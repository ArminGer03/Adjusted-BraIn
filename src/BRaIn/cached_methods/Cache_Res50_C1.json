[{"bug_id": 105, "bug_title": "FileProducer truncates message bodies > 256KB", "bug_description": "Thanks to NIO&apos;s awesomely intuitive behavior, the FileProducer is only writing out the first & last buffers it reads.  Apparently ByteBuffer needs to be cleared after each FileChannel write?", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.1.0", "fixed_version": "camel-1.2.0", "fixed_files": ["org.apache.camel.component.file.FileProducer.java"], "label": 1, "es_results": []}, {"bug_id": 161, "bug_title": "URL link broken in org.apache.camel.Processor API doc", "bug_description": "\"Message Transformer\" link is broken.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.1.0", "fixed_version": "camel-1.2.0", "fixed_files": ["org.apache.camel.Processor.java"], "label": 1, "es_results": []}, {"bug_id": 140, "bug_title": "XsltBuilder instance not threadsafe and cannot be used more then once.", "bug_description": "XsltBuilder uses a ResultHandler instance variable to hold the result of the transformation. That makes it thread unsafe and at the same time, as it&apos;s never cleared/reset, it can&apos;t be used more then once.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.1.0", "fixed_version": "camel-1.2.0", "fixed_files": ["org.apache.camel.builder.xml.XsltBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 191, "bug_title": "Catch clause with pipeline does not proceed", "bug_description": "Catch clause with pipeline does not proceed. This is because the pipeline is designed to reject exceptions. The catch processor should give the exchange a fresh start\nThis started from thread http://www.nabble.com/Camel-1.2---CatchProcessor-not-working--t4690438s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.processor.TryProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 197, "bug_title": "Processor invocation order", "bug_description": "Please see attached patch to reproduce.\nI made a small change to an existing unit test and created a route like this:\nfrom(\"direct:a\").intercept(interceptor1).process(orderProcessor).intercept(interceptor2).to(\"direct:d\"); \nI think one would expect to have one route and the invocation order to be interceptor1 -> orderProcessor -> interceptor2 -> direct:d.\nInstead we have:\n1. two routes\n1. interceptor1 and interceptor2 are invoked twice\n2. orderProcessor is never invoked\n3. direct:d is invoked twice, i think.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.model.ToType.java", "org.apache.camel.model.InterceptType.java", "org.apache.camel.model.ResequencerType.java", "org.apache.camel.model.ChoiceType.java", "org.apache.camel.model.RoutesType.java", "org.apache.camel.model.OutputType.java", "org.apache.camel.model.ThreadType.java", "org.apache.camel.model.ExceptionType.java", "org.apache.camel.model.ThrottlerType.java", "org.apache.camel.impl.RouteContext.java", "org.apache.camel.builder.InterceptorBuilderTest.java", "org.apache.camel.model.CatchType.java", "org.apache.camel.model.XmlParseTest.java", "org.apache.camel.model.InterceptorType.java", "org.apache.camel.model.ExpressionNode.java", "org.apache.camel.model.RouteType.java", "org.apache.camel.spring.xml.SpringXmlRouteBuilderTest.java", "org.apache.camel.model.ProcessorType.java", "org.apache.camel.model.ProceedType.java", "org.apache.camel.model.InterceptorRef.java"], "label": 1, "es_results": []}, {"bug_id": 194, "bug_title": "Converters are not found properly", "bug_description": "There is default converter from dom.Document -> xml.transform.DOMSource\nUnfortunately when you try to send DOM document (implementation) and convert it to Source there is no converter found, because Document cannot be assigned to implementation without a cast.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.impl.converter.DefaultTypeConverter.java"], "label": 1, "es_results": []}, {"bug_id": 189, "bug_title": "System.out in source code - FtpProducer", "bug_description": "I was browsing the source for the FTP component to see how the inner works.\nI discovered a System.out statement in the class FtpProducer:\n            System.out.println(sb.toString() + \" = \" + success);", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.component.file.remote.FtpProducer.java"], "label": 1, "es_results": []}, {"bug_id": 203, "bug_title": "FileComponent sometimes creates file instead of directory", "bug_description": "When using the FileComponent (\"file://\") to publish to a non-existent directory tree, the last part of the path is created as file instead of directory. It only happens when setting the filename in the header (FileComponent.HEADER_FILE_NAME).\nThis is because the method that determines the file name (which checks whether a directory exists), is called before the directory tree is built.\nSee also http://www.nabble.com/Problem-with-concurrent-dir-file-access--tf4724448s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.component.file.FileProducer.java"], "label": 1, "es_results": []}, {"bug_id": 232, "bug_title": "Fix performance counters", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.management.InstrumentationLifecycleStrategy.java", "org.apache.camel.model.FromType.java", "org.apache.camel.model.ResequencerType.java", "org.apache.camel.model.RoutesType.java", "org.apache.camel.spi.LifecycleStrategy.java", "org.apache.camel.Route.java", "org.apache.camel.management.JmxInstrumentationUsingDefaultsTest.java", "org.apache.camel.impl.RouteContext.java", "org.apache.camel.management.ManagedRoute.java", "org.apache.camel.management.CamelNamingStrategy.java", "org.apache.camel.impl.DefaultLifecycleStrategy.java", "org.apache.camel.model.RouteType.java", "org.apache.camel.management.ManagedEndpoint.java", "org.apache.camel.osgi.OsgiComponentResolver.java"], "label": 1, "es_results": []}, {"bug_id": 247, "bug_title": "some issues with camel-cxf component", "bug_description": "1. input params List shouldn&apos;t be null, at lease it should be a empty List,\nso use\nList<Object> params = new ArrayList<Object>();\ninstead of \nList<Object> params = null;\nin CamelInvoker.java\nto avoid input params is null\n2. Cxf Producer should only copy back exchange when ExchangePattern is not InOnly\nso  use\nif (exchange.getPattern() != ExchangePattern.InOnly) {\n        \texchange.copyFrom(cxfExchange);\n}\ninstead of \n    exchange.copyFrom(cxfExchange);", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.component.cxf.HelloServiceImpl.java", "org.apache.camel.component.cxf.HelloService.java", "org.apache.camel.component.cxf.CxfRouterTest.java", "org.apache.camel.component.cxf.CxfProducer.java", "org.apache.camel.component.cxf.CamelInvoker.java", "org.apache.camel.component.cxf.CXFGreeterRouterTest.java"], "label": 1, "es_results": []}, {"bug_id": 271, "bug_title": "CxfEndpointBeanDefinitionParser throws a ClassCastException when definining inner list elements in spring configuration", "bug_description": "In the CxfEndpointBeanDefinitionParser, the list elements being read are cast to com.sun.xml.bind.v2.schemagen.xmlschema.List class which is never true...\nWhy don&apos;t we just leave it as a object or atleast cast it to a java.util.List?\nFor getting the error just add the following configuration to a cxf endpoint:\n  <cxf:cxfEndpoint id=\"toEndpoint1\" \n                   address=\"http://localhost:9091/GreeterContext/GreeterPort\" \n                   serviceClass=\"demo.router.Greeter\">\n      <cxf:features>\n          <cxfcore:logging/>\n      </cxf:features>\n  </cxf:cxfEndpoint>\ncxfcore points to the \"http://cxf.apache.org/core\" namespace.\nI have attached a patch for this problem & would appreciate if someone applied it.\nThanks\nBalaji\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.component.cxf.spring.CxfEndpointBeanDefinitionParser.java"], "label": 1, "es_results": []}, {"bug_id": 320, "bug_title": "DefaultParameterMappingStrategy does not load its defaults", "bug_description": "This is preventing BeanProcessor from using bean methods with a Message parameter, or bean methods not named \"process\" that have an Exchange as their parameter.  For example, to map to a Message the it tries using the ParameterExpression bodyAs(Message.class) instead of inmessageExpression().", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.component.bean.DefaultParameterMappingStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 329, "bug_title": "Throwing the  IllegalArgumentException if there are already another type bean which name is same with the URI schema", "bug_description": "\nPlease see the discussion below\nhttp://www.nabble.com/%22cxf%22-name-conflict-in-the-spring-context-td15427019s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.component.cxf.CamelInvoker.java", "org.apache.camel.impl.DefaultComponentResolver.java"], "label": 1, "es_results": []}, {"bug_id": 340, "bug_title": "Mina TCP does not support InOut pattern", "bug_description": "See this thread:\nhttp://www.nabble.com/camel-mina-TCP-InOut-exchange-td15530602s22882.html\nThis builder does not support InOut when a client send a textline to localhost:8888.\nThe client never receives a response.\nurl = \"mina:tcp://localhost:8888?textline=true\";\nfrom(uri).process(new Processor() {\n                    public void process(Exchange e) \n{\n                        String in = e.getIn().getBody(String.class);\n                        // append newline at end to denote end of data for textline codec\n                        e.getOut().setBody(\"Hello \" + in + \"\\n\");\n                    }\n                });\nAttached is a patch that fixes this by introducing a new parameter to the URL for fluent builder sync=true (ie is there a better name?)\nUsing this url instead and the patch then Camel will send a response back to the client:\nmina:tcp://localhost:8888?textline=true&sync=true\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.component.mina.MinaConsumer.java", "org.apache.camel.component.mina.MinaComponent.java"], "label": 1, "es_results": []}, {"bug_id": 374, "bug_title": "HttpBinding.writeResponse does not set the response status code (i.e. it is always 200)", "bug_description": "When writing out an Http response, the HttpBinding should check for the existing of the header HttpProducer.HTTP_RESPONSE_CODE and set it in the response if it&apos;s there.  This allows requests made via the http component to be output with the correct status code info.  If also allows processors in the flow to control the status code by setting the header themselves.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.component.http.HttpBinding.java"], "label": 1, "es_results": []}, {"bug_id": 430, "bug_title": "consumeDelete=false option of etl sample does not seem to work", "bug_description": "In org/apache/camel/example/etl/EtlRoutes.java, the following line:\n\n    from(\"jpa:org.apache.camel.example.etl.CustomerEntity?consumeDelete=false?consumer.delay=3000&consumeLockEntity=false\")\n        .setHeader(FileComponent.HEADER_FILE_NAME, el(\"${in.body.userName}.xml\"))\n        .to(\"file:target/customers?append=false\");\n\n\nseems to successfully dump the database entries to files in the target/customers directory.  Unless I&apos;m misunderstanding the sample, the consumeDelete=false is meant to ensure that the dump of the database does not delete the entries from the database.  However, when running the etl sample querying the customer table in the database, the list is empty.  I suspect the consumeDelete=false isn&apos;t working.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.3.0", "fixed_files": ["org.apache.camel.example.etl.EtlRoutes.java"], "label": 1, "es_results": []}, {"bug_id": 457, "bug_title": "Camel should raise an exception if parsing of xslt file fails", "bug_description": "Consider a Camel route that involves an xslt component. When Camel sets up the route, the XsltBuilder ignores any errors coming from the xerces xml parser and continues constructing the route even though the xslt transformer is not initialized. \nLater when the route is executed Camel correctly checks if the transformer is initialized and raises an error if not:\njava.lang.IllegalArgumentException: No transformer configured!\nHowever in case of a parse error in the xslt file, the whole route becomes unusable and the setup routine that constructs the route should fail with an appropriate error message. A sanity check needs to be added in method\norg.apache.camel.builder.xml.XsltBuilder.setTransformerSource(javax.xml.transform.Source).\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.builder.xml.XsltBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 476, "bug_title": "Add support for errorHandler in the xml (spring) DSL", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.spring.processor.SpringFaultRouteTest.java"], "label": 1, "es_results": []}, {"bug_id": 412, "bug_title": "OSX mvn -e -o org.apache.camel:camel-maven-plugin:dot fails due to the lack of dot.exe", "bug_description": "Though the intent seemed to be there, the DotMojo does not skip conversion of the file if there is no value for <executable/> in the pom.xml.  The parameter was defaulted.\n#. removed the default.\n#. added log message.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.maven.DotMojo.java"], "label": 1, "es_results": []}, {"bug_id": 499, "bug_title": "camel-core does not compile on HP-UX", "bug_description": "camel-core/src/main/java/org/apache/camel/model/ProcessorType.java:[871,12] inconvertible types\n found   : org.apache.camel.model.ProcessorType<Type>\n required: org.apache.camel.model.InterceptType\ncamel-core/src/main/java/org/apache/camel/model/ProcessorType.java:[872,39] inconvertible types\n found   : org.apache.camel.model.ProcessorType<Type>\n required: org.apache.camel.model.InterceptType\ncamel-core/src/main/java/org/apache/camel/model/ProcessorType.java:[884,12] inconvertible types\n found   : org.apache.camel.model.ProcessorType<Type>\n required: org.apache.camel.model.InterceptType\ncamel-core/src/main/java/org/apache/camel/model/ProcessorType.java:[885,38] inconvertible types\n found   : org.apache.camel.model.ProcessorType<Type>\n required: org.apache.camel.model.InterceptType ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.model.ProcessorType.java"], "label": 1, "es_results": []}, {"bug_id": 510, "bug_title": "Intermittent test failure in MultiCastAggregatorTest", "bug_description": "There is a race condition in the test that both in/out aggregators are aggregating the output.  It looks like the InAggregator should to make a copy of the exchange.  The attached patch also gets rid of a unused method.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.processor.MultiCastAggregatorTest.java"], "label": 1, "es_results": []}, {"bug_id": 519, "bug_title": "XML Syntax does not support enabling/disabling stream caching", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.model.RouteType.java"], "label": 1, "es_results": []}, {"bug_id": 536, "bug_title": "camel-cxf component not propagating context data", "bug_description": "For POJO data formats the CxfProducer currently invokes the following method to invoke on a target server\nresult = client.invoke(operation, parameters.toArray());\nThis fails to setup the Request and Response contexts for the out (request) message and in (response) message. It should instead be copying the relevant data from the consumer and invoking the following operation on the Client interface\nObject[] invoke(BindingOperationInfo oi,   Object[] params,  Map<String, Object> context) throws Exception;\nLikewise the PAYLOAD and MESSAGE data formats are not setting up their contexts before they call the dispatch operation. In fact there is comments in the current codebase...\n                // invoke the message prepare the context\n                Map<String, Object> context = new HashMap<String, Object>();\n                Map<String, Object> requestContext = new HashMap<String, Object>();\n                Map<String, Object> responseContext = new HashMap<String, Object>();\n                // TODO Get the requestContext from the CamelExchange\n                context.put(CxfClient.REQUEST_CONTEXT, requestContext);\n                context.put(CxfClient.RESPONSE_CONTEXT, responseContext);\nThe fix should also include a fix for all three data format types", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.cxf.CxfProducerRouterTest.java", "org.apache.camel.component.cxf.invoker.CxfClient.java", "org.apache.camel.component.cxf.CxfProducer.java", "org.apache.camel.component.cxf.CxfBinding.java", "org.apache.camel.component.cxf.CxfProducerTest.java"], "label": 1, "es_results": []}, {"bug_id": 307, "bug_title": "when renaming a file, we should ensure that the new file name does not exist first (deleting any existing files if they are there)", "bug_description": "See this discussion for more detail : http://www.nabble.com/From-file-to-ftp-tp14921828s22882p15016618.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.file.strategy.RenameFileProcessStrategy.java", "org.apache.camel.component.file.strategy.DeleteFileProcessStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 561, "bug_title": "some samples' poms are using apache-activemq module which will introduce some lower version of Spring and wstx-asl to the eclipse workspace", "bug_description": "When using mvn eclipse:eclipse to setup the workspace for camel example to run. \nI found some camel-cxf related example can&apos;t work in eclipse which is caused by using the lower version of wstx-asl.\nWe can using the activemq-core instead of apache-activemq to fix this issue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.example.cxf.Server.java"], "label": 1, "es_results": []}, {"bug_id": 566, "bug_title": "CamelInvoker fail if operationName in the wsdl portype begin with uppercase", "bug_description": "the generated method name in java file is always begin with lower case, but the operationName in the servicemodel begin with uppercase per the wsdl, this mismatch will cause the invocation fail throwing NO_OPERATION_FOUND exception\nwe should set CxfConstants.OPERATION_NAME as the one from servicemodel", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.cxf.CamelInvoker.java"], "label": 1, "es_results": []}, {"bug_id": 574, "bug_title": "Multicast default excecutor do not create the enough thread for parallel processing ", "bug_description": "When I added the loan broker example CAMEL-556, I found there is no performance improvement when using the parallelly mulitcast.\nAfter digging into to code, I found we should create the enough thread for the parallel processing.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 593, "bug_title": "[patch] Make sure streams get closed", "bug_description": "a couple places streams are opened and are not closed. patch fixes this.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.util.PackageHelper.java", "org.apache.camel.component.file.FileEndpoint.java", "org.apache.camel.converter.NIOConverter.java", "org.apache.camel.model.RedeliveryPolicyType.java", "org.apache.camel.management.CamelNamingStrategy.java", "org.apache.camel.component.bean.BeanInfo.java", "org.apache.camel.processor.DeadLetterChannel.java", "org.apache.camel.util.ResolverUtil.java", "org.apache.camel.converter.ObjectConverter.java"], "label": 1, "es_results": []}, {"bug_id": 597, "bug_title": "[patch] faulty ctor initialization", "bug_description": "JmsQueueEndpoint has faulty initialization logic, checking a parameter and if null, setting that parameter. The code should be setting the member variable instead.\nCode was\n     public JmsQueueEndpoint(String uri, JmsComponent component, String destination,\n             JmsConfiguration configuration, QueueBrowseStrategy queueBrowseStrategy) {\n         super(uri, component, destination, false, configuration);\n         this.queueBrowseStrategy = queueBrowseStrategy;\n         if (queueBrowseStrategy == null) \n{\n            queueBrowseStrategy = createQueueBrowseStrategy();\n         }\n     }\npatch fixes this (two places)", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.jms.JmsQueueEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 558, "bug_title": "Scanning for Type Converters does not work in OSGi", "bug_description": "Even the core type converters could not be loaded:\n18:26:04,982 | ERROR | ExtenderThread-4 | ResolverUtil | g.apache.camel.util.ResolverUtil  364 | Could not search jar file &apos;/org/apache/camel/converter&apos; for classes matching criteria: annotated with @Converterdue to an IOException: /org/apache/camel/converter (No such file or directory) ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.util.ResolverUtil.java"], "label": 1, "es_results": []}, {"bug_id": 618, "bug_title": "Oracle AQ contains a typo \"JMSXRecvTimeStamp\" -- the JMS spec lists  the correct spelling as \"JMSXRcvTimeStamp\". Camel's JmsBinding also has the correct spelling, but can be patched to prevent Oracle AQ from puking.", "bug_description": "http://www.nabble.com/%28Fixed-%29-Updated---Help-w--Oracle-JMS-and-AQjmsException-%28JMS-147%29-td17989368s22882.html\nOracle&apos;s Message implementation (AQjmsMessage) has a typo, I am pretty certain that it exists as \"JMSXRecvTimeStamp\". To workaround this typo I patched org.apache.camel.component.jms.JMSBinding to include the misspelled property name in set of ignored headers/properties.\nAdditionally, I believe Oracle might be setting null values for empty or missing standard and optional JMS reserved properties/headers. Most notably JMSReplyTo. This is resulting in JMSBinding attempting to set a null Destination on the JMSReplyTo header which then results in an Oracle exception. A quick patch was to check for null values and ignore them in the method JmsBinding.appendJmsProperties() for the JMSReplyTo header. A better fix is probably to prevent empty/blank headers or properties from originating at the Oracle level... assuming nulls/blanks are not allowed by the JMS 1.1 spec.\nAnyhoo, I will attach some different patches to this issue for your review. \nCheers!\nSeon", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.jms.JmsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 612, "bug_title": "Exchange should end in error when no choice in a ChoiceType matches", "bug_description": "When you define a route with a choice() and no matching when() clause is found, the Exchange just ends successfully without doing anything.  In my mind, it should fail by default in this case (or we should at least have an easy way to get this behavior).\n\nfrom(DIRECT_ROUTER).choice()\n  .when(xpath(\"/oc:order_confirmation/oc:order/oc:customer/@category = &apos;140&apos;\"))\n    .to(DIRECT_CUSTOMER1)\n  .when(xpath(\"/oc:order_confirmation/oc:order/oc:customer/@category = &apos;116&apos;\"))\n    .to(DIRECT_CUSTOMER2);\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.model.ChoiceType.java"], "label": 1, "es_results": []}, {"bug_id": 577, "bug_title": "ResolverUtil cannot resolve class under osgi enviroment", "bug_description": "in method of \nprotected void find(Test test, String packageName, ClassLoader loader) \nit try to load class  from directory or jar, it works for standalone mode.\nBut if the classloader of the method is osgi bundle class loader, it will fail since the getResource(packgename) return neither directory nor jar\nfor example, package name is org/apache/camel/convert, then what returned is /org/apache/camel/convert, so both loadImplementationsInDirectory and loadImplementationsInJar doesn&apos;t work in this case", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.util.ResolverUtil.java"], "label": 1, "es_results": []}, {"bug_id": 653, "bug_title": "Bean method lost when sending a message from one JMS queue to another", "bug_description": "I noticed a problem in JmsBinding.java. If Camel has read a JMS message from a queue and that message has a bean method specified in the header (\"org.apache.camel.MethodName\"), the method header will removed if that message is written to another queue. Here&apos;s the offending code: \n    protected boolean shouldOutputHeader(org.apache.camel.Message camelMessage, String headerName, \n                                         Object headerValue) \n{ \n        return headerValue != null && !getIgnoreJmsHeaders().contains(headerName) \n               && ObjectHelper.isJavaIdentifier(headerName); \n    }\n \n\"org.apache.camel.MethodName\" fails the check isJavaIdentifier and is excluded from the headers written to the new message. I&apos;m not sure the purpose of this check, but this might be an unintended side effect. \nThe call chain is something like this: \nJmsProducer.process \nJmsBinding.makeJmsMessage \nJmsBinding.appendJmsProperties \nJmsBinding.shouldOutputHeader \nUpdate: Experimenting later with WebLogic&apos;s JMS, I noticed that WebLogic (9.2) does not allow this header name. Maybe this was the original motivation for this check. In order to be compatible with WebLogic, perhaps the method header name should changed to something like \"CamelMethod\". I believe this would correct both problems.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.jms.JmsMessage.java", "org.apache.camel.component.jms.JmsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 657, "bug_title": "CsvDataFormat.marshal never uses custom CSVConfig", "bug_description": "Affected class: org.apache.camel.dataformat.csv.CsvDataFormat \nMethod \"marshal\" has a strange behaviour. The CSVConfig for the CSVWriter can not be set via setConfig, because \"marshal\" has this line:\n\nCSVConfig conf = createConfig();\n\n\nTherefore you can not use your own CSVConfig to adjust marshalling. \nFix proposal:\n\nremove method createConfig\nsee unmarshaling with corresponding getter/setter\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.dataformat.csv.CsvDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 685, "bug_title": "JMS - propation of message properties should honor jms spec that only primitives is allowed", "bug_description": "See the user forum:\nhttp://www.nabble.com/Fwd%3A-Using-Apache-Camel-as-Transport-for-Apache-CXF-with-SOAP-over-JMS-td18314917s22882.html\nBasically JmsBinding should check for primitives before setting the object property on the jms message.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.jms.JmsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 691, "bug_title": "CxfExchange  do not copy the exchange pattern when copying itself.", "bug_description": "When I applied the CAMEL-688 patch , I found the CxfSoapMessageProviderTest failed. \nAfter digging into the code , I found CxfExchange do not copy the exchange pattern when the exchange copying itself.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.2.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.cxf.CxfExchange.java"], "label": 1, "es_results": []}, {"bug_id": 694, "bug_title": "CXF component's CamelDestination gets configured twice", "bug_description": "CamelTransportFactory does not need to call\n\"configurer.configurationBean(destination)\" as it has been performed\nby CamelDestination..initConfig() in its constructor.  \n\nIndex: components/camel-cxf/src/main/java/org/apache/camel/component/cxf/transport/CamelTransportFactory.java\n===================================================================\n--- components/camel-cxf/src/main/java/org/apache/camel/component/cxf/transport/CamelTransportFactory.java\t(revision 675685)\n+++ components/camel-cxf/src/main/java/org/apache/camel/component/cxf/transport/CamelTransportFactory.java\t(working copy)\n@@ -26,7 +26,6 @@\n \n import org.apache.camel.CamelContext;\n import org.apache.cxf.Bus;\n-import org.apache.cxf.configuration.Configurer;\n import org.apache.cxf.service.model.EndpointInfo;\n import org.apache.cxf.transport.AbstractTransportFactory;\n import org.apache.cxf.transport.Conduit;\n@@ -87,12 +86,7 @@\n     }\n \n     public Destination getDestination(EndpointInfo endpointInfo) throws IOException {\n-        CamelDestination destination = new CamelDestination(camelContext, bus, this, endpointInfo);\n-        Configurer configurer = bus.getExtension(Configurer.class);\n-        if (null != configurer) {\n-            configurer.configureBean(destination);\n-        }\n-        return destination;\n+        return new CamelDestination(camelContext, bus, this, endpointInfo);\n     }\n \n     public Set<String> getUriPrefixes() {\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.cxf.transport.CamelTransportFactory.java", "org.apache.camel.language.bean.BeanExpression.java"], "label": 1, "es_results": []}, {"bug_id": 713, "bug_title": "FileProducer - consumes file instead of producing!", "bug_description": "The strange code in the FileProducer that if the exchange is out capable it will poll the file instead bites us big time.\nJames must have had a beer and a fancy moment when he created the code. Something with a dynamic receipentlist where the endpoints should be polled from a file but the expression uses a Producer when the endpoints is created. For the file it should consume instead but it doesn&apos;t. Any basically it should consume instead.\nIt also bites us when we send an InOut exchange to the FileProducer it will not produce the file but consume it and there is no file so nothing happens.\nThis code\n\npublic class FileBackupTest extends ContextTestSupport {\n\n    public void testMailGeneration() throws Exception {\n        MockEndpoint mock = getMockEndpoint(\"mock:result\");\n        mock.expectedMessageCount(1);\n\n        template.requestBody(\"seda:mails\", \"Hello World\");\n\n        mock.assertIsSatisfied();\n    }\n\n    protected RouteBuilder createRouteBuilder() throws Exception {\n        return new RouteBuilder() {\n            public void configure() throws Exception {\n                from(\"seda:mails\")\n                    .to(\"file:target/mails\")\n                    .to(\"mock:result\");\n            }\n        };\n    }\n\n\nWould not create a file in target/mails folder as we are using template.request that is an InOut MEP.\nI will fix it by removing the strange consumer code in the FileProducer and change the fancy dynamic recepientlist test (SimulatorTest), to not use file based endpoints.\nIt could be a blocker for the 1.4 release and we should consider creating a new RC!", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.file.FileComponent.java", "org.apache.camel.processor.SimulatorTest.java", "org.apache.camel.component.file.FileProducer.java"], "label": 1, "es_results": []}, {"bug_id": 737, "bug_title": "When getContext().addInterceptorStrategy(new Tracer());  is added in a RouteBuilder class, then the timer component does not work", "bug_description": "When \ngetContext().addInterceptorStrategy(new Tracer());\nis added in a RouteBuilder Class, then the timer component does not start.\ne.g. \npublic class RouteBuilder extends RouteBuilder {\n    getContext().addInterceptorStrategy(new Tracer());\n    public void configure() \n{\n\n        from(\"timer://export\")\n        .to(\"bean:myBean\");\n    }\n}", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.log.LogFormatter.java", "org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.processor.interceptor.TraceFormatter.java"], "label": 1, "es_results": []}, {"bug_id": 741, "bug_title": "CamelConduit's should check the TargetReference when create its instance", "bug_description": "CAMEL-726 shows a ws-address error when using Camel transport for CXF.\nAfter running the code , I found there is no To address tag  generated in the soap header. It was caused by the CamelConduit can set right targe reference.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.cxf.transport.CamelConduit.java"], "label": 1, "es_results": []}, {"bug_id": 755, "bug_title": "AMQP Tests are disabled", "bug_description": "The AMQP tests are currently disabled. Adding an older version of MINA (than Camel uses) & commons lang to the classpath got the tests passing except for ObjectMessage type test.\nSending a TextMessage and BytesMessage work fine so I&apos;m opening up a separate JIRA for the ObjectMessage issue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.amqp.AMQPRouteTest.java"], "label": 1, "es_results": []}, {"bug_id": 765, "bug_title": "InstrumentationProcessor should catch Throwable", "bug_description": "The InstrumentationProcessor currently only catches exception and sets it in the exception.   Runtime errors and throwables can be thrown all the way out of the DeadLetterChannel&apos;s process method, out of the reach of exception policy.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.management.InstrumentationProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 773, "bug_title": "Trace does not look at camel.trace property", "bug_description": "This has a simple fix - it was a spelling error ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 758, "bug_title": "SFTP Producer - Does not handle folder already exists when trying to create the remote folder", "bug_description": "The SFTP (the secure) has this bug that it doesn&apos;t check if the folder already exsits before it tries to create the remote folder.\nSee nabble:\nhttp://www.nabble.com/SftpProducer-bug-td18640519s22882.html\nBela the reported want to contribute a patch later, or he will be able to test it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.file.remote.FtpProducer.java", "org.apache.camel.component.file.remote.SftpProducer.java", "org.apache.camel.component.file.remote.RemoteFileConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 793, "bug_title": "smtp connection without authentication (no username and password given) does not work", "bug_description": "Connections to smtp servers with no authentication fail. The mail.smtp.auth is always set to true. The patch sets it to false if no username is given.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.mail.MailConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 803, "bug_title": "ValidatingProcessor not thread-safe", "bug_description": "The method org.apache.camel.processor.validation.ValidatingProcessor.process() is executed concurrently but error information is stored in a shared errorHandler. Because each thread calls errorHandler.reset() it may clean errors written by another thread. The easiest way to fix this issue would be a synchronized block:\nValidatingProcessor.java\n...\nsynchronized (errorHandler) {\n    errorHandler.reset();\n    validator.setErrorHandler(errorHandler);\n    \n    DOMResult result = new DOMResult();\n    validator.validate(source, result);\n    \n    errorHandler.handleErrors(exchange, schema, result);\n}\n... \n\n\nThe disadvantage of this solution is that is serializes threads. A locally created error handler would be preferrable ...", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.processor.validation.DefaultValidationErrorHandler.java", "org.apache.camel.processor.validation.ValidatingProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 806, "bug_title": "Strict Uri option validation fails HTTP Get Requests", "bug_description": "The new strict URI validation requires that the named parameter options be validated prior to calling endpoint. This has the unfortunate side effect of failing exchanges to Endpoints where the parameter options are variable e.g. a GET request to HTTP server. In cases such as these it would be useful to be able to circumvent this strict validation.\nThe attached test illustrates the issue.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.impl.DefaultEndpoint.java", "org.apache.camel.Endpoint.java", "org.apache.camel.component.http.HttpComponent.java", "org.apache.camel.impl.DefaultComponent.java", "org.apache.camel.component.http.HttpGetWithQueryParamsTest.java", "org.apache.camel.component.http.HttpGetWithHeadersTest.java", "org.apache.camel.component.cxf.CxfSoapEndpoint.java", "org.apache.camel.component.http.HttpEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 811, "bug_title": "delayer pattern does not work in Spring DSL", "bug_description": "You can&apos;t configure the delayer pattern properly in the Spring DSL currently. I will be attaching a patch for this shortly.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.model.DelayerType.java", "org.apache.camel.spring.SpringCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 792, "bug_title": "Exception is not logged or handled when error handler is LoggingErrorHandler and processing occurs inside Pipeline", "bug_description": "Exception throw inside Pipeline is not reported or handled when error handler is LoggingErrorHandler.\nAttached unit test fails - exchange with exception is not passed to \"mock:error\" endpoints.\nTest will pass when you change error handler to default (DeadLetterErrorHandler)\nor when processor throwing an exception is processed not inside Pipeline.\nAfter some digging inside source code I noticed Pipeline source code fragment.\n\n    public void process(Exchange exchange) throws Exception {\n        AsyncProcessorHelper.process(this, exchange);\n    }\n\n\nAsyncProcessorHelper.process(..) never throws exception directly.\nInstead it passes exception information in exchange object properties.\nProblem is LoggingErrorHandler doesn&apos;t use it correctly because (code snipped from LoggingErrorHandler):\n\n    public void process(Exchange exchange) throws Exception {\n        try {\n            output.process(exchange);\n        } catch (Throwable e) {\n            if (!customProcessorForException(exchange, e)) {\n                logError(exchange, e);\n            }\n        }\n    }\n\n\nit is logging only exception returned directly by output.process call.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.processor.LoggingErrorHandler.java"], "label": 1, "es_results": []}, {"bug_id": 826, "bug_title": "File polling consumer does not remove file lock if exchange fails", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.file.FileRenameRouteTest.java", "org.apache.camel.component.file.FileConsumer.java", "org.apache.camel.component.file.strategy.NoOpFileProcessStrategy.java", "org.apache.camel.component.file.FileNoOpRouteTest.java", "org.apache.camel.component.file.FileDeleteRouteTest.java", "org.apache.camel.component.file.FileFilterOnNameRouteTest.java", "org.apache.camel.component.file.FileProcessStrategy.java", "org.apache.camel.impl.DefaultConsumer.java", "org.apache.camel.component.file.FileRouteTest.java", "org.apache.camel.component.file.strategy.FileProcessStrategySupport.java"], "label": 1, "es_results": []}, {"bug_id": 818, "bug_title": "Should preserve the exchange type in the routing slip", "bug_description": "Currently, the routing slip will create a new InOut exchange for each target based on the target endpoint type. This means that someone calling from a jbi endpoint (or other endpoint with custom exchange) into the routing slip will have their exchange type changed (to mostly likely DefaultExchange). I will be including a patch to fix this issue shortly. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.processor.RoutingSlip.java", "org.apache.camel.processor.routingslip.RoutingSlipDataModificationTest.java"], "label": 1, "es_results": []}, {"bug_id": 834, "bug_title": "Trailing slash missing in SftpConsumer", "bug_description": "When this route \n\n    <camel:route>\n        <camel:from uri=\"sftp://someuser@hostname/path/to/directory/?password=secret&amp;directory=true&amp;binary=true&amp;consumer.useFixedDelay=false&amp;consumer.setNames=true&amp;consumer.recursive=false\" />\n        <camel:to uri=\"file:///C:/camel/output/\" />\n    </camel:route>\n\n\nis configured the files in the remote directory are download, but subsequent calls of the method \"pollDir\" result in this exception:\n\n[ Thread: 1 RemoteFileComponent] SftpConsumer                   WARN  Caught SftpException:2: No such file\n[ Thread: 1 RemoteFileComponent] SftpConsumer                   WARN  Doing nothing for now, need to determine an appropriate action\n[ Thread: 1 RemoteFileComponent] ScheduledPollConsumer          WARN  An exception occured while polling: Endpoint[sftp://someuser@hostname/path/to/directory/?password=secret&amp;directory=true&amp;binary=true&amp;consumer.useFixedDelay=false&amp;consumer.setNames=true&amp;consumer.recursive=false]: No such file 2: No such file\n\n\nAs you can see the slash is definitely in the configured route. If I check for the missing slash and add it to the dir variable (line 115 in SftpConsumer) then the consumer works:\n\n    \tif(!dir.startsWith(\"/\")) {\n    \t\tdir = \"/\" + dir;\n    \t}\n\n\nDidn&apos;t have the time to have a closer look. The root because of the missing slash is somewhere else.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.file.remote.FtpServerTestSupport.java"], "label": 1, "es_results": []}, {"bug_id": 842, "bug_title": "handleFault does not interrupt processing on successful retry", "bug_description": "See following thread for more details.\nhttp://www.nabble.com/Handling-JBI-faults-td19090487s22882.html#a19090487", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.processor.HandleFaultProcessor.java", "org.apache.camel.processor.FaultRouteTest.java"], "label": 1, "es_results": []}, {"bug_id": 857, "bug_title": "DeadLetterChannel - maximum redelivery is not corrent and redelivery counter is wrong when failure handled", "bug_description": "The unit test from CAMEL-794 demonstrates a few issues with the DLC in Camel\n\nthe maximum redelivery is not reached - eg. setting it to 2 will only perform 1 normal attempt + 1 redelivery, and not as expected 2 redeliveries\nwhen the exchanged could not be redelivered at all then the redeliverycounter has already been incremented and thus is off by +1\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.builder.BuilderWithScopesTest.java", "org.apache.camel.processor.RedeliveryPolicyPerExceptionTest.java", "org.apache.camel.processor.RedeliveryPolicy.java", "org.apache.camel.processor.DeadLetterChannel.java", "org.apache.camel.processor.ValidationFinallyBlockNoCatchTest.java"], "label": 1, "es_results": []}, {"bug_id": 860, "bug_title": "DeadLetterChannel does not set the EXCEPTION_CAUSE_PROPERTY", "bug_description": "The DeadLetterChannel does not set the EXCEPTION_CAUSE_PROPERTY.\nIt looks like a bug in the method:\npublic boolean process(final Exchange exchange, final AsyncCallback callback, final RedeliveryData data) \nThe property is set by getting the Exception from the exchange:\nexchange.setProperty(EXCEPTION_CAUSE_PROPERTY, exchange.getException());\nbut several lines before the Exception ist set to null in exchange:\nif (exchange.getException() != null) {\n                Throwable e = exchange.getException();\n                exchange.setException(null); // Reset it since we are handling it.\nto fix the bug, i think it is simply done by preserving the Throwable like this\nThrowable e = exchange.getException();\nif (exchange.getException() != null) {\n                exchange.setException(null); // Reset it since we are handling it.\n...\nexchange.setProperty(EXCEPTION_CAUSE_PROPERTY, e);\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.processor.DeadLetterChannelTest.java", "org.apache.camel.builder.ErrorHandlerTest.java", "org.apache.camel.processor.DeadLetterChannel.java"], "label": 1, "es_results": []}, {"bug_id": 851, "bug_title": "camel:dot - spring xml files located in different folder ", "bug_description": "camel:dot doesn&apos;t support loading spring .xml files using the configuration option to the plugin. Currently it loads from META-INF/spring/*.xml\nThe other two: run and embedded has this feature.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.maven.EmbeddedMojo.java", "org.apache.camel.maven.DotMojo.java"], "label": 1, "es_results": []}, {"bug_id": 873, "bug_title": "Current mina component cannot tansfer the exchange fault message body", "bug_description": "I just found MinaTransferExchangeOptionTest don&apos;t show how to set fault message and the fault message is not a part of MinaPayloadHolder.\nYou can&apos;t set the exception on the exchange in the processor , sine the camel DeadLetterChannel will try to redeliver the message.\nSo we just need to support put the exception which need to send back to the camel client into the fault message body.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.mina.MinaTcpWithInOutUsingPlainSocketTest.java", "org.apache.camel.component.mina.MinaTcpWithIoOutProcessorExceptionTest.java", "org.apache.camel.component.mina.MinaConsumer.java", "org.apache.camel.component.mina.MinaPayloadHolder.java", "org.apache.camel.component.jms.JmsIOConverter.java", "org.apache.camel.component.mina.MinaProducer.java", "org.apache.camel.component.mina.MinaEndpoint.java", "org.apache.camel.component.mina.MinaConverterTest.java", "org.apache.camel.converter.IOConverter.java", "org.apache.camel.component.mina.MinaComponent.java", "org.apache.camel.converter.NIOConverter.java", "org.apache.camel.component.mina.MinaPayloadHelper.java", "org.apache.camel.Exchange.java", "org.apache.camel.component.mina.MinaUdpWithInOutUsingPlainSocketTest.java", "org.apache.camel.component.mina.MinaConverter.java", "org.apache.camel.component.mina.MinaTransferExchangeOptionTest.java"], "label": 1, "es_results": []}, {"bug_id": 882, "bug_title": "SftpEndpoint ignores custom ssh port", "bug_description": "SftpEndpoint ignores custom ssh port. It works only if server runs on port 22. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.4.0", "fixed_files": ["org.apache.camel.component.file.remote.SftpEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 871, "bug_title": "camel-spring - data format using camel xml elements does not work", "bug_description": "If you have a route that uses data format and you configure the data format using camel xml elements it doesn&apos;t work.\nSee nabble: \nhttp://www.nabble.com/xstream-in-xml-configuration-td19227970s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.CamelContext.java", "org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.spring.CamelContextFactoryBean.java", "org.apache.camel.model.dataformat.DataFormatType.java", "org.apache.camel.spi.RouteContext.java", "org.apache.camel.impl.DefaultRouteContext.java", "org.apache.camel.spring.handler.CamelNamespaceHandler.java", "org.apache.camel.model.MarshalType.java", "org.apache.camel.model.UnmarshalType.java"], "label": 1, "es_results": []}, {"bug_id": 886, "bug_title": "<jaxb> element should not require contextPath attribute", "bug_description": "In the Java DSL, you can say from(\"foo\").marshal().jaxb().to(\"bar\")\nIn XML I think it would look like this:\n<from uri=\"...\" />\n<marshal>\n  <jaxb />\n</marshal>\n<to uri=\"...\" />\nThe problem is that you can&apos;t use the <jaxb /> element without the contextPath attribute, which the XSD says is required.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.model.dataformat.JaxbDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 906, "bug_title": "JmsProducer - option requestTimeout is not configured properly", "bug_description": "When sending a JMS message to a destination using camel-jms (JMSProducer) I can&apos;t get it to use my requestTimeout option in the endpoint configuration. It always default to 20000.\n\ntemplate.requestBody(\"activemq:queue:slow?requestTimeout=1000\", \"Hello World\");\n\n\nThis doesn&apos;t work as expected with a timeout of only 1 sec.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.jms.JmsEndpoint.java", "org.apache.camel.component.jms.JmsConfiguration.java", "org.apache.camel.component.jms.JmsProducer.java"], "label": 1, "es_results": []}, {"bug_id": 912, "bug_title": "camel-cxf component is not passing request context through correctly", "bug_description": "CxfProducer.process() is not setting up the request context correctly during createCxfMessage, so later in that method propagateContext has no request data to propagate.  Also, we need the exchange properties to be added to the request context, so that non-camel components can pass properties into the cxf request context.\nI have coded a fix for this, along with a unit test of course.  I&apos;ll attach the patch to this JIRA.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.cxf.CxfProducerContextTest.java", "org.apache.camel.component.cxf.CxfBinding.java"], "label": 1, "es_results": []}, {"bug_id": 914, "bug_title": "Camel Maven plugin should not look for routes in target/test-classes", "bug_description": "Currently the Maven Plugin attempts to load Camel Routes from both target/classes and target/test-classes.  This causes an issue when you have the same package defined for testing as you have in your Camel Context.  An example problem is using a super class for a test which is only available a test dependency which results in a ClassNotFoundException.\nI would propose one of the following:\n\nPrevent the plugin from using target/test-classes altogther\nAdd a mojo parameter to allow for routes under test-classes to be scanned\n\t\nThis should default to False\nIf this parameter is set to True then the plugin should add dependencies marked as \"test\" to the classpath\n\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.maven.RunMojo.java"], "label": 1, "es_results": []}, {"bug_id": 922, "bug_title": "Remove deps on persistence-api jar, non compliant with the spec", "bug_description": "I mentioned a while ago a warning I couldn&apos;t explain in eclipse related to the use of @UniqueConstraint in camel-jpa.\nIt turned out to be due to the fact that camel-jpa has a dependency on persistence api, but used for testing openjpa that has a dependency on geronimo-jpa_3.0_spec.  So camel-jpa pulls in two different implementation of the same spec.  So far so good.  It also turns out that the definition of the @UniqueConstraint annotation is different in the two jars, the persistence-api (and the camel-jpa code as implemented) being non spec compliant.  I found other inconsistencies in the persistence-api impl and it seems to be based on an older version of the spec.\n(More precisely persistence api annotates the @interface UniqueConstraint with @Target(\n{TYPE}\n) whereas the spec and the geronimo impl use: @Target({}))\nMy recommendation is to replace the dependency on persistence-api with the geronimo version.  I will commit a change for this in the next hour or so, but leave this issue open.  If anybody has any objections please shout and I will revert the change.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.bam.model.ProcessDefinition.java", "org.apache.camel.bam.model.ActivityDefinition.java", "org.apache.camel.processor.idempotent.jpa.MessageProcessed.java"], "label": 1, "es_results": []}, {"bug_id": 807, "bug_title": "HttpProducer premature out message creation", "bug_description": "I believe there is incorrect flow in the HttpProducer which can subvert error handling in the route:\nin Camel 1.4.0 in HttpProducer on line 76 a \"blank\" out message is set on the exchange:\nMessage out = exchange.getOut(true);\n        try {\n            int responseCode = httpClient.executeMethod(method);\n...\n1. If an IO error occurs on connect, the out message will be passed to the error handling code and if the originating endpoint was a JmsMessage it will cause an NPE when JmsMessage.getMessageId() is called. It also obscures the offending message I believe.\n2. A similar situation will happen if the HTTP service returned an error code; the out message will contain a reply message even though the processing resulted in an error (e.g. HTTP Status 500) \n3. Also, shouldn&apos;t this code test whether the exchange expects an \"out\" altogether, and if not, perhaps, put just put the return HTTP status into the \"in\" message header.\n--Bill", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.http.HttpPostWithBodyTest.java", "org.apache.camel.component.http.HttpProducer.java"], "label": 1, "es_results": []}, {"bug_id": 970, "bug_title": "createConnector attribute for CamelJMXAgentType should default to 'true'", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.spring.CamelJMXAgentType.java"], "label": 1, "es_results": []}, {"bug_id": 969, "bug_title": "CamelContext.setRoutes() documentation is misleading", "bug_description": "The javaDoc for CamelContext.setRoutes() states:\n\"Sets the routes for this context, replacing any current routes\"\nYet when used, and the context started, startRouteDefinitions() is used first, causing any existing routes created through the definitions (such as those from RouteBuilders) to be re-created and started before the new routes provided to setRoutes() are started.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.CamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 1003, "bug_title": "seda component - will lose message if its stopped while it polls", "bug_description": "See attached patch for details\nUnit test inspired by nabble:\nhttp://www.nabble.com/Wait-for-condition-td20064420s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.component.seda.SedaConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 939, "bug_title": "Investigate test failures on AIX", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.ruby.RubyTest.java", "org.apache.camel.builder.script.Jsr223Test.java", "org.apache.camel.LanguageTestSupport.java", "org.apache.camel.language.script.PythonLanguageTest.java", "org.apache.camel.dataformat.csv.CsvRouteTest.java", "org.apache.camel.builder.xml.XPathTest.java", "org.apache.camel.spring.CamelContextFactoryBean.java", "org.apache.camel.model.XmlParseTest.java"], "label": 1, "es_results": []}, {"bug_id": 1034, "bug_title": "Messages in StreamResequencer between 2 ActiveMQ queues seem to be stuck", "bug_description": "Perhaps some subtle configuration issue, but a Camel stream resequencer that is a route between 2 ActiveMQ queues fails to deliver all ordered messages. In fact, using the defaults for a StreamResequencer only 1 out of 100 messages are delivered. However, all 100 messages have been removed from the Producer queue by the resequencer and are not moved to the consumer queue either (even unordered). \nRelevant points:\n\nBroker is run external to Camel route (non-embedded, no VM transport)\nProducer is a Camel ProducerTemplate\nConsumer is a Camel route that listens on another delivery queue and delivers message to a simple stdout Processor\nResequencer uses stream defaults (100, 1000ms)\nTrace warns of some Converter override (see attached trace) and throws a breadcrumb error\nResequencer fails delivery whether using body (long) or seqnum header (long)\nall JMS components are configured for AutoAcks\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.processor.StreamResequencer.java", "org.apache.camel.processor.resequencer.ResequencerEngine.java"], "label": 1, "es_results": []}, {"bug_id": 1053, "bug_title": "spring integration xsd change breaks component", "bug_description": "Thread is here http://www.nabble.com/-HEADS-UP--camel-spring-integration-issue-td20331450s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.spring.integration.SpringIntegrationConsumer.java", "org.apache.camel.component.spring.integration.adapter.CamelTargetAdapter.java", "org.apache.camel.component.spring.integration.HelloWorldService.java", "org.apache.camel.component.spring.integration.adapter.AbstractCamelAdapter.java", "org.apache.camel.component.spring.integration.converter.SpringIntegrationConverter.java", "org.apache.camel.component.spring.integration.SpringIntegrationProducerTest.java", "org.apache.camel.component.spring.integration.adapter.config.CamelTargetAdapterParser.java", "org.apache.camel.component.spring.integration.SpringIntegrationOneWayConsumerTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationProducer.java", "org.apache.camel.component.spring.integration.adapter.CamelSourceAdapterTest.java", "org.apache.camel.component.spring.integration.adapter.ConfigurationTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationBinding.java", "org.apache.camel.component.spring.integration.adapter.CamelSourceAdapter.java", "org.apache.camel.component.spring.integration.SpringIntegrationEndpoint.java", "org.apache.camel.component.spring.integration.SpringIntegrationTwoWayConsumerTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationMessage.java", "org.apache.camel.component.spring.integration.adapter.CamelTargetAdapterTest.java"], "label": 1, "es_results": []}, {"bug_id": 1057, "bug_title": "XQuery creates an empty out message that makes it impossible to chain more processors behind it", "bug_description": "When using XQuery the getOut() function is called on the message so if it is not used at a place where the out is filled an empty out message will be created. With this xquery cannot be used for example in setBody, setHeader... and anywhere where the out is not filled.\nAn example where this causes real trouble:\n<choice>\n  <when>\n    <xquery>...</xquery>\n    <to uri=\"direct:follow\" />\n  </when>\n</choice>\n...\n<from uri=\"direct:follow\">\n<to ...>\n<to ...>\nIn the patch the out property in the xquery context is filled only if it is not null. With it all of my pipelines seem to work well for me.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.xquery.XQueryBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 1062, "bug_title": "PipelineConcurrentTest.java fails randomly", "bug_description": "The test fails with an IllegalStateException: Queue full.  I believe the reason for this is that there are 10000 messages sent, but the default queue size is 1000.   On a fast machine, the queue fills up before the consumer gets a chance to pop the messages from the queue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.processor.PipelineConcurrentTest.java"], "label": 1, "es_results": []}, {"bug_id": 1070, "bug_title": "org.apache.camel.Message - hasAttachments is buggy", "bug_description": "I must use\n                        if (exchange.getIn().getAttachments().size() > 0) {\nInstead of\n                        if (exchange.getIn().hasAttachments()) {\nAs the latter always returns false. Or at least returns false even though the size is > 0", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.impl.DefaultMessage.java"], "label": 1, "es_results": []}, {"bug_id": 1088, "bug_title": "Cannot get the soap header when the camel-cxf endpoint working in the PAYLOAD data fromat", "bug_description": "When I added a unit test to show how to get the SOAP header from a PAYLOAD camel-cxf endpoint , I found the soap header stuff is not working.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.cxf.invoker.PayloadMessage.java", "org.apache.camel.component.cxf.interceptors.SoapMessageInInterceptor.java", "org.apache.camel.component.cxf.invoker.PayloadInvokingContext.java", "org.apache.camel.component.cxf.CxfPayLoadMessageRouterTest.java", "org.apache.camel.component.cxf.interceptors.XMLMessageInInterceptor.java", "org.apache.camel.component.cxf.interceptors.SoapMessageOutInterceptor.java", "org.apache.camel.component.cxf.interceptors.AbstractMessageInInterceptor.java"], "label": 1, "es_results": []}, {"bug_id": 1090, "bug_title": "ThroughputLogger incorrectly reports duration 0", "bug_description": "from the apache-camel-load test:\n2008-11-14 15:27:01,114 [mponent@1296d1d] INFO  aset:myDataSet?produceDelay=10 - Sent: 600 messages so far. Last group took: 0 millis which is: 85.106 messages per second. average: 80.128\nThe ThroughputLogger used by DataSet resets the duration before reporting it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.processor.ThroughputLogger.java"], "label": 1, "es_results": []}, {"bug_id": 1103, "bug_title": "Delayer not working as expected.", "bug_description": "The delayer expression is not evaluated in my route builder:\nfrom(waitQueueName).delayer(header(HEADER_EXECUTION_TIME)).bean(routerBean).recipientList(header(HEADER_TARGET_ROUTE));\nWhile the recipientList expression is working the header expression is always ignored. I stepped through the code and saw that in DelayerType.createAbsoluteTimeDelayExpression() \"expr.getLanguage()\" is always null:\n    private Expression createAbsoluteTimeDelayExpression(RouteContext routeContext) {\n        ExpressionType expr = getExpression();\n        if (expr != null) {\n            if (ObjectHelper.isNotNullAndNonEmpty(expr.getLanguage())) \n{\n                return expr.createExpression(routeContext);\n            }\n        }\n        return null;\n    }\nI have seen that this was changed in 1.5.0 (http://issues.apache.org/activemq/browse/CAMEL-811). ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.model.DelayerType.java"], "label": 1, "es_results": []}, {"bug_id": 1095, "bug_title": "camel-jetty - Content-Type should be set on response if provided as OUT header", "bug_description": "The Content-Type is not set in HttpBinding if an end-user has provided one as a OUT header.\nSo if an user exposes a Jetty service and let us requests fly in that is text/plain and want to return a response that is image/jpeg or the likes the content type can not be set as:\n            exchange.getOut().setHeader(\"Content-Type\", \"image/jpeg\");", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.http.HttpMessage.java", "org.apache.camel.component.http.HttpProducer.java", "org.apache.camel.component.http.HttpComponent.java", "org.apache.camel.component.jetty.JettyHttpComponent.java", "org.apache.camel.component.http.HttpOperationFailedException.java", "org.apache.camel.component.http.HttpBinding.java", "org.apache.camel.component.http.HttpEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1107, "bug_title": "camel-http - does not remove httpClient.xxx URI options", "bug_description": "When using the options to configure httpClient using URI option, they should be removed from the uri that is left over to the HTTPProducer.\n\nhttp://someserver?httpClient.soTimeOut=5000\n\n\nshould remove the httpClient.xxx so it&apos;s\n\nhttp://someserver\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.http.HttpMessage.java", "org.apache.camel.component.http.HttpProducer.java", "org.apache.camel.component.http.HttpComponent.java", "org.apache.camel.component.jetty.JettyHttpComponent.java", "org.apache.camel.component.http.HttpOperationFailedException.java", "org.apache.camel.component.http.HttpBinding.java", "org.apache.camel.component.http.HttpEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1093, "bug_title": "camel-jetty - Exchange failures should not be returned as 200", "bug_description": "The code below:\n\n    protected RouteBuilder createRouteBuilder() throws Exception {\n        return new RouteBuilder() {\n            public void configure() throws Exception {\n                errorHandler(noErrorHandler());\n                from(\"jetty:http://localhost:8080/myapp/myservice\").process(new MyBookService());\n            }\n        };\n    }\n\n    public class MyBookService implements Processor {\n        public void process(Exchange exchange) throws Exception {\n            throw new IllegalArgumentException(\"Damm\");\n        }\n    }\n\n\nWill return http response code 200. We should let end users easily set what response code they want and in case of route failures we should probably return 500 and return the stracktrace in the body", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.http.HttpMessage.java", "org.apache.camel.component.http.HttpProducer.java", "org.apache.camel.component.http.HttpComponent.java", "org.apache.camel.component.jetty.JettyHttpComponent.java", "org.apache.camel.component.http.HttpOperationFailedException.java", "org.apache.camel.component.http.HttpBinding.java", "org.apache.camel.component.http.HttpEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1111, "bug_title": "HttpProducer - Side effect of using URI-based endpoint config disables POST method use", "bug_description": "Hello all,\nI&apos;m using Camel HTTP component to send data across to HTTP web services.  Of course we need to POST this data over, and it&apos;s all been peachy until I started on a storycard to implement error handling and retry on timeouts.\nFollowing the docco here (http://activemq.apache.org/camel/http.html), I made the same change to my endpoint to include ?httpClient.SoTimeout=5000, and that&apos;s when the problems started.\nAfter stepping through the code, I have now found that Camel thinks \"httpClient.soTimeout\" is actually a query string parameter that I want to pass across to the remote service, when it is not - it&apos;s just a configuration option for the HTTPClient.  Perhaps Camel needs a QueryStringFilter (or something like it) so that it can distinguish configuration options as opposed to real GET method parameters, or maybe for now ignore any parameters beginning with httpClient.\nFiling as a bug for now because at the very least, the documentation should be updated to highlight the limitation of only allowing GET methods when URI-based configuration is employed.  I haven&apos;t identified a workaround yet (using Spring DSL) but I&apos;m sure it won&apos;t be too difficult.\nThanks!\nJason", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.http.HttpMessage.java", "org.apache.camel.component.http.HttpProducer.java", "org.apache.camel.component.http.HttpComponent.java", "org.apache.camel.component.jetty.JettyHttpComponent.java", "org.apache.camel.component.http.HttpOperationFailedException.java", "org.apache.camel.component.http.HttpBinding.java", "org.apache.camel.component.http.HttpEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1118, "bug_title": "SFTP endpoint does not peform host key verification", "bug_description": "Camel does allow users to configure JSch with a known_hosts file. This makes the users vulnerable to man in the middle type attacks.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.file.remote.RemoteFileConfiguration.java", "org.apache.camel.component.file.remote.UriConfigurationTest.java", "org.apache.camel.component.file.remote.SftpEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1122, "bug_title": "Camel should propagate exception because in InOnly scenarios", "bug_description": "In the case an exception is handled by the DeadLetterChannel, Camel will currently set the exception to null and add an exchange property containing the exception. We should also add this as a message header so that it will be propagated to external services, like a JBI service.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.processor.DeadLetterChannel.java"], "label": 1, "es_results": []}, {"bug_id": 1108, "bug_title": "ParallelProcessing and executor flags are ignored in Multicast processor", "bug_description": "The parallelProcessing and executor arguments are ignored in the constructor of MulticastProcessor. \nHere is a call that MulticastType makes\norg.apache.camel.model.MulticastType.java\n    return new MulticastProcessor(list, aggregationStrategy, isParallelProcessing(), threadPoolExecutor);\n\n\nMulticastProcessor ignores parallelProcessing and executor arguments and invokes a chain constructor with \"..,false, null,..\"\norg.apache.camel.processor.MulticastProcessor.java\n    public MulticastProcessor(Collection<Processor> processors, AggregationStrategy aggregationStrategy, boolean parallelProcessing, ThreadPoolExecutor executor) {\n        this(processors, aggregationStrategy, false, null, false);\n    }\n\n    public MulticastProcessor(Collection<Processor> processors, AggregationStrategy aggregationStrategy, boolean parallelProcessing, ThreadPoolExecutor executor, boolean streaming) {\n        notNull(processors, \"processors\");\n        this.processors = processors;\n        this.aggregationStrategy = aggregationStrategy;\n        this.isParallelProcessing = parallelProcessing;\n        if (isParallelProcessing) {\n            if (executor != null) {\n                this.executor = executor;\n            } else { \n                // setup default Executor\n                this.executor = new ThreadPoolExecutor(processors.size(), processors.size(), 0, TimeUnit.MILLISECONDS, new ArrayBlockingQueue<Runnable>(processors.size()));\n            }\n        }\n        this.streaming = streaming;\n    }\n\n\nAttached a patch but have not tested it. Also need a unit test for this.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1125, "bug_title": "MockEndpont.expectedMinimumMessageCount() does not work right", "bug_description": "In MockEndpoint.assertIsSatisfied(), don&apos;t use the right counter for assertion.\n\n      \n      if (expectedMinimumCount >= 0) {\n            int receivedCounter = getReceivedCounter();\n            assertTrue(\"Received message count \" + receivedCounter + \", expected at least \" + expectedCount, expectedCount <= receivedCounter);\n        }\n\n\nit should be change to \n\n      \n      if (expectedMinimumCount >= 0) {\n            int receivedCounter = getReceivedCounter();\n            assertTrue(\"Received message count \" + receivedCounter + \", expected at least \" + expectedMinimumCount, expectedMinimumCount <= receivedCounter);\n        }\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.mock.MockEndpoint.java", "org.apache.camel.builder.ValueBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 1145, "bug_title": "CxfExchange does not copy unit of work and thus its null and thus NPE is thrown", "bug_description": "William could you check this error reported by end user\nSee nabble:\nhttp://www.nabble.com/UnitOfWork-td20829434s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.cxf.CxfExchange.java"], "label": 1, "es_results": []}, {"bug_id": 1037, "bug_title": "Messages in Resequencer between 2 JMS queues get stuck", "bug_description": "Martin describes the issue as follows in CAMEL-1034:\n\"The issue with the regular resequencer (the one that extends the BatchProcessor) remains because the process(Exchange) method is empty. In addition to the BatchProcessor&apos;s polling consumer, an additional JmsConsumer is created by the JMS endpoint that competes with the polling consumer. The JmsConsumer then calls the empty process(Exchange) method and the exchange is lost.\"", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.processor.StreamResequencer.java", "org.apache.camel.model.ResequencerType.java", "org.apache.camel.model.ProcessorType.java", "org.apache.camel.processor.BatchProcessor.java", "org.apache.camel.processor.Resequencer.java", "org.apache.camel.processor.AggregatorTest.java", "org.apache.camel.model.AggregatorType.java", "org.apache.camel.processor.Aggregator.java"], "label": 1, "es_results": []}, {"bug_id": 1188, "bug_title": "Exceptions thrown from when - methodCall are not caught by onException", "bug_description": "See the route below. beanOne throws ExceptionOneException and beanTwo throws ExceptionTwoException. ExceptionOneException it caught and handled as it&apos;s supposed to.  But ExceptionTwoException goes all the way through, without  being caught and routed to the exceptionTwoQueue. Is it the when tag or the methodCall tag that is the because of this?\n\n<camelContext id=\"MyCamelContext xmlns=\"http://activemq.apache.org/camel/schema/spring\">\n\t<endpoint id=\"myMainQueue\" uri=\"activemq:${my.project.queue.main}\"/>\n\t<endpoint id=\"exceptionOneQueue\" uri=\"activemq:${my.project.queue.exceptionOne}\"/>\n\t<endpoint id=\"exceptionTwoQueue\" uri=\"activemq:${my.project.queue.exceptionTwo}\"/>\n\t<route>\t\t\n\t\t<from ref=\"myMainQueue\" />\n\t\t<onException>\n\t\t\t<exception>my.project.queue.ExceptionOneException</exception>\n\t\t\t<redeliveryPolicy maximumRedeliveries=\"0\" />\n\t\t\t<handled>\n\t\t\t\t<constant>true</constant>\n\t\t\t</handled>\n\t\t\t<to ref=\"exceptionOneQueue\"/>\n\t\t</onException>\n\t\t<onException>\n\t\t\t<exception>my.project.queue.ExceptionTwoException</exception>\n\t\t\t<redeliveryPolicy maximumRedeliveries=\"0\" />\n\t\t\t<handled>\n\t\t\t\t<constant>true</constant>\n\t\t\t</handled>\n\t\t\t<to ref=\"exceptionTwoQueue\"/>\n\t\t</onException>\n\t\t<onException>\n\t\t\t<exception>java.lang.Exception</exception>\n\t\t\t<redeliveryPolicy maximumRedeliveries=\"0\" />\n\t\t\t<handled>\n\t\t\t\t<constant>false</constant>\n\t\t\t</handled>\n\t\t</onException>\n\t\t<unmarshal>\n\t\t\t<jaxb prettyPrint=\"true\" contextPath=\"my.project.domain\" />\n\t\t</unmarshal>\n\t\t<choice>\n\t\t\t<when>\n\t\t\t\t<methodCall bean=\"beanTwo\" method=\"methodFromBeanTwo\"/>\n\t\t\t</when>\n\t\t\t<otherwise>\n\t\t\t\t<to uri=\"bean:beanOne?methodName=methodFromBeanOne\" />\n\t\t\t</otherwise>\n\t\t</choice>\n\t</route>\n</camelContext>\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.ProducerTemplate.java", "org.apache.camel.builder.ValueBuilder.java", "org.apache.camel.impl.DefaultComponent.java"], "label": 1, "es_results": []}, {"bug_id": 1196, "bug_title": "MockEndpoint - sleep for empty test does not work", "bug_description": "See nabble:\nhttp://www.nabble.com/MockEndpoint---sleep-for-empty-tests-is-flawed--td21067367s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.mock.MockEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1193, "bug_title": "Interceptors for multicast", "bug_description": "MulticastType override the method wrapProcessorInInterceptors() and adds a StreamCachingInterceptor.\nThis is giving a problem for the debugger in FUSE Integration designer. FID debugger works using the Debug Interceptor, because we are not add an interceptor for Multicast node, it will not stop at that node even if we place a break point.\ncamel-dev mailing list thread related to this: http://www.nabble.com/Camel-Multicast-and-Interceptors-td21053645s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.model.MulticastType.java", "org.apache.camel.builder.RouteBuilderTest.java"], "label": 1, "es_results": []}, {"bug_id": 1199, "bug_title": "Throttler appears to \"throttle\" per thread instead of throttling across multiple threads", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.processor.Throttler.java", "org.apache.camel.processor.ThrottlerTest.java"], "label": 1, "es_results": []}, {"bug_id": 1225, "bug_title": "Duplicate type converter toString(Source) in two classes in camel-core", "bug_description": "See nabble\nhttp://www.nabble.com/Seda-StaticMethodTypeConverter-TransformerException-td21287333s22882.html\nThis type converter is needed by JBI containers so we should make sure the fix is correct.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.converter.IOConverter.java"], "label": 1, "es_results": []}, {"bug_id": 1213, "bug_title": "camel-jms consumer does not set camel message id based on JMS message id if the next node is not JMS based", "bug_description": "The code in org.apache.camel.component.jms.copyFrom looks odd\nI think the ! should be removed\nBEFORE\n\n        if (!copyMessageId) {\n            setMessageId(that.getMessageId());\n        }\n\n\nAFTER\n\n        if (copyMessageId) {\n            setMessageId(that.getMessageId());\n        }\n\n\nI will add a unit test (JmsToFileMessageIdTest) based on end user having some trouble with it", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.jms.JmsMessage.java", "org.apache.camel.converter.jaxp.BytesSource.java", "org.apache.camel.converter.jaxp.StringSource.java"], "label": 1, "es_results": []}, {"bug_id": 1249, "bug_title": "MailConfiguration injected into MailComponent by Spring is ignored.", "bug_description": "A Camel org.apache.camel.component.mail.MailComponent is configured via Spring as:\n\n<bean id=\"mailbox\" class=\"org.apache.camel.component.mail.MailComponent\">\n  <property name=\"configuration\">\n    <bean id=\"mailbox_config\" class=\"org.apache.camel.component.mail.MailConfiguration\">\n      <property name=\"protocol\"><value>pop3</value></property>\n      <property name=\"host\"><value>mail.abc.com</value></property>\n      <property name=\"username\"><value>test</value></property>\n      <property name=\"password\"><value>test</value></property>\n    </bean>\n  </property>\n</bean>\n\n\nIt is silly to use the hard-coded URI in Java code to create a mail endpoint. Instead I want to use the above method to specify everything (I mean everything) about how to access a mail server (send or receive) in different deployments.  Up to Camel 1.5, line 73 of MailComponent.createEndpoint() ignored the MailConfiguration variable MailComponent.configuration and created a new one to parse the URI. This defeats the Spring injection above, which is recommended by Camel&apos;s own User&apos;s Guide.\nLine 73 and 74 should be changed from:\n\n        MailConfiguration config = new MailConfiguration();\n        config.configure(url);\nto\n\n        configuration.configure(url);\nIn addition, if the uri parameter equals the component name, createEndpoint() should not parse it at all, so that the following route builder will create the mail endpoint solely according to the Spring injection of MailConfiguration:\n\n    from(\"mailbox\").to(\"my_queue\");", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.mail.MailConfiguration.java", "org.apache.camel.component.mail.MailComponent.java", "org.apache.camel.component.mail.MailEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1265, "bug_title": "Expression should be renamed to Language", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.model.language.LanguageExpression.java"], "label": 1, "es_results": []}, {"bug_id": 1270, "bug_title": "Starting Camel using Main from camel:run or Main.class - countdown latch is hanging", "bug_description": "The countdown latch in MainSupport is not completed when Main is stopping.\nThen we have a hanging thread. Can bee seen using ctrl + \\", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.util.MainSupport.java", "org.apache.camel.spring.Main.java"], "label": 1, "es_results": []}, {"bug_id": 1274, "bug_title": "Jetty HTTP SelectChannelConnector is not closed when CamelContext is stopped", "bug_description": "When the camel-jetty component is shut down, it leaves Jetty SelectChannelConnector instances active.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 1278, "bug_title": "VelocityEndpoint reads velocity templates using default encoding - add parameter to set encoding", "bug_description": "VelocityEndpoints reads templates using default encoding, so I cannot get correct content while running on Windows and Linux.\nThe reason is that there is used InputStreamReader constructor without encoding parameter.\nI&apos;ve added simple support for setting encoding - see attached patch", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.velocity.VelocityEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 122, "bug_title": "port the error handler / dead letter channel / redelivery policy over to JAXB2 so you can fully customize it via XML", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.spring.processor.SpringFaultRouteTest.java"], "label": 1, "es_results": []}, {"bug_id": 1283, "bug_title": "SQL component does not handle wrong number of parameters", "bug_description": "NPE thrown when null body sent to to SQL endpoint. When to many parameters are set then exception is thrown but nothing wrong happens when to less parameters are set.\nIt should be possible to set null as a body as there might be parameterless statement to execute. There should be consistent Exception thrown when wrong number of parameters is set.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.component.sql.SqlRouteTest.java", "org.apache.camel.component.sql.SqlProducer.java"], "label": 1, "es_results": []}, {"bug_id": 1292, "bug_title": "SQLComponent should preserve headers", "bug_description": "\n                from(\"direct:start\")\n                    .setHeader(\"foo\", constant(\"bar\"))\n                    .to(\"sql:select * from projects\")\n                    .to(\"mock:result\");\n\n\nThe foo header should be preserved so when its routed to the mock its still there.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.sql.SqlProducer.java"], "label": 1, "es_results": []}, {"bug_id": 1301, "bug_title": "NPE in FactoryFinder.findClass when context classloader is null", "bug_description": "My program works fine, when I run it as a normal Windows app.  But when I try to run it as a service, it throws a \"Could not auto create component: http\" which is caused by a NullPointerException which is caused by Thread.currentThread().getContextClassLoader() returning null.\nA quick search found this ActiveMQ bug: http://issues.apache.org/activemq/browse/AMQ-1229 which is essentially identical.  If it&apos;s been fixed in ActiveMQ, probably you can just merge the fix from org.apache.activemq.util.FactoryFinder into org.apache.camel.util.FactoryFinder.\nIncidentally, the fact that the context classloader is null has also been reported as a bug: http://issues.apache.org/jira/browse/DAEMON-100", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.util.ObjectHelper.java", "org.apache.camel.util.FactoryFinder.java", "org.apache.camel.impl.DefaultComponentResolver.java"], "label": 1, "es_results": []}, {"bug_id": 1205, "bug_title": "@EndpointInject can not lookup bean if endpoint is defined directly as a consumer (from)", "bug_description": "I was messing with camel-spring and routes using spring DSL\nI had this part of unit test code\n\n    @EndpointInject(name = \"myFileEndpoint\")\n    protected Endpoint inputFile;\n\n\nTo lookup an endppoint with the given name myFileEndpoint\nBut if I define it directly then it cannot be found\n\n        <route>\n            <from id=\"myFileEndpoint\" uri=\"file://target/antpathmatcher?consumer.recursive=true&amp;filter=#myFilter\"/>\n            <to uri=\"mock:result\"/>\n        </route>\n\n\nSo I have to define it as a loose endpoint as:\n\n        <endpoint id=\"myFileEndpoint\" uri=\"file://target/antpathmatcher?consumer.recursive=true&amp;filter=#myFilter\"/>\n\n        <route>\n            <from ref=\"myFileEndpoint\"/>\n            <to uri=\"mock:result\"/>\n        </route>\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.builder.xml.Namespaces.java", "org.apache.camel.spring.handler.CamelNamespaceHandler.java"], "label": 1, "es_results": []}, {"bug_id": 1321, "bug_title": "Classloading leak in camel-jaxb component", "bug_description": "See message thread:\nhttp://www.nabble.com/Classloading-leak-in-camel-jaxb-component-td21879801s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.converter.jaxb.JaxbConverter.java"], "label": 1, "es_results": []}, {"bug_id": 1330, "bug_title": "camel-cxf Soap-Fault not returned as 500 http error code", "bug_description": "Here is the mail thread which talks about it.\nhttp://markmail.org/message/4qvwufpboro3lj34?q=camel-cxf+Soap-Fault+not+returned+as+500+http+error+code", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.cxf.CXFGreeterRouterTest.java"], "label": 1, "es_results": []}, {"bug_id": 1352, "bug_title": "HttpHeaderFilterStrategy tries to filter out \"http.requestMethod\" but does not", "bug_description": "I actually found this in 1.5.0, but a quick look in ViewVC indicates that it still happens in 1.6.0 and in the trunk.\nHttpHeaderFilterStrategy uses String.toLowerCase on \"http.responseCode\" because of the capital C.\nIt should also use toLowerCase on \"http.requestMethod\" because of the capital M, but it doesn&apos;t.\nThus the http.requestMethod header leaks into the HTTP protocol headers.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.impl.DefaultHeaderFilterStrategy.java", "org.apache.camel.component.http.HttpHeaderFilterStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 1356, "bug_title": "TryProcessor sets \"caught.exception\" header to in message, while there can be out already", "bug_description": "Error exists in org.apache.camel.processor.TryProcessor.handleException(Exchange, Throwable). If your exchange has already out message (that can happen if you set out and then throw an exception. Then the exception caught is set on in message. When it reaches a pipeline it will be lost after first hop.\nI believe TryProcessor should do what Pipelient does - copy out to in in new exchange if out exists.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.processor.TryProcessor.java", "org.apache.camel.processor.TryProcessorHandleTest.java"], "label": 1, "es_results": []}, {"bug_id": 1401, "bug_title": "JaxbDataFormat is not thread-safe", "bug_description": "JaxbDataFormat reuses the Marshaller/Unmarshaller instances.\nHowever, according to the https://jaxb.dev.java.net/faq/index.html#threadSafety, Marshaller/Unmarshaller instances are NOT thread safe, so you can&apos;t use them from separate threads at the same time.\nDue to this we are getting exceptions from the JAXB implementation (like ClassCastException or \"org.xml.sax.SAXException: FWK005 parse may not be called while parsing.\"). Everything works nice with our custom JaxbDataFormat that creates new Unmarshaller/Marshaller instance on every request.\nAlso, lazy-creating instances (like JAXBContext) in getter methods is not thread-safe as well (because explicit synchronization is required).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.converter.jaxb.JaxbDataFormat.java", "org.apache.camel.example.JAXBConvertTest.java", "org.apache.camel.converter.jaxb.JaxbConverter.java", "org.apache.camel.converter.jaxb.FallbackTypeConverter.java"], "label": 1, "es_results": []}, {"bug_id": 1384, "bug_title": "ExchangeHelper should respect ExchangePattern.InOptionalOut ", "bug_description": "The utility method org.apache.camel.util.ExchangeHelper.copyResults() is use by many core classes. However, this method does not properly support MEP InOptionalOut.\nAssuming in an InOptionalOut exchange, having no out message means just that  no out message should be sent, then the following lines in this method\n\n//\n            Message out = source.getOut(false);\n            if (out != null) {\n                result.getOut(true).copyFrom(out);\n            } else {\n                // no results so let us copy the last input\n\n\nshould be changed to:\n\n//\n            Message out = source.getOut(false);\n            if (out != null) {\n                result.getOut(true).copyFrom(out);\n            } else if (result.getPattern() == ExchangePattern.InOptionalOut) {\n                result.setOut(null);\n            } else {\n                // no results so let us copy the last input\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.component.dataset.DataSetConsumer.java", "org.apache.camel.component.dataset.DataSetEndpoint.java", "org.apache.camel.util.ExchangeHelper.java", "org.apache.camel.impl.DefaultProducerTemplate.java"], "label": 1, "es_results": []}, {"bug_id": 1366, "bug_title": "EndpointMessageListener should respect ExchangePattern", "bug_description": "In all current releases, org.apache.camel.component.jms.EndpointMessageListener.onMessage() has the following logic (line 90 in 1.6.0 code):\n\n// send the reply\nif (rce == null && body != null && !disableReplyTo) {\n    sendReply(replyDestination, message, exchange, body);\n}\n\n\nThis logic should also respect ExchangePattern of the exchange, so I propose a change to:\n\n// send the reply\nif (rce == null && body != null && exchange.isOutCapable()) {\n    sendReply(replyDestination, message, exchange, body);\n}\n\n\nThis change allows a processing pattern where the route may change the ExchangePattern using methods like RouteBuilder.inOnly() to switch the MEP at will so that the reply is send at a later time (true asynchronous exchange).  This processing pattern is particularly useful for integrating long running services. For example,\n\n// Java DSL\nfrom(\"activemq:my_queue?exchangePattern=InOnly\").to(\"predict_weather://?reply_later=true\");\n// or\nfrom(\"activemq:my_queue2\").inOnly().to(\"predict_weather://?reply_later=true\");\n\n\nThe flaw of the current logic makes it impossible to do true asynchronous exchange, because 1) it does not respect the ExchangePattern; 2) if property \"disableReplyTo\" is used, the \"org.apache.camel.jms.replyDestination\" property will not be set (see method createExchange in the same file), thus downstream cannot find the reply destination.\nThe proposed change can also deprecate the disableReplyTo property and put the MEP concept into good use.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.component.jms.EndpointMessageListener.java"], "label": 1, "es_results": []}, {"bug_id": 1406, "bug_title": "DelayProcessor should handle interrupting while shutting down", "bug_description": "See nabble:\nhttp://www.nabble.com/Re%3A-Delayer%3A-%22Transport-disposed%22-at-JVM-Shutdown-td22202577s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.processor.DelayerTest.java", "org.apache.camel.processor.DelayProcessorSupport.java"], "label": 1, "es_results": []}, {"bug_id": 1379, "bug_title": "Mina configuration is shared between endpoints", "bug_description": "Establishing a MINA endpoint with a custom codec, and then establishing another without a custom codec is a problem. The second endpoint inherits the first endpoint&apos;s codec.\nMy recommendation is to not share configuration data between endpoint creation. I recommend that the MINA component instantiate a new configuration for each new endpoint instead of copying the previous configuration.\nAs a workaround the user can specify \"codec\" as a URI parameter with no value.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.component.mina.MinaCustomCodecTest.java", "org.apache.camel.component.mina.MinaEncodingTest.java", "org.apache.camel.component.mina.MinaComponent.java"], "label": 1, "es_results": []}, {"bug_id": 1426, "bug_title": "Camel Spring configuration doen't support to scan the SpringRouteBuilder", "bug_description": "Here is the mail thread which talks about it.\nhttp://www.nabble.com/Error%3A-This-SpringBuilder-is-not-being-used-with-a-SpringCamelContext-and-there-is-no-applicationContext-property-configured-to22326547s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.spring.SpringRouteBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 1412, "bug_title": "Can not load the QueueBrowserStrategy in OSGI environment", "bug_description": "Here is the mail thread which discusses about it.\nhttp://www.nabble.com/Classloading-and-OSGI-to22303475.html#a22303475", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.component.jms.JmsComponent.java"], "label": 1, "es_results": []}, {"bug_id": 1444, "bug_title": "camel-mina - UDP protocol could have an issue if used in same camel context as both client and server sending to localhost", "bug_description": "A mina bytebuffer could be shared in a mina session. It should not be.\nSee nabble, that could lead to that problem the end user has:\nhttp://www.nabble.com/Camel-1.6-2.0-MINA-UDP-issue-td22426433s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.component.mina.MinaProducer.java", "org.apache.camel.component.mina.MinaTcpLineDelimiterUsingPlainSocketTest.java", "org.apache.camel.component.mina.MinaComponent.java", "org.apache.camel.component.mina.MinaTcpWithIoOutProcessorExceptionTest.java", "org.apache.camel.component.mina.MinaConsumer.java", "org.apache.camel.component.mina.MinaUdpWithInOutUsingPlainSocketTest.java", "org.apache.camel.component.mina.MinaConverter.java"], "label": 1, "es_results": []}, {"bug_id": 1454, "bug_title": "CXF component running in Payload mode does not work with Holders", "bug_description": "If you convert the CxfWsdlFirstTest to run in PAYLOAD mode (by simply changing the endpoint URI in Spring xml), the client.getPerson() invocation will fail.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.component.cxf.feature.PayLoadDataFormatFeature.java", "org.apache.camel.component.cxf.interceptors.AbstractMessageInInterceptor.java", "org.apache.camel.component.cxf.CxfProducer.java"], "label": 1, "es_results": []}, {"bug_id": 1461, "bug_title": "A request route with a topic node incurs a 20 second wait and refers to the wrong MEP.", "bug_description": "If a route contains a node that publishes to a topic, the route is incorrectly suspended for a default 20 seconds at the topic node.  Further, JmsProducer.java checks the MEP of the original request Exchange and not the endpoint of the topic.\nFor example, say I have a route built like this:\n\nfrom(\"activemq:queue:request\").\n  to(\"generate_news\").\n  to(\"activemq:topic:news\").\n  to(\"do_something_else\");\n\n\nThe original request is expecting a reply. However, after the \"news\" is pumped into the news topic, there is a default 20 second wait (requestTimeout).  This wait always results in the exception: \"The OUT message was not received within: 20000 millis on the exchange...\" \nAfter reading the JmsProducer code, I changed the route to the following:\n\nfrom(\"activemq:queue:request\").\n  to(\"generate_news\").\n  to(\"activemq:topic:news?exchangePattern=InOnly\").\n  to(\"do_something_else\");\n\n\nThis reveals the root of the bug, which is in the first few lines of method org.apache.camel.component.jms.JmsProducer.process(Exchange):\n\n//\n    public void process(final Exchange exchange) {\n        final org.apache.camel.Message in = exchange.getIn();\n\n        if (exchange.getPattern().isOutCapable()) {\n\n\nThe above if statement checks the MEP of the original request&apos;s Exchange and not the new endpoint of the news topic. This makes the above \"?exchangePattern=InOnly\" configuration useless, because the original request MEP is InOut.  The result is that after that 20 second time-out, the temporary queue for the original request has expired, so the whole request failed. Note that the next node \"do_something_else\" is never reached due to the time-out exception.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-1.6.1", "fixed_files": ["org.apache.camel.component.jms.JmsProducer.java", "org.apache.camel.component.jms.JmsBinding.java", "org.apache.camel.component.jms.EndpointMessageListener.java"], "label": 1, "es_results": []}, {"bug_id": 1508, "bug_title": "TypeConverter - Can find Object type converter even though there is a more specialized converter to use", "bug_description": "The code travels down the super to fast. And the code that checks for Object and inherited types in the end of the lookup code should be run as last resort.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.0-M2", "fixed_files": ["org.apache.camel.impl.converter.DefaultTypeConverter.java"], "label": 1, "es_results": []}, {"bug_id": 1519, "bug_title": "Spring DSL - Transacted routes with policy seems to not work - same route in Java DSL works ", "bug_description": "In 1.x see rev 762400 where I added an unit test demonstrating the bug\nThe TX interceptor gets applied wrong with Spring DSL versus Java DSL.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-1.6.1", "fixed_files": ["org.apache.camel.spring.CamelContextFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 1544, "bug_title": "Multiple consumers getting created in custom endpoint", "bug_description": "If you restart the camel context ,you will get into trouble of creating multiple consumers.\nHere is the mail thread with talks about it.\nhttp://www.nabble.com/Multiple-consumers-getting-created-in-custom-endpoint-td22928845.html#a22928845", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.1", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.impl.DefaultCamelContextTest.java"], "label": 1, "es_results": []}, {"bug_id": 1568, "bug_title": "Converting from stream/reader to string changes line breaks other than \"\\n\"", "bug_description": "The IOConverter#toString(Reader/Stream) copies the input data line by line. A line is considered to be terminated by any one of a line feed (&apos;\\n&apos;), a carriage return (&apos;\\r&apos;), or a carriage return followed immediately by a linefeed. Unfortunately, the lines are always concatenated with \"\\n\", so that HL7 messages (which use \"\\r\") become unusable after conversion.\nAlso see http://www.nabble.com/HL7-messages-become-unusable-after-convertBodyTo%28String.class%29-td23219748.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-1.6.1", "fixed_files": ["org.apache.camel.converter.IOConverter.java"], "label": 1, "es_results": []}, {"bug_id": 1583, "bug_title": "ldap component is not thread safe", "bug_description": "JNDI context objects are not thread-safe. The LDAP component shares a directory context across all threads that use the component. This is not safe.\nIn addition the LDAP component will attempt to establish a connection on instantiation of the component, and not when the component is required to process requests. If the LDAP server is not ready e.g. temporarily unavailable then the entire Camel application will stop.\nJNDI directory contexts should be established when a consuming thread needs it and should be released when the thread is finished with the component i.e.:\n\nctx = new InitialDirContext(env);\ntry {\n  ...\n} finally {\n  ctx.close();\n}\n\n\nThe above will release the connection with the LDAP server as soon as possible. The existing component relies on JNDI to release the socket in its own time (several seconds later).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-1.6.1", "fixed_files": ["org.apache.camel.component.ldap.LdapProducer.java"], "label": 1, "es_results": []}, {"bug_id": 1586, "bug_title": "camel-cxf producer loses the content-type ", "bug_description": "camel-cxf doesn&apos;t put the Protocol-Header into the camel message header, so we lost the content-type message.\nHere is the mail thread for the discussion.\nhttp://www.nabble.com/camel-cxf-content-type-response-header-tt23370337.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-1.6.1", "fixed_files": ["org.apache.camel.component.cxf.CxfRawMessageRouterTest.java", "org.apache.camel.component.cxf.CxfHeaderFilterStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 1614, "bug_title": "Warning \"Disabling JMSReplyTo as this Exchange is not OUT capable\" on false positives in JBossAS5", "bug_description": "see: http://www.nabble.com/Disabling-JMSReplyTo-as-this-Exchange-is-not-OUT-capable-td23524909s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.jms.JmsProducer.java"], "label": 1, "es_results": []}, {"bug_id": 756, "bug_title": "AMQP component cannot send an ObjectMessage", "bug_description": "Sending a TextMessage and BytesMessage work fine but when you try to send a Serializable object it gets lost somehow... see the testJmsRouteWithObjectMessage test in http://svn.apache.org/repos/asf/activemq/camel/trunk/components/camel-amqp/src/test/java/org/apache/camel/component/amqp/AMQPRouteTest.java to see what happens.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-2.0-M2", "fixed_files": ["org.apache.camel.component.amqp.AMQPRouteTest.java"], "label": 1, "es_results": []}, {"bug_id": 1497, "bug_title": "JmsPollingConsumer - receive does not work", "bug_description": "The timeout values to set on Spring JmsTemplate is wrong. We should uses the provided constants on JmsTemplate for this\n\n\t/**\n\t * Timeout value indicating that a receive operation should\n\t * check if a message is immediately available without blocking.\n\t */\n\tpublic static final long RECEIVE_TIMEOUT_NO_WAIT = -1;\n\n\t/**\n\t * Timeout value indicating a blocking receive without timeout.\n\t */\n\tpublic static final long RECEIVE_TIMEOUT_INDEFINITE_WAIT = 0;\n\n\nIs the correct values. Looks like the values was reverted in the camel-jms code ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-1.6.1", "fixed_files": ["org.apache.camel.component.jms.JmsPollingConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 1760, "bug_title": "Unable to read post params from request", "bug_description": "If consuming messsages from jetty endpoint there is no way to read post parameters from HttpServletRequest. \nIn example  getting HttpServletRequest  from body :\nHttpServletRequest req = exchange.getIn().getBody(HttpServletRequest.class);\nreq.getParameterMap() returns allways empty map\nThe problem is that jetty Request.extractParameters() method is trying  to read post parameters from Request.getInputStream().  But unfortunately someone strips the input stream before and req.getInputStream() returns allways 0 bytes\nThe workaround for me is to extend DefaultHttpBinding as described in:  http://camel.apache.org/jetty.html\nMyHttpBinding .java\npublic class MyHttpBinding extends DefaultHttpBinding {\n    \n    public void readRequest(HttpServletRequest request, HttpMessage message) {\n       request.getParameterMap();\n       super.readRequest(request,message);\n    }\n}\n\n\ncalling  request.getParameterMap()  will cache parameters inside jetty Request and it&apos;s possible to query params later,  without having inputStream\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.http.DefaultHttpBinding.java", "org.apache.camel.component.jetty.HttpRouteTest.java"], "label": 1, "es_results": []}, {"bug_id": 1828, "bug_title": "jaxb prettyPrint does not work", "bug_description": "Here is the mail thread with discuss about it.\n http://www.nabble.com/JAXB-pretty-print-marshaling-td24449564.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.example.DataFormatTest.java", "org.apache.camel.converter.jaxb.JaxbDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 1912, "bug_title": "DefaultCamelContext.removeEndpoints() fails due to ConcurrentModificationException", "bug_description": "The removeEndpoints() method iterates over the endpoints map, and attempts to remove entries inside of the iterator loop.  This causes near immediate ConcurrentModificationException&apos;s.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.5.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 2112, "bug_title": "Problem using a CamelTransportFactory from a junit 4 unit test or together with <context:annotation-config/>", "bug_description": "I recently tried to convert my unit tests to junit 4 using the new Spring test framework.\nSince then I get the following exception when calling the test.\nError creating bean with name &apos;org.apache.camel.component.cxf.transport.CamelTransportFactory#0&apos;: Injection of resource methods failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No matching bean of type [java.lang.String] found for dependency [collection of java.lang.String]: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {}\nThe same happens when I activate /<context:annotation-config/>/\nI think it has to do with autowiring. Do you have any idea what goes wrong here and how I can fix it? ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.cxf.transport.CamelTransportFactory.java"], "label": 1, "es_results": []}, {"bug_id": 522, "bug_title": "Problem with scanning Jar files for Converters if application is loaded with webstart", "bug_description": "I&apos;ve encountered a problem with the scanning of jar files with the @Converter annotation if the Jars are loaded with the webstart classloader. Because the URL starts with http: the conversion to a file fails in ResolverUtil. \nI&apos;ve attached a patch that uses an UrlResource if the url starts with http: and this works if the jar-resource has the version information added to the jar file in the JNLP-file (like <jar href=\"camel-core-1.4-SNAPSHOT.jar\"/>). But if the version is added separately (like <jar href=\"activemq-core.jar\" version=\"5.1.0\"/>) it fails because the classloader returns the name of the jar-file without the version information (like http://....../activemq-core.jar) and this file does not exist. \nI&apos;m currently trying if it is possible to check whether the application is started via webstart (there is a class Tool that should only exist in the webstart classloader) and in this case I&apos;ll check if the filename of the jar-file is returned correctly. If yes maybe we can provide another patch to get rid of the problem described above.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.3.0", "fixed_version": "camel-1.5.0", "fixed_files": ["org.apache.camel.util.ResolverUtil.java"], "label": 1, "es_results": []}, {"bug_id": 2275, "bug_title": "camel-jpa JPAEndpoint eat up the because of  InvalidPayloadException", "bug_description": "Here is the mail thread discuss this issue.\nhttp://old.nabble.com/TypeConverter-%3A-Should-it-be-blocking-for-camel-route-or-not-in-case--of-error---td26723837.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.InvalidPayloadRuntimeException.java", "org.apache.camel.component.jpa.JpaEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1966, "bug_title": "CamelNamespaceHandler is not thread safe", "bug_description": "Here is the mail thread[1] which discusses about it.\n[1] http://www.nabble.com/Occasional-CamelNamespaceHandler-exception-when-running-with-Spring-DM-1.2-td25195171.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.spring.handler.CamelNamespaceHandler.java"], "label": 1, "es_results": []}, {"bug_id": 2327, "bug_title": "The NP check of the remote service is wrong in CamelServiceExporter ", "bug_description": "We should check the export service instead of the camelContext.\n components/camel-spring/src/main/java/org/apache/camel/spring/remoting/CamelServiceExporter.jav(revision 895109)\n+++ components/camel-spring/src/main/java/org/apache/camel/spring/remoting/CamelServiceExporter.jav(working copy)\n@@ -83,7 +83,7 @@\n         }\n         Endpoint endpoint = CamelContextHelper.getMandatoryEndpoint(camelContext, uri);\n\nnotNull(camelContext, \"service\");\n+        notNull(getService(), \"service\");\n         Object proxy = getProxyForService();\n\n         consumer = endpoint.createConsumer(new BeanProcessor(proxy, camelContext));", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.3", "fixed_files": ["org.apache.camel.spring.remoting.CamelServiceExporter.java"], "label": 1, "es_results": []}, {"bug_id": 2397, "bug_title": "camel-cxf producer should copy the inMessage headers to the outMessage", "bug_description": "If we try to chain some different endponit together in the camel route, we need to make sure the in message header be copied to the out message. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.3", "fixed_files": ["org.apache.camel.component.cxf.CxfProducerContextTest.java", "org.apache.camel.component.cxf.CxfProducer.java", "org.apache.camel.component.cxf.CxfProducerTest.java"], "label": 1, "es_results": []}, {"bug_id": 2188, "bug_title": "JcrProducer node creation throws javax.jcr.PathNotFoundException when using exchange ID as node name", "bug_description": "To reproduce, use the camel-jcr component without specifying a node name in the message properties.  JcrProducer will attempt to use the exchange ID as the node name, but in ServiceMix 4, this ID contains path separator characters, which results in a PathNotFound exception when the producer attempts to create the node.  IMHO, the producer should attempt to create parent nodes as needed when the value of either the exchange ID or the property named JcrConstants.JCR_NODE_NAME contain path separators.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.jcr.JcrEndpoint.java", "org.apache.camel.component.jcr.JcrProducer.java"], "label": 1, "es_results": []}, {"bug_id": 2763, "bug_title": "MailBinding cannot handle the subject header with NO_ASCII code rightly.", "bug_description": "Here is the mail thread which discusses about it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.4.0", "fixed_version": "camel-1.6.3", "fixed_files": ["org.apache.camel.component.mail.MailRouteTest.java", "org.apache.camel.component.mail.MailBinding.java"], "label": 1, "es_results": []}, {"bug_id": 1624, "bug_title": "Subject not correctly set in xmpp message in both chat producers.", "bug_description": "The code in XmppPrivateChatProducer and XmppGroupChatProducer does not call setSubject() on the org.jivesoftware.smack.packet.Message object.\nSetting a subject property on the org.apache.camel.Message object produces a header on the Smack object via its superclass org.jivesoftware.smack.packet.Packet, but that never gets reflected in the Message object because the Message object does not override the setProperty() method on Packet.\n\nprivate void send(String camelConnectionString) {\n\t\tfinal CamelContext camel = new DefaultCamelContext();\n\t\tcamel.start();\n\t\tEndpoint endpoint = camel.getEndpoint(camelConnectionString);\n\t\tExchange exchange = endpoint.createExchange(ExchangePattern.InOut);\n\t\tProducer producer = endpoint.createProducer();\n\t\tproducer.start();\n\n\t\t// send the message\n\t\tMessage message = exchange.getIn();\n\t\tmessage.setBody(\"The message body\");\n\t\tmessage.setHeader(\"subject\", \"Message subject for filtering.\");\n\t\tproducer.process(exchange);\n}\n\n\nThe annoying part about this, is that I am trying to integrate with a receiving system that handles messages using the subject as a key to determine how the message should be handled.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.xmpp.GoogleTalkTest.java", "org.apache.camel.component.xmpp.XmppBinding.java"], "label": 1, "es_results": []}, {"bug_id": 1646, "bug_title": "camel-restlet - Should be InOut exchange pattern", "bug_description": "camel-restlet creates exchanges that are inOnly. But they should be InOut.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.restlet.RestletEndpoint.java", "org.apache.camel.component.restlet.RestletBinding.java", "org.apache.camel.component.restlet.DefaultRestletBinding.java", "org.apache.camel.component.restlet.RestletProducer.java", "org.apache.camel.component.restlet.RestletComponent.java"], "label": 1, "es_results": []}, {"bug_id": 1676, "bug_title": "Endpoint.getParticipant should return Endpoint.getUser() if participant is null.", "bug_description": "The participant should default to the user  if the participant is null.\n\n    public String getParticipant() {\n        return participant != null ? participant : getUser();\n    }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.xmpp.XmppEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1633, "bug_title": "XMPPConsumer.processPacket does not correctly handle received non-message packets.", "bug_description": "I am currently porting an application using an in house Smack integration to Apache Camel.\nSpecifically, I am getting failures with multiuser chat where messages stop being received in the middle of the message stream I am sending.\nI have yet to verify the issue exists with private chat as well, but the XMPPConsumer source looks like there will be a similar issue.\nThe XMPPConsumer class registers itself for all packet types in the doStart method, but in the processPacket method immediately casts the received Packet to Message.\nI have found with the in house integration that Smack sends several types of Packets, and I could not find assurance that it would not call the packet listener with a null message.\nA simple if((null != packet) && (packet instanceof Message))  should be used to prevent improper packets from being utilized.\nFYI: the above if statement should also prevent packets from building up in the Smack message queue, since all messages will be processed without throwing an exception.\nSo a call to muc.nextMessage() is unnecessary, and actually detrimental (since if the next packet is a message, it will be dropped without processing).\nIt may be wise to actually use a try/catch block to prevent exceptions from being thrown by the processPacket method, since messages that throw exceptions seem to stay in the Smack message queue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.xmpp.XmppConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 1728, "bug_title": "camel-bean - should throw MethodNotExist exception if method name provided but not found on bean", "bug_description": "See nabble\nhttp://www.nabble.com/AmbiguousMethodCallException-td24042324s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-2.0-M3", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 1724, "bug_title": "camel-mail - sending mails with text/plain could give a content type error by java mail", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.mail.MailBinding.java"], "label": 1, "es_results": []}, {"bug_id": 1736, "bug_title": "camel-cxf defaultHeaderFilterStrategy does not filter the Camel releates headers", "bug_description": "Here is the mailing thread[1] which discuss this issue.\n[1] http://www.nabble.com/-camel-cxf--Stripping-headers-td24143323.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.cxf.CxfHeaderFilterStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 1743, "bug_title": "Validator component lacks a DOMSource converter from an InputStream", "bug_description": "The Validator component requires the payload to be of type DOMSource so it converters it to this type.\nBut we lack a XmlConverter from an InputStream to this type.\nSee nabble:\nhttp://www.nabble.com/XML-Validation-throws-NoXmlBodyValidationException-td24147734.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.itest.jetty.JettyValidatorTest.java", "org.apache.camel.converter.jaxp.XmlConverter.java"], "label": 1, "es_results": []}, {"bug_id": 1752, "bug_title": "SpringCamelContext will get into endless loop of calling start method if the route service calling the ApplicationContext refresh method in it is initial method", "bug_description": "Here is the mail thread which discusses this issue.\nhttp://www.nabble.com/StackOverFlow-error-with-Camel-1.6.1-tp24172060p24172060.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.spring.SpringCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 1786, "bug_title": "camel-irc parsing of channel name and parameters is incorrect", "bug_description": "If you try and pass some parameters to camel-irc it turns out that camel joins a different IRC channel than expected, for example a URI like:\nirc://camel-con@irc.codehaus.org:6667/#camel-test?nickname=camel-con\ncauses camel to join #camel-test?nickname=camel-con and not #camel-test with a nickname of camel-con.  Will attach a patch that addresses this and updates the unit test.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.irc.IrcRouteTest.java", "org.apache.camel.component.irc.IrcConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 1795, "bug_title": "camel-jms - wrong defaults causes message listener to use new thread when receiving a message", "bug_description": "JmsConfiguratiuon have a bad default\nmaxMessagesPerTask=1\nIt should be -1 as we do not want to strict the DefaultMessageListenerContainer to only handle 1 message per thread.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.jms.JmsConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 1922, "bug_title": "BeanEndpoint fails in the presence of bridge methods", "bug_description": "When a message is sent to a bean: endpoint, camel searches for a method with the right signature.  If multiple methods are found, an exception is thrown.\nIf a have an interface like\npublic interface <T> I {\n  T function(T arg);\n}\nand a class like\npublic class C implements I<Number> {\n  Number function(Number arg) \n{\n    return new Double(arg.doubleValue() * 2);\n  }\n}\n\nthen I can&apos;t use an instance of C as an endpoint.\n\nThe problem is that under the hood java has created a bridge method, and the \"real\" class C looks like\n\npublic class C implements I<Number> {\n  Number function(Number arg) {    return new Double(arg.doubleValue() * 2);  }\n\n  Object function(Object arg) \n{\n    return function((Number) arg);\n  }\n}\nCamel then discovers that there are two methods it could call (f(Object) and f(Number)) and throws an exception even though there&apos;s no real ambiguity because both methods do the same thing.\nI recommend rejecting bridge methods in BeanInfo.isValidMethod with somehting like\nif (method.isBridgeMethod()) return false;\nDoes this sound reasonable?", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 1983, "bug_title": "camel-cxf should let users to set out SOAP headers", "bug_description": "User reported trouble setting out SOAP headers in 1.x \nhttp://www.nabble.com/setting-soap-headers-in-camel-cxf-router-td25068148.html#a25068148\nWe should also test/fix it in 2.x as well as adding documentation on how to set/get SOAP headers in POJO and PAYLOAD modes.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.cxf.soap.headers.CxfMessageHeadersRelayTest.java", "org.apache.camel.component.cxf.soap.headers.HeaderTesterImpl.java"], "label": 1, "es_results": []}, {"bug_id": 2067, "bug_title": "Mail Component: Character \"|\" in recipient addresses causes failure", "bug_description": "It looks like there is minor bug in regex used in MailBinding.java, it causes \"|\" to act as delimiter, splitting email address into 2 parts.  \nFor ex: Using email address as - first|last@test.com causes it to split into two parts:\n1) first\n2) last@test.com\n\"|\" is acceptable character in email as per wikipedia http://en.wikipedia.org/wiki/Email_address. I did not bother to read RFC specification.\nHere is snippet of code causing issue:\n private static void appendRecipientToMimeMessage(MimeMessage mimeMessage, String type, String recipient)\n        throws MessagingException {\n        // we support that multi recipient can be given as a string seperated by comma or semi colon\n        String[] lines = recipient.split(\"[,|;]\");\n        for (String line : lines) {\nRegex above should either be \"[,;]\" or \";|,\"\nAlso, having email addresses separate out by , or ; seems to be issue as it can be part of name. ex: \"firstName, I like comma <first.last@test.com>\" is valid. Above code will still because failure for such messages. Can camel support addresses as List/Array rather than comma delimited string?\nSee Forum thread: http://www.nabble.com/Camel-Mail-Component%3A-%22|%22-in-email-causes-message-to-fail-td25883099.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.mail.MailBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2072, "bug_title": "camel-mail - extracting mails with headers using multi values inserts duplicates", "bug_description": "Sending a message with headers that contains 2 values such a String[] with 2 values will result on the other side when consuming that mail message to contain a header with 2 entries. There should only be 1 entry with the 2 values.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.mail.MailBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2075, "bug_title": "camel-bean - When a method name is given it should be more strict to be sure always invoking a method with that name", "bug_description": "You could potential invoke another method name if you have annotated that with a Camel annotation. Even if you have told Camel that the method should be named foo etc.\nWhat Camel should do is to ensure that only methods with the name foo is possible to invoke.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 2074, "bug_title": "camel-bean - should skip getter/setter methods when looking for method candidates", "bug_description": "If you have a POJO and you have a setter for a Camel concept such as a org.apache.camel.TypeConverter then the bean component could consider invoking the setTypeConverter method as a candidate.\nIn fact it should skip any getter/setter all together.\nFor example this pojo\n\npublic class RiderOrderService {\n\n    private TypeConverter converter;\n\n    public void setConverter(TypeConverter converter) {\n        this.converter = converter;\n    }\n\n    public String handleCustom(GenericFile file) {\n    ...\n    }\n\n    public String handleXML(Document doc) {\n    ...\n    }\n\n\nCamel should only consider the 2 handle methods. The setter should be skipped.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java", "org.apache.camel.util.IntrospectionSupportTest.java", "org.apache.camel.util.IntrospectionSupport.java"], "label": 1, "es_results": []}, {"bug_id": 2084, "bug_title": "http polling consumer does not support timeout", "bug_description": "When using the consumer template to receive from a remote http service using camel-http the method receive(5000) that uses a 5 sec timeout does not work as expected.\nCamel does not use the 5000 value.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.http.DefaultHttpBinding.java", "org.apache.camel.component.http.HttpPollingConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 2090, "bug_title": "camel-jms - Option autoStartup does not work", "bug_description": "It does in fact auto startup it anyway.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.jms.JmsConsumer.java", "org.apache.camel.component.jms.JmsEndpointConfigurationTest.java", "org.apache.camel.component.jms.EndpointMessageListener.java", "org.apache.camel.component.jms.JmsConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 2145, "bug_title": "camel-http - creating http uri may contain special endpoint options", "bug_description": "For example sending to an endpoint as follows\n\nto(\"http://www.google.com?throwExceptionOnFailure=false\");\n\n\nCould potentially added throwExceptionOnFailure as a parameter but its an endpoint parameter not a HTTP parameter.\nThis issue is for components that build on top of camel-http.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 2158, "bug_title": "onException - Should look up the hierarchy for exact matches to avoid being handled by Exception.class where an exact match existed", "bug_description": "See nabble\nhttp://old.nabble.com/Exception-handling-...-onException-to26215607.html\n\n                onException(UnmarshalException.class).handled(true).to(\"mock:ue\");\n                \n                onException(Exception.class).handled(true).to(\"mock:exception\");\n\n                from(\"direct:start\")\n                    .throwException(new UnmarshalException(\"Could not unmarshal\", new IllegalArgumentException(\"Damn\")));\n\n\nThe code above should be handled by the UnmarshalException.class as its the best candidate.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.processor.exceptionpolicy.DefaultExceptionPolicyStrategy.java", "org.apache.camel.processor.exceptionpolicy.ExceptionPolicyStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 2168, "bug_title": "Logger logs INFO level messages at DEBUG level", "bug_description": "The Camel logger logs INFO level messages at the DEBUG level. CAMEL-1048 fixed the problem in one of two cases (logging without an exception), but it is still broken for logging with an exception. Patch to fix this is attached.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.1", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.processor.Logger.java"], "label": 1, "es_results": []}, {"bug_id": 2245, "bug_title": "Routing slip does not stop when Exchange is failed", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.2", "fixed_version": "camel-1.6.3", "fixed_files": ["org.apache.camel.processor.RoutingSlip.java", "org.apache.camel.processor.routingslip.RoutingSlipWithExceptionTest.java"], "label": 1, "es_results": []}, {"bug_id": 2445, "bug_title": "BatchProcesser.processExchange needs to catch Throwable", "bug_description": "If an aggregator sends an exchange to a processor that throws an Error this causes the thread started by BatchProcessor to exit, exchanges will then accumulate in the aggregator until an OutOfMemoryError occurs.\nThis patch sorts that out and adds a unit test, however there&apos;s another problem that I&apos;m still looking into, namely that BatchProcessor just uses an instance of LoggingErrorHandler as it&apos;s exception handler, so any exceptions/errors caught by this processor will just get logged and not follow the normal Camel error handling.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-1.6.2", "fixed_version": "camel-1.6.3", "fixed_files": ["org.apache.camel.processor.BatchProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1488, "bug_title": "BeanInfo.overridesExistingMethod() does not handle overloaded methods correctly.", "bug_description": "Camel can fail to determine the appropriate method to call on a bean that has overloaded (vs. overridden) methods. It will always call the first overloaded method, even if the parameter is not the same type as that of the message being processed.\nThe bug is in BeanInfo.overridesExistingMethod. Here&apos;s the offending code:\n            for (int i = 0; i < info.getMethod().getParameterTypes().length; i++) {\n                Class type1 = info.getMethod().getParameterTypes()[i];\n                Class type2 = methodInfo.getMethod().getParameterTypes()[i];\n                if (!type1.equals(type2)) \n{\n                    continue;\n                }\n            }\n            // same name, same parameters, then its overrides an existing class\n            return info;\nIf the parameter types don&apos;t match, the continue statement is not going to do what you&apos;d want. The author obviously intended the \"continue\" to continue with the next methodInfo. Instead, it checks the next parameter and will always return the current methodInfo  if it reaches this point.\nHere&apos;s a unit test that exemplifies the issue:\n----------------------------------\npackage biz.firethorn.hostinterface.camel;\nimport java.lang.reflect.Method;\nimport junit.framework.Assert;\nimport junit.framework.TestCase;\nimport org.apache.camel.CamelContext;\nimport org.apache.camel.Exchange;\nimport org.apache.camel.Message;\nimport org.apache.camel.RuntimeCamelException;\nimport org.apache.camel.component.bean.AmbiguousMethodCallException;\nimport org.apache.camel.component.bean.BeanInfo;\nimport org.apache.camel.component.bean.MethodInvocation;\nimport org.apache.camel.impl.DefaultCamelContext;\nimport org.apache.camel.impl.DefaultExchange;\nimport org.apache.camel.impl.DefaultMessage;\npublic class BeanInfoTest extends TestCase {\n\tpublic void test() throws Exception \n{\n\t\t\n\t\tCamelContext camelContext = new DefaultCamelContext();\n\t\tBeanInfo beanInfo = new BeanInfo(camelContext, Bean.class);\n\t\t\n\t\tMessage message = new DefaultMessage();\n\t\tmessage.setBody(new RequestB());\n\t\tExchange exchange = new DefaultExchange(camelContext);\n\t\texchange.setIn(message);\n\t\t\n\t\tMethodInvocation methodInvocation = beanInfo.createInvocation(new Bean(), exchange);\n\t\tMethod method = methodInvocation.getMethod();\n\t\t\n\t\tAssert.assertEquals(\"doSomething\", method.getName());\n\t\t\n\t\tAssert.assertEquals(RequestB.class, method.getGenericParameterTypes()[0]);\n\t}\n}\nclass Bean {\n\tpublic void doSomething(RequestA request) {\t\n\t}\n\tpublic void doSomething(RequestB request) {\n\t}\n}\nclass RequestA {\n\tpublic int i;\n}\nclass RequestB {\n\tpublic String s;\n}", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M1", "fixed_version": "camel-1.6.1", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 1492, "bug_title": "Allow parementer append for jms bindings with no endpoint", "bug_description": "See test failures in https://issues.apache.org/activemq/browse/AMQ-2182", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M1", "fixed_version": "camel-2.0-M2", "fixed_files": ["org.apache.camel.component.jms.JmsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 1504, "bug_title": "HTTP_URI and HTTP_PATH message headers not concatenated when sending messages to http endpoint", "bug_description": "When a message is sent to an http endpoint, the path specified in the HTTP_PATH header is ignored.\nIn the HttpProducer.createMethod() of the camel-http component, the URI is taken from the HTTP_URI header or the endpoint, but the HTTP_PATH header is not concatenated. \nSee also the discussion on the mailing list: http://www.nabble.com/Setting-a-path-in-message-header-with-Camel-http-2.0M1-td22781504.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M1", "fixed_version": "camel-2.0-M2", "fixed_files": ["org.apache.camel.component.http.HttpProducer.java"], "label": 1, "es_results": []}, {"bug_id": 1535, "bug_title": "java.lang.Error thrown in mock/dataset assertions causes the async processing to not complete as just catches Exception", "bug_description": "java.lang.AssertionException extends java.lang.Error\nSo the mock endpoint should deal with this and catch Throwable instead of Exception when it handles failures.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M1", "fixed_version": "camel-1.6.1", "fixed_files": ["org.apache.camel.processor.TransformProcessor.java", "org.apache.camel.component.mock.MockEndpoint.java", "org.apache.camel.component.dataset.DataSetConsumer.java", "org.apache.camel.component.dataset.DataSetSedaTest.java", "org.apache.camel.builder.ErrorHandlerBuilderRef.java", "org.apache.camel.component.file.DirectoryCreateIssueTest.java", "org.apache.camel.processor.RoutePerformanceTest.java"], "label": 1, "es_results": []}, {"bug_id": 123, "bug_title": "add an 'on commit / on rollback' hook so that non-transactional components can do things like delete files when the processing has completed", "bug_description": "e.g. file / FTP should only delete the file after successful processing has occurred etc", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M1", "fixed_version": "camel-2.0-M2", "fixed_files": ["org.apache.camel.Exchange.java", "org.apache.camel.impl.DefaultExchange.java"], "label": 1, "es_results": []}, {"bug_id": 1632, "bug_title": "File component - from endpoint is not set on file consumer", "bug_description": "So when you later call exchange.getFromEndpoint() it returns null. What it should return is the file/ftp endpoint that it was consumed from.\nSee nabble:\nhttp://www.nabble.com/Get-The-intercepted-endpoint-when-using-inetrcept%28%29-td23543993s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M1", "fixed_version": "camel-2.0-M2", "fixed_files": ["org.apache.camel.component.file.GenericFileExchange.java", "org.apache.camel.component.file.remote.RemoteFileEndpoint.java", "org.apache.camel.component.file.remote.RemoteFileExchange.java", "org.apache.camel.component.file.FileEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1642, "bug_title": "Splitter with streaming - GenericFile is not detected when creating the java.util.Scanner", "bug_description": "getScanner should recognize GenericFile as type so we can leverage the underlying java.io.File handle.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M1", "fixed_version": "camel-2.0-M2", "fixed_files": ["org.apache.camel.util.ObjectHelper.java", "org.apache.camel.builder.ExpressionBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 1719, "bug_title": "bean component should thrown NoTypeConverterException when bean parameter binding cannot convert to method signature type", "bug_description": "See nabble:\nhttp://www.nabble.com/Error-not-raised-by-Camel-if-the-%40Header-type-define-in-a-bean-is-not--correct-td24051086s22882.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.0-M3", "fixed_files": ["org.apache.camel.component.bean.MethodInfo.java"], "label": 1, "es_results": []}, {"bug_id": 1735, "bug_title": "camel-example-etl - bug in sample about type converter", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.0-M3", "fixed_files": ["org.apache.camel.component.jpa.JpaProducer.java", "org.apache.camel.component.jpa.JpaEndpoint.java", "org.apache.camel.component.jpa.JpaConsumer.java", "org.apache.camel.component.jpa.JpaTest.java", "org.apache.camel.example.etl.CustomerTransformer.java", "org.apache.camel.example.etl.EtlRoutes.java"], "label": 1, "es_results": []}, {"bug_id": 1753, "bug_title": "validator component should be configurable for either Sax or Dom as MSV component requires Dom", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.0-M3", "fixed_files": ["org.apache.camel.component.validator.msv.MsvComponent.java", "org.apache.camel.processor.validation.DefaultValidationErrorHandler.java", "org.apache.camel.processor.validation.ValidatorErrorHandler.java", "org.apache.camel.processor.validation.ValidatingProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1755, "bug_title": "Failover load balancer - should prepare exchange before failover", "bug_description": "When the failover load balancer does a fail over it only clears the exception.\nWhen you use dead letter channel or default error handler with redeliveries enabled they will have such information.\nThis should also be cleared before failover. So we have a clean and fresh start.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.0-M3", "fixed_files": ["org.apache.camel.processor.loadbalancer.FailOverLoadBalancer.java"], "label": 1, "es_results": []}, {"bug_id": 1774, "bug_title": "cxfbean does not propagate in http response code to Camel header", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M1", "fixed_version": "camel-2.0-M3", "fixed_files": ["org.apache.camel.component.cxf.cxfbean.CxfBeanTest.java", "org.apache.camel.component.cxf.cxfbean.CxfBeanBinding.java", "org.apache.camel.component.cxf.transport.CamelDestination.java", "org.apache.camel.component.cxf.jaxrs.testbean.CustomerService.java", "org.apache.camel.component.cxf.cxfbean.DefaultCxfBeanBinding.java", "org.apache.camel.component.cxf.cxfbean.CxfBeanDestination.java"], "label": 1, "es_results": []}, {"bug_id": 1813, "bug_title": "HttpProducer does not copy the in message header to out message", "bug_description": "Here is the mail thread which discuss about it.\nhttp://www.nabble.com/HTTP-component-and-IN-head-loss-to24393415.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.0-M3", "fixed_files": ["org.apache.camel.component.http.HttpGetTest.java", "org.apache.camel.component.http.HttpGetWithHeadersTest.java", "org.apache.camel.component.http.HttpProducer.java"], "label": 1, "es_results": []}, {"bug_id": 1824, "bug_title": "GenericFileConsumer fails to pick up all files in a directory if there are more files than maxMessagePerPoll setting", "bug_description": "Camel adds filenames it is processing to an in memory, in progress map.  Once it is finished processing it removes them.  If we specific a maxMessagesPerPoll number less then the files in a directory, it only processes up to the number of files we specify for each poll.  It then removes the rest from the in progress map and tries them again next poll.  This is the code (from GenericFileConsumer:processBatch()) for the last part:\n       for (int index = 0; index < exchanges.size() && isRunAllowed(); index++) \n{\n            GenericFileExchange<T> exchange = (GenericFileExchange<T>) exchanges.poll();\n            String key = exchange.getGenericFile().getFileName();\n            endpoint.getInProgressRepository().remove(key);\n        }\n\nUnfortunately, as you can see it uses exchanges.size() to determine how many file names to remove (i.e. how many times to loop), however exchanges.poll() removes one from the head of exchanges for each loop.  This means that the exchanges.size() reduces by one for each loop, which means it only cleans up half of the filenames that are in exchanges, which means these files are never picked up again as Camel thinks it is still processing them.\n\nThe fix is to replace the for loop with a while:\n\n        while ((exchanges.size() > 0) && isRunAllowed()) {            GenericFileExchange<T> exchange = (GenericFileExchange<T>) exchanges.poll();            String key = exchange.getGenericFile().getFileName();            endpoint.getInProgressRepository().remove(key);        }\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.0-M3", "fixed_files": ["org.apache.camel.component.file.FileOperations.java", "org.apache.camel.component.file.GenericFileOperations.java", "org.apache.camel.component.file.GenericFileConsumer.java", "org.apache.camel.processor.LoggingErrorHandler.java", "org.apache.camel.impl.DefaultUnitOfWork.java", "org.apache.camel.component.file.strategy.GenericFileDeleteProcessStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 1825, "bug_title": "GenericFileDeleteProcessStrategy in some cases seems to delete a file, but then throw an exception saying it cannot delete the file, even though it has.", "bug_description": "In some cases, GenericFileDeleteProcessStrategy seems to delete a file, but then it throws an exception saying it can&apos;t delete the file, even though it has.  Because this happens, it causes a rollback even though the file has been processed and deleted correctly.  This only seems to happen on our Linux box which is using a network share to process files from.  We have managed to work around it by checking if the file exists again straightaway after \"operations.deleteFile(...)\", and this always says the file does not exist as expected, so not sure why the call  \"operations.deleteFile(...)\" returns false.  The modified code below works around this issue, and logs a warning if the file still exists (though you may wish to throw and exception instead), but so far the log statement has not been called in our test runs even though it does enter the \" if (!deleted)\" block.\n    @Override\n    public void commit(GenericFileOperations<T> operations, GenericFileEndpoint<T> endpoint, GenericFileExchange<T> exchange, GenericFile<T> file) throws Exception {\n        // must invoke super\n        super.commit(operations, endpoint, exchange, file);\n        boolean deleted = operations.deleteFile(file.getAbsoluteFilePath());\n        if (!deleted) {\n            final File javaFile = new File(file.getAbsoluteFilePath());\n            if (javaFile.exists()) \n{\n                log.warn(\"Cannot delete file: \" + file);\n            }\n        }\n    }", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.0-M3", "fixed_files": ["org.apache.camel.component.file.FileOperations.java", "org.apache.camel.component.file.GenericFileOperations.java", "org.apache.camel.component.file.GenericFileConsumer.java", "org.apache.camel.processor.LoggingErrorHandler.java", "org.apache.camel.impl.DefaultUnitOfWork.java", "org.apache.camel.component.file.strategy.GenericFileDeleteProcessStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 1848, "bug_title": "MockEndPoint yields nullpointer due to mixed up typeconversion", "bug_description": "MockEndpoint yields a failed unit test with very confusing message:\n        resultEndpointHub.expectedHeaderReceived(\"portalId\", 30);\nYields: ... expected <null> but got ...\nOf course the expected value of null must never happen if a value is put in there", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.0.0", "fixed_files": ["org.apache.camel.component.mock.MockEndpointTest.java", "org.apache.camel.processor.BatchProcessor.java", "org.apache.camel.component.mock.MockEndpoint.java", "org.apache.camel.processor.aggregate.DefaultAggregationCollection.java"], "label": 1, "es_results": []}, {"bug_id": 1844, "bug_title": "Files size after simply route from file to file is smaller then the original size.", "bug_description": "I&apos;ve got very simply camel route:\nCamelContext context = new DefaultCamelContext();\ncontext.addRoutes(new RouteBuilder() {\n  public void configure() \n{\n    from(\"file://\" + INBOX_DIR + \"?noop=false\").to(\"file://\" + OUTBOX_DIR);\n  }\n});\ncontext.start();\nAll this route does, is to copy a file from INBOX_DIR to OUTBOX_DIR.\nAnd everything goes fine for smaller files, but when I put something bigger into INBOX_DIR (on my machine problem starts with files bigger than ~100MB) things go wrong.\nThe file in OUTBOX_DIR has different size than the original one. In general it is smaller than the original (its size varies - can be 3/4 of the original, can be 1/2 etc).\nWith Camel 1.6.1 with noop set to \"true\" the file in OUTBOX was bigger than the original one (even 1,5-2 times larger !). With Camel 2.0-M2 setting noop to true or false didn&apos;t matter - the file in OUTBOX was smaller than the original one.\nIt is possible to avoid this bug, by setting delay but this is no good. To handle large files, I&apos;d have to set a very high value which would slow down the whole application.\nI&apos;ve tried all possible configurations noop=true/false, readLock, readLockTimeout but without success.\nTested on both Apache Camel 1.6.1 and 2.0-M2 with similar results.\nMy environment:\nJava: 1.6.0.14\nOS: Linux, Kubuntu 9.04\nAttached you will find a Maven project which exposes the bug.\nUnpack it, run\nmvn assembly:assembly\nnext run\nrun.sh\nand copy big file to inbox dir and observe resulting file in outbox dir.\nTo kill the application hit CTRL+C ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.0.0", "fixed_files": ["org.apache.camel.component.file.strategy.FileProcessStrategyFactory.java"], "label": 1, "es_results": []}, {"bug_id": 1851, "bug_title": "Route JMX attributes not being updated", "bug_description": "Some routes are not getting instrumented using JMX and we get the following WARN for those on startup;\nInstrumentationLifecycleStrategy: Route has not been instrumented for endpoint: ...\nIt turns out that its only those routes that have an onException handler as the first processor in the route, e.g.\n<route id=\"per-message-route\" errorHandlerRef=\"RouteErrorHandler\">\n  <from uri=\"jms:MSG_IN\"/>\n  <onException>\n    <exception>can.retry.Exception</exception>\n    <redeliveryPolicy ref=\"UnlimitedRedeliveryPolicyConfig\"/>\n    <handled><constant>true</constant></handled>\n  </onException>\n  <bean ref=\"Formatter\" method=\"formatInput\"/>\n...8<... \nMore info and proposed fix at http://www.nabble.com/Routes-JMX-attributes-not-updated-in-2.0-M2-td24631265.html#a24639433", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.0.0", "fixed_files": ["org.apache.camel.management.InstrumentationLifecycleStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 1852, "bug_title": "The broker example does not work in camel 2.0 M3", "bug_description": "There some NPE when you run the example of loan broker which is caused by the change of CAMEL-1722.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M3", "fixed_version": "camel-2.0.0", "fixed_files": ["org.apache.camel.loanbroker.queue.version.BankResponseAggregationStrategy.java", "org.apache.camel.loanbroker.queue.version.LoanBroker.java", "org.apache.camel.loanbroker.webservice.version.BankResponseAggregationStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 1864, "bug_title": "interceptSendToEndpoint does not trigger endpoints defined in onException", "bug_description": "The reason is that onException is initialized before interceptors", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M3", "fixed_version": "camel-2.0.0", "fixed_files": ["org.apache.camel.CamelContext.java", "org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.processor.SendProcessor.java", "org.apache.camel.util.ExchangeHelper.java", "org.apache.camel.model.RoutesDefinition.java", "org.apache.camel.processor.intercept.InterceptSendToIssue.java", "org.apache.camel.spring.CamelContextFactoryBean.java", "org.apache.camel.model.InterceptSendToEndpointDefinition.java", "org.apache.camel.processor.intercept.FromFileInterceptSendToIssue.java"], "label": 1, "es_results": []}, {"bug_id": 1883, "bug_title": "EXCEPTION_CAUGHT header should be set to exception caught", "bug_description": "Currently whatever exception is caught in doCatch there is a EXCEPTION_CAUGHT header set to the exception that was set on the exchange itself. As doCatch() traverses causes of exception it might happen that something else is caught and something else is in EXCEPTION_CAUGHT header.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M3", "fixed_version": "camel-2.0.0", "fixed_files": ["org.apache.camel.processor.CatchProcessor.java", "org.apache.camel.processor.TryProcessorTest.java", "org.apache.camel.processor.TryProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1885, "bug_title": "Stream Caching is enabled by default when using CamelContextFactoryBean", "bug_description": "According to the documentation Stream Caching should be disabled by default in Camel 2.0. The default is overriden when using CamelContextFactoryBean because of this line:\n    private Boolean streamCache = Boolean.TRUE;\nUnit test showing the problem:\npublic class DefaultStreamCachingTest {\n     @Test\n     public void test() throws Exception {\n         ApplicationContext appContext = new ClassPathXmlApplicationContext(new String[] \n{\"context.xml\"}\n);\n         DefaultCamelContext camelContext = (DefaultCamelContext) appContext.getBean(\"camelContext\");\n         assertFalse(camelContext.isStreamCacheEnabled());\n     }\n}\ncontext.xml:\n<beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:camel=\"http://camel.apache.org/schema/spring\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://camel.apache.org/schema/spring http://camel.apache.org/schema/spring/camel-spring.xsd\">\n    <camel:camelContext id=\"camelContext\" />\n</beans>\nCurrently one must use:\n    <camel:camelContext id=\"camelContext\" streamCache=\"false\" />", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M3", "fixed_version": "camel-2.0.0", "fixed_files": ["org.apache.camel.spring.CamelContextFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 1927, "bug_title": "FileConsumer preMove setting causes filename to be left in in progress repository.", "bug_description": "If the FileConsumer preMove setting is used, then files are picked up the first time only, as the original file name is added to the in progress repository, whereas the new file name is removed from the in progress repository (though nothing is actually removed as the new file name is not in the in progress repository).\nThis causes a few problems:\ni) If the maxMessagePerPoll is used, then messages not processed the first time are never picked up again.\nii) If the intention is to reprocess the same file, or a new file with the same name, then it will never be picked up again after the first time.\niii)  As the wrong file names are removed from the in progress repository, it results in a slow memory leak in the in progress repository.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M3", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.file.GenericFileConsumer.java", "org.apache.camel.component.file.GenericFileOnCompletion.java"], "label": 1, "es_results": []}, {"bug_id": 1957, "bug_title": "The construction method of SourceCache is wrong  ", "bug_description": "The construction of SourceCache is wrong , it will cause the NPE when you enable the stream cache.\n\n  public SourceCache(String data) {\n        new StringSource(data);\n        // It should be changed to \n        // super(data);\n    }\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.converter.stream.SourceCache.java", "org.apache.camel.converter.stream.StreamCacheConverterTest.java"], "label": 1, "es_results": []}, {"bug_id": 1963, "bug_title": "Graceful shutdown problem", "bug_description": "While running camel-example-spring-jms in 2.0M3, I&apos;ve noticed that GracefulShutdownService can&apos;t stop Camel. It calls\n    Main.getInstance().stop();\nbut this call blocks and never returns. With enabled debug logging for org.springframework.jms, console outputs\n[aultMessageListenerContainer-1] efaultMessageListenerContainer DEBUG Waiting for shutdown of message listener invokers\n[aultMessageListenerContainer-1] efaultMessageListenerContainer DEBUG Still waiting for shutdown of 1 message listener invokers\nRelated forum thread:\nhttp://www.nabble.com/Garceful-shutdown-bug-in-2.0M3--td25113519.html\nDado", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M3", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.processor.RedeliveryPolicy.java", "org.apache.camel.impl.MainSupport.java", "org.apache.camel.spring.Main.java", "org.apache.camel.processor.Pipeline.java", "org.apache.camel.impl.ServiceSupport.java", "org.apache.camel.processor.RedeliveryErrorHandler.java"], "label": 1, "es_results": []}, {"bug_id": 1925, "bug_title": "HTTP component throwing IllegalArgumentException in M3 release while bridging between jetty and http", "bug_description": "The CamelHttpUri header is not set to the right value while routing between jetty and http components. Because of which the construction of the HttpMethod in the http component fails and throws up an IllegalArgumentException.\nDetailed discussion regarding this issue can be found in this following thread.\nhttp://www.nabble.com/HTTP-component-throwing-IllegalArgumentException-in-M3-release.-tt25057133.html#a25057133\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M3", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.servlet.CamelHttpTransportServlet.java", "org.apache.camel.component.http.HttpProducer.java", "org.apache.camel.component.servlet.HttpClientRouteTest.java", "org.apache.camel.component.jetty.CamelContinuationServlet.java", "org.apache.camel.component.http.HttpComponent.java", "org.apache.camel.component.jetty.JettyHttpComponent.java", "org.apache.camel.component.http.HttpEndpoint.java", "org.apache.camel.component.http.CamelServlet.java"], "label": 1, "es_results": []}, {"bug_id": 2060, "bug_title": "Camel Bean component should not remember the last called method name if the bean's method name is not explicitly specified.", "bug_description": "Here is the mail thread which discuses this issue.\nhttp://www.nabble.com/Bean-endpoint-in-a-route-is-holding-reference-to-the-last-used-methodName-and-does-not-invoke-Camel%27s-Bean-binding-tp25838095p25838095.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M3", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.bean.BeanProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1930, "bug_title": "Synchronized access to XPathExpression resulting in contention for multiple consumers", "bug_description": "Hi,\nI&apos;m using Camel to do some JMS message routing. Messages are XML so xpath is a natural choice.\nHowever when using a choice with an xpath expression, the XPathBuilder creates one XPathExpression object. According to the specification, these objects are not thread safe so synchronizing looks natural. But then, using multiple jms consumers is totally useless since no concurrent evaluations can be made.\nXPathExpression objects would rather need to be stored in a ThreadLocal to avoid synchronization and contention.\nCheers,\nFabrice", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M3", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.builder.xml.XPathBuilder.java", "org.apache.camel.builder.xml.MessageVariableResolver.java"], "label": 1, "es_results": []}, {"bug_id": 2092, "bug_title": "velocity endpoint should not remember the last used template uri", "bug_description": "If there are two velocity endpoint linked with the pipeline, you will find the last velocity will not effect anymore.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M1", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.velocity.VelocityEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2133, "bug_title": "The converter's MandatoryConvert method does not throw the exception when the value is null", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M1", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java", "org.apache.camel.builder.ExpressionBuilder.java", "org.apache.camel.converter.ConverterTest.java", "org.apache.camel.impl.converter.DefaultTypeConverter.java"], "label": 1, "es_results": []}, {"bug_id": 2251, "bug_title": "CXFBean should not call the BusFactory.getDefaultBus() ", "bug_description": "The defaultBus could be polluted in the container like ServceMix.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M2", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.cxf.cxfbean.CxfBeanEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2447, "bug_title": "cxfbean should  propagate CONTENT_TYPE for other camel component to use", "bug_description": "There is no \"content-type\" header return from the below route\n\n <route>\n\n      <from uri=\"jetty:http://localhost:9006/employeesBase?matchOnUriPrefix=true\"/>\n\n      <to uri=\"cxfbean:EmployeesPOJO\"/>\n\n  </route>\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0-M3", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.cxf.cxfbean.CxfBeanTest.java", "org.apache.camel.component.cxf.cxfbean.DefaultCxfBeanBinding.java"], "label": 1, "es_results": []}, {"bug_id": 1190, "bug_title": "SoapMessageInInterceptor imports test generated code(pizza.types.CallerIDHeaderType)", "bug_description": "org.apache.camel.component.cxf.interceptors.SoapMessageInInterceptor imports a class generated from test code \nimport org.apache.camel.pizza.types.CallerIDHeaderType;\nThis makes life a bit annoying when trying to debug/develop with camel-cxf without generating the full test code.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-1.6.0", "fixed_files": ["org.apache.camel.component.cxf.interceptors.SoapMessageInInterceptor.java"], "label": 1, "es_results": []}, {"bug_id": 1414, "bug_title": "JMSMessage vanishes attachments", "bug_description": "When using this kind of route\n<from uri=\"activemq:queue:test\"/>\n<camel:process ref=\"mailProcessor\" />\n<to uri=\"smtp://localhost:25?to=user@localhost\" />\nand trying to enrich the message in the mailProcessor with\nexchange.getIn().addAttachment(\"attachement.txt\",\n                new DataHandler(\"Hello world\", \"text/plain\"));\nThe received mail doesn&apos;t contains any attachment.\nIf the input \"from\" is a \"direct\" instead of activemq, it works fine.\nInspecting source code,  MessageSupport.copyFrom(Message that) does\ngetAttachments().putAll(that.getAttachments());\nbut the child class JmsMessage doesn&apos;t.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.component.spring.integration.SpringIntegrationMessage.java", "org.apache.camel.component.jms.JmsMessage.java"], "label": 1, "es_results": []}, {"bug_id": 1416, "bug_title": "Using # notation to reference CXF serviceClass is not working", "bug_description": "See issue reported in mailing list.\nhttp://www.nabble.com/camel-cxf-endpoint---error-%3A-Failed-to-convert-property-value-of-type--...-tp22312601p22312601.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.component.cxf.CxfComponent.java", "org.apache.camel.component.cxf.CxfSpringEndpoint.java", "org.apache.camel.component.cxf.spring.CxfEndpointBeanDefinitionParser.java"], "label": 1, "es_results": []}, {"bug_id": 1421, "bug_title": "Cxf Endpoint String bean properites are not merged", "bug_description": "CxfEndpointBeanDefinitionParser maintains a property map that can be overridden by user provided property map.  They should be merged.\nPlease see the email thread. \nhttp://www.nabble.com/camel-cxf---dataformat-tp22332652p22332652.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.0-M1", "fixed_files": ["org.apache.camel.component.cxf.ServiceClassRefTest.java", "org.apache.camel.component.cxf.spring.CxfEndpointBeanDefinitionParser.java"], "label": 1, "es_results": []}, {"bug_id": 1506, "bug_title": "Recursively scan multipart nodes of an email for attachments - not just top level nodes", "bug_description": "The current code will only scan the top level of a multipart message. This misses any attachments that are under another node.\nAll unit tests still run for me after applying this patch.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.0-M2", "fixed_files": ["org.apache.camel.component.mail.MailMessage.java", "org.apache.camel.component.mail.MailConfiguration.java", "org.apache.camel.component.mail.MailUtils.java", "org.apache.camel.component.mail.MailBinding.java"], "label": 1, "es_results": []}, {"bug_id": 1716, "bug_title": "Simple language support for AND / OR to concat multiple expressions fails if string constants are enclosed in single quotation marks", "bug_description": "The current implementation for CAMEL-1637 fails if the string constants in the two expressions are enclosed in single quotation marks as given in the following example \n\n${in.header.foo} == &apos;abc&apos; and ${in.header.bar} == &apos;123&apos;\n\n\nThis is due to an issue with the GROUP_PATTERN regular expression in SimpleLanguageSupport.java. The matching pattern in between the single quotes is greedy and thus does not match correctly.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.0-M3", "fixed_files": ["org.apache.camel.spring.spi.TransactionErrorHandlerBuilder.java", "org.apache.camel.language.simple.SimpleLanguageSupport.java", "org.apache.camel.language.SimpleOperatorTest.java"], "label": 1, "es_results": []}, {"bug_id": 1924, "bug_title": "Unit test failures on Windows in 2.0.0 release", "bug_description": "3 failures in camel-core that do not seem to occur on linux and mac.\nFailure Details\n\ttestCannotDeleteFile\n\tjunit.framework.AssertionFailedError: Should have thrown an exception\norg.apache.camel.component.file.strategy.GenericFileDeleteProcessStrategyTest:125\n\ttestCacheStreamToFileAndNotCloseStream\n\tjunit.framework.AssertionFailedError: we should have no temp file expected:<1> but was:<0>\norg.apache.camel.converter.stream.CachedOutputStreamTest:117\n\ttestRouteIsCorrectAtRuntime\n\tjunit.framework.AssertionFailedError: expected:<4> but was:<1>\norg.apache.camel.processor.ChoiceWithEndTest:39", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.converter.stream.CachedOutputStreamTest.java", "org.apache.camel.converter.stream.CachedOutputStream.java", "org.apache.camel.processor.ChoiceWithEndTest.java", "org.apache.camel.component.file.strategy.GenericFileDeleteProcessStrategyTest.java"], "label": 1, "es_results": []}, {"bug_id": 1934, "bug_title": "Unit test failure in Windows for StreamCache. Temp file is not deleted", "bug_description": "The temporary file is not deleted. I think the file is still open in some other stream. The problem is I have no idea how to find this place. Like in issue  \t CAMEL-1924 this only happens on windows as on unix you can delete a file even if there are open streams.\ntestStreamCacheToFileShouldBeDeletedInCaseOfException\n\tjava.lang.AssertionError: There should be no files expected:<1> but was:<0>\norg.apache.camel.component.jetty.HttpStreamCacheFileTest:71", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.jetty.HttpStreamCacheFileTest.java", "org.apache.camel.converter.stream.CachedOutputStreamTest.java", "org.apache.camel.converter.stream.CachedOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 1948, "bug_title": "ServiceSupport - incorrect state after restart", "bug_description": "This hampers restart of services by example the overhauled JMX in CAMEL-1933.\nWhen a service was restarted it had the following incorrect state:\n\nstarted = true\nstarting = false\nstopped = true\nstopping = false\n\nThe stopped should have been changed to false as its started.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.impl.ServiceSupport.java"], "label": 1, "es_results": []}, {"bug_id": 1982, "bug_title": "JMSBinding needs to be able to deal with an null endpoint", "bug_description": "migrating activemq from 2.0-M2 to 2.0.0 some tests fail with internal NPE - problem is activemq camel uses JMSBinding() no arg creation and some of the methods do not check for a null endpoint.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.jms.JmsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 1997, "bug_title": "PackageScanDefinition missing public setter methods making it difficult to configure CamelContext via CamelContextFactoryBean using regular Spring definition", "bug_description": "In order to use spring features like \"depends-on\" and PropertyPlaceholderConfigure, it is desirable to configure the CamelContext using the standard Spring boolean definition (bean and property tags, etc), instead of the shortcut xbean style definition (<camelContext> tag).\nHowever, the org.apache.camel.spring.CamelContextFactoryBean, which is typically used to configure the CamelContext bean, uses the org.apache.camel.model.PackageScanDefinition class to hold the package scanning information. This class does not have any public setter for its private properties and can only be configured via xbean. This makes it very hard to configure the CamelContext bean using the standard Spring bean definition. Please add the public setters to this class so it can be used by Spring directly.\nAt present, a workaround is to implement a subclass of the org.apache.camel.model.PackageScanDefinition to add the missing setters. A sample Spring definition would look like this:\n    <bean id=\"camel\" class=\"org.apache.camel.spring.CamelContextFactoryBean\" depends-on=\"my-other-bean\">\n        <property name=\"trace\" value=\"false\"/>\n        <property name=\"packageScan\">\n            <bean class=\"com.mypackage.CamelPackageScanDefinition\">\n                <property name=\"packageList\"><value>$\n{route.packageScan}\n</value></property>\n            </bean>\n        </property>\n        </bean>\n    <bean id=\"template\" class=\"org.apache.camel.spring.CamelProducerTemplateFactoryBean\">\n        <property name=\"camelContext\" ref=\"camel\"/>\n    </bean>\nIn the above sample, the CamelPackageScanDefinition is the custom subclass.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.model.PackageScanDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 1977, "bug_title": "Camel HTTP binding incorrectly filters out the headers", "bug_description": "Camel does not filter out internal headers when creating HTTP response. The internal headers like \"CamelBeanMultiParameterArray\" occur in HTTP response.\nThe reason is that in the following piece of code (from DefaultHttpBinding.java, around line 157):\n\n        for (String key : message.getHeaders().keySet()) {\n            String value = message.getHeader(key, String.class);\n            if (headerFilterStrategy != null\n                    && !headerFilterStrategy.applyFilterToCamelHeaders(key, value, exchange)) {\n                response.setHeader(key, value);\n            }\n        }\n\n\nthe \"key\" returned is lowercase, even in case the real header is \"CamelBeanMultiParameterArray\" (because CaseInsensitiveMap is used, which turns all keys to lowercase). Since pattern match in DefaultHeaderFilterStrategy is case-sensitive, the header is not filter out when filling the response headers.\nThe suggested solution is to make pattern case-insensitive.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.http.HttpHeaderFilterStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 2018, "bug_title": "Timer Endpoint starts the thread before configuration", "bug_description": "The Timer endpoint does not appear to allow configuration using Spring XML.  When debugging, the Timer is created (with default configuration) and then the configuration magic happens\nTimerComponent.createEndpoint calls\nnew TimerEndpoint() which calls\ncomponent.getTimer(this)\nWhich creates the timer before the properties are set", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.timer.TimerEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 1994, "bug_title": "WS-Adressing: No headers in the respose, [MAPs retrieved from message null]", "bug_description": "I am using a cxf endpoint configured in POJO mode and running in wsdl-first-approach. When a web-service client(Soap-UI) send a request with headers, it expects headers back with appropriate response Action defined in wsdl, however, with this configuration no headers are not being sent back.\nError seen on debug logs:\nContextUtils                   DEBUG retrieving MAPs from context property javax.xml.ws.addressing.context.inbound\nContextUtils                   WARN  WS-Addressing - failed to retrieve Message Addressing Properties from context\nMAPAggregator                  DEBUG MAPs retrieved from message null\nContextUtils                   DEBUG retrieving MAPs from context property javax.xml.ws.addressing.context.outbound\nContextUtils                   WARN  WS-Addressing - failed to retrieve Message Addressing Properties from context\nContextUtils                   DEBUG retrieving MAPs from context property javax.xml.ws.addressing.context.outbound\nContextUtils                   WARN  WS-Addressing - failed to retrieve Message Addressing Properties from context\nPlease find attached detailed logs, a sample project, and a sample xml request.\nSome more details:\nEndpoint configuration: CxfEndpoint, WSAdressing Feature enabled, Wsdl First approach\nCamel Version: 2.0.0\nCXF Version: 2.2.2", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.cxf.CxfConsumer.java", "org.apache.camel.component.cxf.DefaultCxfBinding.java", "org.apache.camel.component.cxf.CxfBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2076, "bug_title": "Setting global error handler in camel-spring may not work correctly", "bug_description": "Really weird since recently upgrading junit in camel-spring some unit tests started failing based on global error handling configuration with camel-spring.\nI have tracked it down to a bug how error handlers was resolved when using spring XML only. It always worked with Java DSL.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.builder.ErrorHandlerBuilderRef.java"], "label": 1, "es_results": []}, {"bug_id": 2088, "bug_title": "camel-bean - Null String bodies is converted to a logging message instead of being null", "bug_description": "See nabble\nhttp://www.nabble.com/ToStringTypeConverter-and-null-message-bodies-td25978515.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.impl.converter.ToStringTypeConverter.java"], "label": 1, "es_results": []}, {"bug_id": 2089, "bug_title": "camel-jms - Fixed ReplyTo configured on endpoint should also work for consumers", "bug_description": "Endpoint option: ReplyTo is currently only applicable for producers. It should also work for consumers.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.jms.tx.NonTransactedInOutForJmsWithTxnMgrTest.java", "org.apache.camel.component.jms.tx.QueueToQueueRequestReplyTransactionTest.java", "org.apache.camel.component.jms.EndpointMessageListener.java", "org.apache.camel.component.jms.JmsConfiguration.java", "org.apache.camel.component.jms.JmsRouteRequestReplyTest.java"], "label": 1, "es_results": []}, {"bug_id": 2095, "bug_title": "Support Issue: Routing a TextMessage from a <camel:proxy> to a JMS queue", "bug_description": "I did not find any example or documentation for using the <camel:proxy> with a simple send(String textMessage) method and a route sending the textMessage String as body to a JMS queue (for example).\nThe thing is that <camel:proxy> creates a messages with body of type BeanInvocation. I would expect that Camel provides some builtin transformer/converter to change the body to the String argument, so that the message on the queue is of type TextMessage.\nDid I miss something? ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.impl.converter.ToStringTypeConverter.java", "org.apache.camel.component.bean.CamelInvocationHandler.java"], "label": 1, "es_results": []}, {"bug_id": 2058, "bug_title": "File Component - Rename operations fail sometimes on certain filesystems (Windows)", "bug_description": "On Windows (don&apos;t know if there are other platforms that suffer from this problem), the file component fails to successfully rename files with the File.renameTo operation.  It fails when the rename is performed immediately after closing a file.  On Windows this usually indicates that some other process has the file open.  This occurs due to things like Virus scanners which keep the file open for very short periods of time.  Given a slight pause the rename would succeed.\nThis is a serious problem which effectively makes useless options like \"tempPrefix\" in the File Producer and \"localWorkDirectory\" with the FTP Consumer.  Workarounds like \"disable your virus scanner\" don&apos;t cut it for everyone (me specifically) as I&apos;m system privilege restricted from doing so, and even then, there&apos;s no guarantee that other windows processes might not do similar things (file indexers, etc).\nThe Java spec doesn&apos;t define the behavior of the rename operation and specifically says that this can vary from implementation to implementation / filesystem to filesystem.  Second, rename doesn&apos;t say why it fails, it merely returns false which is very unhelpful.\nA couple ways to fix:\n1).  Provide an option to disable this optimization.  ie, a \"alwaysCopyInsteadOfRename\" or something (clean, simple, easy).\nThis would be a simple fix.  More or less just a few clauses/tests in GenericFileProducer around any \"is local\" checks....\n2).  Attempt a copy instead if the rename fails\n\nmaybe after a brief pause\nmaybe even after a number of rename attempts\nmaybe watch to see the file is closed prior to a rename attempt.\n\nRename operation failures may affect other things as well like certain locking schemes.\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4167147", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.file.GenericFileProducer.java", "org.apache.camel.component.file.remote.FtpOperations.java", "org.apache.camel.component.file.FileEndpoint.java", "org.apache.camel.component.file.remote.SftpOperations.java", "org.apache.camel.component.file.FileOperations.java", "org.apache.camel.component.file.strategy.GenericFileProcessStrategySupport.java", "org.apache.camel.component.file.GenericFileOperations.java", "org.apache.camel.component.file.strategy.MarkerFileExclusiveReadLockStrategy.java", "org.apache.camel.component.jpa.JpaWithNamedQueryTest.java", "org.apache.camel.util.FileUtil.java", "org.apache.camel.component.file.strategy.FileBigFileCopyManually.java", "org.apache.camel.component.jpa.QueryBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 2102, "bug_title": "Transacted routes do not allow custom error handlers to mark exception as handled AND set rollback only", "bug_description": "If a route is transacted using transacted(), and a custom error handler is registered using onException() which handles the exception using handled(), creates a custom error response, and marks the transaction to be rolled back using rollback(), Camel wraps a non-existent &apos;null&apos; exception with a TransactedRuntimeCamelException in TransactionErrorHandler.wrapTransactedRuntimeException() and throws the TRCE, causing the custom error response to be ignored. \nRoute looks like:\n\t\t// configure error handler for all routes\n\t\tonException(Exception.class).handled(true).to(\"bean:transformerBean?method=exceptionToResponse\").rollback();\n\t\tfrom(\"cxfrs://bean:categoriesEndpoint\").transacted()\n\t\t\t.choice()...\nBasically the application wants to handle exceptions by sending a custom error message back to the route client, and also mark transactions for rollback. \nAttached patch checks for the presence of an exception, and only raises a TRCE if its not null. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.model.RollbackDefinition.java", "org.apache.camel.processor.RollbackProcessor.java", "org.apache.camel.spring.spi.SpringTransactionPolicy.java", "org.apache.camel.model.TransformDefinition.java", "org.apache.camel.model.ProcessorDefinition.java", "org.apache.camel.spring.spi.TransactionErrorHandler.java", "org.apache.camel.spring.interceptor.SpringTransactionalClientDataSourceUsingTransactedTest.java", "org.apache.camel.CamelExchangeException.java"], "label": 1, "es_results": []}, {"bug_id": 2128, "bug_title": "Rollback does not work for transactional routes using camel-cxf", "bug_description": "I have a request reply service that should be able to do three different\nthings:\n1)  no exception occurs in the implementation: The jms Message should be\ncommitted and the normal reply should be sent\n2) The implementation throws an exception defined in the service contract:\nThe jms message should be committed and a fault should be sent\n3) The implemementation throws another kind of exception: The message delivery should\nbe rolled back so it can be received again\nCase 1 and 2 currently work as expected.\nFor Case 3 a fault is returned to the caller and the transaction is committed which is wrong. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-1.6.2", "fixed_files": ["org.apache.camel.component.cxf.transport.CamelDestination.java"], "label": 1, "es_results": []}, {"bug_id": 2132, "bug_title": "camel-jms - Sending Object message should fail if payload cannot be converted to serializable", "bug_description": "If not a null message is created and send.\nSee nabble:\nhttp://old.nabble.com/JPA%3A-Could-not-pass-Object-from-jpa-to-jms-to26160124.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.jms.JmsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2139, "bug_title": "Some methods of SftpOperations create streams that are never closed", "bug_description": "Where SftpOperations creates streams for storing or retrieving files via ChannelSftp, these streams are not always closed. For example, the retrieveFileToStreamInBody method creates an OutputStream object, but never closes it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.file.remote.FtpOperations.java", "org.apache.camel.component.file.remote.SftpOperations.java"], "label": 1, "es_results": []}, {"bug_id": 2144, "bug_title": "A null-Message to a cxf-endpoint (e.g. HTTP-GET) can disable accepting regular requests", "bug_description": "Start a fine configured system with came-cxf consumer endpoint using http. Make some tests with valid SOAP-Messages. Then just send a HTTP-Get-Request to the endpoint (e.g. with Browser). You&apos;ll get some kind of Exception saying\njava.lang.ClassCastException: org.apache.cxf.message.MessageContentsList cannot be cast to org.w3c.dom.Node\nOk, this could be interpreted as \"There is no content to convert into Node...\" as, if you debug, you&apos;ll see, that an empty MessageContentsList is provided by camel-cxf as the body of the camel message. Of course, if the Request would contain a body, the camel body would be a non empty MessageContentsList and the type converter mechanism would find a way to convert the list into a w3c Document, but that&apos;s not the point. The Bug is, that this null-body-request puts the DefaultTypeConverter into a state, that let always return null for bodies of type MessageContentsList, even if it&apos;s not empty (which is normal). I think that there is certain &apos;misses&apos;-map responsible for that kind of &apos;miss&apos;-state...", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.cxf.converter.CxfConverter.java"], "label": 1, "es_results": []}, {"bug_id": 2175, "bug_title": "Initialization code of camel-cxf is not thread safe", "bug_description": "getCxfBinding is not thread safe in case multiple threads hit a CXF webserive at once and it has not been initialized before.\nCode like this\n\n    public CxfBinding getCxfBinding() {\n        if (cxfBinding == null) {\n            cxfBinding = new DefaultCxfBinding();   \n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Create default CXF Binding \" + cxfBinding);\n            }\n        }\n        \n        if (!cxfBindingInitialized.getAndSet(true) \n                && cxfBinding instanceof HeaderFilterStrategyAware) {\n            ((HeaderFilterStrategyAware)cxfBinding)\n                .setHeaderFilterStrategy(getHeaderFilterStrategy());\n        }\n        return cxfBinding;\n    }\n\n\nIs a false sense as the getAndSet will let other threads pass it with a cxfBinding that still may not have been initialized.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.restlet.RestletEndpoint.java", "org.apache.camel.component.cxf.CxfEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2183, "bug_title": "camel-cxf - Empty body causes http error 500 even for GET when using cxfbean", "bug_description": "CxfSoapBinding has a bug that it mandates a body to exist. But when you send a GET then there is of course no body.\nThe code below fixed this in the method getCxfInMessage\n\n        // body can be empty in case of GET etc.\n        InputStream body = message.getBody(InputStream.class);\n        if (body != null) {\n            answer.setContent(InputStream.class, body);\n        } else if (message.getBody() != null) {\n            // fallback and set the body as what it is\n            answer.setContent(Object.class, body);\n        }\n\n\nWillem Tam can you review if that is okay? Maybe the code can be more finer to check the HTTP method and in cases of GET etc. it allows no body.\nI will commit my fix to get the unit tests passing.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.cxf.CxfSoapBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2196, "bug_title": "camel-cxf - CxfProducer in POJO mode should be able to use POJO object directly without wrapped as List", "bug_description": "See camel-example-cxf-async\nIn the client code that sends to the server using CxfProducer. I have to wrap my POJO in a List object.\nCxfProducer should be smarter and be able to do this itself.\nIt can do this by converting the payload to an iterator and use that to construct the list.\nThen it works for POJO objects directly.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.cxf.CxfProducer.java", "org.apache.camel.example.client.CamelClient.java"], "label": 1, "es_results": []}, {"bug_id": 2201, "bug_title": "RedeliveryPolicy - Setting delay > maxDelay should should also set max value", "bug_description": "See nabble\nhttp://old.nabble.com/maximumRedeliveryDelay-applies-even-if-you-are-not-using-exponential-back-off-to26415728.html\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.processor.RedeliveryPolicy.java"], "label": 1, "es_results": []}, {"bug_id": 2215, "bug_title": "CaseInsensitiveMap should be serializable", "bug_description": "CaseInsensitiveMap should be fully serializable\nSee nabble\nhttp://old.nabble.com/DataSet-header-%7BCamelDataSetIndex%3D0%7D-causes-failure-to26487985.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.util.CaseInsensitiveMap.java", "org.apache.camel.util.CaseInsensitiveMapTest.java"], "label": 1, "es_results": []}, {"bug_id": 2222, "bug_title": "camel-bean - Classes enhanced by CGLIB should skip cglib methods", "bug_description": "See nabble\nhttp://old.nabble.com/AmbiguousMethodCallException-on-transactional-spring-bean-to26496269.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 2221, "bug_title": "Inconsistent out message when using recipient list", "bug_description": "When using the Dynamic Recipient List pattern, there&apos;s an inconsistency between two ways of of defining that pattern:\n\nwhen using a @RecipientList-annotated method with a beanRef, you get the list of endpoints as the out message body\nwhen using the same method with a recipientList(bean(...)) call, you get the aggregated result of the invoked endpoints\n\nI think it would be better if both options returned the same response message and I would suggested the aggregated result of the invoked endpoints is the more appropriate one.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-1.6.3", "fixed_files": ["org.apache.camel.component.bean.MethodInfo.java"], "label": 1, "es_results": []}, {"bug_id": 2227, "bug_title": "Mock component - expectedBodiesReceived(List) does not work as expected", "bug_description": "Java chooses the varargs methods always instead of the List method.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.1.0", "fixed_files": ["org.apache.camel.component.mock.MockEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2269, "bug_title": "doCatch() - Without any exception given should report that as a problem", "bug_description": "Code like this\n\n                    .doTry()\n                        .to(\"mock:b\")\n                        .throwException(new IllegalArgumentException(\"Damn\"))\n                    .doCatch()\n                        .to(\"mock:catch\")\n                    .end();\n\n\nShould report a failure in the route as doCatch must have at least 1 exception as parameter.\nIt should correctly be\n                    .doCatch(Exception.class)\nWe could also consider using Excpetion.class as default but in Java you must also provide the exception to catch and thus its more intuitive if its 100% like Java", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.model.CatchDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 2303, "bug_title": "GroupedExchange in Aggregator is not working correctly", "bug_description": "See nabble\nhttp://old.nabble.com/Help%21-org.apache.camel.impl.GroupedExchange-does-not-exist-in-the-latest-download-ts26841584.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.processor.aggregator.AggregateGroupedExchangeTest.java", "org.apache.camel.model.AggregateDefinition.java", "org.apache.camel.processor.BatchProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 2059, "bug_title": "Creating a transacted pipeline with custom error handling is difficult", "bug_description": "When trying to add error logging to a transacted pipeline, I discovered the following issues:\n(1) onException/onCompletion do not work in a transacted route\n(2) if the transacted tag is not the first element in the route, all the endpoints above it will be silently ignored\n(3) custom error handlers interact very awkwardly with transacted routes (specifically, a non-transacted error handler will not work at all on a transacted route)\nI was able to solve my problem using doTry/doCatch; this may be a good candidate for adding to the docs.\nI suggest that, if the above issues are not corrected (especially 2+3, which may not be route designs that it is desired to support), they should trigger errors in the corresponding pipeline.\nI have attached a tgz of examples showing problems (1) and (2) above, as well as my successful route.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.model.OnExceptionDefinition.java", "org.apache.camel.model.TransactedDefinition.java", "org.apache.camel.model.OnCompletionDefinition.java", "org.apache.camel.spring.CamelContextFactoryBean.java", "org.apache.camel.model.InterceptSendToEndpointDefinition.java", "org.apache.camel.model.ProcessorDefinition.java", "org.apache.camel.model.InterceptFromDefinition.java", "org.apache.camel.model.InterceptDefinition.java", "org.apache.camel.model.PolicyDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 2336, "bug_title": "Spring DSL - Some route scoped concerns such as onException etc. can vanish in some edge cases", "bug_description": "The JAXB creates the Spring DSL model in a bit wacky way so we need to work on it a bit before Camel can create the runtime routes.\nIn some cases some of the cross functions such as onCompletion, onException could potentially vanish. \nThe problem is really that we should have divided the route into a upper / lower section where upper is the cross functions and the lower the actual route.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.processor.OnCompletionAndInterceptAndOnExceptionGlobalTest.java", "org.apache.camel.spring.CamelContextFactoryBean.java", "org.apache.camel.model.OnExceptionDefinition.java", "org.apache.camel.model.TransactedDefinition.java", "org.apache.camel.model.OnCompletionDefinition.java", "org.apache.camel.model.InterceptSendToEndpointDefinition.java", "org.apache.camel.model.ProcessorDefinition.java", "org.apache.camel.model.InterceptFromDefinition.java", "org.apache.camel.model.InterceptDefinition.java", "org.apache.camel.model.PolicyDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 2249, "bug_title": "Wrong handling of useMessageIDAsCorrelationID", "bug_description": "Camel jms seems to contain two bugs in correlation id handling.\nThe first shows when you have a sender that has UseMessageIDAsCorrelationID=\"false\" and a server that has UseMessageIDAsCorrelationID=\"true\". If you send a message with correlationId=\"a\" then the response message will contain correlationId=\"<request message id>\". Even if this could be a valid behaviour as you wanted UseMessageIDAsCorrelationID=\"true\" I do not think it makes sense as the sender will not be able to correlate the message. So for this case I propose to only set the correlation id to the request message id on the server if the correlation id of the request was not set.\nThe second bug seems to hide the first bug. Perhaps someone found a quick (and wrong way to make the tests work). It shows when you set UseMessageIDAsCorrelationID=\"true\" on both client and server. If you send a message with correlation id = \"a\" the client sends it out with this correlation id. The server then sets the correlation id to the request message id (first bug). Then on the client the reply is received. After that the correlation id is set back to \"a\" on the client. So the tests think all is well. This part of the code should be removed.\nI have marked both problems in the code with FIXME markers in my patch. I can also provide a patch with the solution but first I wanted to only show the problem and provide a failing test. \nHope my explanations were not to confused ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.jms.JmsSimpleInOnlyNoMutateTest.java", "org.apache.camel.component.jms.EndpointMessageListener.java", "org.apache.camel.component.jms.JmsProducer.java"], "label": 1, "es_results": []}, {"bug_id": 2387, "bug_title": "GenericFileConverter should honor charset from Exchange", "bug_description": "GenericFileConverter should pass in Exchange as parameter to converter so it can leverage any CHARSET set on the Exchange such as from a .convertBodyTo(String.class, \"UTF-8\");", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.file.GenericFileConverter.java"], "label": 1, "es_results": []}, {"bug_id": 2436, "bug_title": "BeanProcessor - Should differentiate between void and method returning null values", "bug_description": "When invoking a bean method it should better detect whether the bean is a void or has a return type.\nAnd if it returns null it should be regarded as a valid response and set as body.\nSee nabble\nhttp://old.nabble.com/filtering-messages-ts27403559.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.Exchange.java", "org.apache.camel.component.bean.BeanProcessor.java", "org.apache.camel.component.bean.MethodInfo.java", "org.apache.camel.component.bean.BeanWithPropertiesAndHeadersAndBodyInjectionTest.java"], "label": 1, "es_results": []}, {"bug_id": 2621, "bug_title": "File consumer - Polling from network share on Windows may regard files as not a file", "bug_description": "\nfile.isFile()\n\n\nMay return false on Windows if consuming from a network share etc. So we should just regard anything that is not a directory as a file.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.file.FileConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 2662, "bug_title": "ftp - exception thrown from begin should remove the file from in progress so the file can be polled on subsequent calls", "bug_description": "The SFTP component can throw an exception in its begin logic, which causes Camel to not remove the file from its internal in progress cache.\nSee nabble\nhttp://old.nabble.com/SFTP-rename-problems-ts28254146.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.file.GenericFileConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 2678, "bug_title": "Removing entries from FileIdempotentRepository do not get persisted", "bug_description": "Removing an entry (file name) programatically from a file idempotent repository has only effect on the cache and does not get persisted on the file repository. \nThere are some situations in which one may need to rerun a file through a workflow and it makes neccessary to remove the file from the idempotent repository. A specific problem arises when upon a server restart, the file is not picked up by the workflow because the remove was not persisted/spooled on the disk.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.processor.idempotent.FileIdempotentRepository.java"], "label": 1, "es_results": []}, {"bug_id": 2745, "bug_title": "ExpressionDefinition toString  print the expression and expression value at the same time", "bug_description": "When I running the test which need to call the ExpressionDefinition toString method, I found the expression is\n\n    simple{bodyAs(java.lang.String.class)bodyAs[java.lang.String]}\n\n\nWe need to avoid adding the expression when the ExpressionDefinition has the expression value.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.model.language.ExpressionDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 3319, "bug_title": "cxfEndpoint of camel-cxf does not support  the shcemaLocation element", "bug_description": "Here is the mail thread[1] which discusses about it.\n[1]http://camel.465427.n5.nabble.com/camel-cxf-exception-when-parsing-cxf-schemaLocation-element-td3253254.html#a3253254", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.0.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.CxfRsSpringEndpointTest.java", "org.apache.camel.component.cxf.spring.CxfRsClientFactoryBeanDefinitionParser.java", "org.apache.camel.component.cxf.spring.CxfRsClientFactoryBeanTest.java", "org.apache.camel.component.cxf.spring.CxfRsServerFactoryBeanDefinitionParser.java", "org.apache.camel.component.cxf.spring.CxfRsServerFactoryBeanTest.java", "org.apache.camel.component.cxf.spring.NamespaceHandler.java", "org.apache.camel.component.cxf.CxfSpringRouterTest.java", "org.apache.camel.component.cxf.spring.CxfEndpointBeanTest.java", "org.apache.camel.component.cxf.spring.CxfEndpointBean.java", "org.apache.camel.component.cxf.spring.CxfEndpointBeanWithBusTest.java"], "label": 1, "es_results": []}, {"bug_id": 7304, "bug_title": "InterceptSendToEndpoint does not work where uri needs to be normalized", "bug_description": "interceptSendToEndpoint(\"sftp://hostname:22/testDirectory?privateKeyFile=/user/.ssh.id_rsa\") is not intercepted because uri passed to InterceptSendToEndpointDefinition is not normalized.\nAs a result InterceptSendToEndpointDefinition createProcessor() method fails to match EndpointHelper.matchEndpoint(routeContext.getCamelContext(), uri, getUri()) and InterceptSendToEndpoint is not created.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.10.4", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.model.InterceptSendToEndpointDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 7338, "bug_title": "CxfClientCallback should not populate camel exchange OUT message if no response comes back to cxf producer and camel exchange pattern is InOnly", "bug_description": "I have a following camel route:\n\n\n\nString destination = \"cxf:http://localhost:9090/test?dataFormat=MESSAGE\";\n\n\n\n    from(\"jms:queue:dslSource\")\n\n        .onException(java.net.ConnectException.class, Exception.class)\n\n            .maximumRedeliveries(3)\n\n            .maximumRedeliveryDelay(3000)\n\n            .retryAttemptedLogLevel(LoggingLevel.INFO)\n\n            .log(LoggingLevel.WARN, \"Failed to send message ${body}\") \n\n            .log(LoggingLevel.WARN, \"Sending message to the error queue: ${body}\")          // body is null here\n\n            .to(\"jms:queue:dslError\")\n\n            .end()\n\n        .log(LoggingLevel.INFO, \"Sending message ...\")\n\n        .setHeader(\"bridgeDestination\", constant(destination))\n\n        .to(destination)\n\n        .log(LoggingLevel.INFO, \"received back: ${body}\")\n\n        .routeId(\"example-dsl\");\n\n\n\nIf an exception is thrown, for instance, a ConnectionException due to unavailability of the backend web service, the onException() route will be executed. The first log endpoint:\n\n\n\n.log(LoggingLevel.WARN, \"Failed to send message ${body}\") \n\n\n\nprints out camel exchange IN message body without any problem. However, the second log endpoint:\n\n\n\n.log(LoggingLevel.WARN, \"Sending message to the error queue: ${body}\")\n\n\n\nonly prints out a \"null\".\nThe reason is that when an exception is thrown, the CxfClientCallback.handleException() is called:\n\n\n\npublic void handleException(Map<String, Object> ctx, Throwable ex) {\n\n    ....\n\n        if (!boi.getOperationInfo().isOneWay()) {\n\n            // copy the InMessage header to OutMessage header                 \n\n            camelExchange.getOut().getHeaders().putAll(camelExchange.getIn().getHeaders());\n\n            binding.populateExchangeFromCxfResponse(camelExchange, cxfExchange, ctx);\n\n            camelAsyncCallback.done(false);\n\n        }\n\n...\n\n\n\nand this line always populates camel exchange OUT message regardless whether there is a CXF response back or not:\n\n\n\ncamelExchange.getOut().getHeaders().putAll(camelExchange.getIn().getHeaders());\n\n\n\nTherefore, the second log endpoint within onException() block has it&apos;s camel exchange IN message been overwritten and replaced with OUT message from previous endpoint&apos;s although it is a message of NULL body.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.10.4", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.cxf.DefaultCxfBinding.java", "org.apache.camel.component.cxf.CxfProducer.java", "org.apache.camel.component.cxf.CxfClientCallback.java"], "label": 1, "es_results": []}, {"bug_id": 7471, "bug_title": "SOAP with attachments not mapped correctly from CXF to Camel for CXF_MESSAGE", "bug_description": "SOAP with attachments messages are not mapped correctly from CXF to Camel and back if using CXF_MESSAGE message format. \nWe are having following simple camel route:\n\n\n\n<route>\n\n    <from uri=\"cxf:bean:Endpoint1\" />\n\n    <to uri=\"cxf:bean:Endpoint2\" />\n\n</route>\n\n\n\nSo mapping between CXF Endpoint 1 and Camel creates attachments in Camel exchange and sets the Camel IN message as a soap message. However, it&apos;s missing a bit of removing the attachments from the message as they are already added to the Camel Exchange.\nMapping from Camel and CXF Endpoint 2 maps Camel IN message (Soap with attachments) to CXF and then adds the attachments from Camel exchange which results in a messed up soap message that appears to contain twice as many attachments and it has messed up MIME boundaries as well as some of them are from the input message and some of them are generated during the last mapping.\nAs a workaround we have added a Camel processor that clears the attachments from the IN Message in Camel .", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.10.4", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.cxf.DefaultCxfBinding.java"], "label": 1, "es_results": []}, {"bug_id": 6703, "bug_title": "camel-mail - Folder is not open", "bug_description": "I think I&apos;ve found a bug in camel-mail source (MailConsumer.java)\nIf you define a mail endpoint which has closeFolder option to true you may encounter a folder is not open exception.\nIn fact, the delete action occured during processBatch which is set as an exchange&apos;s onCompletion. OnCompletion is executed in a new thread and right after that, we have the finally of the try-catch clause which is executed in a concurrent thread.\nTherefore, we can then have a folder that is open at the beginning of the processCommit but becoming closed during the search for an email with the uid because the finally is executed.\nAm I wrong or is it a real bug?\nI&apos;ve attached MailConsumer.java that should fix this bug.\nCheers!", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.10.6", "fixed_version": "camel-2.11.2", "fixed_files": ["org.apache.camel.component.mail.MailConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7308, "bug_title": "Timer component : timer should use StartupListener to be initialized before first fire", "bug_description": "CAMEL-5542 creates a regression for our routes when migrating from ServiceMix 4.4 to ServiceMix 4.5.\nThe secondary route in which we use this timer populates a cache, which must be initialized before other primary routes in the bundle can use it, otherwise requests sent to these other routes will generate errors.\nIn our ServiceMix installation with all the other bundles, the bundle with the timer can take from 1 second to 30 seconds to start-up, depending on the machine speed and the number of other bundles which are installed/started at the same time. \nIt is therefore very difficult to estimate the time to define for the \"delay\" variable. If set too low, the first fire will be ignored and the other primary routes will generate errors until the timer second fire. If set too high, the other primary routes will generate errors until the timer first fire. \nNormally the timer period is set to refresh the cache every several minutes, which leaves a large window where requests end with an error in case the cache failed to initialize.\nHere is a patch with a StartupListener which effectively reduces the window of errors to a fraction of a second.\nAlso, it would help to debug these kind of problems if a \"debug\" log is added when the first timer fire is ignored.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.10.7", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.timer.TimerConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 8035, "bug_title": "CXFRS consumer should set Exchange's charset name, if content type provides one", "bug_description": "This is the JAXRS counterpart to https://issues.apache.org/jira/browse/CAMEL-6188", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.10.7", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.DefaultCxfRsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 8424, "bug_title": "Transaction being propagated ignoring REQUIRES_NEW when using direct component", "bug_description": "I found that when we are using exactly same propagation policy bean in different routes used together with \"direct\" component, then TransactionErrorHandler always propagates current transaction even if our policy is \"PROPAGATION_REQUIRES_NEW\".\nThe failing code is:\n\n\n\n    public TransactionErrorHandler(CamelContext camelContext, Processor output, CamelLogger logger, \n\n            Processor redeliveryProcessor, RedeliveryPolicy redeliveryPolicy, ExceptionPolicyStrategy exceptionPolicyStrategy,\n\n            TransactionTemplate transactionTemplate, Predicate retryWhile, ScheduledExecutorService executorService,\n\n            LoggingLevel rollbackLoggingLevel) {\n\n\n\n        super(camelContext, output, logger, redeliveryProcessor, redeliveryPolicy, null, null, false, retryWhile, executorService);\n\n        setExceptionPolicy(exceptionPolicyStrategy);\n\n        this.transactionTemplate = transactionTemplate;\n\n        this.rollbackLoggingLevel = rollbackLoggingLevel;\n\n        this.transactionKey = ObjectHelper.getIdentityHashCode(transactionTemplate);\n\n    }\n\n\n\n    @Override\n\n    public void process(Exchange exchange) throws Exception {\n\n        // we have to run this synchronously as Spring Transaction does *not* support\n\n        // using multiple threads to span a transaction\n\n        if (exchange.getUnitOfWork().isTransactedBy(transactionKey)) {\n\n            // already transacted by this transaction template\n\n            // so let us just let the error handler process it\n\n            processByErrorHandler(exchange);\n\n        } else {\n\n            // not yet wrapped in transaction so let us do that\n\n            // and then have it invoke the error handler from within that transaction\n\n            processInTransaction(exchange);\n\n        }\n\n    }\n\n\n\nSo then for each policy there is a hash code created, which then is used to verify whether current route is already transacted by this transaction policy.\nThis makes \"PROPAGATION_REQUIRES_NEW\" ignored when used with \"direct\" component.\nSo for example:\n\n\n\n                from(\"activemq:queue:start\").routeId(\"route1\")\n\n                        .transacted(\"PROPAGATION_REQUIRES_NEW\")\n\n                        .setExchangePattern(ExchangePattern.InOnly)\n\n                        .to(\"activemq:queue:result1\")\n\n                        .to(\"direct:route2\")\n\n                        .throwException(new RuntimeException(\"Expected!\"));\n\n\n\n                from(\"direct:route2\").routeId(\"route2\")\n\n                        .transacted(\"PROPAGATION_REQUIRES_NEW\")\n\n                        .setExchangePattern(ExchangePattern.InOnly)\n\n                        .to(\"activemq:queue:result2\");\n\n\n\nThe above route suppose to work in 2 different transactions, as our propagation is REQUIRES_NEW for both of them. But due to hash code verification and optimisation, route2 will participate in same transaction as route1 instead of new.\nThis is rather buggy.\nWill create pull request in minutes.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.10.7", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.spring.spi.TransactionErrorHandler.java"], "label": 1, "es_results": []}, {"bug_id": 7459, "bug_title": "parseQuery Drops Char When Last Parameter is RAW with value ending in ')'", "bug_description": "org.apache.camel.util.URISupport\nWhen processing RAW parameters as part of parseQuery a look ahead to the next char is needed in order to determine the end of the RAW value.  The logic to prevent a StringIndexOutOfBoundsException drops the last char when evaluating for next char when the current char (i) is the second to last char of the string.\nThis becomes an issue when the RAW value ends in &apos;)&apos; \nConsider:\nuri = \"foo=RAW(ba(r))\"\nuri.length() = 14\ni = 12\nuri.charAt(12) = &apos;)&apos;\nuri.charAt(13) = &apos;)&apos;\n(i < uri.legnth() - 2) = 12 < (14 - 2) = 12 < 12 = false\nthus next = \"\\u0000\"\nThe RAW value now ends satisfying the requirements and the char at index 13 is never read.  The resulting parameter is \"foo=RAW(ba(r)\".\nThe logic to prevent the index exception should be \"(i <= uri.legnth() -2)\" or \"(i < uri.legnth() - 1)\"", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.11.0", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.util.URISupport.java", "org.apache.camel.issues.EndpointWithRawUriParameterTest.java"], "label": 1, "es_results": []}, {"bug_id": 7143, "bug_title": "camel-groovy - Evaluation returns 1st result only", "bug_description": "Seems like we have another issue reported which we couldn&apos;t reproduce.\nBut maybe this time we can.\nIssue at SO\nhttp://stackoverflow.com/questions/21221085/strange-behaiour-with-camel-groovy-spring-dsl\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.11.2", "fixed_version": "camel-2.11.4", "fixed_files": ["org.apache.camel.builder.script.ScriptLanguage.java", "org.apache.camel.builder.script.ScriptBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 7239, "bug_title": "Address the SchemaFactory thread safe issue.", "bug_description": "SchemaFactory is not thread safe, we need to do addition work in ValidatorProcessor to avoid the threads issue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.11.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.validator.ValidatorRouteTest.java", "org.apache.camel.processor.validation.ValidatingProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 7287, "bug_title": "SJMS: Cannot connect to durable topic because \"noLocal\" is hardcoded to \"true\"", "bug_description": "See this thread: http://camel.465427.n5.nabble.com/SJMS-issue-with-noLocal-td5748632.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.11.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.sjms.jms.JmsObjectFactory.java"], "label": 1, "es_results": []}, {"bug_id": 7271, "bug_title": "AbstractListGroupedExchangeAggregationStrategy produces failed exchange if first received exchange fails", "bug_description": "If the first exchange received by a (concrete implementation of) AggregationStrategy  contains an exception, then the result of the aggregation will also contain that exception, and so will not continue routing without error. This makes the first received exchange have an effect that subsequent exchanges do not have.\nThe specific use case multicasts to GroupedExchangeAggregationStrategy. The MulticastProcessor.doDone function uses ExchangeHelper.copyResults to copy the aggregated result to the original exchange. The copyResults method copies the exception as well, thereby propagating the error.\n The attached unit test has 3 tests, testAFail, testBFail, and testAllGood. All three of these should pass, but testAFail does not.\nWhat is happening is that AbstractListAggregationStrategy is directly storing its values on and returning the first exchange:\n    public Exchange aggregate(Exchange oldExchange, Exchange newExchange) {\n        List<V> list;\n        if (oldExchange == null) \n{\n\n            list = getList(newExchange);\n\n        }\n else \n{\n\n            list = getList(oldExchange);\n\n        }\n\n        if (newExchange != null) {\n            V value = getValue(newExchange);\n            if (value != null) \n{\n\n                list.add(value);\n\n            }\n        }\n        return oldExchange != null ? oldExchange : newExchange;\n    }\nThe pre-CAMEL-5579 version of GroupedExchangeAggregationStrategy created a fresh exchange to store and return the aggregated exchanges:\n    public Exchange aggregate(Exchange oldExchange, Exchange newExchange) {\n        List<Exchange> list;\n        Exchange answer = oldExchange;\n        if (oldExchange == null) \n{\n\n            answer = new DefaultExchange(newExchange);\n\n            list = new ArrayList<Exchange>();\n\n            answer.setProperty(Exchange.GROUPED_EXCHANGE, list);\n\n        }\n else \n{\n\n            list = oldExchange.getProperty(Exchange.GROUPED_EXCHANGE, List.class);\n\n        }\n\n        if (newExchange != null) \n{\n\n            list.add(newExchange);\n\n        }\n        return answer;\n    }", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.11.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.processor.aggregate.GroupedExchangeAggregationStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 7277, "bug_title": "camel-ssh should close the session when execution is finished.", "bug_description": "Here is the complain from the stackoverfow ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.11.4", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.ssh.SshEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 7286, "bug_title": "amazonSNSEndpoint option is ignored", "bug_description": "I just got an user complain that the amazonSNSEndpoint option of aws-sns was ignored. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.11.4", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.aws.sdb.SdbEndpoint.java", "org.apache.camel.component.aws.ddb.DdbEndpoint.java", "org.apache.camel.component.aws.ses.SesEndpoint.java", "org.apache.camel.component.aws.sqs.SqsEndpoint.java", "org.apache.camel.component.aws.s3.S3Endpoint.java", "org.apache.camel.component.aws.cw.CwEndpoint.java", "org.apache.camel.component.aws.sns.SnsEndpoint.java", "org.apache.camel.component.aws.sns.AmazonSNSClientMock.java", "org.apache.camel.component.aws.sns.SnsComponentConfigurationTest.java"], "label": 1, "es_results": []}, {"bug_id": 7426, "bug_title": "camel-http component should skip reading the form body if it is bridgeEndpoint", "bug_description": "When the camel http related component gets the post request of from, it always parsers the request body to get the from parameter. This feature could cause some trouble when the camel route proxy the request to other endpoint.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.11.4", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.http4.CamelServlet.java", "org.apache.camel.component.http4.DefaultHttpBinding.java", "org.apache.camel.component.http4.HttpMessage.java", "org.apache.camel.component.http.HttpMessage.java", "org.apache.camel.component.http.DefaultHttpBinding.java", "org.apache.camel.component.jetty.HttpProxyRouteTest.java", "org.apache.camel.component.jetty.CamelContinuationServlet.java", "org.apache.camel.component.http.CamelServlet.java", "org.apache.camel.Exchange.java"], "label": 1, "es_results": []}, {"bug_id": 6769, "bug_title": "JndiRegistry - Implement the methods that return empty set", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Proper-way-to-initialize-Transaction-management-for-Camel-in-an-Application-Server-tp5739760.html\nWe should implement the methods that return empty set. So it would work in the situation from the link", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.0", "fixed_version": "camel-2.11.3", "fixed_files": ["org.apache.camel.impl.JndiRegistry.java", "org.apache.camel.spi.Registry.java"], "label": 1, "es_results": []}, {"bug_id": 7062, "bug_title": "Tracer, BacklogTracer and BacklogDebugger should stop/shutdown when Camel does that", "bug_description": "This ensures we cleanup resources nicely.\nAlso the backlog tracer should remove processor definitions when a route is removed to not keep reference to defintions which are no longer in use.\nSee nabble\nhttp://camel.465427.n5.nabble.com/BacklogTracer-memory-leak-tp5744561.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.0", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.processor.interceptor.DefaultChannel.java", "org.apache.camel.processor.interceptor.BacklogTracer.java"], "label": 1, "es_results": []}, {"bug_id": 7120, "bug_title": "NullPointerException on BindyFixedLengthFactory.unbind()", "bug_description": "The Camel Bindy component throws a NullPointerException on unbind when you&apos;ve specified a header, but no footer. It looks like there was a simple copy/paste error in the marshal method. When generating the models, it checks the headerRow for null and then adds the headerRow. It then checks the headerRow (not the footerRow) for null and then adds the footerRow. Later down the chain, the BindyFixedLengthFactory throws a null exception when it tries to call getName() on the model Class.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.0", "fixed_version": "camel-2.11.4", "fixed_files": ["org.apache.camel.dataformat.bindy.fixed.BindyFixedLengthDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 7182, "bug_title": "camel-guice - PostConstruct do not throw checked exception", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/GuiceCamelContext-checked-exception-issue-with-Glassfish4-tp5746869.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.0", "fixed_version": "camel-2.11.4", "fixed_files": ["org.apache.camel.guice.GuiceCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 7185, "bug_title": "APT - Should check inherited class/interface for UriParam when scanning", "bug_description": "For example generation of the file component does not check the super class which has all the options, and therefore we only detect a few options.\n\n\n\n<html>\n\n<header>\n\n<title></title>\n\n</header>\n\n<body>\n\n<h1>file endpoint</h1>\n\n<p>File endpoint.</p>\n\n<table class=&apos;table&apos;>\n\n  <tr>\n\n    <th>Name</th>\n\n    <th>Type</th>\n\n    <th>Description</th>\n\n  </tr>\n\n  <tr>\n\n    <td>copyAndDeleteOnRenameFail</td>\n\n    <td>boolean</td>\n\n    <td></td>\n\n  </tr>\n\n  <tr>\n\n    <td>forceWrites</td>\n\n    <td>boolean</td>\n\n    <td></td>\n\n  </tr>\n\n</table>\n\n<h2>file consumer</h2>\n\n<p>File consumer.</p>\n\n</body>\n\n</html>\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.0", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.tools.apt.EndpointAnnotationProcessor.java", "org.apache.camel.tools.apt.util.Strings.java"], "label": 1, "es_results": []}, {"bug_id": 7200, "bug_title": "getComponentDocumentation do not work in OSGi", "bug_description": "This API returns null in OSGi, eg Karaf.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.0", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 7210, "bug_title": "getComponentDocumentation does not work if component name has dash in name", "bug_description": "For example if component is direct-vm we cannot load the component docs. Also the ftp components has special location.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.0", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.component.file.FileComponent.java", "org.apache.camel.component.dataformat.DataFormatComponent.java", "org.apache.camel.component.file.remote.FtpComponent.java", "org.apache.camel.component.file.remote.SftpEndpoint.java", "org.apache.camel.component.file.remote.SftpComponent.java", "org.apache.camel.component.file.GenericFileComponent.java", "org.apache.camel.component.file.remote.FtpsEndpoint.java", "org.apache.camel.component.file.remote.RemoteFileEndpoint.java", "org.apache.camel.component.file.remote.FtpEndpoint.java", "org.apache.camel.component.file.remote.FtpsComponent.java", "org.apache.camel.impl.UriEndpointComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7432, "bug_title": "camel-mybatis - issues calling Oracle Stored procedure with multiple resultsets", "bug_description": "I am using camel-mybatis (version 2.12.0) component in Fuse 6.1 environment to invoke an oracle SP with 2 resultsets. The request is a HashMap and once mybatis maps the resultsets to java beans, the List is again saved in the original HashMap itself. \nHere is the snippet of SqlMap:\n<select\n    id=\"searchUsers\"\n    parameterType=\"java.util.HashMap\"       \n    statementType=\"CALLABLE\">\n    {call ORACLE.SP_NAME_1(\n        #\n{userId,mode=IN,jdbcType=VARCHAR}\n,\n#\n{maxResultsCount,mode=IN,jdbcType=DECIMAL}\n,\n#\n{view,mode=IN,jdbcType=VARCHAR}\n,\n#\n{statusInfo,mode=OUT,jdbcType=CURSOR,resultMap=statusInfoRowMap}\n,\n#\n{memberInfo,mode=OUT,jdbcType=CURSOR,resultMap=claimInfoRowMap}\n)}\nAnd here is how I invoke the mybatis component:\n<setBody>\n    <groovy>\n[\n    userId:&apos;ID-1234&apos;,\n    maxResultsCount:20,\n    view:&apos;MEMBER&apos;,\n]\n   </groovy>\n</setBody>\n<to uri=\"mybatis:searchUsers?statementType=SelectOne\" />\nSince there are no result object (all the results are stored in the original requested HashMap itself), MyBatisProducer is setting null to exchange OUT message. The original body which contains the results from stored procedure is lost.\nThe Question is: is this the expected behaviour? mybatis component already stores the result in exchange header, so why to update the body as well?\nThe workaround I had to do was - to store the original body to a header, invoke mybatis and reset body from the header (which has the stored procedure result now) : \n<setBody>\n            <groovy>\n                [\n             userId:&apos;ID1234&apos;,\n             maxResultsCount:20,\n             view:&apos;MEMBER&apos;\n          ]\n        </groovy>\n    </setBody>\n    <setHeader headerName=\"originalRequest\">\n        <simple>$\n{body}\n</simple>\n    </setHeader>\n    <to uri=\"mybatis:searchUsers?statementType=SelectOne\" />\n    <setBody>\n        <simple>$\n{in.headers.originalRequest}\n</simple>\n    </setBody>\n    <log message=\"status: $\n{body.statusInfo}\n\" />", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.0", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.mybatis.MyBatisProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7888, "bug_title": "HL7Decoder leaks memory", "bug_description": "Under constant load (i.e. Mina session never idles out), the HL7Decoder leaks memory. In fact, all received messages are appended to Mina&apos;s IOBuffer, which leads to OOME after some time.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.component.hl7.HL7MLLPCodecTest.java", "org.apache.camel.component.hl7.HL7RouteTest.java", "org.apache.camel.component.hl7.HL7MLLPCodecLongTest.java", "org.apache.camel.component.hl7.HL7MLLPCodecStandAndEndBytesTest.java", "org.apache.camel.component.hl7.HL7Converter.java", "org.apache.camel.component.hl7.HL7MLLPCodecPlainStringTest.java", "org.apache.camel.component.hl7.HL7MLLPEncoder.java", "org.apache.camel.component.hl7.HL7MLLPDecoder.java"], "label": 1, "es_results": []}, {"bug_id": 7167, "bug_title": "AbstractListAggregationStrategy : at the end of the split, the body is not replaced by the agregated list", "bug_description": "Using a class that extends AbstractListAggregationStrategy to rebuild a List after the completion of the split because the body not to be replaced by the agregated list at the end of the split.\nIn other words (AbstractListAggregationStrategy.onCompletion(Exchange exchange) is never called.\nHere is what I do :\nfrom(HANDLE_A_LIST)//\n            .split(body(), new ListAggregationStrategy())// body is an arrayList of String\n            .to(\"log:foo\")//\n            .end()// end split\n            // the body is a string instead of a List\n            .end()// end route\nclass ListAggregationStrategy extends AbstractListAggregationStrategy<String>\n    {\n        @Override\n        public String getValue(Exchange exchange)\n        {\n\n            return exchange.getIn().getBody();\n\n        }\n    }\nAs workaround, I use .setBody(property(Exchange.GROUPED_EXCHANGE)) after the end of the split.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.1", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 7754, "bug_title": "Property Trigger.timerZone is declared as constant but not implemented", "bug_description": "Seems that we have a problem with Quartz/Quartz2 components. The doc claims that we can setup this property in the URI \"trigger.timeZone\" (http://camel.apache.org/quartz.html - see specifying timezone ) but quartz don&apos;t use it even if a constant is defined :https://www.dropbox.com/s/1wjt3slsz3jajlh/Screenshot%202014-08-27%2010.29.53.png?dl=0\nWe have the same issue too with quartz2 --> https://www.dropbox.com/s/jcxnn72bzi38qob/Screenshot%202014-08-27%2010.31.34.png?dl=0", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.1", "fixed_version": "camel-2.14.0", "fixed_files": ["org.apache.camel.component.quartz2.QuartzEndpoint.java", "org.apache.camel.component.quartz.QuartzComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7058, "bug_title": "camel-sql - Setting SQL_ROW_COUNT header is not updated if the header already exists", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/bug-sql-SQL-Component-cannot-change-SQL-ROW-COUNT-header-value-tp5744350.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.component.sql.SqlProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7061, "bug_title": "DefaultCxfRsBinding only uses Exchange.getOut()", "bug_description": "When org.apache.camel.component.cxf.jaxrs.DefaultCxfRsBinding is populating the response from an org.apache.camel.Exchange then only the exchange.getOut().getBody() is used.\nIn the SOAP case org.apache.camel.component.cxf.DefaultCxfBinding the response is either uses  exchange.getOut().getBody() or exchange.getIn().getBody()\nSee also Discussion for more details.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.11.3", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.DefaultCxfRsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 7057, "bug_title": "Issue with password having double &&", "bug_description": "there is an issue with handling password with double && even when wrapping it into RAW() constraint \nFor instance the use of URI \nhttp://hostname/script.php?authMethod=Basic&authPassword=RAW(pa&&word)&authUsername=usr\nthrows the exception: \norg.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: http://hostname/script.php?authMethod=Basic&authPassword=RAW(pa&&word)&authUsername=usr due to: Invalid uri syntax: Double && marker found. Check the uri and remove the duplicate & marker. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.11.3", "fixed_files": ["org.apache.camel.impl.DefaultComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7073, "bug_title": "camel-restlet - request headers are sent in response", "bug_description": "The fix applied in CAMEL-6879 causes the request headers to be sent back in the response.\nDefaultRestletBinding copies the request headers in &apos;org.restlet.http.headers&apos; header to the response headers in &apos;org.restlet.http.headers&apos;.\nThis causes problems for things like Content-Type header which is a valid request and response header and so the response is sent with the request content-type. Eg, POST requests that should receive JSON in response and send using content-type application/x-www-form-urlencoded will be sent back a response with the same content-type which doesn&apos;t make sense.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.component.restlet.DefaultRestletBinding.java"], "label": 1, "es_results": []}, {"bug_id": 7087, "bug_title": "StreamCache does not reset at the end of the pipeline", "bug_description": "The StreamCache does not reset at the end of the pipeline.\nThe following routes will not work as expected:\n   from(\"direct:c\").noStreamCaching().to(\"direct:d\").to(\"mock:c\");\n   from(\"direct:d\").streamCaching().process(new TestProcessor());\n(the test processor is a processor that reads the InputMessage)\nIf another processor is added after the TestProcessor that does nothing, the routes will work as expected.\nThis case is a bit synthetic, but in real life a route that starts with a InOut CXF endpoint, has an endpoint that generates a Stream and then writes the message content to an in only endpoint (file, FTP, log, etc.) will also fail, even if stream caching is enabled.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.processor.CamelInternalProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 7096, "bug_title": "The ObjectHelper#createIterator utility should properly adhere the java.util.Iterator contract", "bug_description": "According to the java.util.Iterator#next() contract the implementation should throw NoSuchElementException if the iteration has no more elements. However this is currently not the case by the different anonymous class implemetations of this interface through ObjectHelper#createIterator().\nThe side effect of this is that currently this could end up with IndexOutOfBoundsException and what not if one would iterate over the Node(s) of a DOM NodeList. Another example is that calling next() on the Iterator returned for an empty String repeatedly doesn&apos;t throw  any NoSuchElementException although it&apos;s hasNext() method returns false!\nWe should better provide guards for the condition when there&apos;s no more elements available in which case a NoSuchElementException should be thrown.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.11.3", "fixed_files": ["org.apache.camel.util.ObjectHelper.java", "org.apache.camel.util.ObjectHelperTest.java"], "label": 1, "es_results": []}, {"bug_id": 7102, "bug_title": "Broken JUnit classes for testing authentication in camel-jcr", "bug_description": "The following JUnit tests that are supposed to test authentication with the camel-jcr component are currently broken and disabled (annotated @Ignore):\n\nAuthTokenLoginFailureTest\nJcrAuthTokenWithLoginTest\n\nTesting authentication in camel-jcr (i.e. connection URI parsing) is essential and should be fixed.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.component.jcr.AuthTokenLoginFailureTest.java", "org.apache.camel.component.jcr.JcrAuthTokenWithLoginTest.java", "org.apache.camel.component.jcr.JcrAuthTestBase.java", "org.apache.camel.component.jcr.JcrRouteTestSupport.java"], "label": 1, "es_results": []}, {"bug_id": 7111, "bug_title": "Multicast EIP with only one child processor does not call aggregate strategy", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/multicast-aggregation-tp5745571.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.11.3", "fixed_files": ["org.apache.camel.model.MulticastDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 7116, "bug_title": "JettyHttpProducer.doStop does not stop client thread pool", "bug_description": "Case:\nCamelContext with one route :\nfrom(\"direct:a\")\n .routingSlip(generateHttpUrl())\nWhere generateHttpUrl() returns  for example:\n\"jetty:http://someurl.com:666/some?httpClientMinThreads=128&\"\nWhen CamelContext is closing, JettyHttpProducer.doStop is not called.\nResult:\n\nWhen JettyHttpProducer.client is closing, its thread pool is not closed and the threads are non-deamon threads (subject to another JIRA),  so application cannot be shutdown.\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7160, "bug_title": "Throttling has problems with rate changes", "bug_description": "When using the throttler with the header expression for controlling the rate, changing the rate does not work reliably. \nSome more information can be found in the following mail thread:\nhttp://camel.465427.n5.nabble.com/Problems-with-dynamic-throttling-td5746613.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.11.4", "fixed_files": ["org.apache.camel.processor.Throttler.java", "org.apache.camel.processor.ThrottlerTest.java"], "label": 1, "es_results": []}, {"bug_id": 7159, "bug_title": "camel-bindy not picking up @Link annotation items", "bug_description": "It works fine (like in the tests) when you provide bindy with a Map<String, Object> of model objects corresponding to the @Linked-ed classes. We should do better though and try to figure this out for users.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.11.4", "fixed_files": ["org.apache.camel.dataformat.bindy.csv.BindyCsvDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 7163, "bug_title": "BacklogDebugger - Should not change body/header type to string", "bug_description": "When using the backlog debugger then updating the body/headers would currently force those to become string type.\nWe should preserve existing type, and allow end users to specify a new type. And also make it possible to remove body/headers as well.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.management.BacklogDebuggerTest.java", "org.apache.camel.management.mbean.ManagedBacklogDebugger.java", "org.apache.camel.api.management.mbean.ManagedBacklogDebuggerMBean.java", "org.apache.camel.processor.interceptor.BacklogDebugger.java"], "label": 1, "es_results": []}, {"bug_id": 7172, "bug_title": "camel-netty - Some options in netty configuration do not support # lookup", "bug_description": "For example passphrase does not support passphrase=#myPassword, to lookup a bean with that id that produces the password to use.\nThere is a few others as well.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.impl.DefaultComponentTest.java", "org.apache.camel.component.netty.NettyConfiguration.java", "org.apache.camel.impl.DefaultComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7173, "bug_title": "Default value on jmxAgent createConnector should be 'false'", "bug_description": "The defalult value is set to true. Should have been false\nsee http://camel.465427.n5.nabble.com/disabeling-loadStatistics-td5746709.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.13.0", "fixed_files": ["org.apache.camel.core.xml.CamelJMXAgentDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 7146, "bug_title": "NPE in Aggregator when completionSize = 1", "bug_description": "A Camel aggregator with persistence repository cannot have a completionSize of 1. If this is configured, every message produces a NPE with the attached stacktrace. \nI have also attached a small example project that shows the Exception. As soon as the completionSize is > 1, it runs fine.\nThis is just a minor flaw, since I cannot think about a really useful case with completionSize 1, but it worked with earlier versions of Camel. \nAs an alternative (if completionSize 1 should not be used), Camel could throw an error during Context startup when completionSize < 2.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.spi.AggregationRepository.java", "org.apache.camel.processor.aggregate.AggregateProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 7112, "bug_title": "A single call of consumerTemplate.receiveBody consumes more than one messages from a SEDA queue", "bug_description": "When using consumer template&apos;s receiveBody on a SEDA queue that has multiple exchanges, a single call of receiveBody consumed more than one messages from the queue.  This happens for both receiveBody and receiveBodyNoWait.\nI will attach a test project that can produce this issue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.11.4", "fixed_files": ["org.apache.camel.component.seda.SedaEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 7125, "bug_title": "tokenizeXml fails when attributes have a / in them", "bug_description": "tokenizeXml does not work or produce value xml output when attributes contain a /.\nThe test below will fail under 2.12.2\n\n\n\nimport org.apache.camel.EndpointInject;\n\nimport org.apache.camel.Produce;\n\nimport org.apache.camel.ProducerTemplate;\n\nimport org.apache.camel.builder.RouteBuilder;\n\nimport org.apache.camel.component.mock.MockEndpoint;\n\nimport org.apache.camel.test.junit4.CamelTestSupport;\n\nimport org.junit.Test;\n\n\n\npublic class CamelTokenizeXmlTest extends CamelTestSupport {\n\n\n\n  @EndpointInject(uri = \"mock:result\")\n\n  protected MockEndpoint resultEndpoint;\n\n  @Produce(uri = \"direct:start\")\n\n  protected ProducerTemplate template;\n\n\n\n  @Test\n\n  public void testXmlWithSlash() throws Exception {\n\n    String message = \"<parent><child attr=&apos;/&apos; /></parent>\";\n\n    resultEndpoint.expectedBodiesReceived(\"<child attr=&apos;/&apos; />\");\n\n    template.sendBody(message);\n\n    resultEndpoint.assertIsSatisfied();\n\n  }\n\n\n\n  @Override\n\n  protected RouteBuilder createRouteBuilder() {\n\n    return new RouteBuilder() {\n\n      @Override\n\n      public void configure() {\n\n        from(\"direct:start\").split().tokenizeXML(\"child\").to(\"mock:result\");\n\n      }\n\n    };\n\n  }\n\n}\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.11.4", "fixed_files": ["org.apache.camel.processor.SplitTokenizerTest.java", "org.apache.camel.support.TokenXMLExpressionIterator.java"], "label": 1, "es_results": []}, {"bug_id": 7192, "bug_title": "PGPDataFormat: Sub-Key not supported for signing and correct selection of encryption/signing key via KeyFlag", "bug_description": "A PGP key ring with version 4 keys typically consists of a primary key which has sub-keys. There is a use-case where the primary key has only the usage flag (KeyFlag) \"certify\" and one sub-key is makred with the usage flag \"signing\" and the other sub-key is marked with usage flag\"encryption\".  In this case the PGPDataFormat does not use the correct sub-key for signing and encryption.\nIn the patch I enabled the usage of sub-keys for the signing process and I introduced the usage flag (KeyFlag) for finding the correct keys for signing and encryption.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.converter.crypto.PGPDataFormat.java", "org.apache.camel.converter.crypto.PGPDataFormatUtil.java", "org.apache.camel.converter.crypto.PGPDataFormatTest.java"], "label": 1, "es_results": []}, {"bug_id": 7155, "bug_title": "Incorrect implementation of the method StringHelper.hasStartToken()", "bug_description": "From the semantic point of view the method hasStartToken() should return false in case of an expression without language specified, i.e. starting with \"${\". \nHowever, it is correct to return true in current usage of hasStartToken() in camel-core, i.e. checking if an expression is in simple language, since if the expression does not contain the language token, then it is a simple expression.\nThe method calls for checking of simple language should be replaced with a newly created method isSimpleLanguage(String expression) that would check if the language is a simple expression, i.e. hasStartToken(expression, \"simple\") || expression.indexOf(\"${\") >= 0.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.util.StringHelperTest.java", "org.apache.camel.util.StringHelper.java"], "label": 1, "es_results": []}, {"bug_id": 7219, "bug_title": "Language endpoint with constant cannot load from classpath", "bug_description": "A route as\n\n\n\n                from(\"direct:start\")\n\n                    .to(\"language:constant:resource:classpath:org/apache/camel/component/language/hello.txt\")\n\n                    .to(\"mock:result\");\n\n\n\ndoes not work as expected.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.component.language.LanguageComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7230, "bug_title": "SJMS does not respect QoS settings (ttl/persistence) for sending to queues", "bug_description": "Reproduced here:\nhttps://github.com/christian-posta/camel-sandbox/blob/master/camel-2.12.2-issues/src/test/java/posta/SjmsTestFromMailingList.java", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.sjms.support.JmsTestSupport.java", "org.apache.camel.component.sjms.producer.InOnlyProducer.java", "org.apache.camel.component.sjms.producer.InOutProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7254, "bug_title": "MailComponent 'peek' does not work due to programming error", "bug_description": "The MailComponent peek setting doesn&apos;t work due to the following line:\n\n\n\n    private void peekMessage(Message mail) {\n\n        // this only applies to IMAP messages which has a setPeek method\n\n        if (mail.getClass().getName().startsWith(\"IMAP\")) {\n\n            try {\n\n                LOG.trace(\"Calling setPeek(true) on mail message {}\", mail);\n\n                IntrospectionSupport.setProperty(mail, \"peek\", true);\n\n            } catch (Throwable e) {\n\n                // ignore\n\n                LOG.trace(\"Error setting peak property to true on: \" + mail + \". This exception is ignored.\", e);\n\n            }\n\n        }\n\n    }\n\n\n\nThe line that checks the class name for IMAP should be using getSimpleName. Otherwise it&apos;s checking the package name instead.\nThis effectively means that rollback of processing in camel mail is not supported.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.mail.MailConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7258, "bug_title": "org.apache.camel.model.dataformat.XmlJsonDataFormat settings assignments are misplaced.", "bug_description": "\"elementName\" value is assigned to \"encoding\" field, \"arrayName\" is assigned to \"elementName\" field when using XmlJsonDataFormat(Map<String, String> options) constructor.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.dataformat.xmljson.XmlJsonDataFormatTest.java", "org.apache.camel.model.dataformat.XmlJsonDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 7141, "bug_title": "Custom HeaderFilterStrategy does not work when set it on either component or endpoint uri", "bug_description": "Setting custom HeaderFilterStrategy on either NettyHttpComponent or NettyHttpEndpoint uri does not work. The only way to get it to work is to create a NettyHttpBinding bean and set the custom HeaderFilterStrategy as a property of the NettyHttpBinding bean and then set the custom NettyHttpBinding bean onto either NettyHttpComponent and NettyHttpEndpoint uri.\nThe reason it does not work was that when applying the custom HeaderFilterStrategy, it is not set on NettyHttpBinding object as the NettyHttpBinding object always uses default HeaderFilterStrategy. \nI have attached the patch file. It might not be necessary to modify NettyHttpComponent class but I thought it&apos;d be better to initialize configuration, nettyBinding and headerFilterStrategy in their getter method only when needed rather than in constructor of the NettyHttpComponent.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.netty.http.DefaultNettyHttpBinding.java", "org.apache.camel.component.netty.http.NettyHttpEndpoint.java", "org.apache.camel.component.netty.http.NettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7673, "bug_title": "subscribeTopicName leaks out as a property and used as a destination for mqtt producer, causes infinite delivery", "bug_description": "MQTT consumer sets its subscribeTopicName as a exchange property, and it will be used by MQTT producer as a destination if it exists in the route.\nIf you have a following route:\n\n\n\nfrom(\"mqtt:input?subscribeTopicName=topicIn\")\n\n        .to(\"mqtt:output?publishTopicName=topicOut\");\n\n\n\nMQTT consumer put \"topicIn\" to \"MQTTTopicPropertyName\" exchange property, and MQTT producer uses this property to determine a destination topic to publish. Then MQTT producer ignores publishTopicName and send a message to \"topicIn\", so MQTT consumer consumes the message again, eventually it causes infinite delivery. We need to stop this property to be used as a destination for producer.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.14.0", "fixed_files": ["org.apache.camel.component.mqtt.MQTTProducer.java", "org.apache.camel.component.mqtt.MQTTEndpoint.java", "org.apache.camel.component.mqtt.MQTTConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 7224, "bug_title": "camel-smpp - fails to correctly send messages that require UCS-2 encoding", "bug_description": "If I try to send a string without any config options or headers to indicate alphabet/data coding/encoding, it appears to be sent using an 8 bit character set even if a 16 bit UCS-2 encoding is required.\nIf I try to set CamelSmppAlphabet=-1 (for unknown) as a header, an invalid message is sent to the SMPP peer and it is rejected with SMPP error code 00000401\nIf I try to set alphabet=-1 in the endpoint config though and if I also set CamelSmppDataCoding=8 then it correctly sends a UCS-2 message.\nLooking at the code (particularly the method SmppSmCommand.determineCharset()), it seems that it can ONLY send UCS-2 if alphabet == UNKNOWN_ALPHABET.  Using the value UNKNOWN_ALPHABET (-1) as a header triggers the problem with the SMPP peer so I can only set that value in the endpoint config.  This determineCharset() method should also recognise alphabet == ALPHA_UCS2", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.smpp.SmppSmCommand.java"], "label": 1, "es_results": []}, {"bug_id": 7662, "bug_title": "MQTTProducerTest fails once enables it", "bug_description": "MQTTProducerTest fails once you enable this testcase. This is disabled here:\nhttps://github.com/apache/camel/blob/master/components/camel-mqtt/pom.xml#L84\nAlthough this may be a known issue as it&apos;s explicitly disabled, I&apos;m filing this anyway since I couldn&apos;t find any corresponding JIRA.\nIt fails with different error on 2.12.2 and current upstream master, but both case show the producer can&apos;t send messages through the MQTT producer correctly. I think there is an issue in camel-mqtt since I hit same issue in our application (https://issues.jboss.org/browse/SWITCHYARD-2221), but even if the problem is in the testcase, it still needs to be replaced with appropriate testcase to verify MQTT producer works correctly.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.mqtt.MQTTEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 7785, "bug_title": "setUnitOfWork in DefaultExchange throws NPE when called from Splitter.java", "bug_description": "setUnitOfWork in DefaultExchange throws NPE when called from Splitter.java, \nLine 226 of Splitter.java is: exchange.setUnitOfWork(null);\nLine 372 of DefaultExchange.java checks if onCompletions !=null, in this case because Splitter.java initialized unitOfWork to null on line 376 unitOfWork.addSynchronization(onCompletion) will throw an NPE since unitOfWork is still null.\nRecommendation:\nLine 372 of defaultExchange.java should include a check if unitOfWork is not null:\nif(onCompletions !=null && unitOfWork != null)\nSteps to reproduce:\n1. Include a Split EIP in a Camel Route.\n2. Specify an onPrepareRef=<class>\n3. Add an onCompletion step to the exchange in <class>\n4. NPE should be thrown when processing files.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.impl.DefaultExchange.java"], "label": 1, "es_results": []}, {"bug_id": 8001, "bug_title": "SmppUtils.isGsm0338Encodeable() called with arbitrary data", "bug_description": "\nThe method SmppUtils.isGsm0338Encodeable(byte[]) expects the input data to be Latin1 encoded (or a subset of Latin1, such as ASCII)\nThe method SmppSmCommand.determineAlphabet(Message) prepares an argument by calling String.getBytes(charset) without checking the charset is Latin1 or compatible.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.smpp.SmppSmCommand.java", "org.apache.camel.component.smpp.SmppUtils.java"], "label": 1, "es_results": []}, {"bug_id": 7144, "bug_title": "Blueprint route not shutdown when using OSGi service processor", "bug_description": "The route description uses .toString() when shutting down. This causes the shutdown to block if a proxied OSGi service is used as processor (in XML DSL <process ref=\"osgiService/>) and the service is not present at the time the route is shut down.\nSee http://camel.465427.n5.nabble.com/Blueprint-route-not-shutdown-when-using-OSGi-service-processor-td5746114.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.2", "fixed_version": "camel-2.15.0", "fixed_files": ["org.apache.camel.model.ProcessDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 7251, "bug_title": "SqlProducer call twice the getResultSet on the PreparedStatement => This because issue with HSQLDB (NPE)", "bug_description": "The SqlProducer class use the following lines of code :\nResultSet rs = ps.getResultSet();\n                        SqlOutputType outputType = getEndpoint().getOutputType();\n                        log.trace(\"Got result list from query: {}, outputType={}\", rs, outputType);\n                        if (outputType == SqlOutputType.SelectList) \n{\n\n                            List<Map<String, Object>> data = getEndpoint().queryForList(ps.getResultSet());\n\n                           .................................\n\n                        }\n else if (outputType == SqlOutputType.SelectOne) \n{\n\n                            Object data = getEndpoint().queryForObject(ps.getResultSet());\n\n                            .................................\n\n                        }\n else \n{\n\n                            throw new IllegalArgumentException(\"Invalid outputType=\" + outputType);\n\n                        }\n\nThe problem is that the ResultSet is retrieved at the start, and then only used for the log. Later, when the result set is required, a new call to getResultSet is done. It is an issue with HSQL DB (tested with version 2.3.0 and 2.3.2 of HSQL DB), which in this case return null for the second call.\nAs the ResultSet is already available in a variable \"rs\", I would recommand to use this variable to replace the two last call to ps.getResultSet().\nThanks", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.sql.SqlProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7241, "bug_title": "ByteBuffer to String conversion uses buffer capacity not limit", "bug_description": "Camel&apos;s conversion logic for ByteBuffer&apos;s to String&apos;s has a bug where camel uses a ByteBuffers capacity() instead of it&apos;s limit().\nIf you allocate a large byte buffer and only partially fill it with data, and use camel to convert this into a string, camel tries to convert all the bytes, even the non-used ones.\nThis unit test reproduces this bug.\n\n\n\n    @Test\n\n    public void testByteBufferToStringConversion()\n\n    {\n\n        String str = \"123456789\";\n\n        ByteBuffer buffer = ByteBuffer.allocate( 16 );\n\n        buffer.put( str.getBytes() );\n\n\n\n        Exchange exchange = new DefaultExchange( context() );\n\n        exchange.getIn().setBody( buffer );\n\n        assertEquals( str, exchange.getIn().getBody( String.class ) );\n\n    }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.converter.NIOConverter.java", "org.apache.camel.converter.NIOConverterTest.java"], "label": 1, "es_results": []}, {"bug_id": 7275, "bug_title": "Using doTry .. doCatch with recipient list should not trigger error handler during recipient list work", "bug_description": "When you have a route like this\n\n\n\n                from(\"direct:start\")\n\n                    .doTry()\n\n                        .recipientList(constant(\"direct:foo\")).end()\n\n                    .doCatch(Exception.class)\n\n                        .to(\"mock:catch\")\n\n                        .transform().constant(\"doCatch\")\n\n                    .end()\n\n                    .to(\"mock:result\");\n\n\n\nThen if an exception was thrown it should be catch by doCatch\nA similar route with to instead works as expected.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.Exchange.java", "org.apache.camel.processor.TryProcessor.java", "org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 7279, "bug_title": "Yammer now uses bearer token for auth", "bug_description": "... which means camel-yammer cannot connect to the yammer API currently.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.yammer.ScribeApiRequestor.java"], "label": 1, "es_results": []}, {"bug_id": 7276, "bug_title": "camel-quartz - use of management name to provide default scheduler name breaks context isolation", "bug_description": "When using the camel-quartz component in an unmanged context with multiple camel contexts, for example in a JUnit test case, causes the scheduler to be created with the instance name \"DefaultQuartzScheduler\" which is then shared across all camel context&apos;s within the same jvm.\nThis is in contradiction of the previous behaviour that uses `getCamelContext.getName()` which isolates the scheduler by denoting that the default instance is specific to the camel context.\n\n\n\npackage org.apache.camel.component.quartz;\n\n\n\nimport org.apache.camel.CamelContext;\n\nimport org.apache.camel.impl.DefaultCamelContext;\n\nimport org.apache.camel.management.JmxSystemPropertyKeys;\n\nimport org.junit.Test;\n\nimport org.quartz.Scheduler;\n\nimport org.quartz.SchedulerException;\n\n\n\nimport static org.junit.Assert.assertNotEquals;\n\nimport static org.junit.Assert.assertNotSame;\n\n\n\n/**\n\n * Test regression of camel-context isolation of default scheduler instance introduced in CAMEL-7034.\n\n */\n\npublic class QuartzComponentCamelContextSchedulerIsolationTest {\n\n\n\n  @Test\n\n  public void testSchedulerIsolation_unmanaged() throws Exception {\n\n    disableJMX();\n\n    testSchedulerIsolation();\n\n  }\n\n\n\n  @Test\n\n  public void testSchedulerIsolation_managed() throws Exception {\n\n    enableJMX();\n\n    testSchedulerIsolation();\n\n  }\n\n\n\n  private void testSchedulerIsolation() throws Exception {\n\n    CamelContext context = createCamelContext();\n\n    context.start();\n\n\n\n    CamelContext anotherContext = createCamelContext();\n\n    assertNotEquals(anotherContext.getName(), context.getName());\n\n    assertNotEquals(anotherContext, context);\n\n\n\n    assertNotSame(getDefaultScheduler(context), getDefaultScheduler(anotherContext));\n\n  }\n\n\n\n  /**\n\n   * Create a new camel context instance.\n\n   */\n\n  private DefaultCamelContext createCamelContext() {\n\n    return new DefaultCamelContext();\n\n  }\n\n\n\n  /**\n\n   * Get the quartz component for the provided camel context.\n\n   */\n\n  private QuartzComponent getQuartzComponent(CamelContext context) {\n\n    return context.getComponent(\"quartz\", QuartzComponent.class);\n\n  }\n\n\n\n  /**\n\n   * Get the default scheduler for the provided camel context.\n\n   */\n\n  private Scheduler getDefaultScheduler(CamelContext context) throws SchedulerException {\n\n    return getQuartzComponent(context).getFactory().getScheduler();\n\n  }\n\n\n\n  /**\n\n   * Disables the JMX agent.\n\n   */\n\n  private void disableJMX() {\n\n    System.setProperty(JmxSystemPropertyKeys.DISABLED, \"true\");\n\n  }\n\n\n\n  /**\n\n   * Enables the JMX agent.\n\n   */\n\n  private void enableJMX() {\n\n    System.setProperty(JmxSystemPropertyKeys.DISABLED, \"false\");\n\n  }\n\n\n\n}\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.quartz.QuartzComponent.java", "org.apache.camel.component.quartz2.QuartzPropertiesTest.java", "org.apache.camel.component.quartz.QuartzPropertiesTest.java", "org.apache.camel.component.quartz2.QuartzComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7298, "bug_title": "Simple Language - Binary Expression Equality Fails", "bug_description": "When writing the following code, the expression \n\n${headers.true} == ${headers.true}\n\n evaluates to false - rather than the expected true.\n\n\n\n    <camelContext trace=\"false\" xmlns=\"http://camel.apache.org/schema/blueprint\">\n\n        <route>\n\n            <from uri=\"direct:entry\"/>\n\n\n\n            <setHeader headerName=\"true\">\n\n                <constant>true</constant>\n\n            </setHeader>\n\n\n\n            <setBody>\n\n                <simple resultType=\"java.lang.Boolean\">${headers.true} == ${headers.true}</simple>\n\n            </setBody>\n\n\n\n            <log message=\"The expression evaluated to :: ${body}\" />\n\n\n\n        </route>\n\n    </camelContext>\n\n\n\n\n\n\n[                          main] route1                         INFO  The expression evaluated to :: false\n\n\n\nEdit: I have tried debugging this, and it seems that the \"==\" token is being treated as a LiteralExpression - rather than an being a Binary Operator? I&apos;m imaging therefore that the `false` is purely from coercing a String to the java.lang.Boolean resultType?", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.language.simple.SimpleLanguage.java", "org.apache.camel.language.simple.SimpleParserPredicateTest.java", "org.apache.camel.builder.SimpleBuilder.java", "org.apache.camel.language.simple.SimplePredicateParser.java"], "label": 1, "es_results": []}, {"bug_id": 7303, "bug_title": "Simple Language - Header access surrounded with quotes", "bug_description": "Quotes within header access are not currently handled in the same way as the inbuilt headerAs function or the body accessor notation - perhaps these should be aligned?\nFor instance with the following route\n\n\n\n    <camelContext trace=\"false\" xmlns=\"http://camel.apache.org/schema/blueprint\">\n\n        <route>\n\n            <from uri=\"direct:entry\"/>\n\n\n\n            <setHeader headerName=\"some key\">\n\n                <constant>hello world</constant>\n\n            </setHeader>\n\n\n\n            <setBody>\n\n                <simple>${headers[&apos;some key&apos;]}</simple>\n\n            </setBody>\n\n\n\n            <log message=\"The expression evaluated to :: ${body}\" />\n\n\n\n        </route>\n\n    </camelContext>\n\n\n\nOutcome :\n\n[                          main] route1                         INFO  The expression evaluated to :: \n\nExpected :\n\n[                          main] route1                         INFO  The expression evaluated to :: hello world\n\nFor reference, the current notation works within other parts of the language and are handled as expected\n\n${body[&apos;some key&apos;]}\n\n\n${headerAs(&apos;some key&apos;, java.lang.Boolean}\n\nI think the fix might be to use StringHelper.removeQuotes on line #269 within SimpleFunctionExpression.java possibly, such as\n\nremainder = StringHelper.removeQuotes(remainder.substring(1, remainder.length() - 1));\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.language.simple.SimpleTest.java", "org.apache.camel.language.simple.ast.SimpleFunctionExpression.java"], "label": 1, "es_results": []}, {"bug_id": 7310, "bug_title": "Restlet - Need to run in sync mode due bug in restlet", "bug_description": "There is a NPE bug in restlet when the restlet client timeout\nhttps://github.com/restlet/restlet-framework-java/issues/871\nThis causes camel restlet producer to not have its callback called, which can lead to Camel hanging.\nWe need to force sync mode until restlet fixes their NPE bug", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.restlet.RestletProducerTimeoutTest.java", "org.apache.camel.component.restlet.RestletProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7311, "bug_title": "camel-mail - Should not fetch attachments if mapMailMessage=false", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Polling-Email-Component-Runtime-Error-tp5749047.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.mail.MailMessage.java", "org.apache.camel.component.mail.MailEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 7296, "bug_title": "camel-hbase component have some bugs on Consumer side", "bug_description": "After I tried a little bit, I think there are 2 problems on Consumer side:\n\nReturned value is null\nif we specified family and qualifier options in URI, then all returned values will be null.  I did some investigation, and I found the problem is caused by the equals method of HBaseCell class. In poll() method of HBaseConsumer, the resultRow will apply rowModel first before insert the resultCell, see below code:\n\n\n\n             ....\n\n              HBaseData data = new HBaseData();\n\n                HBaseRow resultRow = new HBaseRow();\n\n                resultRow.apply(rowModel);       //the existing HBaseCell in rowModel will be insert into resultRow\n\n                byte[] row = result.getRow();\n\n                resultRow.setId(endpoint.getCamelContext().getTypeConverter().convertTo(rowModel.getRowType(), row));\n\n\n\n                List<KeyValue> keyValues = result.list();\n\n                if (keyValues != null) {\n\n                    for (KeyValue keyValue : keyValues) {\n\n                        String qualifier = new String(keyValue.getQualifier());\n\n                        String family = new String(keyValue.getFamily());\n\n                        HBaseCell resultCell = new HBaseCell();\n\n                        resultCell.setFamily(family);\n\n                        resultCell.setQualifier(qualifier);\n\n                        resultCell.setValue(endpoint.getCamelContext().getTypeConverter().convertTo(String.class, keyValue.getValue()));\n\n                        resultRow.getCells().add(resultCell); //will fail to insert if some cell with same Family and Qualifier exists\n\n                    }\n\n                    ...\n\n              }\n\n\n\nthat means we can&apos;t specify family and qualifier option when consuming.\nremove option\nthe default value of remove option is true, that mean, after scan the table, all scanned row will be removed. I think it&apos;s not we expect, so user may have to add option remove=false to stop that. however this option doesn&apos;t describe on website http://camel.apache.org/hbase.html .\n\nBesides these problem, I think some descriptions on http://camel.apache.org/hbase.html are not correct, for example the demos and Header Options. please have a check.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.hbase.HBaseConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7341, "bug_title": "CXFRS: InInterceptor defined in Spring is ignored", "bug_description": "A CXF InInterceptor added to the rsServer via Spring is not added to the interceptor chain and therefore is not executed.\n\n\n\n<cxf:rsServer id=\"service\" address=\"/exampleService\"\n\n              serviceClass=\"com.example.service.ExampleService\"\n\n              loggingFeatureEnabled=\"true\"\n\n              xmlns:cxf=\"http://camel.apache.org/schema/cxf\">\n\n    <cxf:providers>\n\n        <bean class=\"com.fasterxml.jackson.jaxrs.json.JacksonJaxbJsonProvider\"/>\n\n    </cxf:providers>\n\n    <cxf:inInterceptors>\n\n        <bean class=\"com.example.service.ExampleInInterceptor\"/>\n\n    </cxf:inInterceptors>\n\n</cxf:rsServer>\n\n\n\n\n\n\npublic class ExampleInInterceptor extends AbstractPhaseInterceptor<Message> {\n\n    public ExampleInInterceptor() {\n\n        super(Phase.RECEIVE);\n\n    }\n\n\n\n    @Override\n\n    public void handleMessage(Message message) throws Fault {\n\n        ...\n\n    }\n\n}\n\n\n\nThe same configuration works with Camel Version 2.12.2.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.CxfRsEndpoint.java", "org.apache.camel.component.cxf.jaxrs.CxfRsSpringEndpointTest.java"], "label": 1, "es_results": []}, {"bug_id": 7347, "bug_title": "camel-netty - Should return 404 instead of 503 if context-path did not match a route", "bug_description": "We should return 404 instead as the service is there but the resource requested was not found.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.netty.http.NettyHttpTwoRoutesMatchOnUriPrefixTest.java", "org.apache.camel.component.netty.http.NettyHttpTwoRoutesStopOneRouteTest.java", "org.apache.camel.component.netty.http.handlers.HttpServerMultiplexChannelHandler.java"], "label": 1, "es_results": []}, {"bug_id": 7359, "bug_title": "Simple Language - Additional after text after inbuilt function call is ignored", "bug_description": "The following Simple expression is valid and runs OK - however it may have been appropriate to report an error to the developer.\n\n\n\n            <setBody>\n\n                <simple>${bodyAs(java.lang.String) Additional text ignored...}</simple>\n\n            </setBody>\n\n\n\nThe above seems a somewhat contrived example; However this is a more &apos;realistic&apos; scenario in which the behaviour is not unexpected -\n\n\n\n            <setBody>\n\n                <simple>${bodyAs(java.lang.String).toUpperCase()}</simple>\n\n            </setBody>\n\n\n\nThe above simple expression will simply set the body to be of type java.lang.String, however will not invoke the subsequent toUpperCase() call - likewise no error is reported to the developer.\nCamel has the same issue when using the function of headerAs and mandatoryBodyAs.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.language.simple.SimpleTest.java", "org.apache.camel.language.simple.ast.SimpleFunctionExpression.java"], "label": 1, "es_results": []}, {"bug_id": 7350, "bug_title": "JaxbDataFormat should not parse the Schema over and over again", "bug_description": "Currently the createMarshaller and createUnmarshaller methods parse the javax.xml.validation.Schema from scratch, which is a severe overhead considering that a Schema instance is threadsafe and it is  encuraged to re-use it as much as possible \n(see http://xerces.apache.org/xerces2-j/javadocs/api/javax/xml/validation/Schema.html) \nI extended the DataFormat and cached the parsed Schema and did a small benchmark that parsed an input document 64000 times:\nJaxbDataFormat: 3:18\nAltJaxbDataFormat: 0:35", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.converter.jaxb.JaxbDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 7363, "bug_title": "camel headers are not preserverd in camel-ahc component", "bug_description": "The documentation of the camel-ahc component states:\n... All headers from the IN message will be copied to the OUT message, so headers are preserved during routing. Additionally Camel will add the HTTP response headers as well to the OUT message headers.\nHowever this is not true. Camel headers are not copied from the in to the out message.\nI have looked up other components doing http requests (http4 and cxf) and these components do copy the headers from the in to the out message.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.ahc.DefaultAhcBinding.java", "org.apache.camel.component.ahc.AhcProduceGetHeadersTest.java"], "label": 1, "es_results": []}, {"bug_id": 7379, "bug_title": "allChannels should not be static variable for the NettyProducer", "bug_description": "A user complained that other netty channels would be closed if he just shutdown one of netty producer.  We should not  store the all channels into a static variable. \nhttp://camel.465427.n5.nabble.com/NettyProducer-stop-will-close-all-channels-belong-to-other-NettyProducer-tp5750374.html ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.netty.NettyProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7391, "bug_title": "camel-netty - NettyProduce should use timer from component instead of creating new timer per producer", "bug_description": "We should use the shared timer, instead of creating a new timer per producer.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.netty.NettyProducer.java", "org.apache.camel.component.netty.NettyClientBossPoolBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 7424, "bug_title": "StaxSource does not produce correct SAX events with default namespace", "bug_description": "See full description here: CXF-5733", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.13.2", "fixed_files": ["org.apache.camel.converter.jaxp.StaxSource.java"], "label": 1, "es_results": []}, {"bug_id": 7428, "bug_title": "Simple Language - Operators are not evaluated for setting body or headers", "bug_description": "Operators are not evaluated if using simple for setting bodies or headers:\n\n\n\nfrom(\"direct:simple\")\n\n    .setHeader(\"myHeader\").simple(\"true == true\", Boolean.class)\n\n    .log(\"header = [${header.myHeader}]\")\n\n    .setBody(simple(\"true == true\", Boolean.class))\n\n    .log(\"body = [${body}]\");\n\n\n\nOutput is as follows:\n\n\n\nINFO  header = [false]\n\nINFO  body = [false]\n\n\n\nThe outcome should be true in both cases.\nAlso, see http://stackoverflow.com/questions/23523409/camel-how-to-set-boolean-header-parameter-using-simple-comparison/23560989#23560989", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.language.simple.SimpleLanguage.java", "org.apache.camel.language.simple.SimpleParserPredicateTest.java", "org.apache.camel.builder.SimpleBuilder.java", "org.apache.camel.language.simple.SimplePredicateParser.java"], "label": 1, "es_results": []}, {"bug_id": 7462, "bug_title": "camel-netty-http does not use \"Expect: 100-continue\" correctly", "bug_description": "Camel-netty-http component, when sending HTTP request with:\n\nExpect: 100-continue\n\n\n\nheader, always sends the HTTP body with first request and treats:\n\nHTTP/1.1 100 Continue\n\n\n\nas final response.\nAdditionally org.apache.camel.component.netty.http.handlers.HttpServerChannelHandler sends HTTP/1.1 100 Continue partial response after the same partial response was send by org.jboss.netty.handler.codec.http.HttpChunkAggregator#messageReceived\nAttached: wireshark session dump.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.netty.http.handlers.HttpClientChannelHandler.java", "org.apache.camel.component.netty.http.handlers.HttpServerChannelHandler.java"], "label": 1, "es_results": []}, {"bug_id": 7541, "bug_title": "RedisIdempotentRepository does not work out of box", "bug_description": "IdempotentRepository need to use contains to check if the message is already there before adding the message. RedisIdempotentRepository doesn&apos;t do that, so RedisComponentSpringIntegrationTest is failed.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.redis.processor.idempotent.RedisIdempotentRepository.java", "org.apache.camel.component.redis.RedisComponentSpringIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 7545, "bug_title": "Having multiple camelContext in same blueprint bundle may only register the 1st in the OSGi service registry", "bug_description": "But its correct in JMX etc.\nBut the osgi service registry is used by the Camel Karaf commands so they can only see the 1st CamelContext\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.core.osgi.OsgiCamelContextPublisher.java"], "label": 1, "es_results": []}, {"bug_id": 7570, "bug_title": "enrich does not send out ExchangeSendingEvent nor ExchangeSentEvent ", "bug_description": "When using enrich DSL, camel doesn&apos;t send out ExchangeSendingEvent nor ExchangeSentEvent  for management.\nYou can find more information about it here", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.processor.Enricher.java"], "label": 1, "es_results": []}, {"bug_id": 7557, "bug_title": "CxfRsProducer does not copy headers between Camel and CXF messages in a proxy mode", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.CxfRsProducer.java", "org.apache.camel.component.cxf.jaxrs.CxfRsProducerTest.java"], "label": 1, "es_results": []}, {"bug_id": 7586, "bug_title": "NotCompliantMBeanException : Attribute MessageHistory has more than one getter", "bug_description": "Hello, I wasn&apos;t able to subscribe on the mailing list, so I&apos;m posting my issue directly here.\nIn my project I need to use some ManagedCamelContextMBean, which I am trying to access through JMX.newMBeanProxy\nHowever, it is not working as I&apos;m getting a NotCompliantMBeanException because the attribute MessageHistory is said to have more than one getter.\nI checked the source code of newMBeanProxy, then the JMX 1.4 specification, and then Camel&apos;s source code, and it appears that ManagedCamelContextMBean is indeed not respecting the standard MBean.\nThe problem is that two methods are defined in ManagedCamelContextMBean : isMessageHistory() and getMessageHistory()\nSince the return type is boolean, isMessageHistory is considered to be a getter, which makes two getter according to the JMX specification and is blocking the newMBeanProxy() method.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.api.management.mbean.ManagedCamelContextMBean.java", "org.apache.camel.management.mbean.ManagedCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 7598, "bug_title": "Camel does not clear the jpa session after each processed message batch", "bug_description": "We are using apache camel to poll from a database. As we want processed rows only to be updated we have disabled consumeDelete on the JPA endpoint. \nDuring testing we found a large memory leak: all polled entities are kept in the session cache (we are using hibernate as persistence provider). \nThe issue seems to be in the JpaConsumer. In method poll() it calls enitityManager.joinTransaction() at the beginning and entitiyManager.flush() at the end of the method but it never calls  entityManager.clear(). As camel is reusing the underlying session during each poll() this causes the first level entity cache to grow indefinitely. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.jpa.JpaConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7636, "bug_title": "camel-jms - Reply manager during timeout must use a catch to ensure onEviction can return correclty", "bug_description": "Got a tidbit of a problem\n<raul>\t 2014-07-24 02:12:02,250 | WARN  | CorrelationTimeoutMap | 123 - org.apache.camel.camel-core - 2.13.1 | Exception happened during eviction of entry ID org.apache.camel.component.jms.reply.TemporaryQueueReplyHandler@7f3a3f81, won&apos;t evict and will continue trying: java.lang.NullPointerException\nThis could lead to endless retry if processing the timeout keeps causing the same exception.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.jms.reply.CorrelationTimeoutMap.java"], "label": 1, "es_results": []}, {"bug_id": 8032, "bug_title": "FileUtil leaks FileInputStream when renameFile fails due to permission issue", "bug_description": "I have a simple camel route:\n\n\n\n<camelContext xmlns=\"http://camel.apache.org/schema/spring\">\n\n    <route>\n\n        <from uri=\"file:C:/tmp/data/in?include=.*$&amp;move=C:/tmp/data/done/${file:onlyname}-${exchangeId}\" />\n\n        <setHeader headerName=\"CamelFileName\">\n\n            <simple>${file:onlyname}-${exchangeId}</simple>\n\n        </setHeader>\n\n        <to uri=\"file:C:/tmp/data/out\" />\n\n    </route>\n\n</camelContext>\n\n\n\nIf the destination folder \"C:/tmp/data/done/\" for the move operation does not allow writing, then the file dropped to the \"C:/tmp/data/in/\" folder will be repeatedly polled, processed and rolled back due to \"Access is denied\" exception.\nEven if we fix the permission issue on the folder \"C:/tmp/data/done/\" to allow writing, the problem still persists and above endless cycle continues. However the reason for the issue will be a bit different now. It is caused by deletion failure to the file from \"C:/tmp/data/in/\" folder after successful FileUtil.renameFile() operation due to fact that something is still holding the file handle. \nThe root because is in the function FileUtil.copyFile():\n\n\n\npublic static void copyFile(File from, File to) throws IOException {\n\n        FileChannel in = new FileInputStream(from).getChannel();\n\n        FileChannel out = new FileOutputStream(to).getChannel();\n\n        try {\n\n            if (LOG.isTraceEnabled()) {\n\n                LOG.trace(\"Using FileChannel to copy from: \" + in + \" to: \" + out);\n\n            }\n\n\n\n            long size = in.size();\n\n            long position = 0;\n\n            while (position < size) {\n\n                position += in.transferTo(position, BUFFER_SIZE, out);\n\n            }\n\n        } finally {\n\n            IOHelper.close(in, from.getName(), LOG);\n\n            IOHelper.close(out, to.getName(), LOG);\n\n        }\n\n    }\n\n\n\nIf the destination folder \"C:/tmp/data/done/\" for move operation is not allowed for writing, the creation of the FileOutputStream will throw an exception straight away. However, because both FileInputStream and FileOutputStream are created outside the try{}...finally{} block, the FileInputStream is never closed. It still holds handle to the file and caused FileSystem unable to delete it. Therefore caused the whole route to fail. \nThe solution is quite simple, we just need to create the Input/Output streams inside try{}...finally{} loop to make sure that the Input/Output streams get closed if something happens during creating of these objects.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.util.FileUtil.java"], "label": 1, "es_results": []}, {"bug_id": 7921, "bug_title": "The soapAction HTTP header is not correctly set when running the CXF client in POJO mode using Camel", "bug_description": "The soapAction HTTP header is not correctly set when running the CXF client in POJO mode using Camel.\nThe root because seems to be that the service name from the generated service class differs from the service name in the WSDL.\nFor me it is unclear if this should be an issue with the cxf-codegen-plugin or with the Camel CXF component. When running the CXF client without Camel then the soapAction HTTP header is correctly set and the issue does not occur. So that&apos;s why I first report the bug with Camel.\nPossible workarounds I found are:\n1) Explicitly specifying the correct serviceName as CXF endpoint attribute.\n2) Explicitly setting the soapAction header in the Camel route prior to calling the CXF endpoint.\nBoth workarounds are not desirable, because they are easily forgotten and CXF does not throw an exception when you do. According to the basic profile v1.0 the soapAction HTTP header must match the value in the WSDL and receiving SOAP servers may throw a SOAP Fault if it doesn&apos;t. Some SOAP servers do throw an exception when the soapAction HTTP header is invalid. Resulting in communication failures between some SOAP client/server combinations.\nI created a test project to verify the above behaviour with the following tests:\n1) CXF in PAYLOAD with Camel. => OK\n2) CXF in POJO mode without Camel => OK\n3) CXF in POJO mode with Camel => NOT OK\n4) CXF in POJO mode with service name set => OK\n5) CXF in POJO mode with soapAction set => OK\nI run the test project with multiple combinations of Camel and CXF. The following combinations I have tried:\n1) Camel 2.12.3 and CXF 2.7.10 (Apache Servicemix 5.0.0 setup)\n2) Camel 2.12.4 and CXF 2.7.11 (Apache Servicemix 5.0.5 setup)\n3) Camel 2.13.2 and CXF 2.7.11 (Apache Servicemix 5.1.3 and 5.3.0 setup)\n4) Camel 2.14.0 and CXF 3.0.1\nIn the example project the mismatch occurs between an annotation in the generated service class:\n\n\n\n@WebService(targetNamespace = \"http://finalist.nl/ai/\", name = \"ICamelCxfTestService\")\n\n\n\nand the definition of the service name in the WSDL:\n\n\n\n<wsdl:service name=\"CamelCxfTestService\">\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.3", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.component.cxf.CxfEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 7593, "bug_title": "camel-netty-http consumer uses a fixed 1mb chunked frame size", "bug_description": "The option to configure the setting was only exposed on the shared netty http server config. We should also expose it for the non shared out of the box netty http consumer.\nPeople today will run into this problem\nWhen sending an HTTP request bigger than 1MB, netty HTTP throws an exception - org.jboss.netty.handler.codec.frame.TooLongFrameException: HTTP content length exceeded 1048576 bytes .", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.netty.http.NettyHttpConfiguration.java", "org.apache.camel.component.netty.http.HttpServerPipelineFactory.java"], "label": 1, "es_results": []}, {"bug_id": 7630, "bug_title": "BlueprintPropertiesParser does not handle the Blueprint Encryption: EncryptionOperationNotPossibleException", "bug_description": "When using encryption feature which provides by Karaf, BlueprintPropertiesParser has more than one PropertyPlaceholder. It just throw the EncryptionOperationNotPossibleException out if the property is not encrypted. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.blueprint.BlueprintPropertiesParser.java"], "label": 1, "es_results": []}, {"bug_id": 7642, "bug_title": "Netty consumer should return error on invalid request", "bug_description": "f you send the corrupted request to the Netty consumer...\n> header1: value1\n> GET /some/resource HTTP/1.1\n> header2: value2\n...Netty will hang on the open connection, instead of returning error immediately.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.netty.http.handlers.HttpServerMultiplexChannelHandler.java", "org.apache.camel.component.netty.http.handlers.HttpServerChannelHandler.java", "org.apache.camel.component.netty.handlers.ClientChannelHandler.java"], "label": 1, "es_results": []}, {"bug_id": 7603, "bug_title": "camel-cxfrs need to store the security context information into the message header", "bug_description": "We need to store the security context information when generate the camel exchange from CXF request message as camel-cxf does.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.DefaultCxfRsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 7659, "bug_title": "camel-atom filter always returns the last consumed entry from feed more than once", "bug_description": "I have problem with camel setup to consume feeds atom://atomUri?splitEntries=true&filter=true&throttleEntries=false, feed processor does not skip last consumed entry.\nThe source of problem is in UpdateDateFilter class, it filters only entries older than last update and entries of same pubdate are once more\nconsumed.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.atom.UpdatedDateFilter.java"], "label": 1, "es_results": []}, {"bug_id": 7714, "bug_title": "AdviceWith - Does not honor autoStartup option", "bug_description": "If setting a route or camel context to autoStartup=false, and then advicing that route, then it will always be started.\nWe should honor the auto startup option and only start it if that option is default or true.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.CamelContext.java", "org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.impl.DefaultCamelContextAutoStartupTest.java", "org.apache.camel.management.mbean.ManagedCamelContext.java", "org.apache.camel.RuntimeConfiguration.java", "org.apache.camel.api.management.mbean.ManagedCamelContextMBean.java"], "label": 1, "es_results": []}, {"bug_id": 7762, "bug_title": "Camel CxfPayload issue when using namespace with no prefix (xmlns:xmlns)", "bug_description": "When using Camel CXF in PAYLOAD mode.If the client sends a SOAP request with body having no namespace prefix. The element which get for the Payload body has an attribute \"xmlns:xmlns\".\nYou can find more information in the stack overflow site", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.cxf.DefaultCxfBinding.java", "org.apache.camel.component.cxf.CxfPayload.java", "org.apache.camel.component.cxf.DefaultCxfBindingTest.java"], "label": 1, "es_results": []}, {"bug_id": 7768, "bug_title": "Handle fault - Should convert the payload to String using type converter", "bug_description": "We should use the type covnerters from Camel to convert the payload correctly.\nAs if you have a DOM object then we just do a .toString which just prints it as Document #0 etc.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.processor.interceptor.HandleFaultInterceptor.java"], "label": 1, "es_results": []}, {"bug_id": 7767, "bug_title": "Mock - Defining assertion on message doest work if using convertTo", "bug_description": "See\nhttp://www.manning-sandbox.com/thread.jspa?threadID=41025&tstart=0\nThe reason is when you use a method in the fluent builder that returns a ValueBuilder then that didn&apos;t detect the predicate.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.util.MessageHelper.java", "org.apache.camel.processor.onexception.OnExceptionHandledTest.java", "org.apache.camel.issues.TryCatchWithSplitIssueTest.java", "org.apache.camel.builder.ValueBuilder.java", "org.apache.camel.component.mock.AssertionClause.java"], "label": 1, "es_results": []}, {"bug_id": 7795, "bug_title": "Regression: MDC may lose values after when Async Routing Engine is used", "bug_description": "CAMEL-6377 introduced some optimisations in the MDC Logging mechanism which make it lose MDC values when the async routing engine is used.\nIf we are using an async component such as CXF, the response or done() callback will be issued from a thread NOT managed by Camel. Therefore, we need the MDCCallback to reset ALL MDC values, not just the routeId (as was intended by the commits that caused the regression).\nThe situation may be salvaged by the fact that underlying MDC implementations use an InheritableThreadLocal, so the first requests after system initialisation may see correct behaviour, because the MDC values from the requesting thread is propagated to the newly initialised threads in the underlying stack&apos;s ThreadPool, as the coreThreads are being initialised within the context of the original threads which act like parent threads.\nBut after those first attempts, odd behaviour is seen and all responses from the async endpoint come back without an MDC.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.impl.MDCUnitOfWork.java"], "label": 1, "es_results": []}, {"bug_id": 7798, "bug_title": "Exchange formatter configured on Log Component may lead to incoherent results", "bug_description": "Currently configuring a custom ExchangeFormatter at the Log Component level may lead to incoherent results if there are multiple Log endpoints in the context, with different log options (e.g. showBody=true, showBody=false).\nThis is because the component looks up the ExchangeFormatter in the registry, sets its properties and then remembers the result for the subsequent endpoint initialisations. This is incorrect.\nThe correct procedure is:\n\nLook up the ExchangeFormatter each time, obtaining a new fresh copy each time. This can be ensured by using @scope=prototype in Spring/Blueprint in the bean definition.\nSet its properties, according to the current endpoint&apos;s properties.\nUse the resulting ExchangeFormatter for that endpoint only.\nSubsequent endpoint initialisations must repeat the same procedure.\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.log.LogComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7856, "bug_title": "camel-cxf producer HttpAPI should not send the message body when using DELETE method", "bug_description": "Here is the discussion in stackoverflow.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.CxfRsEndpoint.java", "org.apache.camel.component.cxf.jaxrs.CxfRsProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7885, "bug_title": "Timer - Restarting a timer endpoint may not trigger at expected time the first time", "bug_description": "If you stop a timer route, and that route has an inflight exchange causing the stop route to force stop due timeout. Then that timer task is still running in the background.\nAnd if you restart the timer route, then it will reuse the old timer instance, which may be still running, and therefore the first trigger time may not happen at the time you would expect.\nFor example from timer:foo?period=2s to trigger every 2s.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.timer.TimerEndpoint.java", "org.apache.camel.component.timer.TimerComponent.java", "org.apache.camel.component.timer.TimerConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7886, "bug_title": "Charset not correctly set from content-type", "bug_description": "When a \"Content-Type\" header is explicitely set with charset attached before invoking the camel http4 component, the charset is not taken into account.\nThis is because ContentType.create is invoked with the complete Content-Type when in fact it expects a mediaType.\n(see lines 414-416: camel-http4 version 2.14.0)\n   if (contentTypeString != null) \n{\n\n      contentType = ContentType.create(contentTypeString);\n\n   }\n\nFor example, if header is set to \"application/json; charset=UTF-8\" the contentType returned by ContentType.create(contentTypeString) is null.\nAnd therefore, further down (line 444)  charset is not extracted even though it has been specified.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.http4.HttpProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7910, "bug_title": "Netty {Client|Server}ChannelHandler need to pass the close and open event around", "bug_description": "If add other handler interesting about close and open event and they are set behind the netty  \n{Client|Server}ChannelHandler, they cannot not get the event as current netty  {Client|Server}\nChannelHandler  doesn&apos;t pass these event around.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.12.4", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.netty.handlers.ClientChannelHandler.java", "org.apache.camel.component.netty.handlers.ServerChannelHandler.java"], "label": 1, "es_results": []}, {"bug_id": 7107, "bug_title": "camel-sjms NullPointerException in case of connection loss", "bug_description": "When connection to a broker (HornetQ) is destroyed due to networking issues or broker internal issues or restarts the getProducers().borrowObject() returns null which is not handled properly.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.12.3", "fixed_files": ["org.apache.camel.component.sjms.producer.InOnlyProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7323, "bug_title": "createRouteStatisticEndpointJson - Returns invalid json if no routes", "bug_description": "If there is no routes, we have an extra } which should not be there\n\n\n\n{\n\n  \"routes\": {\n\n    }\n\n  }\n\n}\n\n\n\nParse error on line 5:\n... {            }}}\n-------------------^\nExpecting &apos;EOF&apos;", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.13.1", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.management.ManagedCamelContextTest.java"], "label": 1, "es_results": []}, {"bug_id": 7321, "bug_title": "JcrConsumer freezes in some environments", "bug_description": "In January Willem Jiang reported that the JUnit test JcrConsumerTest fails in his environment. This could not be reproduced in other environments at that time. Willem Jiang applied the following fix to the pom.xml in order make the test pass:\ncamel-jcr/pom.xml\n\n\n[...]\n\n  <build>\n\n    <plugins>\n\n     <plugin>\n\n        <groupId>org.apache.maven.plugins</groupId>\n\n        <artifactId>maven-surefire-plugin</artifactId>\n\n        <configuration>\n\n          <forkMode>pertest</forkMode>\n\n        </configuration>\n\n     </plugin>\n\n    </plugins>\n\n  </build>\n\n[...]\n\n\n\nThis made all tests run in separate processes, but was also an indicator for potential multi-threading issues in JcrConsumer. I was recently able to reproduce the problem by running Maven within Eclipse. The problem is indeed a synchronization issue and should be addressed with this ticket.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.13.1", "fixed_files": ["org.apache.camel.component.jcr.JcrConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7337, "bug_title": "PGPDataFormat unmarshal does not close the stream correctly", "bug_description": "The PGPDataFormat does not close some streams during unmarshaling.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.13.1", "fixed_files": ["org.apache.camel.converter.crypto.PGPKeyAccessDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 7399, "bug_title": "Camel Salesforce integration tests fail with ClassNotFoundException", "bug_description": "An upgrade to XStream version has broken the alias cache manipulation in XmlRestProcessor. Earlier versions of XStream used to only throw a ClassNotFoundException when CachingMapper.realClass call failed to find a mapped class, but recent versions now add the exception to its cache. This makes it imperative that the cache be flushed when no real class is found. As a result, the cache lookup optimization now only avoids flushing in case the alias mapping hasn&apos;t changed, which still works for repeated invocation of the same endpoint which requires using an XML alias for Salesforce&apos;s RESTish API. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.13.1", "fixed_files": ["org.apache.camel.component.salesforce.internal.processor.XmlRestProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 7398, "bug_title": "Salesforce recently changed their login API to use new OAuth fields, which breaks the camel-salesforce component", "bug_description": "Salesforce added new fields to the login API response. These fields should be added to the LoginToken DTO. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.13.1", "fixed_files": ["org.apache.camel.component.salesforce.internal.dto.LoginToken.java"], "label": 1, "es_results": []}, {"bug_id": 7409, "bug_title": "Camel ZipIterator should not eat the IOException", "bug_description": "If there are some thing wrong with the Zip file which is split by ZipIterator, we cannot find any warning or exception from the camel route. We should not let ZipIterator eat up the exception without do anything.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.dataformat.zipfile.ZipIterator.java", "org.apache.camel.dataformat.zipfile.ZipSplitterRouteIssueTest.java"], "label": 1, "es_results": []}, {"bug_id": 7427, "bug_title": "camel-netty-http component should skip reading the form body if it is bridgeEndpoint", "bug_description": "We need to fix the same issue of CAMEL-7426 in camel-netty-http", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.13.2", "fixed_files": ["org.apache.camel.component.netty.http.DefaultNettyHttpBinding.java", "org.apache.camel.component.netty.http.NettyHttpBridgeRouteUsingHttpClientTest.java", "org.apache.camel.component.netty.http.handlers.HttpServerChannelHandler.java", "org.apache.camel.component.netty.http.NettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7431, "bug_title": "Impossible to use an existing javax.mail.Session with DefaultJavaMailSender", "bug_description": "When using DefaultJavaMailSender with an existing javax.mail.Session instance (i.e. retrieved from JNDI) it is not possible to leave host and port unconfigured.\nMy JavaMailSender is configured as follows:\n    @Bean\n    public JavaMailSender mailSender() \n{\n\n        JavaMailSender jms = new DefaultJavaMailSender();\n\n        jms.setSession(mailSession);\n\n        jms.setProtocol(\"smtp\");\n\n        return jms;\n\n    }\n\nmailSession is a preconfigured Session instance retrieved from JNDI registry. I do not know about it&apos;s configuration.\nWhen calling the send() method, it tries to connect using the connect(host, port, username, password) method passing the (unconfigured) host and port which defaults to localhost:0 and then fails.\nIn case the session is supplied, it should call connect() instead.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.mail.DefaultJavaMailSender.java"], "label": 1, "es_results": []}, {"bug_id": 7518, "bug_title": "FileUtil.renameFile - should return true if file was renamed using copy/delete approach", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Bug-in-org-apache-camel-util-FileUtil-renameFile-introduced-in-2-13-0-tp5752450.html\nRegression introduced by CAMEL-6458", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.13.2", "fixed_files": ["org.apache.camel.util.FileUtil.java"], "label": 1, "es_results": []}, {"bug_id": 7546, "bug_title": "Avoid clash of CamelContext managementName in OSGi", "bug_description": "There can be a potential clash in OSGi when there are 2+ camelContext in the same bundle, as they will by default reuse the same managementName in the mbean naming, where it uses the symbolic name of the OSGi bundle.\nWe should detect this clash and use the counter to make it unique.\nThis is part of CAMEL-6972\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.13.2", "fixed_files": ["org.apache.camel.core.osgi.OsgiCamelContextPublisher.java", "org.apache.camel.core.osgi.OsgiManagementNameStrategy.java", "org.apache.camel.core.osgi.OsgiCamelContextNameStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 7407, "bug_title": "FTP route considers files as in progress after error in poll", "bug_description": "When an error occurs in the middle of polling (e.g. read timeout on ftp server), all files that have been read up to that point are stored in the inProgressRepospitory (last line GenericFileConsumer.isValidFile()). Due to the error, those files are not passed on to the ftp route, but remain in the inProgressRepository. The effect is that those files are never picked up unless we clear the inProgressRepository somehow (e.g. by restarting). When polling fails, files should not be allowed to remain in the inProgressRepository", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.file.GenericFileConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7418, "bug_title": "org.apache.camel.impl.JndiRegistry.findByTypeWithName", "bug_description": "I guess this line isn&apos;t correct:\nif (type.isInstance(pair.getClass()) || type.getName().equals(pair.getClassName()))\nThe variable \"pair.getClass()\" always returns \"javax.naming.NameClassPair\" or its subclasses and the method \"isInstance\" works only with Instances, but does not Classes.\n I think the correct code should be:\nif (type.isAssignableFrom(Class.forName(pair.getClassName())))\nI&apos;ve tried to test a transacted route, but i could not because the error: \nFailed to create route route1 at: >>> Transacted[] <<< in route: Route(route1)[[From[direct:start]] -> [Transacted[]]] because of No bean could be found in the registry of type: PlatformTransactionManager", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.14.0", "fixed_files": ["org.apache.camel.impl.JndiRegistry.java"], "label": 1, "es_results": []}, {"bug_id": 7582, "bug_title": "Python scripts with multiple statements return null", "bug_description": "As reported here: \nhttp://camel.465427.n5.nabble.com/Return-Value-from-Python-Script-Not-In-Message-Body-tt5724056.html#none\nIt seems that when calling python script that has more than a single statement  the return value from the script is null. Here is a simple example in groovy that  shows this:\n\n\n\npackage com.i am.examples\n\n\n\nimport org.apache.camel.*\n\nimport org.apache.camel.impl.*\n\nimport org.apache.camel.builder.*\n\n\n\nString script = URLEncoder.encode(&apos;\"Hello world!\"&apos;, \"UTF-8\") // this works - script returns \"Hello world!\"\n\n//String script = URLEncoder.encode(&apos;bar = \"baz\"; \"Hello world!\"&apos;, \"UTF-8\") // this fails - script returns null\n\n\n\nCamelContext camelContext = new DefaultCamelContext()\n\ncamelContext.addRoutes(new RouteBuilder() {\n\n        def void configure() {\n\n            from(\"direct:python\")\n\n            .to(\"language:python:\" + script)\n\n        }\n\n    }\n\n)\n\n\n\ncamelContext.start()\n\n\n\nProducerTemplate t = camelContext.createProducerTemplate()\n\ndef result = t.requestBody(&apos;direct:python&apos;, &apos;foo&apos;)\n\nprintln result\n\n\n\ncamelContext.stop()\n\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.14.0", "fixed_files": ["org.apache.camel.builder.script.ScriptBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 8100, "bug_title": "Collection should be optional for getDbStats", "bug_description": "Collection should be optional for getDbStats on route description", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.15.0", "fixed_files": ["org.apache.camel.component.mongodb.MongoDbEndpoint.java", "org.apache.camel.component.mongodb.MongoDbOperation.java", "org.apache.camel.component.mongodb.MongoDbOperationsTest.java", "org.apache.camel.component.mongodb.MongoDbProducer.java"], "label": 1, "es_results": []}, {"bug_id": 8437, "bug_title": "Simple bean call does not like parenthesis in parameter values", "bug_description": "The simple language fails calling methods with parameters that contains \")\".\nFirst use case, direct use:\n\n\n\n<route>\n\n    <from uri=\"timer:foo?repeatCount=1\"/>\n\n    <setBody>\n\n        <groovy>[\")\", 42]</groovy>\n\n    </setBody>\n\n    <setHeader headerName=\"test\">\n\n        <simple>${body.contains(\")\")}</simple>\n\n    </setHeader>\n\n    <log message=\"Body: ${body}, contains: ${header.test}\"/>\n\n</route>\n\n\n\nIt throws an error.\nSecond use case, with a usage of a parameter:\n\n\n\n<route>\n\n    <from uri=\"timer:foo?repeatCount=1\"/>\n\n    <setBody>\n\n        <groovy>[\")\", 42]</groovy>\n\n    </setBody>\n\n    <setProperty propertyName=\"query\">\n\n        <constant>)</constant>\n\n    </setProperty>\n\n    <setHeader headerName=\"test\">\n\n        <simple>${body.contains(${property.query})}</simple>\n\n    </setHeader>\n\n    <log message=\"Body: ${body}, contains: ${header.test}\"/>\n\n</route>\n\n\n\nIt doesn&apos;t throw any exception but doesn&apos;t work.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.15.1", "fixed_files": ["org.apache.camel.util.ObjectHelper.java", "org.apache.camel.component.bean.MethodInfo.java", "org.apache.camel.util.ObjectHelperTest.java"], "label": 1, "es_results": []}, {"bug_id": 7443, "bug_title": "Remote Print URI changed to UNC Name", "bug_description": "I&apos;m using a printing URI in the form of lpr://lh.abc.de:631/bbk-test (Hostname changed) and getting an PrintException:\nReason: javax.print.PrintException: No printer found with name: \\\\lhgoe.gbv.de\\bbk-test. Please verify that the host and printer are registered and reachable from this machine.\nWhich is true, since Camel Printer has changed my URI to a Windows UNC (see backslashes).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.0", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.printer.PrinterProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7478, "bug_title": "Simple Language - Length of array properties is not correctly evaluated", "bug_description": "If the exchange body is an array, then body.length returns correctly the length of the array. However, if the array is a property of an object, then not the correct value is returned:\nMyClass.java\n\n\npublic class MyClass {\n\n    public Object[] getMyArray() {\n\n        return new Object[]{\"Hallo\", \"World\", \"!\"};\n\n    }\n\n}\n\n\n\nAccessing the property myArray with Simple:\n\n\n\n<setHeader headerName=\"mySimpleHeader\">\n\n    <simple>body.myArray.length</simple>\n\n</setHeader>\n\n<log message=\"mySimpleHeader = ${header.mySimpleHeader}\" />\n\n\n\nJava:\n\n\n\nfinal ProducerTemplate template = main.getCamelTemplate();\n\ntemplate.sendBody(\"direct:start\", new MyClass());\n\n\n\nLog:\n\n[main] route1 INFO  mySimpleHeader = 1\n\n\n\nThe return value should be 3 instead of 1.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.language.simple.SimpleTest.java", "org.apache.camel.component.bean.BeanInfo.java", "org.apache.camel.language.bean.BeanExpression.java"], "label": 1, "es_results": []}, {"bug_id": 7483, "bug_title": "SmppConfiguration has wrong value setDataCoding", "bug_description": "// SmppConfiguration has wrong value setDataCoding\npublic void setDataCoding(byte dataCoding) \n{\n\n        this.alphabet = dataCoding;\n\n    }", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.smpp.SmppConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 7502, "bug_title": "camel-elastichsearch - starts up an instance even though IP specified", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/camel-elasticsearch-component-starts-up-an-instance-even-though-IP-specified-tp5751825.html\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.13.2", "fixed_files": ["org.apache.camel.component.elasticsearch.ElasticsearchEndpoint.java", "org.apache.camel.component.elasticsearch.ElasticsearchConfigurationTest.java"], "label": 1, "es_results": []}, {"bug_id": 7506, "bug_title": "[NettyHttp] Remove headerFilterStrategy option after resolving", "bug_description": "In Camel Netty HTTP component, we don&apos;t remove headerFilterStrategy option from the parameters, so it is propagated as a query parameter.\nWhen I create producer URL like this - netty-http:http://host.com?headerFilterStrategy=#headerFilterStrategy&foo=bar I expect only foo=bar to be send as a query to to endpoint (while headerFilterStrategy=#headerFilterStrategy should be resolved from the registry and removed from the parameters). This is how it works in Jetty component for example.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.component.netty.http.NettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7513, "bug_title": "Using JPA entities as the argument in Aggregator using POJO", "bug_description": "I have an Aggregator POJO with this method :\npublic Map<Hoteles, List<EventoPrecio>> agregaEventoPrecio(Map<Hoteles, List<EventoPrecio>> lista, EventoPrecio evento) \nWith this route :\nfrom(\"timer://tesipro?fixedRate=true&period=60000\").\nbeanRef(\"uploadARIService\", \"getEventosPrecio\").\naggregate(constant(true), AggregationStrategies.bean(AgregadorEventos.class, \"agregaEventoPrecio\")).\ncompletionSize(100).\nlog(\"Ejecucion de Quartz \");\nAnd I get this error :\nError occurred during starting Camel: CamelContext(249-camel-9) due Parameter annotations at index 1 is not supported on method: public java.util.HashMap com.tesipro.conectores.interfaces.tesiproconpush.camel.AgregadorEventos.agregaEventoPrecio(java.util.HashMap,com.tesipro.conectores.domain.EventoPrecio)          \nIt seems the problem is that annotations are not supported in the aggregator arguments nor in the argument class.\nhttps://github.com/apache/camel/blob/3f4f8e9ddcc8de32cca084927a10c5b3bceef7f9/camel-core/src/main/java/org/apache/camel/processor/aggregate/AggregationStrategyBeanInfo.java#L67", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.12.4", "fixed_files": ["org.apache.camel.processor.aggregator.AggregationStrategyBeanAdapterAllowNullTest.java", "org.apache.camel.processor.aggregate.AggregationStrategyBeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 7524, "bug_title": "CxfProducer created from wsdl detected as provider", "bug_description": "In custom bus extension following code used to detect web provider:\n\n\n\nJaxWsServiceFactoryBean factory =...;\n\n                JaxWsImplementorInfo jaxWsImplementorInfo = jaxwsfactory.getJaxWsImplementorInfo();\n\n                isProvider = null != jaxWsImplementorInfo && jaxWsImplementorInfo.isWebServiceProvider();\n\n\n\nBut org.apache.camel.component.cxf.WSDLServiceFactoryBean used setServiceClass(Provider.class); which because \n\n\n\norg.apache.cxf.jaxws.support.JaxWsImplementorInfo.isWebServiceProvider() {\n\n        return Provider.class.isAssignableFrom(implementorClass);\n\n    }\n\n\n\nreturn true always.\nI can&apos;t set ServiceClass for CxfEndpoint because another case will executed.\nI propose to override service class for this case", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.13.2", "fixed_files": ["org.apache.camel.component.cxf.CxfEndpoint.java", "org.apache.camel.component.cxf.WSDLServiceFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 7560, "bug_title": "camel-test - AdviceWith transacted does not work", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Problem-with-adviceWith-on-2-13-x-tp5752421.html\nThe work-around when using transacted is to set isUseAdvice return false.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.builder.AdviceWithTasks.java", "org.apache.camel.model.ProcessorDefinitionHelper.java"], "label": 1, "es_results": []}, {"bug_id": 7562, "bug_title": "camel-test - AdviceWith in CBR may add twice", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Camel-AdviceWith-issues-tp5752786.html\nWhen using advice-with for a CBR it may add to the when clauses 2 times.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.model.ProcessorDefinitionHelper.java"], "label": 1, "es_results": []}, {"bug_id": 7571, "bug_title": "Potential NPE in rabbitmq producer if header has null value and DEBUG logging enabled", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Bug-in-RabbitMQProducer-tp5753248.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.rabbitmq.RabbitMQProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7572, "bug_title": "Using custom id in CBR will output id duplicated when dumping route as XML using JMX", "bug_description": "If setting a custom id on a CBR then the dump route as xml JMX operation will include the id duplicated in the otherwise tag\n\n\n\n<routes xmlns=\"http://camel.apache.org/schema/spring\">\n\n    <route id=\"route1\">\n\n        <from uri=\"file:src/data?noop=true\"/>\n\n        <to uri=\"activemq:personnel.records\" customId=\"true\" id=\"amq\"/>\n\n    </route>\n\n    <route id=\"route2\">\n\n        <from uri=\"activemq:personnel.records\"/>\n\n        <choice customId=\"true\" id=\"myChoice\">\n\n            <when id=\"when1\">\n\n                <xpath>/person/city = &apos;London&apos;</xpath>\n\n                <to uri=\"file:target/messages/uk\" id=\"to1\"/>\n\n            </when>\n\n            <otherwise customId=\"true\" id=\"myChoice\">\n\n                <to uri=\"file:target/messages/others\" id=\"to2\"/>\n\n            </otherwise>\n\n        </choice>\n\n    </route>\n\n</routes>\n\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.model.RouteDefinitionHelper.java"], "label": 1, "es_results": []}, {"bug_id": 7577, "bug_title": "camel-zipfile - ZipIterator should be closable", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/File-component-does-not-delete-file-tp5753140p5753207.html\nThe ZipIterator should be Closable so the splitter eip closes it correctly.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.13.3", "fixed_files": ["org.apache.camel.dataformat.zipfile.ZipIterator.java"], "label": 1, "es_results": []}, {"bug_id": 7578, "bug_title": "camel-bindy - pattern attribute should not be ignored if locale is not set", "bug_description": "This code doesn&apos;t pad field mandant with \"0\" if locale is not set: \n\n\n\n@CsvRecord(separator = \",\")\n\npublic class Unity {\n\n    @DataField(pos = 1, pattern = \"000\")\n\n    public float mandant;\n\n\n\nRoute:\n\n\n\nfinal BindyCsvDataFormat bindy = new BindyCsvDataFormat(Unity.class);\n\nfrom(\"direct:start\")\n\n    .marshal(bindy)\n\n    .log(\"${body}\");\n\n\n\nTesting with:\n\n\n\nfinal Unity unity = new Unity();\n\nunity.mandant = 50f;\n\nfinal ProducerTemplate template = context.createProducerTemplate();\n\n\n\nThis prints:\n\n50.0\nOnly when setting the locale, pattern is not ignored:\n\n\n\nbindy.setLocale(Locale.US.getISO3Country());\n\n\n\nThis prints:\n\n050", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.dataformat.bindy.FormatFactory.java"], "label": 1, "es_results": []}, {"bug_id": 7448, "bug_title": "throttle EIP - unchanged value", "bug_description": "Throttler Documentation [1] states \"If the header is absent, then the Throttler uses the old value. So that allows you to only provide a header if the  value is to be changed\".\nhowever if the expression evaluates to null (header missing from message) the Throttler throws an exception (Throttler.java:108).\nThe workaround is to ensure that all messages carry the value (if the value is the same no changes will take affect). Adding an option to turn this on and off (e.g. allowNullException) would make it much easier to use (as per camel-users thread [2]).\n[1] http://camel.apache.org/throttler.html\n[2] http://camel.465427.n5.nabble.com/throttle-EIP-unchanged-value-td5751300.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.13.3", "fixed_files": ["org.apache.camel.processor.Throttler.java", "org.apache.camel.processor.ThrottlerNullEvalTest.java"], "label": 1, "es_results": []}, {"bug_id": 7757, "bug_title": "camel-restlet  2.13.1  throwing EOFException  on reading ZipInputStream ", "bug_description": "Please refer to : http://camel.465427.n5.nabble.com/Came-2-13-1-Reading-ZipInputStream-EOFException-tt5755726.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.restlet.DefaultRestletBinding.java"], "label": 1, "es_results": []}, {"bug_id": 8063, "bug_title": "Persistent tail tracking picks random tail tracker from mongoDB collection", "bug_description": "There is a bug the method \"initialize\" in \"MongoDbTailTrackingManager.java\": \n...\n if (trackingObj == null) \n{\n\n            dbCol.insert(filter, WriteConcern.SAFE);\n\n            trackingObj = dbCol.findOne();\n\n }\n...\nIf no tail tracking object exists in the database, a new one will be inserted (that&apos;s okay), but the query \"dbCol.findOne()\" fetches ANY tail tracking object from the database (and not the one we&apos;ve just inserted). \nIn my oppinition, this is a bug and should be corrected like this: \n...\nDBObject filter = new BasicDBObject(\"persistentId\", config.persistentId);\ntrackingObj = dbCol.findOne(filter);\n if (trackingObj == null) \n{\n\n            dbCol.insert(filter, WriteConcern.SAFE);\n\n            trackingObj = dbCol.findOne(filter);\n\n }\n\n(not tested).\nA workaround can be implemented by putting each persistent tail tracker in it&apos;s own collection, so I marked this issue as \"minor\".\nMore details: http://camel.465427.n5.nabble.com/MongoDB-Persistent-tail-tracking-with-concurrent-tailable-consumers-td5759131.html\nBest regards,\nJoerg Peschke", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.15.0", "fixed_files": ["org.apache.camel.component.mongodb.MongoDbTailTrackingManager.java"], "label": 1, "es_results": []}, {"bug_id": 7644, "bug_title": "Scala camel DSL creates numerous DefaultCamelContext instances", "bug_description": "Since the camel DSL is invoked prior to `.addRoutesToCamelContext(CamelContext)` being invoked there is no camel context set on the delegate java RouteBuilder which causes it to create a new context when the first dsl method is invoked.\nWith the implementation of CAMEL-7327 introduced in 2.13.1 which stores created camel contexts in a set in `Container.Instance#CONTEXT`; this causes instances of DefaultCamelContext to be leaked, they are never removed from the static set. This is especially aparrent during unit testing.\nThe following test shows that an additional context is registered for the scala route builder as opposed to java. Verification of the leak can be requires profiler and capturing of heap after termination of the test case (in ParentRunner.java).\n\n\n\npackage org.apache.camel.scala.dsl.builder;\n\n\n\nimport com.google.common.collect.Sets;\n\n\n\nimport org.apache.camel.CamelContext;\n\nimport org.apache.camel.ProducerTemplate;\n\nimport org.apache.camel.builder.RouteBuilder;\n\nimport org.apache.camel.impl.DefaultCamelContext;\n\nimport org.apache.camel.spi.Container;\n\nimport org.junit.After;\n\nimport org.junit.Before;\n\nimport org.junit.Test;\n\n\n\nimport java.lang.ref.WeakReference;\n\nimport java.util.Set;\n\n\n\nimport static org.junit.Assert.assertEquals;\n\n\n\npublic class BuggyScalaTest implements Container {\n\n\n\n  Set<CamelContext> managed = Sets.newHashSet();\n\n\n\n  @Before\n\n  public void setUp() throws Exception {\n\n    Container.Instance.set(this);\n\n  }\n\n\n\n  @After\n\n  public void tearDown() throws Exception {\n\n    Container.Instance.set(null);\n\n  }\n\n\n\n  @Test\n\n  public void testNameJava() throws Exception {\n\n    DefaultCamelContext defaultCamelContext = new DefaultCamelContext();\n\n    defaultCamelContext.addRoutes(new RouteBuilder() {\n\n      @Override\n\n      public void configure() throws Exception {\n\n        from(\"direct:start\").log(\"a message\");\n\n      }\n\n    });\n\n    defaultCamelContext.start();\n\n\n\n    ProducerTemplate producerTemplate = defaultCamelContext.createProducerTemplate();\n\n    producerTemplate.start();\n\n    producerTemplate.sendBody(\"direct:start\", \"\");\n\n    producerTemplate.stop();\n\n    defaultCamelContext.stop();\n\n\n\n    assertEquals(1, managed.size());\n\n  }\n\n\n\n  @Test\n\n  public void testNameScala() throws Exception {\n\n    DefaultCamelContext defaultCamelContext = new DefaultCamelContext();\n\n    defaultCamelContext.addRoutes(new SimpleRouteBuilder());\n\n    defaultCamelContext.start();\n\n\n\n    ProducerTemplate producerTemplate = defaultCamelContext.createProducerTemplate();\n\n    producerTemplate.start();\n\n    producerTemplate.sendBody(\"direct:start\", \"\");\n\n    producerTemplate.stop();\n\n    defaultCamelContext.stop();\n\n\n\n    assertEquals(1, managed.size()); // will equal 2\n\n  }\n\n\n\n  @Override\n\n  public void manage(CamelContext camelContext) {\n\n    managed.add(camelContext);\n\n  }\n\n}\n\n\n\n\n\n\n  package org.apache.camel.scala.dsl.builder\n\n\n\n  import org.apache.camel.scala.dsl.builder.RouteBuilder\n\n\n\n  class SimpleRouteBuilder extends RouteBuilder {\n\n    from(\"direct:start\").log(\"a message\")\n\n  }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.15.0", "fixed_files": ["org.apache.camel.main.Main.java", "org.apache.camel.main.MainSupport.java", "org.apache.camel.builder.BuilderSupport.java"], "label": 1, "es_results": []}, {"bug_id": 7565, "bug_title": "SFTP using PollEnrich with \"disconnect=true\" and \"delete=true\" does NOT delete the file", "bug_description": "Attached are two test cases - one with a \"non-pollEnrich\" test (which works fine) and one with a \"pollEnrich\" test (which fails).\nIn stepping through some of the code, it appears that the \"disconnect\" and \"delete\" are on two different threads (true for both scenarios).  However, for the \"non-pollEnrich\" test, there seems to be a cycle that allows the timing of the two threads to NOT be an issue.  For the \"pollEnrich\" test, that cycle doesn&apos;t seem to occur.  \nMy uneducated guess is that both tests (code executions) are checking to see if the \"from\" has completed (including performing the delete) before disconnecting.  This makes sense for the \"non-pollEnrich\" test, but for the \"pollEnrich\" it should be checking to see if the \"pollEnrich\" is done, not the \"from\".\nPlease note that if you do not indicate \"disconnect=true\", file deletion occurs as expected.  This seems to be broken in 2.12.x through 2.13.1 (not sure if it goes back further or not).\nI have attached two different test cases to show the different behaviors (\"non-pollEnrich\" vs \"pollEnrich\").", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.1", "fixed_version": "camel-2.15.1", "fixed_files": ["org.apache.camel.component.file.remote.RemoteFileConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7587, "bug_title": "MessageHistory stores passwords in plain text", "bug_description": "The MessageHistory feature currently keeps passwords in plain text in case they are part of the URI. \nMessageHelper.doDumpMessageHistoryStacktrace() does some sanitizing, but only for the from node - other nodes/processors are currently not sanitized. \nIn order to prevent handling sensitive information in the message history in general, I would suggest to sanitize the URI already when storing a MessageHistory item. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.util.MessageHelper.java"], "label": 1, "es_results": []}, {"bug_id": 7595, "bug_title": "camel-jdbc - Overrides with old headers when used the 2nd time in a route", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/I-m-having-problems-with-the-jdbc-component-header-CamelJdbcUpdateCount-tp5753590p5753603.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.jdbc.JdbcProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7650, "bug_title": "camel-restlet - Return 405 if method not allowed instead of 404", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Camel-Restlet-2-13-1-Consumer-Questions-tp5754698.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.restlet.MethodBasedRouter.java", "org.apache.camel.component.restlet.RestletMultiMethodsEndpointTest.java", "org.apache.camel.component.restlet.RestletRouteBuilderTest.java"], "label": 1, "es_results": []}, {"bug_id": 7679, "bug_title": "[cxfrs] Second argument is null when consumer invoke the bean with two arguments", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.bean.MethodInfo.java", "org.apache.camel.component.cxf.jaxrs.testbean.CustomerServiceResource.java"], "label": 1, "es_results": []}, {"bug_id": 7706, "bug_title": "Camel JGroups does not disconnect shared JChannel correctly", "bug_description": "Camel JGroups component uses a JGroups JChannel object in JGroupsEndpoint, which is shared by its JGroupsProducer and JGroupsConsumer. \nHowever, both the producer and consumer call disconnect() independently in their doStop() methods. This can leave a producer or consumer disconnected if the other is stopped.\nThe channel connect/disconnect could use a shared counter to determine when the channel should be disconnected. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.13.3", "fixed_files": ["org.apache.camel.component.jgroups.JGroupsProducer.java", "org.apache.camel.component.jgroups.JGroupsEndpoint.java", "org.apache.camel.component.jgroups.JGroupsConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7718, "bug_title": "Constants used in camel-infinispan component have restricted access", "bug_description": "Constants used for setting headers in camel-infinispan are not accessible in application because class containing them  is not public. \nThese constants are used in unit tests and also in example in the documentation of the component, so it seems this is minor mistake and also a bug.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.13.3", "fixed_files": ["org.apache.camel.component.infinispan.InfinispanConstants.java"], "label": 1, "es_results": []}, {"bug_id": 7715, "bug_title": "SjmsConsumer and SjmsProducer do not remove thread pool when stop", "bug_description": "SjmsConsumer and SjmsProducer always register a new ThreadPool on Camel context ExecutorServiceManager every time a new instance is created for an endpoint.  If consumer or producer is stopped or removed or even component is removed, thread pool still exists. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.sjms.SjmsProducer.java", "org.apache.camel.component.sjms.SjmsConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7735, "bug_title": "ProducerTemplate - Sending manually created Exchange causes 2x sent event notification", "bug_description": "Only when you manually create  the exchange and pass it to the producer template send method there will be 2x sent event in the notifier.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.impl.ProducerCache.java"], "label": 1, "es_results": []}, {"bug_id": 7736, "bug_title": "Failure to create producer during routing slip or similar eip causes exchange causes error handler not to react properly", "bug_description": "If an endpoint.createProducer throws an exception from a dynamic eip, then the exchange is kept marked as inflight, and the error handler does not react as soon as possible and as expected.\nThis was working in Camel 2.10.x etc.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.impl.ProducerCache.java"], "label": 1, "es_results": []}, {"bug_id": 7731, "bug_title": "DefaultBeanRowMapper broken", "bug_description": "DefaultBeanRowMapper current version does not upper-case when it should, due to what appears to be a copy paste error (note the toLowerCase in both branches)\nif (toUpper) \n{\n\n\tchar upper = Character.toLowerCase(ch);\n\n\tsb.append(upper);\n\n\t// reset flag\n\n\ttoUpper = false;\n\n}\n else \n{\n\n\tchar lower = Character.toLowerCase(ch);\n\n\tsb.append(lower);\n\n}\n\nshould be\n\nif (toUpper) {\n\n\tchar upper = Character.toUpperCase(ch);\n\n\tsb.append(upper);\n\n\t// reset flag\n\n\ttoUpper = false;\n\n} else {\n\tchar lower = Character.toLowerCase(ch);\n\tsb.append(lower);\n}", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.jdbc.DefaultBeanRowMapper.java"], "label": 1, "es_results": []}, {"bug_id": 7751, "bug_title": "Trace interceptor use add instead of remove in removeTraceHandler", "bug_description": "Class https://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/processor/interceptor/Tracer.java implements add instead of remove in method: \n    public void removeTraceHandler(TraceEventHandler traceHandler) \n{\n\n        this.traceHandlers.add(traceHandler);\n\n    }", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.processor.interceptor.Tracer.java"], "label": 1, "es_results": []}, {"bug_id": 7763, "bug_title": "NettyServerBootstrapConfiguration should provide default SSL settings", "bug_description": "Currently NettyServerBootstrapConfiguration doesn&apos;t provide default values of keyStoreFormat and securityProvider options. This is not in sync with description of these options in Netty doc (1).\nFor example the following configuration:\n\n\n\n<bean id=\"httpsConfiguration\" class=\"org.apache.camel.component.netty.http.NettySharedHttpServerBootstrapConfiguration\">\n\n  <property name=\"port\" value=\"10020\"/>\n\n  <property name=\"host\" value=\"0.0.0.0\"/>\n\n  <property name=\"keyStoreResource\" value=\"jsse/localhost.ks\"/>\n\n  <property name=\"trustStoreResource\" value=\"jsse/localhost.ks\"/>\n\n  <property name=\"passphrase\" value=\"changeit\"/>\n\n</bean>\t \n\n\n\n...won&apos;t work until I add keyStoreFormat and securityProvider explicitly to it:\n\n\n\n  <property name=\"keyStoreFormat\" value=\"JKS\"/>\n\n  <property name=\"securityProvider\" value=\"SunX509\"/>\n\n\n\nSolution:\nWe should add keyStoreFormat=JKS and securityProvider=SunX509 defaults to the NettyServerBootstrapConfiguration.\n(1) http://camel.apache.org/netty.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.14.0", "fixed_files": ["org.apache.camel.component.netty.NettyServerBootstrapConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 7665, "bug_title": "RabbitMq do not reject messages when consumer or endpoint fail.", "bug_description": "RabbitMQ does not execute command to reject a message when consumers or end point throws an exception or shutdown is executed by karaf.\nWe need execute channel.basicReject(deliveryTag, false) to send message to Dead Letter Exchange.\nIn current implementation the message is always acknowledged. \nScenario:\n\n\n\nfrom(\"rabbitmq://localhost:5672/myFailTest?\"\n\n+\"routingKey=myFailTest&queue=myFailTest&exchangeType=direct\"\n\n+ \"&vhost=test&durable=true&autoDelete=false\"\n\n+ \"&autoAck=false&username=guest&password=guest\")\n\n.to(\"ftp://localhost/notExists?connectTimeout=100&timeout=100\");\n\n\n\nUsing errorHandler(deadLetterChannel(\"..another queue\") the message is Redelivered in same queue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.14.0", "fixed_files": ["org.apache.camel.component.rabbitmq.RabbitMQConsumer.java", "org.apache.camel.component.rabbitmq.RabbitMQConstants.java", "org.apache.camel.component.rabbitmq.RabbitMQEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 7830, "bug_title": "RestLetHeaderFilterStrategy should filter the header of \"Transfer-Encoding\"", "bug_description": "As we don&apos;t set the transfer_encoding protocol header for the restlet service, we need to remove the transfer_encoding header which could let the client wait forever.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.13.3", "fixed_files": ["org.apache.camel.component.restlet.RestletSetBodyTest.java", "org.apache.camel.component.restlet.RestletHeaderFilterStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 7931, "bug_title": "JCR need to support multi-valued property", "bug_description": "Regarding to the issue at Camel User Forum as [[camel-jcr] The trouble about the JCR multi-valued property when using JCR_GET_BY_ID](http://camel.465427.n5.nabble.com/camel-jcr-The-trouble-about-the-JCR-multi-valued-property-when-using-JCR-GET-BY-ID-td5757029.html).  Here is the fix from https://github.com/apache/camel/pull/287.patch  from  charlee. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.component.jcr.JcrGetNodeByIdTest.java", "org.apache.camel.component.jcr.JcrProducer.java", "org.apache.camel.component.jcr.JcrNodePathCreationTest.java"], "label": 1, "es_results": []}, {"bug_id": 7935, "bug_title": "JcloudsPayloadConverter.toPayload(InputStream) cannot deal with FileInputStreamCache", "bug_description": "StackOverflowError if body is FileInputStreamCache.\nhttp://camel.465427.n5.nabble.com/camel-jclouds-StackOverflowError-if-body-is-FileInputStreamCache-tp5757810.html ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.13.3", "fixed_files": ["org.apache.camel.component.jclouds.JcloudsPayloadConverter.java"], "label": 1, "es_results": []}, {"bug_id": 7966, "bug_title": "Does not set the blank or empty string to the mail recipients", "bug_description": "Here is the user complain in the mailing list.\n\n\n\nI recently had a production issue where there was an exception thrown when \n\nthe CC or the BCC headers were set to \"\": \n\n\n\ncom.sun.mail.smtp.SMTPAddressFailedException: 554 5.1.1 Invalid recipient address \n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.13.3", "fixed_files": ["org.apache.camel.component.mail.MailBinding.java"], "label": 1, "es_results": []}, {"bug_id": 7981, "bug_title": "JMX - Routes with transacted does not enlist processor mbeans", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Not-all-processors-are-listed-in-JMX-preventing-detailed-route-statistics-profiling-tp5757634p5758257.html\nRoutes with < transacted > does not enlist mbeans under processor, but you have mbeans in routes / consumers etc.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.util.ServiceHelper.java", "org.apache.camel.processor.RedeliveryErrorHandler.java"], "label": 1, "es_results": []}, {"bug_id": 7977, "bug_title": "sftp compression option is not set rightly", "bug_description": "The setting of sftp compression should skip the space, according to the user report.\n\n\n\nsession.setConfig(\"compression.s2c\", \"zlib@openssh.com, zlib, none\");\n\nsession.setConfig(\"compression.c2s\", \"zlib@openssh.com, zlib, none\");\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.file.remote.SftpOperations.java"], "label": 1, "es_results": []}, {"bug_id": 7784, "bug_title": "Camel : RSS - Ignores posts with identical published or updated date.", "bug_description": "When running with a RSS feed which has multiple entries with the same date only the first one is read, the rest are ignored because of the date filter.  I have provided a rudimental fix and unit test for this issue, this hashes the entries to detect duplicates.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.rss.UpdatedDateFilter.java"], "label": 1, "es_results": []}, {"bug_id": 8086, "bug_title": "Possible memoryleak when convertBodyTo is used in a dynamicRouter", "bug_description": "We have implemented a while loop using a dynamicRouter.\nThe dynamicRouter looks like this:\n<dynamicRouter>\n  <header>someheadername</header>\n</dynamicRouter>\nwhere someheadername refers to another route using direct:routename\nThe route that handles direct:routename looks like this:\n<bean ref=\"someref\"/>\n<convertBodyTo type=\"java.lang.String\"/>\nThe someref-bean just puts some data in the body and header and would also be responsible to set the value of someheadername=null to exit the dynamicRouter.\nDuring execution of these routes we see that heapusage increases until OOM if the dynamicRouter does not exit before OOM. The number of instances of DefaultMessage also keeps increasing.\nIf we remove the <convertBodyTo> from the route we do not get OOM and the number of instances of DefaultMessage is stable and low.\nThe same also happens if we replace <convertBodyTo> with a <transform>.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.builder.sql.SqlBuilder.java", "org.apache.camel.processor.TransformProcessor.java", "org.apache.camel.processor.SetBodyProcessor.java", "org.apache.camel.processor.ConvertBodyProcessor.java", "org.apache.camel.util.ExchangeHelper.java"], "label": 1, "es_results": []}, {"bug_id": 7821, "bug_title": "BridgePropertyPlaceholderConfigurer fails to use custom PropertiesParser", "bug_description": "A BridgePropertyPlaceholderConfigurer configured to use a custom PropertiesParser like this:\n\n\n\n  <bean id=\"bridgePropertyPlaceholder\" class=\"org.apache.camel.spring.spi.BridgePropertyPlaceholderConfigurer\"\n\n      p:location=\"classpath:application.properties\"\n\n      p:parser-ref=\"jasyptParser\" />\n\n\n\nfails to actually use the parser to parse properties.\nIt seems that org.apache.camel.spring.CamelContextFactoryBean#initPropertyPlaceholder, which is called from afterPropertiesSet, overwrites the custom parser with the default parser from the properties component (on line 279).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.spring.spi.BridgePropertyPlaceholderConfigurer.java"], "label": 1, "es_results": []}, {"bug_id": 7678, "bug_title": "Update camel-rabbitmq URI for consumers", "bug_description": "The camel-rabbitmq requires a mandatory amqp exchange as the first parameter in the URI.  Other options are specfied after the \"?.  This is appropriate for producers, but not for consumers.  Subscribers should specify the queue name as the first parameter.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.restlet.DefaultRestletBinding.java", "org.apache.camel.component.rabbitmq.RabbitMQEndpointTest.java", "org.apache.camel.component.rabbitmq.RabbitMQComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7989, "bug_title": "FileIdempotentRepository should create the file store on startup", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Problems-configuring-FileIdempotentRepository-tp5758212.html\nThe file store should be created on startup so the file store is always available.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.processor.FileIdempotentConsumerCreateRepoTest.java", "org.apache.camel.processor.idempotent.FileIdempotentRepository.java"], "label": 1, "es_results": []}, {"bug_id": 7988, "bug_title": "file consumer - Should call abort in case read lock cannot be acquired if exception was thrown", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Possible-issue-with-FileLockExclusiveReadLockStrategy-leaves-orphaned-camelLock-file-tp5758142.html\nThis could cause a .camelLock orphaned file to be there causing the file to not be eligible for consuming on next poll.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.file.GenericFileConsumer.java", "org.apache.camel.component.file.strategy.FileLockExclusiveReadLockStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 8030, "bug_title": "camel-netty need to release the thread pool when shutdown", "bug_description": "CAMEL-7915 introduced a side effect that doesn&apos;t shutdown the thread pool when the netty producer is shutdown.\nWe need to clean up these thread when shutting down the Netty endpoint.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.netty.NettyWorkerPoolBuilder.java", "org.apache.camel.component.netty.NettyProducer.java", "org.apache.camel.component.netty.NettyClientBossPoolBuilder.java", "org.apache.camel.component.netty.NettyServerBossPoolBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 8033, "bug_title": "Exchange Leak Caused By pollEnrich", "bug_description": "There can be a potential issue in PollEnricher as it should use a try .. catch to deal with any runtime exceptions that may be thrown from the polling consumer api.\nThis is mandated by the async routine engine.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.processor.PollEnricher.java"], "label": 1, "es_results": []}, {"bug_id": 8036, "bug_title": "JettyComponent should not setup the security handler more than once", "bug_description": "As we create multiple consumer for the rest component, it could introduce an issue that camel could add security handler more than once if user setup the security handler on the rest endpoint.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.jetty.HttpBasicAuthComponentConfiguredTest.java", "org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 8073, "bug_title": "Camel may clear attachments during routing", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Camel-cleans-attachments-if-error-is-thrown-during-routing-tp5759410.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.impl.MessageSupport.java"], "label": 1, "es_results": []}, {"bug_id": 8081, "bug_title": "Multicast Aggregator should keep processing other exchange which is not timeout", "bug_description": "It makes sense the multicast aggregator keep processing the exchange even some exchange are timeout. \nHere is a thread in the camel user mailing list talks about it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.processor.MulticastParallelAllTimeoutAwareTest.java", "org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 8082, "bug_title": "CxfRs producer should close the connection if MEP is InOnly", "bug_description": "When using cxfRs to send a rest message in InOnly mode, Camel do not close\nthe javax.ws.rs.core.Response object. As we are InOnly, the reponse object is not passed through the exchange, so the caller can not close it either.\nHere is the mail thread which talks about it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.CxfRsProducer.java"], "label": 1, "es_results": []}, {"bug_id": 8092, "bug_title": "cxf:producer Matrix Params are missing", "bug_description": "In cxfrs component the producer strips away the matrix param.\nSteps to reproduce:\nWeb service proxying configuration (camel-proxy-example). \n<camelContext id=\"camel\" xmlns=\"http://camel.apache.org/schema/spring\">\n<route>\n<from uri=\"cxfrs://bean://rsServer?bindingStyle=SimpleConsumer\"/>  \n<to uri=\"cxfrs://bean://rsClient\"/>  \n</route>  \n</camelContext>\nBut we can see that the Matrix Parameters are completely stripped away in CxfRsProducer.  The SimpleConsumer as the binding style the matrix param will be sent as header params.  But shouldn&apos;t the matrix params part of the URL. \nID: 15 \nAddress: http://localhost:8001/test/services/proxyServer/boxing;state=Current\nHttp-Method: GET \nContent-Type: \nHeaders: \n{Accept=[application/xml], accept-encoding=[gzip, deflate], accept-language=[en-us], Authorization=[xxxx], connection=[keep-alive], Content-Length=[0], Content-Type=[null], host=[localhost:8001]}\n \n-------------------------------------- \n--------------------------- \nID: 16 \nAddress: http://localhost:29090/MyServer/boxing\nHttp-Method: GET \nContent-Type: / \nHeaders: \n{Accept=[application/xml], state=[Current],  user-agent=[xxxxxx], accept-encoding=[gzip, deflate], Content-Length=[0], Authorization=[xxxxx], org.apache.cxf.request.uri=[/services/proxyServer/boxing;state=Current], host=[localhost:8001], connection=[keep-alive], accept-language=[en-us], org.apache.cxf.message.Message.PATH_INFO=[/boxing], org.apache.cxf.request.method=[GET], Content-Type=[*/*]}\n \nI have came across similar issue posted in the past.  CAMEL-5405 CXF Transport loses HTTP Matrix parameters  But we are using camel 2.14.0 version. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.CxfRsProducer.java"], "label": 1, "es_results": []}, {"bug_id": 8121, "bug_title": "Infinite Loop Within Camel if the temp file directory is not writable", "bug_description": "If the target &apos;tmp&apos; directory (specified by java.io.tmpdir) used by Camel, does not have proper privileges, then Camel enters infinite loop. The offending code is found in org.apache.camel.util.FileUtil.createNewTempDir().\nWe need to check the if the temp file is writable.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.util.FileUtil.java"], "label": 1, "es_results": []}, {"bug_id": 8123, "bug_title": "Mina2 SSL initiates handshake backwards", "bug_description": "The Mina2Consumer and Mina2Producer are configured backwards for SSL handshake initiation. The consumer is trying to initiate the handshake rather than the producer. You can see this by setting up a SSL enabled consuming route and telnet to the port, you will immediately get some data sent from the server to the client rather than the server waiting on the client to initiate the handshake.\nThe issue is Mina2Consumer:160, it is setting UseClientMode to true, it should be false. Mina2Producer line 313 needs to also change to the following:\n            SslFilter filter = new SslFilter(configuration.getSslContextParameters().createSSLContext(), configuration.isAutoStartTls());\nfilter.setUseClientMode(true);\nconnector.getFilterChain().addFirst(\"sslFilter\", filter);\nAs far as I can tell this has been a bug since this component was introduced.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.mina2.Mina2Producer.java", "org.apache.camel.component.mina2.Mina2Consumer.java"], "label": 1, "es_results": []}, {"bug_id": 8134, "bug_title": "We should not add synchronisation if the CachedOutputStream closedOnCompletion option is false", "bug_description": "CachedOutputStream adds synchronization into exchange even the closedOnCompletion option is false, it could cause OOM error with the HttpProducer endpoint does \"unlimited/limited\" redelivery. \nHere is the discussion about this issue. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.converter.stream.CachedOutputStreamTest.java", "org.apache.camel.converter.stream.CachedOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 8156, "bug_title": "SNSClient should setup endpoint before creating the topic", "bug_description": "When using camel-aws-sns endpoint, it always create the topic on the default endpoint.\nHere is the mailing thread about it ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.aws.sns.SnsEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 8168, "bug_title": "Mina Does Not Unbind From Listening Port", "bug_description": "If a bundle includes a mina consumer as the one below, when you stop the bundle, the port is not released. Therefore, when restarting the bundle, you&apos;ll get a port already in use bind exception.\n\n\n\n<from uri=\"mina2:tcp://0.0.0.0:12345?sync=true\"/>\n\n\n\nNote that the problem does not exist if you&apos;re using an IP address other than 0.0.0.0. For example:\n\n\n\n<from uri=\"mina2:tcp://{{host.name}}:12345?sync=true\"/>\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.mina2.Mina2Consumer.java", "org.apache.camel.component.mina.MinaTcpTest.java", "org.apache.camel.component.mina2.Mina2TcpTest.java"], "label": 1, "es_results": []}, {"bug_id": 8327, "bug_title": "ContextTestSupport does not support weaveByType in test cases", "bug_description": "Update AdviceWithTypeTest test class to contain the below. On Camel-2.13.2 and below the test case would pass, on 2.13.3 and above the test case fails.\nAdviceWithTypeTest.java\n\n\n    public void testUnknownType2() throws Exception {\n\n       \n\n        context.getRouteDefinitions().get(0).adviceWith(context, new AdviceWithRouteBuilder() {\n\n            @Override\n\n            public void configure() throws Exception {\n\n                weaveByType(ChoiceDefinition.class).replace().to(\"mock:baz\");\n\n            }\n\n        });\n\n       \n\n        getMockEndpoint(\"mock:baz\").expectedMessageCount(1);\n\n        template.sendBody(\"direct:start\", \"World\");\n\n        assertMockEndpointsSatisfied();\n\n    }\n\n \n\n \n\n    @Override\n\n    protected RouteBuilder createRouteBuilder() throws Exception {\n\n        return new RouteBuilder() {\n\n            @Override\n\n            public void configure() throws Exception {\n\n                // START SNIPPET: e5\n\n                from(\"direct:start\")\n\n                    .transform(simple(\"Hello ${body}\"))\n\n                    .log(\"Got ${body}\")\n\n                    .to(\"mock:result\")\n\n                    .choice()\n\n                    .when(header(\"foo\").isEqualTo(\"bar\"))\n\n                       .to(\"mock:resultA\")\n\n                    .otherwise()\n\n                       .to(\"mock:resultB\");\n\n                // END SNIPPET: e5\n\n            }\n\n        };\n\n    }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.3", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.model.ProcessorDefinitionHelperTest.java", "org.apache.camel.model.ProcessorDefinitionHelper.java"], "label": 1, "es_results": []}, {"bug_id": 8519, "bug_title": "Salesforce component security listener does not replace old auth header", "bug_description": "The security listener incorrectly adds a copy of the auth header for streaming api client. Although this causes the exchange to ultimately fail, the BayeuxClient reconnects and proceeds without raising errors in Camel route. However, this ends up logging multiple warnings from the security listener and the BayeuxClient. \nFixing the security listener to reset the auth header will optimize this process and let the BayeuxClient continue with the existing exchange. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.13.4", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.salesforce.internal.client.SalesforceSecurityListener.java", "org.apache.camel.component.salesforce.internal.streaming.SubscriptionHelper.java"], "label": 1, "es_results": []}, {"bug_id": 7826, "bug_title": "Olingo2 component sets incorrect value for \"id\" element in create entry endpoint", "bug_description": "The serviceRoot property is not set correctly to the OData base URI when creating the EntityProviderWriteProperties, which causes the generated links in body to be incorrect. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.component.olingo2.api.impl.Olingo2AppImpl.java", "org.apache.camel.component.olingo2.api.Olingo2AppIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 7829, "bug_title": "Olingo2 component should handle 200 OK responses from OData servers for PUT method", "bug_description": "Some OData servers, like the JBoss Data Virtualization server based on OData4J library respond with HTTP 200 OK instead of 204 No Content for PUT methods. \nThe component needs to be able to handle this scenario. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.component.olingo2.api.impl.Olingo2AppImpl.java"], "label": 1, "es_results": []}, {"bug_id": 7839, "bug_title": "Xpath is not namespace aware in choice", "bug_description": "I have a route (XML definition) containing a choice based on xpath expressions. The xpath expressions are using custom namespaces. After migration from camel 2.13.2 to camel 2.14.0 the namespaces are not registered to the XpathExpression and XPathBuilder anymore.\nXPath in \"setProperty\" definitions referencing the same namespaces still work fine.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.model.ProcessorDefinitionHelperTest.java", "org.apache.camel.model.ProcessorDefinitionHelper.java"], "label": 1, "es_results": []}, {"bug_id": 7842, "bug_title": "Avoid using InputStreamEntity for byte[] input", "bug_description": "Sending byte[] input via http in some circumstances causes ugly NonRepeatableEntityException because of the internal usage of InputStreamEntity in the HttpEntityConverter which is not repeatable. The usage of ByteArrayEntity seems more resonable for this input.\nA workaround is to explicitly convertbodyto string but this is taken with care because of encoding when creating string and StringEntity later on!", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.http4.HttpEntityConverter.java"], "label": 1, "es_results": []}, {"bug_id": 7866, "bug_title": "barcodeFormat is always QR CODE", "bug_description": "I&apos;m testing the camel-barcode component with the following Spring XML.\n\n\n\n    <camelContext xmlns=\"http://camel.apache.org/schema/spring\">\n\n        <dataFormats>\n\n            <barcode id=\"QR_CODE\" width=\"200\" height=\"200\" imageType=\"JPG\" barcodeFormat=\"QR_CODE\"/>\n\n            <barcode id=\"PDF_417\" width=\"200\" height=\"200\" imageType=\"JPG\" barcodeFormat=\"PDF_417\"/>\n\n        </dataFormats>\n\n\n\n        <route>\n\n            <from uri=\"direct:QR_CODE\"/>\n\n            <marshal ref=\"QR_CODE\"/>\n\n            <to uri=\"file:target/out?fileName=qr_code.jpg\"/>\n\n        </route>\n\n    \n\n        <route>\n\n            <from uri=\"direct:PDF_417\"/>\n\n            <marshal ref=\"PDF_417\"/>\n\n            <to uri=\"file:target/out?fileName=pdf_417.jpg\"/>\n\n        </route>    \n\n    </camelContext>\n\n\n\nBut pdf_417.jpg seems to be QR CODE, not PDF 417.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.dataformat.barcode.BarcodeTestBase.java", "org.apache.camel.model.dataformat.BarcodeDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 7870, "bug_title": "[camel-barcode] Writer/Reader hints should be re-optimized when setBarcodeFormat method called.", "bug_description": "BarcodeDataFormat#setBarcodeDataFormat() should optimize writer/reader hints such like setBarcodeImageType().\nsee https://github.com/apache/camel/pull/284", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.dataformat.barcode.BarcodeDataFormatTest.java", "org.apache.camel.dataformat.barcode.BarcodeDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 7868, "bug_title": "wrong concatenation of parameters in JettyHttpComponent", "bug_description": "See http://camel.465427.n5.nabble.com/bug-restConfiguration-jetty-endpointProperty-td5757065.html for details.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7869, "bug_title": "endpointProperty does not work (restConfiguration - jetty)", "bug_description": "See http://camel.465427.n5.nabble.com/bug-restConfiguration-jetty-endpointProperty-td5757065.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 7855, "bug_title": "If you suspend a JMS route that is stopped, calling resume on it does not work and reports no error", "bug_description": "If a route that consumes from JMS is in state Stopped, and first Suspend and then Resume are called upon it, the route reports itself as being in state Started. However, it is not actually consuming anything from the endpoint.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.12.5", "fixed_files": ["org.apache.camel.component.jms.JmsConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7874, "bug_title": "json DataFormat: The prettyPrint option does not work as expected", "bug_description": "See http://camel.465427.n5.nabble.com/Problems-prettyPrinting-JSON-after-camel-2-14-0-upgrade-td5756738.html#a5757104 for a background.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.component.jackson.JacksonJsonDataFormatTest.java", "org.apache.camel.component.jackson.SpringJacksonJsonDataFormatTest.java", "org.apache.camel.component.jackson.JacksonMarshalTest.java", "org.apache.camel.model.dataformat.JsonDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 7876, "bug_title": "GroupIterator should respect the exchange's CHARSET_NAME property.", "bug_description": "Line 141 of GroupIterator calls toString on it&apos;s ByteArrayOutputStream, thereby utilising the default encoding set for the JVM.\nThis can cause issues when say splitting an exchange&apos;s body using token/groups with a different encoding from that of the JVM&apos;s default.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.15.0", "fixed_files": ["org.apache.camel.util.GroupIterator.java", "org.apache.camel.util.GroupIteratorTest.java", "org.apache.camel.builder.ExpressionBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 7899, "bug_title": "camel-jetty should support to define multiple http method for the rest service", "bug_description": "Camel complains Failed to start route route3 because of Multiple consumers for the same endpoint is not allowed, when define the rest service on the same path.\n\n\n\n restConfiguration()\n\n                .component(\"jetty\")\n\n                .host(\"localhost\").port(8080)\n\n                .contextPath(\"rest\")\n\n                .bindingMode(RestBindingMode.auto)\n\n                .dataFormatProperty(\"prettyPrint\", \"true\");\n\n        rest(\"/say\")\n\n                .get(\"/hello\").to(\"direct:hello\")\n\n                .get(\"/bye\").consumes(\"application/json\").to(\"direct:bye\")\n\n                .post(\"/bye\").to(\"direct:bye\");\n\n\n\n        from(\"direct:hello\")\n\n                .transform().constant(\"Hello World\");\n\n        from(\"direct:bye\")\n\n                .transform().constant(\"Bye World\");\n\n\n\nHere is the discussion in the nabble.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.component.netty4.http.DefaultContextPathMatcher.java", "org.apache.camel.component.netty4.http.rest.RestNettyHttpPojoInOutTest.java", "org.apache.camel.component.netty4.http.rest.RestPathMatchingTest.java", "org.apache.camel.component.netty4.http.RestContextPathMatcher.java", "org.apache.camel.component.netty4.http.handlers.HttpServerMultiplexChannelHandler.java", "org.apache.camel.component.netty.http.RestContextPathMatcher.java", "org.apache.camel.component.netty.http.rest.RestNettyHttpPojoInOutTest.java", "org.apache.camel.component.netty.http.DefaultContextPathMatcher.java", "org.apache.camel.component.netty.http.handlers.HttpServerMultiplexChannelHandler.java", "org.apache.camel.component.netty.http.rest.RestPathMatchingTest.java", "org.apache.camel.component.servlet.rest.RestServletPojoInOutTest.java", "org.apache.camel.component.jetty.JettyHttpComponent.java", "org.apache.camel.component.http.HttpServletResolveConsumerStrategy.java", "org.apache.camel.component.jetty.rest.RestJettyPojoInOutTest.java", "org.apache.camel.component.http.CamelServlet.java"], "label": 1, "es_results": []}, {"bug_id": 7890, "bug_title": "XmlConverter.toSAXSourceFromStream does not set setNamespaceAware", "bug_description": "XmlConverter.toSAXSourceFromStream does not set\n\n\n\nsfactory.setNamespaceAware(true);\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.13.3", "fixed_files": ["org.apache.camel.converter.jaxp.XmlConverter.java"], "label": 1, "es_results": []}, {"bug_id": 7900, "bug_title": "hdfs2 - chunkSize not honored", "bug_description": "camel-hdfs2 component exposes chunkSize param, but that value is not correctly used in \nhttps://github.com/apache/camel/blob/master/components/camel-hdfs2/src/main/java/org/apache/camel/component/hdfs2/HdfsFileType.java#L79\nand the constant value:\n\n\n\npublic static final int DEFAULT_BUFFERSIZE = 4096;\n\n\n\nalways overrides it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.13.3", "fixed_files": ["org.apache.camel.component.hdfs.HdfsFileType.java", "org.apache.camel.component.hdfs2.HdfsFileType.java"], "label": 1, "es_results": []}, {"bug_id": 7916, "bug_title": "OsgiServiceRegistry forces name property", "bug_description": "Several components does search the registry by calling findByTypeWithName. This method needs in OsgiServiceRegistry that every service has a name property which is not forced by spec and therefor optional. Maybe add a different \"name\" in those cases.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.13.3", "fixed_files": ["org.apache.camel.core.osgi.OsgiServiceRegistry.java"], "label": 1, "es_results": []}, {"bug_id": 7973, "bug_title": "CircuitBreakerLoadBalancer fails on async processors", "bug_description": "The CircuitBreakerLoadBalancer works fine on direct synchronous processor, but it seems to not behave as expected in case of async processor.\nTo reproduce the error, it&apos;s enough to add a .threads(1) before the mock processor in the CircuitBreakerLoadBalancerTest routeBuilder configuration.\nThis misbehaviour seems to be related to the use of the AsyncProcessorConverterHelper to force any processor to behave like asynchronous. \nI&apos;m going to propose a patch with the failing test and a proposal of solution.\nEDIT:\nthe patch contains the fix also to other unexpected behaviour of the CircuitBreaker.\nThe second problem addressed is that, after the opening of the circuit, the RejectedExecutionException raised by the circuit breaker is set in the Exchange, but it doesn&apos;t return. This because the processor will receive the Exchange even if the circuit is open. In this case also, if the CircuitBreaker is instructed to react only to specific Exception, it will close the circuit after the following request, because the raised exception would be a RejectedExecutionException instead of the one specified in the configuration.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.processor.CircuitBreakerLoadBalancerTest.java", "org.apache.camel.processor.loadbalancer.CircuitBreakerLoadBalancer.java"], "label": 1, "es_results": []}, {"bug_id": 7990, "bug_title": "IdempotentConsumer - If no messageId should allow Camel error handler to react", "bug_description": "See SO\nhttp://stackoverflow.com/questions/26453348/camel-onexception-does not-catch-nomessageidexception-of-idempotentconsumer\nThe idempotent consumer should set the exchange on the exchange and invoke the callback, that is an internal routing engine bug in the implementation of that eip.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.processor.idempotent.IdempotentConsumer.java", "org.apache.camel.processor.IdempotentConsumerNoMessageIdTest.java"], "label": 1, "es_results": []}, {"bug_id": 7968, "bug_title": "Container has undefined concurrency behaviour", "bug_description": "The implementation of Container.Instance is not ThreadSafe. It is also not defined what happens when multiple Containers race on the singleton.\nInstead of using a Container singleton approach. It might be better to have a singleton ContainerRegistry that can handle concurrent/multiple Containers", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.15.0", "fixed_files": ["org.apache.camel.spi.Container.java"], "label": 1, "es_results": []}, {"bug_id": 7994, "bug_title": "SJMSComponent effectively ignores setConnectionCount()", "bug_description": "SJMSComponent effectively ignores setConnectionCount()\nThis is because the JmsProducer, by default, pre-populates sessions+producers on Producer.doStart(), which is called by the CamelContext in a single thread and the way the ConnectionFactoryResource is configured, it returns the most recently idle connection (So really it always returns the first connection opened). So all sessions being pre-populated by all SJMS endpoints always use the same single connection.\nThis patch attached makes the ConnectionFactoryResource effectively return connections in a round-robbin manor. Unit test is included.\nThis is important because TCP has a certain overhead per socket which makes it impossible to achieve much more than 6k~ messages a second, regardless of the CPU and IO resources available to a broker. To fully utilize a broker multiple connections must be opened.\nThis is also important if your ConnectionFactory represent a cluster (Like in the case of HornetQ), where subsequent connections from the factory are balanced across nodes in the cluster.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.15.0", "fixed_files": ["org.apache.camel.component.sjms.jms.ConnectionFactoryResource.java", "org.apache.camel.component.sjms.SjmsProducer.java", "org.apache.camel.component.sjms.jms.ConnectionFactoryResourceTest.java"], "label": 1, "es_results": []}, {"bug_id": 8062, "bug_title": "camel-rx - EndpointSubscriber does not call unsubscribe to stop the consumer", "bug_description": "When rx is done with the subscription it does not trigger the unsubscribe callback so we can stop the consumer.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.rx.support.EndpointSubscription.java", "org.apache.camel.rx.support.EndpointSubscribeFunc.java"], "label": 1, "es_results": []}, {"bug_id": 8049, "bug_title": "DefaultRestletBinding can not deal with multi-valued HTTP request parameters", "bug_description": "Multi-valued HTTP request parameters are a common practice yet, with the provided binding, Camel Restlet component can not deal with them.\nIn DefaultRestletBinding, we can see the following problematic code:\n\n\n\npublic void populateRestletRequestFromExchange(Request request, Exchange exchange) {\n\n...\n\n     form.add(key, value.toString());\n\n...\n\n}\n\n\n\nwhere the value is always treated as a String, even though it can be a Collection.\nAnd similarly:\n\n\n\npublic void populateExchangeFromRestletRequest(Request request, Response response, Exchange exchange) throws Exception {\n\n...\n\n     for (Map.Entry<String, String> entry : form.getValuesMap().entrySet()) {\n\n          ...\n\n     }\n\n...\n\n}\n\n\n\nwhere getValuesMap() effectively disregards all duplicate key names.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.restlet.DefaultRestletBinding.java"], "label": 1, "es_results": []}, {"bug_id": 7922, "bug_title": "MQTT endpoint misses QoS > 0 messages due to startup timing issue", "bug_description": "When the MQTT Endpoint is started the MQTT connection is immediately established, causing an immediate influx of persisted messages (put on the topic when the client was not available). \nIssue is that at this point, most likely no consumers are available yet to process these messages.\nReceiving a PUBLISH message\nPublish message are received without any consumers. Result : message with QoS > 0 that were put on the topic while the client was not connected are never processed.\n\nDaemon Thread [hawtdispatch-DEFAULT-3] (Suspended (breakpoint at line 815 in CallbackConnection))\t\n\n\tCallbackConnection.toReceiver(PUBLISH) line: 815\t\n\n\tCallbackConnection.processFrame(MQTTFrame) line: 732\t\n\n\tCallbackConnection.access$1500(CallbackConnection, MQTTFrame) line: 51\t\n\n\tCallbackConnection$6.onTransportCommand(Object) line: 392\t\n\n\tTcpTransport.drainInbound() line: 709\t\n\n\tTcpTransport$6.run() line: 588\t\n\n\tNioDispatchSource$3.run() line: 209\t\n\n\tSerialDispatchQueue.run() line: 100\t\n\n\tSimpleThread.run() line: 77\t\n\n\n\nNo consumers registered yet\nOnly when this finishes will Camel be able to process the messages.\n\nDaemon Thread [localhost-startStop-1] (Suspended (breakpoint at line 164 in MQTTEndpoint))\t\n\n\towns: SpringCamelContext  (id=92)\t\n\n\towns: Object  (id=143)\t\n\n\towns: StandardContext  (id=144)\t\n\n\tMQTTEndpoint.addConsumer(MQTTConsumer) line: 164\t\n\n\tMQTTConsumer.doStart() line: 35\t\n\n\tMQTTConsumer(ServiceSupport).start() line: 61\t\n\n\tSpringCamelContext(DefaultCamelContext).startService(Service) line: 2158\t\n\n\tSpringCamelContext(DefaultCamelContext).doStartOrResumeRouteConsumers(Map<Integer,DefaultRouteStartupOrder>, boolean, boolean) line: 2452\t\n\n\tSpringCamelContext(DefaultCamelContext).doStartRouteConsumers(Map<Integer,DefaultRouteStartupOrder>, boolean) line: 2388\t\n\n\tSpringCamelContext(DefaultCamelContext).safelyStartRouteServices(boolean, boolean, boolean, boolean, Collection<RouteService>) line: 2318\t\n\n\tSpringCamelContext(DefaultCamelContext).doStartOrResumeRoutes(Map<String,RouteService>, boolean, boolean, boolean, boolean) line: 2091\t\n\n\tSpringCamelContext(DefaultCamelContext).doStartCamel() line: 1951\t\n\n\tSpringCamelContext(DefaultCamelContext).doStart() line: 1777\t\n\n\n\nThese messages will never be picked up.\nPerhaps it&apos;s more the responsibility of the consumer / producer to start a connection when they get attached to the endpoint ? ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.mqtt.MQTTConsumer.java", "org.apache.camel.component.mqtt.MQTTProducer.java", "org.apache.camel.component.mqtt.MQTTEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 8053, "bug_title": "Memory leak when adding/removing a lot of routes", "bug_description": "Dynamically adding/removing routes to camel causes registrations in org.apache.camel.builder.ErrorHandlerBuilderRef.handlers (Map<RouteContext, ErrorHandlerBuilder>) for RouteContext instances. Those never get removed and can cause leaks if memory consuming objects are attached in the RouteContext for example constant definitions.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.builder.ErrorHandlerBuilder.java", "org.apache.camel.builder.ErrorHandlerBuilderSupport.java", "org.apache.camel.builder.ErrorHandlerBuilderRef.java"], "label": 1, "es_results": []}, {"bug_id": 8094, "bug_title": "camel-netty: Do not use org.jboss.netty.util.internal.ExecutorUtil as it breaks the camel-netty Karaf feature", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.netty.NettyClientBossPoolBuilder.java", "org.apache.camel.component.netty.NettyWorkerPoolBuilder.java", "org.apache.camel.component.netty.NettyServerBossPoolBuilder.java", "org.apache.camel.component.netty.NettyProducer.java"], "label": 1, "es_results": []}, {"bug_id": 7953, "bug_title": "Hazelcast seda documentation is misleading about pollInterval", "bug_description": "Actual documentation says about property pollInterval used by hazelcast seda consumer: \"How frequent to poll from the SEDA queue\". Unfortunately this is wrong since pollInterval is used as blocking timeout while polling a Java BlockingQueue (http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/BlockingQueue.html#poll%28long,%20java.util.concurrent.TimeUnit%29) which might lead to wrong usage. Property might considered to be renamed to e.g. blockingTimeout. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.15.0", "fixed_files": ["org.apache.camel.component.hazelcast.HazelcastSedaConfigurationTest.java", "org.apache.camel.component.hazelcast.seda.HazelcastSedaConfiguration.java", "org.apache.camel.component.hazelcast.seda.HazelcastSedaConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 7882, "bug_title": "camel-syslog's CamelSyslogTimestamp header is suddenly a GregorianCalendar", "bug_description": "This is mostly to help others facing the same issue, as we just spent 4x3 hours figuring this out.\nAn app, that consumes syslog entries and forwards JMS (over OpenMQ), was upgraded from camel 2.13.0 to 2.14.0.\nAfterwards many things broke upstream, which was masked by poor logging in some Glassfish servers (truncating stacktraces).\nTurns out that the CamelSyslogTimestamp header was silently discarded, being a java.util.GregorianCalendar instance.\nForcing it into a java.util.Date before routing to the JMS endpoint restored service throughout the valuechain.\nAs I wrote; this is mostly a FYI to others .\nBut perhaps the documentation might reflect the lack of guaranteed conversion?", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.component.syslog.SyslogDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 8077, "bug_title": "NullPointerException in getRouteDefinition before context is started", "bug_description": "Not sure if this occurs in 2.14.0. Does not occur in 2.13.3.\nI am extending CamelSpringTestSupport with:\n@Override\npublic boolean isUseAdviceWith() \n{\n\n    return true;\n\n}\n\nIn a @Before method I call context.getRouteDefintion(\"some.id\")\nIt throws a NullPointerException:\norg.apache.camel.impl.DefaultCamelContext.getRouteDefinition(DefaultCamelContext.java:1464)\nIf I put context.start() at the top of the @Before method, it works.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 8106, "bug_title": "XML parsing error is ignored by xtoknize XML tokenizer", "bug_description": "XML parsing exceptions are ignored by xtokenize XML tokenizer and this is leading to the same token extracted repeated times.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.support.XMLTokenExpressionIterator.java"], "label": 1, "es_results": []}, {"bug_id": 8008, "bug_title": "SubmitMulti and DataSm not checking CamelSmppAlphabet header", "bug_description": "According to the documentation, the CamelSmppAlphabet header is used for SubmitSm, SubmitMulti and DataSm message types\nLooking at the code, I noticed it was only used for SubmitSm and not the other two.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.component.smpp.SmppDataSmCommand.java", "org.apache.camel.component.smpp.SmppSubmitMultiCommand.java"], "label": 1, "es_results": []}, {"bug_id": 8118, "bug_title": "BigDecimalPatternFormat overwrites Locale setting", "bug_description": "Because of this change request (https://issues.apache.org/jira/browse/CAMEL-7742) was a new feature implemented that allows pattern annotations for BigDecimal fields in CSV model classes for the Camel Bindy component.\nThe problem with that is, that the usage of this feature overwrites the current Locale setting of the environment. For example, if the current Locale was set to \"German\" and the provided pattern for the BigDecimal field requires \"US\" to unmarshal the numbers in the CSV file, then the method BigDecimalPatternFormat#parse(String) overwrites the Locale, but doesn&apos;t restore the former setting. This can cause problems for other software components that depends on the Locale setting.\nThe because of the problem can be found here on line 21: Source of BigDecimalPatternFormat\nA possible workaround would be storing the current locale temporarily, overwrite that setting, perform the formatting task and then restore the former locale setting.\nLike in this code example:\n\n\n\nif (getNumberFormat() != null) {\n\n    final Locale currentLocale = Locale.getDefault();\n\n    Locale.setDefault(super.getLocale());\n\n    DecimalFormat df = (DecimalFormat)getNumberFormat();\n\n    df.setParseBigDecimal(true);\n\n    BigDecimal bd = (BigDecimal)df.parse(string.trim());\n\n    if(super.getPrecision() != -1) {\n\n        bd = bd.setScale(super.getPrecision(), RoundingMode.valueOf(super.getRounding()));\n\n    }\n\n    Locale.getDefault(); // what is the purpose of this line?\n\n    Locale.setDefault(currentLocale); // restore the Locale setting\n\n    return bd;\n\n}\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.dataformat.bindy.format.BigDecimalPatternFormat.java"], "label": 1, "es_results": []}, {"bug_id": 7880, "bug_title": "Cannot use custom DataFormats in REST DSL", "bug_description": "See: http://camel.465427.n5.nabble.com/RestBindingProcessor-JSON-Data-Format-Config-td5757103.html\nWhen using custom JSON data format in REST DSL, the service incorrectly rejects messages with request structure and accepts messages with response structure.\nThis is due to org.apache.camel.model.rest.RestBindingDefinition.createProcessor(RouteContext) looking up and configuring the same DataFormat object twice:\n\n\n\nDataFormat json = context.resolveDataFormat(name); \n\nDataFormat outJson = context.resolveDataFormat(name); \n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.model.rest.RestBindingDefinition.java", "org.apache.camel.model.rest.RestConfigurationDefinition.java", "org.apache.camel.spi.RestConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 8125, "bug_title": "PropertyInject gives NullPointerException", "bug_description": "Using the annotation @PropertyInject on a field of the RouteBuilder class gives a NullPointerException\npublic class RouteBuilder extends SpringRouteBuilder \n{\n\n\t\n\n\t@PropertyInject(\"foo.bar\")\n\n\tprivate String fooBar;\n\n        ...\n\n}\n\nUsing the {{ }} notation in endpoint URIs is working though.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.model.ProcessorDefinitionHelper.java", "org.apache.camel.util.CamelContextHelper.java", "org.apache.camel.impl.CamelPostProcessorHelper.java"], "label": 1, "es_results": []}, {"bug_id": 8137, "bug_title": "Simple language does not resolve overloaded method calls", "bug_description": "I am having an issue with the Simple language. I have a property named myFile with a value of a java.nio.file.Path object. When I try to use the following expression \n\n ${property.file.getFileName} \n\n in order to invoke the getFileName() method I get an exception saying:\n\nAmbiguous method invocations possible: [public sun.nio.fs.UnixPath.getFileName(), public abstract java.nio.file.Path java.nio.file.Path.getFileName()]\n\n\n\nI am able to use SpEL if I do\n\n#{properties[myFile].getFileName()}\n\n\n\nIt would be nice if Simple supported this as well so I wouldn&apos;t have to go through hoops in order to use SpEL since I can&apos;t use SpEL to specify parameters in a uri.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.15.0", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java", "org.apache.camel.util.ObjectHelper.java"], "label": 1, "es_results": []}, {"bug_id": 8148, "bug_title": "Avoid possible NPE in Camel Box component on exceptions during initial login", "bug_description": "There is a possible NPE that can happen on exceptions during the initial OAuth flow in Camel Box component. This is due to a listener attribute in LoginAuthFlowUI which is never set by Box.com SDK. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.box.internal.LoginAuthFlowUI.java", "org.apache.camel.component.box.IBoxCollaborationsManagerIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 8153, "bug_title": "Fix potential connection leak in StreamList mode", "bug_description": "When using camel-jdbc component with newly introduced StreamList mode I&apos;ve faced a 100% reproducible connection leak issue.\nMy investigation leads me to Tomcat connection pool implementation - it has problem with returing current connection from Statement object - instead of returning pool specific proxy it returns actual JDBC connection.\nThere is `statement.getConnection()` line in `org.apache.camel.component.jdbc.ResultSetIterator` so in my particular scenario things work like this:\n1) Camel borrows connection from Tomcat pool\n2) Camel leaves JDBC connection and ResultSet intact as we use StreamList mode of camel-jdbc component\n3) Route processes resultset in streaming mode and completes successfully\n4) Camel tries to close connection, but due to connection pool implementation issue it closes actual JDBC connection instead of returing it to the pool\n5) Actual JDBC connection is closed an connection pool is unaware of this fact thinking it&apos;s still open and in use by application\nIt would be more error prone to pass proper connection object to ResultSetIterator along with result set and not rely on statement.getConnection() call.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.jdbc.ResultSetIterator.java", "org.apache.camel.component.jdbc.JdbcProducer.java"], "label": 1, "es_results": []}, {"bug_id": 8160, "bug_title": "Generic methods used as endpoints fail in Java 8", "bug_description": "In Java 8 annotations are duplicated to generics&apos; bridge methods.  Camel, rightly, does not support bridge methods as endpoints.  However, because of this change in the language annotations such as @Consume will end up on them after compilation leaving Camel in an invalid state.\nI will attach a sample project.\nThis ticket is from this discussion thread: http://camel.465427.n5.nabble.com/Changes-in-Java-8-generics-breaking-Camel-td5760638.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.util.ReflectionHelper.java"], "label": 1, "es_results": []}, {"bug_id": 8268, "bug_title": "camel-hbase - The scan method is not thread safe when using a filter list", "bug_description": "The scan method is not thread safe when using a filter list. A concurrent call on the endpoint will produce wrong values because the same FilterList is used between threads. Cloning the passed filter list solve the problem.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.hbase.HBaseProducerTest.java", "org.apache.camel.component.hbase.HBaseEndpoint.java", "org.apache.camel.component.hbase.HBaseProducer.java", "org.apache.camel.component.hbase.mapping.HeaderMappingStrategy.java", "org.apache.camel.component.hbase.model.HBaseRow.java", "org.apache.camel.component.hbase.HBaseConstants.java", "org.apache.camel.component.hbase.model.HBaseCell.java"], "label": 1, "es_results": []}, {"bug_id": 7914, "bug_title": "MQTT Endpoint disconnects on failure. Does not reconnect", "bug_description": "When an error occurs in the Camel MQTT endpoint, the CallbackConnection onFailure callback simply disconnects the connection.\nAt that point there doesn&apos;t seem to be any mechanism to reconnect to the broker.\n\n            public void onFailure(Throwable value) {\n\n                connection.disconnect(new Callback<Void>() {\n\n                    public void onSuccess(Void value) {\n\n                    }\n\n\n\n                    public void onFailure(Throwable e) {\n\n                        LOG.debug(\"Failed to disconnect from \" + configuration.getHost() + \". This exception is ignored.\", e);\n\n                    }\n\n                });\n\n            }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.mqtt.MQTTConfiguration.java", "org.apache.camel.component.mqtt.MQTTEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 8505, "bug_title": "Missed CamelSchematronValidationStatus header", "bug_description": "CamelSchematronValidationStatus header missed somewhere in this method of SchematronProducer class:\n\n\n\n    /**\n\n     * Sets validation report and status\n\n     *\n\n     * @param exchange\n\n     * @param report\n\n     * @param status\n\n     */\n\n    private void setValidationReport(Exchange exchange, String report, String status) {\n\n        // if exchange pattern is In and Out set details on the Out message.\n\n        Map<String, Object> headers = new HashMap<String, Object>();\n\n        headers.put(Constants.VALIDATION_STATUS, status);\n\n        headers.put(Constants.VALIDATION_REPORT, report);\n\n        exchange.getOut().setHeader(Constants.VALIDATION_REPORT, report);\n\n        if (exchange.getPattern().isOutCapable()) {\n\n            exchange.getOut().setHeaders(exchange.getIn().getHeaders());\n\n            exchange.getOut().getHeaders().putAll(headers);\n\n        } else {\n\n            exchange.getIn().getHeaders().putAll(headers);\n\n        }\n\n    }\n\n\n\nstatus variable value is SUCCESS here, but right after:\n\n\n\n            <to uri=\"schematron:schematron/dogs.sch\" />\n\n\n\nthere is no such header - only CamelSchematronValidationReport present.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.schematron.SchematronProducer.java", "org.apache.camel.component.schematron.SchematronProducerTest.java"], "label": 1, "es_results": []}, {"bug_id": 9035, "bug_title": "unbind smpp connection bug ", "bug_description": "Suppose SMSC allowed one connection to client and due to any reason if session.unbindAndClose failed it will set session to null and do not retry to unbind. \nConnection/Session will remain open on SMSC till its TransactionTimeOut, mostly SMSc Admin set it to 5 to 10 mins. \nFollowing are snippet of org.apache.camel.component.smpp.SmppProducer and details are like \n1- if for any reason Camel try to unbind connection and it got failed below code print log and set session=null in both success/fail cases. \n2- After sending unbind it will try to reconnect. \n3- As SMSc allowed 1 connection which was not unbinded successfull it will not allowed second connection so reconnect will get failed. \n4- Camel call closesession on reconnection failure and will verify if session != null, as session is already null so this code will not send unbind again and apache camel will not able to get connection from SMSc until timeout happen on SMSc and this will results in 10 mins outage. \nIf we change closeSession() like below \ncurrent: \n\n\n\nprivate void  closeSession() { \n\n        if (session != null) { \n\n            session.removeSessionStateListener(this.internalSessionStateListener); \n\n            try { \n\n                Thread.sleep(1000); \n\n                session.unbindAndClose(); \n\n            } catch (Exception e) { \n\n              LOG.warn(\"Could not close session \" + session); \n\n            } \n\n\n\n            session = null; \n\n\n\n        } \n\n\n\n    } \n\n\n\nSuggested: \n\n\n\nprivate void  closeSession() { \n\n        if (session != null) { \n\n            session.removeSessionStateListener(this.internalSessionStateListener); \n\n            try { \n\n                Thread.sleep(1000); \n\n                session.unbindAndClose(); \n\n                session = null; // if we put here then it will retry for unbind \n\n            } catch (Exception e) { \n\n              LOG.warn(\"Could not close session \" + session); \n\n            } \n\n             session = null; // remove his line \n\n        } \n\n    } \n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.smpp.SmppConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 8431, "bug_title": "Consume all files in aws S3 bucket where deleteAfterRead = false", "bug_description": "The current AWS S3Consumer class has a problem if user supply the parameter deleteAfterRead=false.  The S3Consumer will always consume the same files over and over again with the size of getMaxMessagesPerPoll(). \nAfter some code chasing, i think, the root because is because the ListObjectsRequest does not has the previous marker value. Hence, i wonder if we could do something like below:\n\nprivate string marker; // new line to define the marker string \n..\n..\n..\nListObjectsRequest listObjectsRequest = new ListObjectsRequest();\nlistObjectsRequest.setBucketName(bucketName);\nlistObjectsRequest.setPrefix(getConfiguration().getPrefix());\nlistObjectsRequest.setMaxKeys(maxMessagesPerPoll);\nlistObjectsRequest.setMarker(marker); // new line to set the marker   \nObjectListing listObjects = getAmazonS3Client().listObjects(listObjectsRequest);\nmarker = listObjects.getMarker(); // where marker is track          \n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.15.4", "fixed_files": ["org.apache.camel.component.aws.s3.S3Consumer.java"], "label": 1, "es_results": []}, {"bug_id": 8163, "bug_title": "socketFactory must also be set in MailConfiguration when STARTTLS is used", "bug_description": "When using STARTTLS with custom sslContextParameters it fails since the factory setup is only done for smtps/imaps/pops, same problem applies to the \"dummyTrustManager.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.0", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.component.mail.security.SslContextParametersMailRouteTest.java", "org.apache.camel.component.mail.MailConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 8218, "bug_title": "REST DSL with RestletComponent does not support servelet container", "bug_description": "\"I&apos;m trying out the REST DSL in 2.14.1 and I&apos;m not able to get it to work \nwith restlet within a servlet container.\"\nHere is the mail thread about it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.restlet.RestletComponent.java"], "label": 1, "es_results": []}, {"bug_id": 8217, "bug_title": "camel-xmljson option typeHints does not work", "bug_description": "Camel core&apos;s model fails to set the option typeHints to the data format implementation.\nThe data format implementation mixes up option values YES and WITH_PREFIX.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.dataformat.xmljson.XmlJsonOptionsTest.java", "org.apache.camel.model.dataformat.XmlJsonDataFormat.java", "org.apache.camel.dataformat.xmljson.XmlJsonDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 8213, "bug_title": "UseOriginalAggregationStrategy is suspicious to NullPointerException", "bug_description": "The default constructor of UseOriginalAggregationStrategy sets the final field original to null.\nIn one corner case, the aggregate will dereference this null pointer, as no null-pointer check is made.\n\n\n\n    public Exchange aggregate(Exchange oldExchange, Exchange newExchange) {\n\n        if (propagateException) {\n\n            Exception exception = checkException(oldExchange, newExchange);\n\n            if (exception != null) {\n\n                original.setException(exception);\n\n            }\n\n        }\n\n        return original != null ? original : oldExchange;\n\n    }\n\n\n\nThis potential NPE should be fixed, maybe a unit test be added.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.processor.aggregate.UseOriginalAggregationStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 8222, "bug_title": "Jetty component setting responseHeaderSize is impossible", "bug_description": "This is due to a \"copy/paste\" bug in: \norg.apache.camel.component.jetty.JettyHttpComponent at line: 691\n        if (responseBufferSize != null) \n{\n\n            answer.setResponseBufferSize(responseBufferSize);\n\n        }\n        if (responseHeaderSize != null) {\n\n            answer.setResponseBufferSize(responseHeaderSize);\n\n        }\n\nneeds to be:\n\n        if (responseBufferSize != null) {\n            answer.setResponseBufferSize(responseBufferSize);\n        }\n        if (responseHeaderSize != null) \n{\n\n            answer.setResponseHeaderSize(responseHeaderSize);\n\n        }\n\nThis issue seems to be also in the latest master and in other places where the header size is set.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 8233, "bug_title": "Splitter - Option parallelAggregate is not in use when using parallel procession as well", "bug_description": "If both options are true, then parallel aggregate runs in sequence. It was a missing place in the code that wasn&apos;t changed when the parallelAggregate option was added to Camel", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 8236, "bug_title": "WebSphere class loader detection is too sensitive", "bug_description": "The DefaultCamelContext attempts to detect an IBM WebSphere application server by a simple test: loader.getClass().getName().startsWith(\"com.ibm\")\nThis test can introduce very subtle bugs when working with other IBM productes and I suggest to replace it by a list of known class names of WebSphere class loaders. At least, one should add an additional dot in order to avoid matching packages that only start with \"com.ibm\" such as any \"com.ibmfoobar\".", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.impl.WebSpherePackageScanClassResolver.java"], "label": 1, "es_results": []}, {"bug_id": 8272, "bug_title": "Camel-box socks proxy implementation is incomplete", "bug_description": "org.apache.camel.component.box.internal.LoginAuthFlowUI looks for http.route.socks-proxy and sets up a socks proxy for the webClient\norg.apache.camel.component.box.internal.BoxClientHelper just passes the httpParams on to the underlying HttpClient but the box api uses a vanilla DefaultHttpClient which doesn&apos;t talk SOCKS.\nThe attached patch adds socks proxy support to the main restful box transactions.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.15.0", "fixed_files": ["org.apache.camel.component.box.internal.BoxClientHelper.java", "org.apache.camel.component.box.BoxConverter.java"], "label": 1, "es_results": []}, {"bug_id": 8190, "bug_title": "Kafka producer: partition key is optional, not required by kafka client api", "bug_description": "In order to send to kafka, one need to construct KeyedMessage. \n\n \n\n    public KeyedMessage(String topic, K key, Object partKey, V message) {\n\n        this.topic = topic;\n\n        this.key = key;\n\n        this.partKey = partKey;\n\n        this.message = message;\n\n        class.$init$(this);\n\n        if(topic == null) {\n\n            throw new IllegalArgumentException(\"Topic cannot be null.\");\n\n        }\n\n    }\n\n\n\n    public KeyedMessage(String topic, V message) {\n\n        Object var10002 = null;\n\n        Object var10003 = null;\n\n        this(topic, (Object)null, (Object)null, message);\n\n    }\n\n\n\n    public KeyedMessage(String topic, K key, V message) {\n\n\n\nLooks like only topic is required parameter, but partition key is optional. Also, if key is provided by the user I think is makes sense to propagate it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.kafka.KafkaProducer.java", "org.apache.camel.component.kafka.KafkaProducerTest.java"], "label": 1, "es_results": []}, {"bug_id": 8226, "bug_title": "Deprecated feature dataSourceRef not working correctly", "bug_description": "If several sql endpoints are defined using dataSourceRef attribute the latest one will not get dataSourceRef removed in createEndpoint causing validation exception.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.sql.SqlEndpoint.java", "org.apache.camel.component.sql.SqlComponent.java"], "label": 1, "es_results": []}, {"bug_id": 8329, "bug_title": "camel-sql - May not propagate headers for operations with no resultset", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Camel-SQL-CamelSqlRetrieveGeneratedKeys-attribute-delete-all-headers-tp5762512.html\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.sql.SqlProducer.java", "org.apache.camel.component.sql.SqlGeneratedKeysTest.java"], "label": 1, "es_results": []}, {"bug_id": 8338, "bug_title": "ScriptBuilder relies on TCCL to discover engines", "bug_description": "ScriptBuilder uses javax.script.ScriptEngineManager default ctor, which does \n\n\n\n    public ScriptEngineManager() {\n\n        ClassLoader ctxtLoader = Thread.currentThread().getContextClassLoader();\n\n        init(ctxtLoader);\n\n    }\n\n\n\nThis means we can only discover script engines that are visible on the TCCL", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.builder.script.ScriptBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 8339, "bug_title": "Usage of camel-mail depends on TCCL", "bug_description": "javax.mail.Session uses TCCL to load resources\n\n\n\nThread.getContextClassLoader() line: 1432 [local variables unavailable]\t\n\nSession$3.run() line: 1199\t\n\nAccessController.doPrivileged(PrivilegedAction<T>) line: not available [native method]\t\n\nSession.getContextClassLoader() line: 1194\t\n\nSession.loadAllResources(String, Class, StreamLoader) line: 1137\t\n\nSession.loadProviders(Class) line: 917\t\n\nSession.<init>(Properties, Authenticator) line: 216\t\n\nSession.getInstance(Properties, Authenticator) line: 242\t\n\nMailConfiguration.createJavaMailSender() line: 175\t\n\nMailEndpoint.createProducer() line: 71\t\n\n\n\nBoth MailConfiguration & MailProducer seem to rely on this", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.mail.MailConfiguration.java", "org.apache.camel.component.mail.MailComponent.java", "org.apache.camel.component.mail.MailProducer.java"], "label": 1, "es_results": []}, {"bug_id": 8355, "bug_title": "Dynamicity flag on Mongodb endpoint run a dropIndex() command on specified collection", "bug_description": "Using \"dynamicity=true\" on MongoDb endpoint is running a dropIndex command on the specified collection (CamelMongoDbCollection header), for each exchange going through. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.mongodb.MongoDbEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 8353, "bug_title": "FileLockExclusiveReadLockStrategy - Should defer closing channel till release lock", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/FileLockExclusiveReadLockStrategy-doesn-t-hold-lock-and-error-on-commit-tp5762668.html\nThe acquire lock method closes the channel if it acquired the lock, but that would make the lock invalid. Instead the channel should remain open and only closed when the lock is released.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.Exchange.java", "org.apache.camel.component.file.strategy.MarkerFileExclusiveReadLockStrategy.java", "org.apache.camel.component.file.strategy.FileLockExclusiveReadLockStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 8231, "bug_title": "StompEndPoint does not throw exceptions in case of the underlying connection broken at the time of sending a message", "bug_description": "At the time of  sending the message if the message broker (Apollo is used in this case) is down then no exception is raised from the send method of StompEndpoint.  The underlying connection (stomp jms connection) has the error caught and handled. Since the Stompendpoint does not use the CallBack to get notified of the underlying issue, the message sending is assumed successful. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.component.stomp.StompProducer.java", "org.apache.camel.component.stomp.StompEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 8359, "bug_title": "LinkedIn consumer endpoints have to specify an empty 'fields' parameter", "bug_description": "LinkedIn consumer endpoints that take a &apos;fields&apos; parameter should not have to specify an empty value for the optional field. \nThis field is handled for producer endpoints as its default value is set in LinkedInEndpoint.interceptProperties().\nThis field should be automatically set to null if the api method requires it in LinkedInConsumer.interceptPropertyNames(). ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.linkedin.PeopleResourceIntegrationTest.java", "org.apache.camel.component.linkedin.LinkedInConsumer.java", "org.apache.camel.component.linkedin.LinkedInEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 8358, "bug_title": "Avoid using Olingo2 library classes from 'core' package as it has been designated internal and not part of the API exposed using OSGi headers", "bug_description": "Olingo2 library classes in &apos;core&apos; are internal and should not be used. Even though the URI parser classes should really have been exposed through the public API. This includes a public URI parser URIInfo exposed through the api package, but its implementation URIInfoImpl is part of core and hence not exposed. \nThis does not affect any public component classes in the Olingo2 component.  \nSee OLINGO-420. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.olingo2.api.impl.Olingo2AppImpl.java", "org.apache.camel.component.olingo2.Olingo2AppIntegrationTest.java", "org.apache.camel.component.olingo2.Olingo2Configuration.java", "org.apache.camel.component.olingo2.api.impl.AbstractFutureCallback.java", "org.apache.camel.component.olingo2.api.Olingo2AppIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 8352, "bug_title": "NettyServerBossPoolBuilder build() method is package-protected", "bug_description": "The user documentation for camel-netty4 discusses creating worker and boss thread pools that are shared across camel-netty4 consumers.  I am trying to do this, but discovered that the NettyServerBossPoolBuilder.build() method is package-protected, and I am therefore unable to use this builder to create the shared boss pool.  Unit tests pass because the tests are within the same package, but the method needs to be public for anyone else to use.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.netty4.NettyServerBossPoolBuilder.java", "org.apache.camel.component.netty.NettyClientBossPoolBuilder.java", "org.apache.camel.component.netty.NettyServerBossPoolBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 8369, "bug_title": "weaveAddLast does not work property when route ends with a split", "bug_description": "When using AdviceWithRouteBuilder to modify a route definition, weaveAddLast does not work correctly when the route ends with a split.  It appears to add the additional components within the split, not afterwards.\nSee attached testcase.\nAdding a trivial operation after the split in the original route builder fixes it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.builder.AdviceWithBuilder.java", "org.apache.camel.builder.AdviceWithTasks.java", "org.apache.camel.builder.AdviceWithRouteBuilder.java", "org.apache.camel.model.ProcessorDefinitionHelper.java"], "label": 1, "es_results": []}, {"bug_id": 8382, "bug_title": "dumpRoutesAsXml should resolve \"from\" endpoint property values as with \"to\" endpoint URI", "bug_description": "We have defined our routes with property keys for all endpoints (\"from\" and \"to\"). When using the dumpRoutesAsXml operation we see that the \"from uri\" value is the property key, however the \"to uri\" has been resolved to the actual property value. \nFor example, given a RouteBuilder like the following:\n\n\n\n        from(\"{{route1.uri}}\")\n\n            .routeId(\"myRoute\")\n\n            .to(\"{{route2.uri}}\")\n\n\n\ndumpRoutesAsXml outputs the following:\n\n\n\n    <route customId=\"true\" id=\"myRoute\">\n\n        <from uri=\"{{route1.uri}}\"/>\n\n        <to uri=\"jms:myapp.route2\" id=\"to2\"/>\n\n    </route>\n\n\n\nwhere \"myapp.route2\" is the resolved value of \"route2.uri\".\nPerhaps there is a good reason for this behaviour, but it appears to be inconsistent.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.properties.PropertiesComponentEndpointTest.java", "org.apache.camel.model.ProcessorDefinition.java", "org.apache.camel.model.DataFormatDefinition.java", "org.apache.camel.model.RouteDefinitionHelper.java", "org.apache.camel.model.ProcessorDefinitionHelper.java"], "label": 1, "es_results": []}, {"bug_id": 8395, "bug_title": "Handle empty Salesforce picklist types", "bug_description": "Salesforce inbuilt/user customizable picklist types may be empty with no values defined. This breaks sobject-picklist.vm. It should instead generate an empty Enum with no values. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.maven.CamelSalesforceMojo.java"], "label": 1, "es_results": []}, {"bug_id": 8411, "bug_title": "camel-netty-http - Checking for host header is invalid", "bug_description": "See\nhttp://camel.465427.n5.nabble.com/Camel-Netty-Http-HttpHeaders-case-sensitive-tp5763157.html\nThe names() should be removed as it transfers the headers to a linked hash map instead of checking directly using the netty http headers which is case insensitive.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.netty4.http.handlers.HttpServerChannelHandler.java", "org.apache.camel.component.netty.http.handlers.HttpServerChannelHandler.java"], "label": 1, "es_results": []}, {"bug_id": 8417, "bug_title": "RAW_TOKEN_START and RAW_TOKEN_END not removed when query parameter occurs multiple times", "bug_description": "The tokens RAW_TOKEN_START (\"RAW(\") and RAW_TOKEN_END (\")\") are not removed when the parameter value is a list. The bug is in \"resolveRawParameterValues\" in org.apache.camel.util.UriSupport (lines 323, 324):\n\n\n\n                String value = entry.getValue().toString();\n\n                if (value.startsWith(RAW_TOKEN_START) && value.endsWith(RAW_TOKEN_END)) {\n\n\n\nThis does not work when entry.getValue() contains a list.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.13.4", "fixed_files": ["org.apache.camel.util.URISupport.java", "org.apache.camel.issues.EndpointWithRawUriParameterTest.java"], "label": 1, "es_results": []}, {"bug_id": 8425, "bug_title": "Handle invalid client id gracefully in Camel LinkedIn component", "bug_description": "LinkedIn component uses HtmlUnit to perform a login and authorization on user&apos;s behalf for OAuth. LinkedIn returns an error message instead of a login page on an invalid client id, which the component should handle gracefully, instead of the missing HTML element it throws currently. c", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.linkedin.api.LinkedInOAuthRequestFilter.java", "org.apache.camel.component.linkedin.api.AbstractResourceIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 8426, "bug_title": "Handle invalid client id gracefully in Camel Box component", "bug_description": "Box component uses HtmlUnit to perform a login and authorization on user&apos;s behalf for OAuth. Box.com returns an error message instead of a login page on an invalid client id, which the component should handle gracefully, instead of the missing HTML element it throws currently. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.2", "fixed_files": ["org.apache.camel.component.box.internal.LoginAuthFlowUI.java"], "label": 1, "es_results": []}, {"bug_id": 8432, "bug_title": "camel-mqtt: MQTT wildcard ('+') subscription broken", "bug_description": "MQTT single-level wildcards (\"+\" character) are broken within topic subscriptions.  This is due to URL encoding/decoding that is done on the component URI, which results in the \"+\" being replaced with a space.  I also tried with \"%2B\" instead of the \"+\" sign, which gave the same result.  \nA search led me to CAMEL-8056, which mentions needing to override the useRawUri method to return true.  I&apos;m working on a patch and updated unit tests for this.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.mqtt.MQTTComponent.java", "org.apache.camel.component.mqtt.MQTTConfigurationTest.java", "org.apache.camel.component.mqtt.MQTTBaseTest.java"], "label": 1, "es_results": []}, {"bug_id": 8453, "bug_title": "camel-avro throws SAXParseException when used from spring or blueprint", "bug_description": "Getting this exception when using avro dataformat from spring:\norg.xml.sax.SAXParseException; lineNumber: 27; columnNumber: 88; cvc-complex-type.3.2.2: Attribute &apos;instanceClass&apos; is not allowed to appear in element &apos;avro&apos;.\nDon&apos;t think this has ever worked from spring.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.dataformat.avro.AvroMarshalAndUnmarshalSpringTest.java", "org.apache.camel.model.dataformat.AvroDataFormat.java", "org.apache.camel.dataformat.avro.AvroDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 8455, "bug_title": "camel-linkedin - update_key option should be optional in getHistoricalStatusUpdateStatistics", "bug_description": "update_key option should be optional in getHistoricalStatusUpdateStatistics endpoint. Currently it ends with an exception if the option is not specified: org.apache.camel.RuntimeCamelException: Missing properties for getHistoricalStatusUpdateStatistics, need one or more from [end_timestamp, update_key] See the LinkedIn API docs: https://developer-programs.linkedin.com/historical-company-statistics.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.linkedin.CompaniesResourceIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 8456, "bug_title": "Remove addCompanyUpdateComment endpoint from camel-linkedin", "bug_description": "Underlying resource \n\n\n\n/companies/{company-id}/updates/key={update-key}/update-comments \n\n\n\ndoes not exist. It was probably added by mistake instead of \n\n\n\nhttps://api.linkedin.com/v1/people/~/network/updates/key={update-key}/update-comments\n\n\n\n mentioned in the documentation https://developer-programs.linkedin.com/documents/commenting-and-liking-company-share. The resource is already mapped to linkedin://people/addUpdateComment so there is no need to add it to companies prefixed endpoints.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.linkedin.CompaniesResourceIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 8457, "bug_title": "Correct return types of some endpoints in camel-linkedin", "bug_description": "These endpoints return wrong type:\n\ncompanies/getCompanyUpdateComments - Comments/UpdateComments\ncompanies/getUpdateComments - Comments/UpdateComments\npeople/getGroupMembershipSettings - GroupMemberships/GroupMembership\n\nso it results in java.lang.ClassCastException.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.linkedin.PeopleResourceIntegrationTest.java", "org.apache.camel.component.linkedin.CompaniesResourceIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 8464, "bug_title": "Remove likeCompanyUpdate endpoint from camel-linkedin", "bug_description": "Similar as CAMEL-8456. The underlying resource\n\n\n\n/companies/{company-id}/updates/key={update-key}/is-liked\n\n\n\ndoesn&apos;t exist. It was probably added by mistake instead of\n\n\n\nhttps://api.linkedin.com/v1/people/~/network/updates/key={update-key}/is-liked\n\n\n\nmentioned in the documentation https://developer-programs.linkedin.com/documents/commenting-and-liking-company-share. The resource is already mapped to linkedin://people/likeUpdate so there is no need to add it to companies prefixed endpoints.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.linkedin.CompaniesResourceIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 8673, "bug_title": "ConcurrentModificationException when creating dynamic routes", "bug_description": "This ticket refers to CAMEL-7836.\nThe fix seems to be slightly wrong. (We still geht the ConcurrentModificationException)\nThe because seems to be in removeRouteCollection and addRouteCollection. See here:\n\n\n\nvoid removeRouteCollection(Collection<Route> routes) {\n\n        synchronized (routes) {\n\n            this.routes.removeAll(routes);\n\n        }\n\n}\n\n\n\nHere the code synchronizes on the routes parameter, but should be on this.routes (as it getRoutes() does). The same in addRouteCollection().", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 8393, "bug_title": "Redelivery does not work correctly on Dynamic Routers", "bug_description": "When redelivery occurs for dynamic routers, the properties are being kept. So if the dynamic router uses a property to store the current state such as used in example http://camel.apache.org/dynamic-router.html , then the redelivery actually ends up skipping the endpoint that caused the exception\nHere is my dynamic router class\n\npublic class Router {\n\n\tpublic String route(Exchange exchange) {\n\n\t\tBoolean invoked = exchange.getProperty(\"invoked\", Boolean.class);\n\n\t\tif (invoked == null) {\n\n\t\t\texchange.setProperty(\"invoked\", true);\n\n\t\t\treturn \"mock:route\";\n\n\t\t} else\n\n\t\t\treturn null;\n\n\t}\n\n}\n\n\n\nHere is my unit test class\n\n@RunWith(CamelSpringJUnit4ClassRunner.class)\n\n@ContextConfiguration(loader = CamelSpringDelegatingTestContextLoader.class)\n\npublic class DynamicRouterTest {\n\n\n\n\t@Produce(uri = \"direct:start\")\n\n\tprivate ProducerTemplate producerTemplate;\n\n\n\n\t@EndpointInject(uri = \"mock:end\")\n\n\tprivate MockEndpoint end;\n\n\n\n\t@EndpointInject(uri = \"mock:route\")\n\n\tprivate MockEndpoint route;\n\n\n\n\t@Configuration\n\n\tpublic static class JavaConfig extends SingleRouteCamelConfiguration {\n\n\n\n\t\t@Override\n\n\t\tpublic RouteBuilder route() {\n\n\t\t\treturn new SpringRouteBuilder() {\n\n\n\n\t\t\t\t@Override\n\n\t\t\t\tpublic void configure() throws Exception {\n\n\t\t\t\t\tthis.getContext().setTracing(true);\n\n\t\t\t\t\tfrom(\"direct:start\").onException(IOException.class).maximumRedeliveries(-1).end()\n\n\n\n\t\t\t\t\t.dynamicRouter().method(Router.class).to(\"mock:end\");\n\n\t\t\t\t}\n\n\t\t\t};\n\n\t\t}\n\n\n\n\t}\n\n\n\n\t@Test\n\n\tpublic void test() throws InterruptedException {\n\n\t\troute.whenAnyExchangeReceived(new Processor() {\n\n\n\n\t\t\t@Override\n\n\t\t\tpublic void process(Exchange exchange) throws Exception {\n\n\t\t\t\texchange.getIn().setBody(\"mock route\");\n\n\t\t\t}\n\n\t\t});\n\n\t\troute.expectedBodiesReceived(\"before\");\n\n\t\tend.expectedBodiesReceived(\"mock route\");\n\n\n\n\t\tproducerTemplate.sendBody(\"before\");\n\n\t\troute.assertIsSatisfied();\n\n\t\tend.assertIsSatisfied();\n\n\t}\n\n\n\n\t@Test\n\n\tpublic void test_exception() throws InterruptedException {\n\n\t\troute.whenExchangeReceived(1, new Processor() {\n\n\n\n\t\t\t@Override\n\n\t\t\tpublic void process(Exchange exchange) throws Exception {\n\n\t\t\t\texchange.setException(new IOException());\n\n\t\t\t}\n\n\t\t});\n\n\t\troute.whenExchangeReceived(2, new Processor() {\n\n\n\n\t\t\t@Override\n\n\t\t\tpublic void process(Exchange exchange) throws Exception {\n\n\t\t\t\texchange.getIn().setBody(\"mock route\");\n\n\t\t\t}\n\n\t\t});\n\n\n\n                // this bit fails\n\n\t\troute.expectedBodiesReceived(\"before\", \"before\");\n\n\n\n\t\tend.expectedBodiesReceived(\"mock route\");\n\n\t\tproducerTemplate.sendBody(\"before\");\n\n\t\troute.assertIsSatisfied();\n\n\t\tend.assertIsSatisfied();\n\n\t}\n\n}\n\n\n\nThe test method runs successfully but the test_exception method which tests the redelivery does not. Fails with \"java.lang.AssertionError: mock://route Received message count. Expected: <2> but was: <1>\" which shows that the dynamic router only called the mock:route once.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.processor.RoutingSlip.java"], "label": 1, "es_results": []}, {"bug_id": 8302, "bug_title": "Rabbitmq should not require/bind queue if not specified ", "bug_description": "Current implementation is declaring both exchange and queue on any init (producer or consumer). In case of producer one don&apos;t need queue and may not know who going to be client. \nWe can add flag skipQueueDeclare  so that it won&apos;t genrate uuid queue. \ni can provide PR if it helps. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.1", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.component.rabbitmq.RabbitMQEndpoint.java", "org.apache.camel.component.rabbitmq.RabbitMQEndpointTest.java"], "label": 1, "es_results": []}, {"bug_id": 8434, "bug_title": "Camel HDFS2 - Unable to consume an empty file", "bug_description": "It is not possible to consume an empty file with camel HDFS2 consumer.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.hdfs2.HdfsConsumerTest.java", "org.apache.camel.component.hdfs.HdfsConsumerTest.java", "org.apache.camel.component.hdfs2.HdfsInputStream.java", "org.apache.camel.component.hdfs.HdfsInputStream.java", "org.apache.camel.component.hdfs.HdfsFileType.java", "org.apache.camel.component.hdfs.HdfsConsumer.java", "org.apache.camel.component.hdfs2.HdfsConsumer.java", "org.apache.camel.component.hdfs2.HdfsFileType.java"], "label": 1, "es_results": []}, {"bug_id": 8461, "bug_title": "camel-netty-http does not respect client's keep-alive setting", "bug_description": "If we have the camel route which remove the all the header after the netty-http consumer, netty doesn&apos;t  close the connection even the client send the http header connection as closed.\n\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<blueprint xmlns=\"http://www.osgi.org/xmlns/blueprint/v1.0.0\" default-activation=\"eager\">\n\n  <camelContext xmlns=\"http://camel.apache.org/schema/blueprint\">\n\n    <route>\n\n      <from id=\"t1\" uri=\"netty-http:http://localhost:9000/test\"/>\n\n      <removeHeaders pattern=\"*\"/>\n\n      <setBody>\n\n        <constant>Hello, World!</constant>\n\n      </setBody>\n\n      <to uri=\"log:XXX?level=ERROR\"/>\n\n    </route>\n\n  </camelContext>\n\n</blueprint>\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.netty4.http.handlers.HttpServerChannelHandler.java", "org.apache.camel.component.netty4.http.DefaultNettyHttpBinding.java", "org.apache.camel.component.netty4.http.NettyHttpProducerKeepAliveTest.java", "org.apache.camel.component.netty.http.DefaultNettyHttpBinding.java", "org.apache.camel.component.netty.http.NettyHttpProducerKeepAliveTest.java", "org.apache.camel.component.netty.http.handlers.HttpServerChannelHandler.java"], "label": 1, "es_results": []}, {"bug_id": 8462, "bug_title": "HttpServerChannelHandler should not store the instance of HttpRequest", "bug_description": "HttpServerChannelHandler is created per NettyHttpConsumer, we should not store the Request instance in it&apos;s field. \nBY THE WAY, we can alway access the Request Object from the MessageEvent or Context Object from the NettyHttpHandler.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.netty4.http.handlers.HttpServerChannelHandler.java", "org.apache.camel.component.netty.http.handlers.HttpServerChannelHandler.java"], "label": 1, "es_results": []}, {"bug_id": 8484, "bug_title": "File language - Should support file extensions with multiple dots such as tar.gz", "bug_description": "See SO\nhttp://stackoverflow.com/questions/29031551/how-to-use-donefilename-property-of-camel-for-file-name-with-double-extension", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.util.FileUtil.java", "org.apache.camel.util.FileUtilTest.java", "org.apache.camel.language.FileLanguageTest.java", "org.apache.camel.builder.ExpressionBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 8609, "bug_title": "Remove open-jpa bundle from camel-jpa feature", "bug_description": "The camel-jpa component uses open-jpa for testing, but it does not require the bundle in the feature, and could be used with other JPA implementations, such as Hibernate. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.itest.osgi.jpa.JpaBlueprintRouteTest.java", "org.apache.camel.itest.osgi.jpa.JpaRouteTest.java"], "label": 1, "es_results": []}, {"bug_id": 8665, "bug_title": "Throttler EIP - Using method call for message per sec exp fails in spring", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Throtteling-and-dynamically-changing-MaxRequestsPerPeriod-via-Bean-tp5765952.html\nIt works in java dsl but not in xml dsl", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.processor.ThrottlerTest.java", "org.apache.camel.model.ThrottleDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 8649, "bug_title": "Camel RAW() cannot handle String of  %2050", "bug_description": "\n\n\nfile:inbox?fileName=data+.txt (expected \"inbox/data .txt\" and result is OK)\n\nfile:inbox?fileName=data%20.txt (expected \"inbox/data .txt\" and result is OK)\n\nfile:inbox?fileName=RAW(data+.txt) (expected \"inbox/data+.txt\" and result is OK)\n\nfile:inbox?fileName=RAW(data%20.txt) (expected \"inbox/data%20.txt\", but actually it&apos;s \"inbox/data .txt\") - I think it&apos;s WRONG (disobeyed RAW)\n\nfile:inbox?fileName=RAW(data%2520.txt) (expected \"inbox/data%2520.txt\", but actually it&apos;s also \"inbox/data .txt\") - I think it&apos;s WRONG (disobeyed RAW + double URL decode)\n\nAdditional info:\n\nRAW(data%252520.txt) results in \"data%20.txt\" (disobeyed RAW + double URL decode - it means there is not triple URL decode in this case)\n\nRAW(data%2B.txt) results in \"data+.txt\" (disobeyed RAW + single URL decode - it means there is not double URL decode in this case)\n\ndata%252B.txt results in \"data%2B.txt\" (single URL decode - it means there is not double URL decode in this case)\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.util.URISupport.java", "org.apache.camel.util.UnsafeUriCharactersEncoder.java", "org.apache.camel.util.URISupportTest.java"], "label": 1, "es_results": []}, {"bug_id": 8687, "bug_title": "SyslogConverter does not handle the structured data rightly", "bug_description": "As there are &apos; &apos; inside the structured data, we cannot just check &apos; &apos;  for separation of the structured message.\nHere is the discussion about it.\nhttp://camel.465427.n5.nabble.com/Syslog-data-format-incorrect-parsing-of-structured-data-td5766123.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.syslog.NettyRfc5425Test.java", "org.apache.camel.component.syslog.SyslogConverter.java"], "label": 1, "es_results": []}, {"bug_id": 8786, "bug_title": "The ServletContext init parameters check is not right in CamelServletContextListener", "bug_description": "CamelServletContextListener always complains, even the parameter is quite useful. \n\n\n\nThere are 1 ServletContext init parameters, unknown to Camel. Maybe they are  used by other frameworks? [{routeBuilder-routes=com.mycompany.MyRoute}]\n\n\n\nThere is the mail thread  about it ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.servletlistener.CamelServletContextListener.java"], "label": 1, "es_results": []}, {"bug_id": 8780, "bug_title": "Camel exec component have trouble to load arguments list from message header", "bug_description": "When trying to set the following header:\n\n\n\n            <setHeader headerName=\"CamelExecCommandArgs\" >\n\n                <simple>\"--import\" \"--export\"</simple>\n\n            </setHeader>\n\n\n\nWe receive the following error:\n\n\n\norg.apache.camel.TypeConversionException: Error during type conversion from type: java.lang.String to the required type: java.util.List with value \"--import\" \"--export\" due java.lang.NumberFormatException: For input string: \"\"--import\" \"--export\"\"\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.exec.impl.DefaultExecCommandExecutor.java", "org.apache.camel.language.simple.SimpleParserExpressionTest.java", "org.apache.camel.component.snmp.SnmpConverters.java", "org.apache.camel.component.exec.impl.DefaultExecBinding.java"], "label": 1, "es_results": []}, {"bug_id": 8810, "bug_title": "Camel CXF may propagate wrong Content-Length headers", "bug_description": "In some rare cases camel-cxf may propagate wrong Content-Length HTTP headers in routing scenarios.\nThe scenario in question is a simple camel-cxf to camel-cxf scenario with a request-reply pattern. If the called server does respond with a Content-Length (not chunked) and the server does not send any HTTP protocol header that is not filtered (like Content-Length or Content-Type) the headers from the original server response are forwarded.\nIf the payload returned by Camel is longer than the payload returned by the called server (which provided the Content-Length) e.g. because the proxy is working in PAYLOAD mode and the server uses shorter namespace prefixes for the SOAP envelope, the Content-Length will be too short and the calling client may cut off the message.\nSee the attached unit test for details.\nThe reason for that is that the original headers get set when copying the invocation context from the camel exchange. Normally the protocol header map will be overwritten with the map of filtered headers, so this is not an issue, however of the map of filtered headers is completely empty this will not happen. The fix is to remove the copied protocol headers in that case.\nThe situation will rarely occur in real life because the Server header is there most of the time, so the list of filtered protocol headers is not empty. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.cxf.DefaultCxfBinding.java"], "label": 1, "es_results": []}, {"bug_id": 8901, "bug_title": "NBSP characters in camel-kafka:KafkaConfiguration parameter", "bug_description": "There are some rogue NBSP characters that prevent the kafka zookeeper.session.timeout.ms and zookeeper.sync.time.ms from being set.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.kafka.KafkaConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 9670, "bug_title": "Camel-ftp: No error message on invalid credentials", "bug_description": "As result of CAMEL-8718 the connection will be closed on invalid credentials before throwing an exception.\nBut disconnecting the ftpclient will reset the reply string, so the exception message is null.\nThe attached patch will store the reply string before disconnecting.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.2", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.component.file.remote.FtpOperations.java"], "label": 1, "es_results": []}, {"bug_id": 8900, "bug_title": "Javadoc parser in API Component Framework misses first method with void return type in JDK7", "bug_description": "When using JDK 7 to generate Javadoc, the Javadoc parser based on javax.swing.text.html.parser.Parser fails to extract the first method if it has a void return type. For some reason the combination of the Javadoc generated by JDK7 and its Parser class gets the elements in the wrong order, causing the parser to go into the wrong state missing the first method. The next method element puts the parser back in the right state. \nThe fix is to not let the parser come out of the METHOD_SUMMARY if it has not seen any methods at that point. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.3", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.maven.JavadocParserTest.java", "org.apache.camel.maven.JavadocParser.java"], "label": 1, "es_results": []}, {"bug_id": 8954, "bug_title": "Lock information is not handovered together with Exchange on-completion synchronizations", "bug_description": "This applies to the file components when using common read-lock strategies:\n\nmarkerFile - org.apache.camel.component.file.strategy.MarkerFileExclusiveReadLockStrategy\nfileLock - org.apache.camel.component.file.strategy.FileLockExclusiveReadLockStrategy\n\nThis strategies stores lock information in the Exchange properties:\n\nExchange.FILE_LOCK_FILE_ACQUIRED == \"CamelFileLockFileAcquired\"\nExchange.FILE_LOCK_FILE_NAME == \"CamelFileLockFileName\"\nExchange.FILE_LOCK_EXCLUSIVE_LOCK == \"CamelFileLockExclusiveLock\"\nExchange.FILE_LOCK_RANDOM_ACCESS_FILE == \"CamelFileLockRandomAccessFile\"\n\nLock information is stored as scalar values and can hold information about only one single lock.\nWhen there are two Exchanges participates in the route, share UoW, and synchronizations are handovered from one Exchange to another, information about both locks can&apos;t be stored in the Exchange properties and lost. Consequently when on-completion synchronizations are performed, read-lock strategies can&apos;t access information about all the locks and they are not released.\nFor example, after completing this route lock for file1.dat is not released:\n\n\n\nfrom(\"file:data/input-a?fileName=file1.dat&readLock=markerFile\")\n\n    .pollEnrich(\"file:data/input-b?fileName=file2.dat&readLock=markerFile\")\n\n    .to(\"mock:result\");\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.3", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.file.GenericFile.java", "org.apache.camel.component.file.strategy.MarkerFileExclusiveReadLockStrategy.java", "org.apache.camel.component.file.strategy.FileLockExclusiveReadLockStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 8978, "bug_title": "Setting of SOAP headers via the Camel Header \"org.apache.cxf.headers.Header.list\" not working for CXF data format \"PAYLOAD\" ", "bug_description": "In the camel-cxf documentation https://camel.apache.org/cxf.html is described in the chapter \"How to deal with the message for a camel-cxf endpoint in PAYLOAD data format\" that \"You can use the Header.HEADER_LIST as the key to set or get the SOAP headers\".\nBut this only works for getting SOAP headers.\nIf you want to set SOAP headers via the Camel header Header.HEADER_LIST, the headers are not taken into account in the CXF-to-endpoint.\nI analysed the problem and found out that \n\nthe SOAP header list is forwarded to the CXF request context\nthat this header list is overwritten in CxfEndpoint.CamelCxfClientImpl by the CxfPayload.getHeaders() value.\n\nMy suggestion is that we merge the headers from the Camel header and the the CXF payload in CxfEndpoint.CamelCxfClientImpl. See the attached patch.\nWith the merging  we cover all different use cases:\n\nthe headers can be set in the CxfPayload\nthe headers can be set in the Camel header Header.list\nthe headers can be set in the CXFPayload and the CamelHeader Header.list.\n\nAlso the case where the list instance in the CxfPayload is the same as in the Camel header (in this case no merge is necessary) is covered. This case happens if the from-endpoint is also a CXF endpoint and the CXF payload is forwarded to the to-CXF-endpoint.\nI can commit the change. However, before I do it I want to have the agreement from the CXF experts.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.3", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.component.cxf.CxfEndpoint.java", "org.apache.camel.component.cxf.CxfPayLoadSoapHeaderTest.java"], "label": 1, "es_results": []}, {"bug_id": 9255, "bug_title": "documentType not used for XPath predicates in XML DSL", "bug_description": "the documentType paramter is not used for XPath predicates in XML DSL. It works for XPath expression and in Java DSL", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.14.3", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.model.language.XPathExpression.java"], "label": 1, "es_results": []}, {"bug_id": 8065, "bug_title": "Camel will not build on windows (camel-box and camel-api-component-maven-plugin)", "bug_description": "Building camel fails on windows with the error: \nFailed to execute goal org.apache.camel:camel-api-component-maven-plugin:2.15-SNAPSHOT:fromApis (generate-test-component-classes) on project camel-box: Error generating source for com.box.boxjavalibv2.resourcemanagers.IBoxCollaborationsManager: String index out of range: 1\nThis is due to the plugin not escaping the windows file separator before using it in a regex. \nPatch/pull request to follow", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.14.1", "fixed_files": ["org.apache.camel.maven.AbstractApiMethodGeneratorMojo.java", "org.apache.camel.maven.ApiComponentGeneratorMojo.java"], "label": 1, "es_results": []}, {"bug_id": 8430, "bug_title": "Camel HDFS2 - readSuffix option does not work", "bug_description": "Option readSuffix does not work. It staticly uses constant HdfsConstants.DEFAULT_READ_SUFFIX.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.hdfs.HdfsConsumerTest.java", "org.apache.camel.component.hdfs.HdfsInputStream.java", "org.apache.camel.component.hdfs2.HdfsConsumerTest.java", "org.apache.camel.component.hdfs2.HdfsInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 8479, "bug_title": "TrapReceiveTest is failed within Camel 2.15.0", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.1", "fixed_files": ["org.apache.camel.component.snmp.TrapReceiveTest.java", "org.apache.camel.component.snmp.SnmpEndpoint.java", "org.apache.camel.component.snmp.PollOIDTest.java"], "label": 1, "es_results": []}, {"bug_id": 8476, "bug_title": "Unexpected behavior in fault handling with doTry/doCatch", "bug_description": "There seems to be a peculiarity with fault processing when a route consists of a single TryProcessor and the MEP for an exchange is InOut.  The TryProcessor will iterate over any number of processors defined inside of it and during each iteration it sets the out message to the in message and clears the out message reference.  The end result of this is that when the route completes, the out reference is cleared and Exchange.isFailed() will return false (it checks the out message for fault status).  Here Is where things get interesting, if I add a single processor after the doTry block, some logic in Pipeline kicks in that copies the in message to the out message for InOut MEPs before ending the route.\nI have included a unit test which demonstrates the expected behavior and current (unexpected) behavior.  The expected behavior test fails and the unexpected behavior test passes.  Additional details can be found as comments in the unit test and the camel configuration containing the routes under test.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.1", "fixed_files": ["org.apache.camel.impl.DefaultExchange.java", "org.apache.camel.processor.TryProcessor.java", "org.apache.camel.component.spring.integration.adapter.CamelTargetAdapter.java", "org.apache.camel.util.ExchangeHelper.java", "org.apache.camel.processor.RoutingSlip.java", "org.apache.camel.builder.ExpressionBuilder.java", "org.apache.camel.processor.OnCompletionProcessor.java", "org.apache.camel.spring.processor.SpringSetFaultBodyTest.java", "org.apache.camel.impl.InterceptSendToEndpoint.java", "org.apache.camel.impl.DefaultExchangeHolder.java", "org.apache.camel.component.restlet.DefaultRestletBinding.java", "org.apache.camel.processor.interceptor.HandleFaultInterceptor.java", "org.apache.camel.processor.PipelineHelper.java", "org.apache.camel.component.jms.EndpointMessageListener.java", "org.apache.camel.builder.ProcessorBuilder.java", "org.apache.camel.component.cxf.CxfConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 8498, "bug_title": "CamelContextFactoryBean missing setEndpoints method", "bug_description": "The Spring JAXB model class for <camelContext> is missing a setEndpoints() method, which means you cannot add an endpoint outside the route definition using JAXB.  Interestingly, the Blueprint version of CamelContextFactoryBean has this method, so I&apos;m guessing it was just missed.\nImpacted classes:\norg.apache.camel.spring.CamelContextFactoryBean", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.1", "fixed_files": ["org.apache.camel.spring.CamelContextFactoryBeanTest.java", "org.apache.camel.spring.CamelContextFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 8520, "bug_title": "Camel XMPP does not use a DNS resolver to look at SRV records", "bug_description": "Camel XMPP doesn&apos;t use a DNS resolver to look at SRV records, whereas in 2.14.1 it did.\nIn 2.15.0, ConnectionConfiguration calls DNSUtil.resolveXMPPDomain(serviceName) which runs this code:\nDNSUtil.java\n\n\npublic static List<HostAddress> resolveXMPPDomain(final String domain) {\n\n        if (dnsResolver == null) {\n\n            List<HostAddress> addresses = new ArrayList<HostAddress>(1);\n\n            addresses.add(new HostAddress(domain, 5222));\n\n            return addresses;\n\n        }\n\n        return resolveDomain(domain, &apos;c&apos;);\n\n    }\n\n\n\ndnsResolver is never initialised, so it returns the service name, in my case &apos;jabberzac.org&apos;, instead of the actual XMPP server from the SRV Record, &apos;xmpp.jabberzac.org&apos;, which then causes a timeout.\nThe dnsResolver is meant to be instantiated in init(), which is meant to be called by SmackConfiguration, but never is.\nDNSUtil.java\n\n\n    /**\n\n     * Initializes DNSUtil. This method is automatically called by SmackConfiguration, you don&apos;t\n\n     * have to call it manually.\n\n     */\n\n    public static void init() {\n\n        final String[] RESOLVERS = new String[] { \"javax.JavaxResolver\", \"minidns.MiniDnsResolver\",\n\n                        \"dnsjava.DNSJavaResolver\" };\n\n        for (String resolver :RESOLVERS) {\n\n            DNSResolver availableResolver = null;\n\n            String resolverFull = \"org.jivesoftware.smack.util.dns\" + resolver;\n\n            try {\n\n                Class<?> resolverClass = Class.forName(resolverFull);\n\n                Method getInstanceMethod = resolverClass.getMethod(\"getInstance\");\n\n                availableResolver = (DNSResolver) getInstanceMethod.invoke(null);\n\n                if (availableResolver != null) {\n\n                    setDNSResolver(availableResolver);\n\n                    break;\n\n                }\n\n            }\n\n            catch (ClassNotFoundException|NoSuchMethodException|SecurityException|IllegalAccessException|IllegalArgumentException|InvocationTargetException e) {\n\n                LOGGER.log(Level.FINE, \"Exception on init\", e);\n\n            }\n\n        }\n\n    }\n\n\n\n2.14.1 doesn&apos;t seem to have this problem as DNSUtil class in 2.14.1 doesn&apos;t have an init() function which is meant to be &apos;automatically called&apos;, it just has a static code block:\n\n\n\n    static {\n\n        try {\n\n            Hashtable env = new Hashtable();\n\n            env.put(\"java.naming.factory.initial\", \"com.sun.jndi.dns.DnsContextFactory\");\n\n            context = new InitialDirContext(env);\n\n        }\n\n        catch (Exception e) {\n\n            // Ignore.\n\n        }\n\n    }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.1", "fixed_files": ["org.apache.camel.component.xmpp.XmppRouteTest.java", "org.apache.camel.component.xmpp.XmppMultiUserChatTest.java", "org.apache.camel.component.xmpp.XmppRobustConnectionTest.java"], "label": 1, "es_results": []}, {"bug_id": 8521, "bug_title": "camel-script - Should try all classloaders before throwing IAE", "bug_description": "Due CAMEL-8338]", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.1", "fixed_files": ["org.apache.camel.builder.script.ScriptBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 8547, "bug_title": "Usage of camel-xmlbeans depends on TCCL", "bug_description": "xmlbeans marshalling and unmarshalling does not respect the ApplicationContextClassLoader\nCrossRef: https://github.com/wildfly-extras/wildfly-camel/issues/457", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.util.ObjectHelper.java", "org.apache.camel.converter.xmlbeans.XmlBeansDataFormat.java", "org.apache.camel.converter.xmlbeans.XmlBeansConverter.java", "org.apache.camel.converter.xmlbeans.XmlBeansConverterTest.java"], "label": 1, "es_results": []}, {"bug_id": 8540, "bug_title": "S3Consumer uses maxMessagesPerPoll incorrectly", "bug_description": "S3Consumer sets the maximum number of keys to retrieve from S3 to the value of maxMessagesPerPoll property. According to documentation (http://camel.apache.org/batch-consumer.html), the limit can be disabled by setting the value to negative or zero. In case of S3Consumer, setting the maxMessagesPerPoll URI property will have and effect of always empty result.\nS3Consumer.java\n\n\nlistObjectsRequest.setMaxKeys(maxMessagesPerPoll);\n\n\n\nThe consumer must set key limit only when value of maxMessagesPerPoll is greater than 0.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.component.aws.s3.AmazonS3ClientMock.java"], "label": 1, "es_results": []}, {"bug_id": 8584, "bug_title": "Circuit breaker does not honour halfOpenAfter period", "bug_description": "The CircuitBreakerLoadBalancer will always switch to a half-open state immediately after the first rejected message instead of honouring the halfOpenAfter period.\nIt&apos;s due to the failed message count getting reset in the rejectExchange method:\nhttps://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/processor/loadbalancer/CircuitBreakerLoadBalancer.java#L207", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.processor.loadbalancer.CircuitBreakerLoadBalancer.java", "org.apache.camel.processor.CircuitBreakerLoadBalancerTest.java"], "label": 1, "es_results": []}, {"bug_id": 8636, "bug_title": "camel-kafka need to commit the last batch of messages when the auto commit is false", "bug_description": "CAMEL-8085 introduced a new feature of commit the consumer offset in batch mode, but it doesn&apos;t commit the last batch of messages.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.component.kafka.KafkaConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 8702, "bug_title": "when occurs \"Connection reset by peer\",netty4 client's EventLoopGroup select thread will not shut down", "bug_description": "I used netty4 and netty4http as producer ,when occurs  exception \"Connection reset by peer\",netty4 client&apos;s EventLoopGroup \"Selector\" thread and related port won&apos;t shut down.I used thread dump and found ClientChannelHandler&apos;s method exceptionCaught just close channel,it won&apos;t close \"Selector\" thread and port, and the same to NettyProducer.NettyProducerPoolableObjectFactory.destroyObject.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.component.netty4.NettyServerBootstrapConfiguration.java", "org.apache.camel.component.netty4.NettyProducer.java"], "label": 1, "es_results": []}, {"bug_id": 8643, "bug_title": "Http Post from a streaming client sometimes fails to parse", "bug_description": "When upgrading from camel 2.14.1 to 2.15.0 our http posts from clients were sometimes failing.  Traced issue to https://issues.apache.org/jira/browse/CAMEL-5806.  The linked issue added a check of InputStream.available() which on a slow or remote client the binary stream isn&apos;t always immediately available.  Since this is done as part of the initial setup the data is never parsed when it becomes available.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.http4.HttpBinding.java", "org.apache.camel.component.http4.DefaultHttpBinding.java", "org.apache.camel.component.http4.HttpEndpoint.java", "org.apache.camel.component.http.HttpEndpoint.java", "org.apache.camel.component.http.HttpBinding.java", "org.apache.camel.component.http.DefaultHttpBinding.java"], "label": 1, "es_results": []}, {"bug_id": 9013, "bug_title": "Camel HTTP no longer supporting chunked transfer encoding with Tomcat", "bug_description": "When sending a chunked POST whilst running the servlet under Tomcat, camel now fails to read the input stream and sets the body to null.\nchunked-http-failure-test\nThis is due to camel checking the stream for available bytes introduced in CAMEL-5806. For whatever reason the CoyoteInputStream is returning 0 available bytes when handling a chunked request.\n\n\n\n    if (len < 0) {\n\n        InputStream is = request.getInputStream();\n\n        if (is.available() == 0) {\n\n            // no data so return null\n\n            return null;\n\n        }\n\n    }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.0", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.servlet.ServletRestHttpBinding.java", "org.apache.camel.component.servlet.ServletComponent.java"], "label": 1, "es_results": []}, {"bug_id": 8592, "bug_title": "NPE in AbstractListAggregationStrategy if empty list", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/NullPointerException-on-empty-List-in-AbstractListAggregationStrategy-tp5764965.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.processor.aggregate.AbstractListAggregationStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 8597, "bug_title": "Elasticsearch component ignores indexType header set from endpoint URL", "bug_description": "When indexName and indexType is configured only by endpoint URL then indexType is ignored, resp. it is mistakenly replaced with indexName, see https://github.com/apache/camel/blob/master/components/camel-elasticsearch/src/main/java/org/apache/camel/component/elasticsearch/ElasticsearchProducer.java#L112\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.component.elasticsearch.ElasticsearchProducer.java"], "label": 1, "es_results": []}, {"bug_id": 8628, "bug_title": "camel-dozer component fails when multiple expressions are used in a mapping", "bug_description": "The reference to the current exchange is cleared on each invocation of ExpressionMapper in the done() method.  If a mapping file contains multiple expression mappings, the first expression clears out the exchange reference and subsequent expression mappings fail with:\n\njava.lang.IllegalStateException: Current exchange has not been set for ExpressionMapper\n\n\n\nFix is to not clear the reference in ExpressionMapper.done().  We should do this in DozerProducer after all mappings in the mapping file have completed.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.component.dozer.ExpressionMapper.java", "org.apache.camel.component.dozer.ExpressionMappingTest.java", "org.apache.camel.component.dozer.DozerProducer.java"], "label": 1, "es_results": []}, {"bug_id": 8660, "bug_title": "camel-ftp - Disconnect when no messages do not call disconnect", "bug_description": "Its due disconnect is now done as part of UoW to not disconnect before deleting files etc.\nBut if there is no messages then the UoW is not called as no Exchange. In that case we should disconnect if its true.\nSee nabble\nhttp://camel.465427.n5.nabble.com/FTP-disconnect-problem-tp5765934.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.component.file.GenericFileConsumer.java", "org.apache.camel.component.file.remote.RemoteFileConsumer.java", "org.apache.camel.component.file.NewFileConsumerTest.java"], "label": 1, "es_results": []}, {"bug_id": 8626, "bug_title": "Leaking exchangesInFlightKeys in ManagedRoute", "bug_description": "Having a camel context with a single route:\n\n\n\n        onException(Throwable.class)\n\n                .handled(true)\n\n                .process(handleException()); // essentially  doing exchange.setException(someConvertedException);\n\n\n\n        from(\"direct:generalFlow\")\n\n                .routingSlip(property(GeneralFlowRoute.class.getName()));\n\n\n\nstarted from Spring:\n\n\n\n    <camelContext id=\"flows\" xmlns=\"http://camel.apache.org/schema/spring\">\n\n        <template id=\"template\" defaultEndpoint=\"direct:generalFlow\"/>\n\n        <routeBuilder ref=\"generalFlow\"/>\n\n    </camelContext>\n\n\n\n    <bean id=\"generalFlow\" class=\"com.blabla.GeneralFlowRoute\"/>\n\n\n\nDuring performance test both exchangesInFlightKeys  and exchangesInFlightStartTimestamps are accumulating over time.\nBut if the test is run in one thread with debug - nothing is accumulated.\nIssue found after migration from 2.14.1 to 2.15.1", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.management.mbean.ManagedRoute.java", "org.apache.camel.management.ManagedInflightStatisticsTest.java"], "label": 1, "es_results": []}, {"bug_id": 8672, "bug_title": "Restlet Rest Component properties are ignored", "bug_description": "Rest properties for RestletComponent are not picked up by the component because it&apos;s incorrectly looking for \"restle\" properties instead of \"restlet\".\n\n\n\n        // configure component options\n\n        RestConfiguration config = getCamelContext().getRestConfiguration();\n\n        if (config != null && (config.getComponent() == null || config.getComponent().equals(\"restle\"))) {\n\n            // configure additional options on spark configuration\n\n            if (config.getComponentProperties() != null && !config.getComponentProperties().isEmpty()) {\n\n                setProperties(this, config.getComponentProperties());\n\n            }\n\n        }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.component.restlet.RestletComponent.java"], "label": 1, "es_results": []}, {"bug_id": 8682, "bug_title": "Context scoped OnException should not be stopped if a route is stopped", "bug_description": "If you stop a route and uses context scoped error handling, then its services may be stopped when the route stops. This should not happen for context scoped as they are reused by other routes etc.\nThis can also happen for OnCompletion which has same scope as OnException.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.processor.interceptor.DefaultChannel.java"], "label": 1, "es_results": []}, {"bug_id": 8674, "bug_title": "Camel-Netty4 does not set remote UDP address in headers", "bug_description": "It appears that camel-netty4 does not set CamelNettyRemoteAddress in headers of an Exchange generated by an in-only UDP endpoint.\nIt does set CamelNettyRemoteAddress in properties of the exchange.\nCamelNettyRemoteAddress in properties is set from AddressedEnvelope#sender(). CamelNettyRemoteAddress in headers is set from ChannelHandlerContext#channel().remoteAddress().\nIt appears that the latter does not actually contain the remote address. Possible solutions are:\n1) Change NettyEndpoint to set CamelNettyRemoteAddress in headers from the AddressedEnvelope received\n2) Change NettyEndpoint to set CamelNettyRemoteAddress from properties (and to set properties of the Exchange first)", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.netty4.NettyPayloadHelper.java", "org.apache.camel.component.netty4.NettyUdpWithInOutUsingPlainSocketTest.java"], "label": 1, "es_results": []}, {"bug_id": 8607, "bug_title": "Camel endpoint RAW password unsafe characters", "bug_description": "I am creating a camel endpoint such as this (somehost/someport/baseurl have been replaced):\nhttps4://somehost:someport/baseurl?authenticationPreemptive=true&authPassword=RAW(foo%bar)&authUsername=RAW(username)\nThis causes camel to log the entire endpoint, including the user/password:\n(DefaultComponent.java:67) - Supplied URI &apos;https4://somehost:someport/baseurl?authenticationPreemptive=true&authPassword=RAW(foo%bar)&authUsername=RAW(username)&apos; contains unsafe characters, please check encoding\nConsider:\n-It is a security issue to log the username/password\n-Specifiying RAW would allow for special characters, specifically for passwords, as indicated here : https://camel.apache.org/configuring-camel.html, but it seems that UnsafeUriCharactersEncoder is not handling them appropriately.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.http4.HttpEndpointURLTest.java", "org.apache.camel.impl.DefaultComponent.java"], "label": 1, "es_results": []}, {"bug_id": 8683, "bug_title": "Using load balancer in onException adds duplicate outputs for each route defined", "bug_description": "If using load balancer in context scoped onException then it adds duplicate outputs per route you have. So if you have 3 routes, then there is 2 x duplicates as there is 2 additional routes.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.model.LoadBalancerDefinition.java", "org.apache.camel.issues.AdviceWithOnExceptionAndInterceptTest.java", "org.apache.camel.model.LoadBalanceDefinition.java", "org.apache.camel.model.loadbalancer.CustomLoadBalancerDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 8713, "bug_title": "ParallelAggregate option when using parallel mode does not run in parallel", "bug_description": "See CAMEL-7521\nWhen using parallel processing then the boss thread aggregate on the fly is a single threaded boss thread that controls the aggregation. So even if setting parallel aggregate = true, then its still only 1 boss thread.\nWe should have a thread pool (or allow to use the existing pool for parallel processing) for worker threads to do the aggregate work in parallel.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.15.2", "fixed_files": ["org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 8715, "bug_title": "camel-sql - Should close ResultSet", "bug_description": "See SO\nhttp://stackoverflow.com/questions/29933629/apache-camel-sql-component-is-not-closing-resultset\nThe Spring javadocs says\n\n<p><b>NOTE:</b> Any ResultSets opened should be closed in finally blocks\nwithin the callback implementation. Spring will close the Statement\nobject after the callback returned, but this does not necessarily imply\nthat the ResultSet resources will be closed: the Statement objects might\nget pooled by the connection pool, with \n{@code close}\n calls only\nreturning the object to the pool but not physically closing the resources.\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.sql.SqlEndpoint.java", "org.apache.camel.component.sql.SqlProducer.java", "org.apache.camel.component.sql.SqlConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 8745, "bug_title": "Swagger requires context name with quotes ", "bug_description": "\n\n\n    <servlet>\n\n        <servlet-name>ApiDeclarationServlet</servlet-name>\n\n        <servlet-class>org.apache.camel.component.swagger.DefaultCamelSwaggerServlet</servlet-class>\n\n        <init-param>\n\n            <param-name>base.path</param-name>\n\n            <param-value>rest</param-value>\n\n        </init-param>\n\n        <init-param>\n\n            <param-name>api.path</param-name>\n\n            <param-value>api-docs</param-value>\n\n        </init-param>\n\n        <init-param>\n\n            <param-name>api.version</param-name>\n\n            <param-value>1.2.3</param-value>\n\n        </init-param>\n\n        <init-param>\n\n            <param-name>api.title</param-name>\n\n            <param-value>User Services</param-value>\n\n        </init-param>\n\n        <init-param>\n\n            <param-name>api.description</param-name>\n\n            <param-value>Camel Rest Example with Swagger that provides an User REST service</param-value>\n\n        </init-param>\n\n        <init-param>\n\n            <param-name>camelId</param-name>\n\n            <param-value>\"swagger-test\"</param-value>\n\n        </init-param>\n\n        <load-on-startup>2</load-on-startup>\n\n    </servlet>\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.swagger.DefaultCamelSwaggerServletTest.java"], "label": 1, "es_results": []}, {"bug_id": 8923, "bug_title": "Kafka: Topic name lookup from message headers in the producer causes infinite loop", "bug_description": "Refer changes done for CAMEL-7331\nThe change causes a critical bug.\nFor a camel route as follows:\n\n\n\nfrom(\"kafka://<broker>?topic=T1\")\n\n.process(myProcessor)\n\n.to(\"kafka://<broker>?topic=T2\")\n\n\n\nKafkaExchange will be created by KafkaConsumer and the topic will be set to \"T1\" in the header.\nFor the &apos;to&apos; endpoint, KafkaProducer will try to read the topic from the exchange header which is still \"T1\" instead of \"T2\" thereby causing an infinite loop.\nAs discussed in user-group, a different header should be used to read the topic name from message. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.component.kafka.KafkaProducer.java", "org.apache.camel.component.kafka.KafkaEndpoint.java", "org.apache.camel.component.kafka.KafkaConfiguration.java", "org.apache.camel.component.kafka.KafkaProducerTest.java"], "label": 1, "es_results": []}, {"bug_id": 8587, "bug_title": "Exceptions from multicast aggregators are not propagated to the global exception handler", "bug_description": "When a multicast aggregator throws an exception, either directly or by setting the exception to the returned exchange, the exception is just logged instead of being  propagated to the global exception handler.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.model.MulticastDefinition.java", "org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 9311, "bug_title": "Concurrency issue with the dynamic router", "bug_description": "We have a service with two HTTP contexts:\nhttp://0.0.0.0:9000/endpoin1/dynamic\"\nhttp://0.0.0.0:9000/endpoin2/dynamic\"\nBoth endpoints can be called using a HTTP GET method and a dynamic router routes the message differently.\nEach HTTP endpoint  has its own instance of the dynamic router bean. The dynamic router bean does not use any shared state.\nWhen the HTTP endpoints are called sequentially the calls are handled as expected. When the HTTP endpoints are called concurrently then sometimes the the dynamic router of the 9000/endpoin1/dynamic endpoint is re-routing calls originating from 9000/endpoin2/dynamic and vice versa.\nPlease take a look at attached unit test case for more detail.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.processor.RoutingSlip.java", "org.apache.camel.processor.DynamicRouter.java"], "label": 1, "es_results": []}, {"bug_id": 9246, "bug_title": "camel-cxf proxy with MTOM does not get attachment back when RPC/literal style WSDL used", "bug_description": "I have a camel-cxf proxy test case with MTOM enabled. The WSDL used in the project is RPC/Literal style. However, I was unable to get attachment back via response. Please take a look at the test case for more detail.\nAfter some debugging, it looks like that camel-cxf producer did get attachment back. However, there were two attachment related parts. One part had no attachment (empty) but had correct \"Content-ID\". The other part did contain entire attachment but had a rather different \"Content-ID\". For instance:\n\n\n\n19:30:47,110 | INFO  | qtp565617691-367 | IMtomExample                     | 118 - org.apache.cxf.cxf-core - 3.0.4.redhat-620133 | Outbound Message\n\n---------------------------\n\nID: 63\n\nResponse-Code: 200\n\nEncoding: UTF-8\n\nContent-Type: multipart/related; type=\"application/xop+xml\"; boundary=\"uuid:f108bc3f-549a-4baa-ab00-757ff837aacf\"; start=\"<root.message@cxf.apache.org>\"; start-info=\"text/xml\"\n\nHeaders: {accept-encoding=[gzip,deflate], breadcrumbId=[ID-jluomac-52985-1445511717614-11-8], Host=[localhost:7777], Server=[Jetty(8.1.15.v20140411)], User-Agent=[Apache-HttpClient/4.1.1 (java 1.5)]}\n\nPayload: --uuid:f108bc3f-549a-4baa-ab00-757ff837aacf\n\nContent-Type: application/xop+xml; charset=UTF-8; type=\"text/xml\"\n\nContent-Transfer-Encoding: binary\n\nContent-ID: <root.message@cxf.apache.org>\n\n\n\n<soap:Envelope xmlns:soap=\"http://schemas.xmlsoap.org/soap/envelope/\"><soap:Body><ns1:doMtomResponse xmlns:ns1=\"http://cxf.example.com/\"><parameters xmlns:ns2=\"http://cxf.example.com/\"><output>soapui-splash.png</output><dataLength>27186</dataLength><data><xop:Include xmlns:xop=\"http://www.w3.org/2004/08/xop/include\" href=\"cid:15839364-d75d-4cbe-a163-40838bca762a-31@cxf.apache.org\"/></data></parameters></ns1:doMtomResponse></soap:Body></soap:Envelope>\n\n--uuid:f108bc3f-549a-4baa-ab00-757ff837aacf\n\nContent-Type: image/png\n\nContent-Transfer-Encoding: binary\n\nContent-ID: <e4b4a36a-b07a-481d-8bf8-82df5b57f40b-1@cxf.apache.org>\n\nContent-Disposition: attachment;name=\"soapui-splash.png\"\n\n\n\n<-- soapui-splash.png binary attachment starts here -->\n\n...\n\n...\n\n<-- soapui-splash.png binary attachment ends here -->\n\n\n\n--uuid:f108bc3f-549a-4baa-ab00-757ff837aacf\n\nContent-Type: image/png\n\nContent-Transfer-Encoding: binary\n\nContent-ID: <15839364-d75d-4cbe-a163-40838bca762a-31@cxf.apache.org>\n\nContent-Disposition: attachment;name=\"soapui-splash.png\"\n\n\n\n<-- no binary attachment here. empty -->\n\n\n\n--uuid:f108bc3f-549a-4baa-ab00-757ff837aacf--\n\n--------------------------------------\n\n\n\nAnd the response message returned was:\n\n\n\n<soap:Envelope xmlns:soap=\"http://schemas.xmlsoap.org/soap/envelope/\">\n\n   <soap:Body>\n\n      <ns1:doMtomResponse xmlns:ns1=\"http://cxf.example.com/\">\n\n         <parameters xmlns:ns2=\"http://cxf.example.com/\">\n\n            <output>soapui-splash.png</output>\n\n            <dataLength>27186</dataLength>\n\n            <data>\n\n               <xop:Include href=\"cid:15839364-d75d-4cbe-a163-40838bca762a-31@cxf.apache.org\" xmlns:xop=\"http://www.w3.org/2004/08/xop/include\"/>\n\n            </data>\n\n         </parameters>\n\n      </ns1:doMtomResponse>\n\n   </soap:Body>\n\n</soap:Envelope>\n\n\n\nBecause the response pointed to the \"Content-ID\" of \n\n\n\n<xop:Include href=\"cid:15839364-d75d-4cbe-a163-40838bca762a-31@cxf.apache.org\" xmlns:xop=\"http://www.w3.org/2004/08/xop/include\"/>\n\n\n\nand the parts identified by the \"Content-ID: <15839364-d75d-4cbe-a163-40838bca762a-31@cxf.apache.org>\" was actually empty. Therefore, client can not get the attachment back.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.15.5", "fixed_files": ["org.apache.camel.component.cxf.DefaultCxfBinding.java", "org.apache.camel.component.cxf.mtom.CxfMtomPOJOProducerTest.java"], "label": 1, "es_results": []}, {"bug_id": 8708, "bug_title": "SOAP unmarshalling should not fail for Faults that lack an optional Detail element", "bug_description": "Camel-soap code assumes that all Faults that a Camel SOAP endpoint is receiving, are containing a Detail element. Sending a Fault without Detail will result in an NPE. Detail is optional element in SOAP Faults (see http://www.w3.org/TR/soap12-part1/#soapfault) and its absence should not generate an error.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.16.2", "fixed_files": ["org.apache.camel.dataformat.soap12.Soap12UnMarshalTest.java", "org.apache.camel.dataformat.soap.Soap12DataFormatAdapter.java"], "label": 1, "es_results": []}, {"bug_id": 9713, "bug_title": "Can not set custom Jetty HttpClient to producer endpoint", "bug_description": "The camel-jetty page has a \"httpClient\" option that can be set on camel-jetty producer endpoint and it says:\n\n\n\nTo use a shared org.eclipse.jetty.client.HttpClient for all producers created by this endpoint. This option should only be used in special circumstances.\n\nThis option should only be used in special circumstances.\n\n\n\nIn some cases, user might want to set a shard Jetty HttpClient among camel-jetty producer endpoints. For instance, user might use recipientList with camel-jetty producer endpoint to support multiple dynamically created URLs. So setting a shared Jetty HttpClient with a shared thread pool would be ideal to avoid exploding number of producer endpoints + their own producer thread pools in this use case. However, there is no way to set it to camel-jetty producer endpoint.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.1", "fixed_version": "camel-2.17.0", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpComponent.java", "org.apache.camel.component.jetty.JettyHttpEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 8747, "bug_title": "camel-rx - Should leverage UoW when subscribe or observe", "bug_description": "See SO\nhttp://stackoverflow.com/questions/30057358/camel-rx-is-not-deleting-objects-consumed-from-s3\nThe work should be done in an UoW to ensure on completions is executed.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.rx.SendToTest.java", "org.apache.camel.rx.support.EndpointSubscription.java", "org.apache.camel.rx.support.ObserverSender.java"], "label": 1, "es_results": []}, {"bug_id": 8782, "bug_title": "Configuring endpoints using reference lookup may fail with matching primitive types with their Object counterpart types", "bug_description": "Based on PR\nhttps://github.com/apache/camel/pull/522#issuecomment-103620070\nYeah I had a look its due primtive vs object types, that causes Camel to not match the setter method.\neg the method uses a boolean type, but the returned value from the reference lookup is a java.lang.Boolen type. And therefor its not used. I will log a ticket and fix that.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.util.IntrospectionSupport.java"], "label": 1, "es_results": []}, {"bug_id": 8774, "bug_title": "DefaultJettyHttpBinding preserves CONTEXT_ENCODING from the request even HTTP response does not contain the header", "bug_description": "When Http Request is Gzip encoded (CONTENT_ENCODING is set) but the HTTP response is not ( httpExchange.getResponseHeaders().get(HttpHeaders.CONTENT_ENCODING) == null) then Exchange.CONTENT_ENCODING is present in the Camel out message.\nThis is because DefaultJettyHttpBinding:125 preserves all in headers.\nI believe that HTTP protocol headers (except custom ones) from the in shouldn&apos;t be copied to the Camel out message.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.14.3", "fixed_files": ["org.apache.camel.component.http4.HttpProducer.java", "org.apache.camel.component.http4.HttpCompressionTest.java", "org.apache.camel.component.ahc.DefaultAhcBinding.java", "org.apache.camel.component.http.HttpProducer.java", "org.apache.camel.component.jetty.DefaultJettyHttpBinding.java", "org.apache.camel.util.MessageHelper.java", "org.apache.camel.util.MessageHelperTest.java"], "label": 1, "es_results": []}, {"bug_id": 8871, "bug_title": "null body after exception from transform method", "bug_description": "When an exception is thrown from a transform method I lose the content of the message body. This behaviour is unexpected because it does not happen when an exception is thrown from a bean call.\n.bean(ErrorBean.class, ErrorBean.METHOD)\nVs.\n.transform().method(ErrorBean.class, ErrorBean.METHOD)\nI have a testcase that I will try to get attached but here are the routes:\n\n\n\nif (\"testTransformExceptionToErrorRoute\".equals(getTestMethodName())) {\n\n                    from(START)\n\n                    .routeId(\"exception.test.transform\")\n\n                    .transform().method(ErrorBean.class, ErrorBean.METHOD)\n\n                    .to(END);\n\n                } else {\n\n                    from(START)\n\n                    .routeId(\"exception.test.bean\")\n\n                    .bean(ErrorBean.class, ErrorBean.METHOD)\n\n                    .to(END);\n\n                }\n\n\n\n    public static class ErrorBean {\n\n        private static final String METHOD = \"throwException\";\n\n        public static void throwException(Exchange exchange) {\n\n            String body = exchange.getIn().getBody(String.class);\n\n            Assertions.assertThat(body).isEqualTo(BODY);\n\n            throw new NullPointerException();\n\n        }\n\n    }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.processor.ConvertBodyProcessor.java", "org.apache.camel.processor.SetHeaderProcessor.java", "org.apache.camel.processor.SetPropertyProcessor.java", "org.apache.camel.processor.TransformProcessor.java", "org.apache.camel.processor.SetBodyProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 8864, "bug_title": "Camel-Aggregator JDBC repository always overwrites old exchange", "bug_description": "I&apos;m using FlexibleAggregationStrategy to aggregate objects into collection. I found following issue where Aggregator ends up overwriting old collection with the new one.\nJdbcCamelCodec is not marshaling all of the exchange properties. \nE.g. it does not marshal exchange&apos;s &apos;CamelFlexAggrStrCollectionGuard&apos; property.\nSo next time Aggregator tries to fetch BLOB from database, oldExchange does not have above property.\nSo following code creates new collection for each new object, and hence overwrites the old collection.\nprivate Collection<E> FlexibleAggregationStrategy::safeInsertIntoCollection(Exchange oldExchange, Collection<E> oldValue, E toInsert) {\n        Collection<E> collection = null;\n        try {\n            if (oldValue == null || oldExchange.getProperty(COLLECTION_AGGR_GUARD_PROPERTY, Boolean.class) == null) {\n                try {\n                     collection = collectionType.newInstance();    //**EVERYTIME NEW COLLECTION as previous aggregation did not serialize COLLECTION_AGGR_GUARD_PROPERTY to database.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.util.toolbox.FlexibleAggregationStrategy.java", "org.apache.camel.component.leveldb.LevelDBCamelCodec.java", "org.apache.camel.processor.aggregate.cassandra.CassandraCamelCodec.java", "org.apache.camel.Exchange.java", "org.apache.camel.component.hawtdb.HawtDBCamelCodec.java", "org.apache.camel.processor.aggregate.jdbc.JdbcCamelCodec.java"], "label": 1, "es_results": []}, {"bug_id": 8909, "bug_title": "Jasypt CLI outputs help twice", "bug_description": "Using -help results in the following output:\n[janstey@bender apache-camel-2.15.2]$ java -jar lib/camel-jasypt-2.15.2.jar -help\nApache Camel Jasypt takes the following options\n  -h or -help = Displays the help screen\n  -c or -command <command> = Command either encrypt or decrypt\n  -p or -password <password> = Password to use\n  -i or -input <input> = Text to encrypt or decrypt\n  -a or -algorithm <algorithm> = Optional algorithm to use\nError: Command is empty\nApache Camel Jasypt takes the following options\n  -h or -help = Displays the help screen\n  -c or -command <command> = Command either encrypt or decrypt\n  -p or -password <password> = Password to use\n  -i or -input <input> = Text to encrypt or decrypt\n  -a or -algorithm <algorithm> = Optional algorithm to use", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.component.jasypt.Main.java"], "label": 1, "es_results": []}, {"bug_id": 8916, "bug_title": "Support autoCreate=true in ftp/ftps/sftp consumers", "bug_description": "GenericFileEndpoint supports autoCreate option. It may be used to create necessary path for fileconsumer and producers for file/ftp/ftps/sftp endpoints.\nThis flag however isn&apos;t used for ftp/ftps/sftp consumers.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.component.file.remote.FtpConsumerThrowExceptionOnLoginFailedTest.java", "org.apache.camel.component.file.remote.SftpConsumer.java", "org.apache.camel.component.file.remote.FtpConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 8914, "bug_title": "Unable to shutdown endpoint when intercepted with interceptSendToEndpoint", "bug_description": "I&apos;m facing an issue with the version of camel 2.15.2 \nI have a component that create an endpoint. \nThis endpoint override the shutdown and doShutdown method so we can log values at shutdown time. \nBut when this endpoint is intercept with the interceptSendToEnpoint method the shutdown is not called. \nHere is an example of code to reproduce: \nthe Component: \npublic class CustomComponent extends DefaultComponent {\n        @Override\n        protected Endpoint createEndpoint(String uri, String remaining, Map<String, Object> parameters) throws Exception \n{\n\n            return new CustomEndpoint();\n\n        }\n    }\nthe Endpoint: \n    public class CustomEndpoint extends DefaultEndpoint {\n        @Override\n        public Producer createProducer() throws Exception {\n            return new DefaultProducer(this) {\n                @Override\n                public void process(Exchange exchange) throws Exception \n{\n\n                    log.info(exchange.getExchangeId());\n\n                }\n            };\n        }\n        @Override\n        public Consumer createConsumer(Processor processor) throws Exception \n{\n\n            return null;\n\n        }\n\n        @Override\n        public boolean isSingleton() \n{\n\n            return false;\n\n        }\n\n        @Override\n        public void shutdown() throws Exception \n{\n\n            super.shutdown();\n\n            System.out.println(\"SHUTDOWN\");\n\n        }\n\n        @Override\n        protected void doShutdown() throws Exception \n{\n\n            super.doShutdown();\n\n            System.out.println(\"do SHUTDOWN\");\n\n        }\n\n        @Override\n        protected String createEndpointUri() \n{\n\n            return \"myEndpoint\";\n\n        }\n    }\nthe route: \npublic class MyRoute extends RouteBuilder {\n        @Override\n        public void configure() {\n            try \n{\n\n                getContext().addComponent(\"myEndpoint\", new CustomComponent());\n\n            }\n catch (Exception e) \n{\n\n                e.printStackTrace();\n\n            }\n            interceptSendToEndpoint(\"myEndpoint:producer\")\n                    .log(\"INTERCEPTED\");\n            from(\"direct:murex\").routeId(\"Trade Repository Route\")\n                 .to(\"myEndpoint:producer\");\n        }\n    }\nWhen there is an interceptor the shutdown on the endpoint is not called, when there is no interceptor the shutdown is called. \nAfter some debugging I noticed that in the DefaultCamelContext at the shutdown time the shutdown is called on the list of endpoints, when there is an interceptor the list do not contain the CustomEnpoint but only the Interceptor, and because the interceptor does not implement ShutdownableAware the shutdown is not propagate to the underlying endpoint (here the CustomEndpoint) \nWithout the interceptor the CustomEndpoint appears in the list and the shutdown method is called. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.impl.InterceptSendToEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 8927, "bug_title": "camel-ahc-ws - Do not swallow exception when connecting", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Camel-Websocket-Connection-key-not-set-tp5766728p5766730.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.ahc.ws.WsProducer.java", "org.apache.camel.component.ahc.ws.WsEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 8885, "bug_title": "ConsumeLockEntity without ConsumeDelete", "bug_description": "I think that condition at the beginning of the lock function is wrong:\n\n\n\nprotected boolean lockEntity(Object entity, EntityManager entityManager) {\n\n        if (!getEndpoint().isConsumeDelete() || !getEndpoint().isConsumeLockEntity()) {\n\n            return true;\n\n        }\n\n...\n\n\n\nIf I want to just select and then update entity I should set consumeDelete=false, but If so entity will  newer be locked...", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.jpa.JpaConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 8768, "bug_title": "hdfs2 component overwrite option is also being applied to directory filesystem path", "bug_description": "If you need to produce files into an existing HDFS2 path, the default behavoir is overwrite the path, which will delete all existing files on HDFS.  If overwrite option is disabled, then the component will complain that the existing HDFS directory exists and will not work.  \nThe propose solution is to add the following if statement to ignore check if the HDFS directory exists. The overwrite option should only be used for files not directories.\ncode snippet in HdfsOutputStream.java and patch is attached\n if (ret.info.getFileSystem().exists(new Path(ret.actualPath))) {\n                //only check of not directory\n                if (!ret.info.getFileSystem().isDirectory(new Path(ret.actualPath))) {\n                    if (configuration.isOverwrite()) \n{\n\n                        ret.info.getFileSystem().delete(new Path(ret.actualPath), true);\n\n                    }\n else \n{\n\n                        throw new RuntimeCamelException(\"The file already exists\");\n\n                    }\n                }\n            }", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.hdfs.HdfsOutputStream.java", "org.apache.camel.component.hdfs2.HdfsOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 8945, "bug_title": "Loop - Should break out looping if exception happened during routing", "bug_description": "See SO\nhttp://stackoverflow.com/questions/31312281/apache-camel-loop-does-not-stop-on-exception\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.processor.LoopProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 8950, "bug_title": "Injected Quartz2 scheduler does not have access to CamelContext in jobs", "bug_description": "Currently if you inject a scheduler the CamelContext won&apos;t be available for jobs to access. When the scheduler is created automatically, the context is added to the quartz scheduler context so the jobs can access it:\nquartzContext.put(QuartzConstants.QUARTZ_CAMEL_CONTEXT + \"-\" + camelContextName, getCamelContext());", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.quartz2.QuartzComponent.java"], "label": 1, "es_results": []}, {"bug_id": 8951, "bug_title": "RecipientList with RAW parameter do not work", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/java-net-URISyntaxException-using-recipientList-tp5769103.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.util.EndpointHelper.java"], "label": 1, "es_results": []}, {"bug_id": 8963, "bug_title": "camel:route-suspend karaf command does not work as expected", "bug_description": "In previous versions of camel (2.13.2 for sure) it was possible to use camel:route-suspend command and specify route id only. In 2.15.2 its not possible anymore. For example:\n\n\n\nkaraf@root()> camel:route-list\n\n Context                           Route                                  Status\n\n -------                           -----                                  ------\n\n fi-remurex-cpty-context           fi-remurex-institution-queue           Started\n\nkaraf@root()> camel:route-suspend fi-remurex-institution-queue\n\nError executing command camel: route-suspend: argument context is required\n\n\n\nAt the same time, on <tab> key camel:route-suspend command still suggests route, but not context as second parameter. Used route id is unique, so I don&apos;t think its a good idea to ask for context in this case.\nkaraf 3.0.4, camel 2.15.2.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.karaf.commands.RestShow.java", "org.apache.camel.karaf.commands.AbstractRouteCommand.java", "org.apache.camel.karaf.commands.RouteInfo.java", "org.apache.camel.karaf.commands.RouteShow.java", "org.apache.camel.commands.AbstractLocalCamelController.java"], "label": 1, "es_results": []}, {"bug_id": 8964, "bug_title": "CamelContext - API for control routes may cause Route not to update it state", "bug_description": "See CAMEL-8963\nIts the Route instance that do not update it state as well. But the RouteService has the correct state. So one can be Started and the other Suspended.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.util.ServiceHelper.java", "org.apache.camel.impl.RouteSedaSuspendResumeTest.java"], "label": 1, "es_results": []}, {"bug_id": 8905, "bug_title": "encoding problems in jsonpath", "bug_description": "I detected three different encoding problems in jsonpath:\n\nif jsonpath is called with an input stream which has an encoding different from the default encoding (given by Charset.defaultCharset()) then jsonpath still uses the default encoding. Error location in JsonPathEngine:\n        else if (json instanceof InputStream) \n{\n\n            InputStream is = (InputStream) json;\n\n            return path.read(is, Charset.defaultCharset().displayName(), \n\nconfiguration);}\n\n\nif jsonpath is called with a json file whose encoding is different from UTF-8, then jsonpath still parses the document with UTF-8. Error location in JsonPathEngine:\n       else if (json instanceof File) \n{\n\n            File file = (File) json;\n\n            return path.read(file, configuration);\n\n       }\n path.read(file, configuration) uses always UTF-8\n\n\nif jsonpath is called with an URL pointing to a JSON document whose encoding is different from UTF-8, then jsonPath still parses the document with UTF-8. Error location in JsonPathEngine:\n         else if (json instanceof URL) \n{\n\n            URL url = (URL) json;\n\n            return path.read(url, configuration);\n\n         }\npath.read(url, configuration) uses UTF-8\n\nMy solution proposal is to determine the encoding of the JSON documents automatically according to the specification RFC-4627 (https://www.ietf.org/rfc/rfc4627.txt; see chapter 3. Encoding) and then call the method path.read(jsonDocument,foundEncoding,configuration) with the found encoding. See attached patch.\nActually I can commit the patch myself. However, I would like that somebody who is more familiar with jsonpath than I does review my patch.\nSo please tell me if my patch can be accepted or not. I can then do the actual commit or I will discard the patch.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.jsonpath.JsonPathEngine.java"], "label": 1, "es_results": []}, {"bug_id": 8984, "bug_title": "BlueprintCamelContext OSGi service is not unregistered when context is stopped", "bug_description": "org.apache.camel.core.osgi.OsgiCamelContextPublisher doesn&apos;t receive CamelContextStoppingEvent because org.apache.camel.util.EventHelper#doNotifyEvent() checks for started state and org.apache.camel.core.osgi.OsgiCamelContextPublisher is never started.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.blueprint.CamelContextFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 9012, "bug_title": "Olingo2's batch process generates the invalid request", "bug_description": "The syntax check of olingo2 lib was tighten from version 2.0.1.\nAs a result, the batch message generated by camel-ollingo2&apos;s is rejected by olingo2 version 2.0.1 and newer.\nConcretely, the current message includes two extra new lines within the batch part that is rejected by the olingo2 library.\nOLINGO-739\nWe should remove these two extra lines.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.olingo2.api.impl.Olingo2AppImpl.java"], "label": 1, "es_results": []}, {"bug_id": 9019, "bug_title": "ManagedRuntimeEndpointRegistry was not enlisted in JMX", "bug_description": "This is not enlisted as ManagedRuntimeEndpointRegistry but as event notifier because its also that instance.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.management.DefaultManagementLifecycleStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 9029, "bug_title": "JGroups managed routes can be started too early", "bug_description": "JGroupsFilters#dropNonCoordinatorViews doesn&apos;t filter non-view messages properly which can cause the wrong route to be started. This issue was not so easy to detect, as usually the channels used for cluster management doesn&apos;t send any other messages, than view ones.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.jgroups.JGroupsFilters.java"], "label": 1, "es_results": []}, {"bug_id": 9032, "bug_title": "Bean component - Should filter out abstract methods", "bug_description": "If you call a method on a bean then the introspector should filter out abstract methods if there is class inheritance with abstract defined methods.\nSee SO\nhttp://stackoverflow.com/questions/31671894/camel-ambiguousmethodcallexception-abstract-classes", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 9043, "bug_title": "Fix camel-example-cxf-osgi/blueprint examples", "bug_description": "Cannot deploy camel-example-cxf-osgi because of wrong Import-Package requirement. Expression for output filename is not evaluated properly in both examples.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.example.reportincident.ReportIncidentRoutes.java"], "label": 1, "es_results": []}, {"bug_id": 9048, "bug_title": "camel-core causes restart of karaf console if it is refreshed", "bug_description": "Start karaf 4.0.0\nfeature:repo-add mvn:org.apache.cxf.karaf/apache-cxf/3.1.1/xml/features\nfeature:repo-add mvn:org.apache.camel.karaf/apache-camel/2.15.2/xml/features\nfeature:install camel-core\nfeature:install -v wss4j\nThe last feature install causes the karaf she will to restart. The refreshed bundles list shows this:\n    jline/2.12.1 (Wired to org.apache.camel.camel-core/2.15.2 which is being refreshed)\n    org.apache.camel.camel-core/2.15.2 (Should be wired to: org.apache.servicemix.bundles.xalan/2.7.1.7 (through [org.apache.camel.camel-core/2.15.2] osgi.wiring.package; filter:=\"(osgi.wiring.package=org.apache.xalan.xsltc.trax)\"; resolution:=optional))\nSo this shows that the immediate reason is that jline was refreshed. As jline is used by the she will it also restarts.\nNow it might seem strange that jline depends on camel-core. I had a similar issue in activemq-core Activator. It probed the classloaders of all bundles for well known interfaces to find extensions. I think camel-core does the same. The problem here is that jline has a dynamic import package: *. So the bundle classloader of jline is able to find any camel interface and will then have a wire to camel-core. So if then there is a refresh of camel-core it also will be refreshed.\nThis issue can hit all bundles that have a dynamic import package *. \nthe solution is to not actually load interface classes but rather check the bundle wiring if there is a wiring to an interface package. This will then not change the wirings and so not because these problems.\nI will try to provide a fix for the problem.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.impl.osgi.Activator.java"], "label": 1, "es_results": []}, {"bug_id": 9059, "bug_title": "Jetty exposes its endpoint without component name", "bug_description": "A simple route such as\n\n\n\n        from(\"jetty:http://0.0.0.0:8080/ping\").transform(constant(\"PONG\\n\"));\n\n\n\nEnlists the jetty endpoint as just: http://0.0.0.0:8080/ping. But then its mistakenly the http component. It should be the full url with jetty as prefix.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 9049, "bug_title": "Websocket Component not shutting down embedded jetty server on component shutdown", "bug_description": "When stopping the websocket component, it has been observed that the embedded Jetty server does not completely shut down. This causes a bind error when attempting to start the component again. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.3", "fixed_files": ["org.apache.camel.component.websocket.WebsocketProducer.java", "org.apache.camel.component.websocket.WebsocketConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 9087, "bug_title": "camel-pgevent payload always null", "bug_description": "(note, camel-pgevent doesn&apos;t seem to have a component listed in JIRA)\nHaving the following reproducer\n\n\n\n@ContextName(\"myCdiCamelContext\")\n\npublic class MyRoutes extends RouteBuilder {\n\n\n\n    @Inject\n\n    @Uri(\"pgevent://localhost:5432/postgres/foobar?user=postgres&pass=mysecretpassword\")\n\n    private Endpoint listenEndpoint;\n\n\n\n    @Inject\n\n    @Uri(\"pgevent://localhost:5432/postgres/foobar?user=postgres&pass=mysecretpassword\")\n\n    private Endpoint notifyEndpoint;\n\n\n\n    @Inject\n\n    @Uri(\"timer:foo?period=5000\")\n\n    private Endpoint timerEndpoint;\n\n\n\n    @Inject\n\n    @Uri(\"log:output\")\n\n    private Endpoint resultEndpoint;\n\n\n\n    @Inject\n\n    private SomeBean someBean;\n\n\n\n    @Override\n\n    public void configure() throws Exception {\n\n        from(timerEndpoint)\n\n                .transform().simple(\"hello\")\n\n                .to(notifyEndpoint);\n\n\n\n        from(listenEndpoint)\n\n            .to(resultEndpoint);\n\n    }\n\n}\n\n\n\ntogether with\n\n\n\ndocker run --name some-postgres -p 5432:5432 -e POSTGRES_PASSWORD=mysecretpassword -d postgres\n\n\n\nthe notify payload is always null\n\n2015-08-19 08:40:51,216 [0 - timer://foo] DEBUG SendProcessor                  - >>>> Endpoint[pgevent://localhost:5432/postgres/foobar?pass=mysecretpassword&user=postgres] Exchange[Message: hello]\n\n2015-08-19 08:40:51,227 [C EventLoop (2)] DEBUG SendProcessor                  - >>>> Endpoint[log://output] Exchange[Message: null]\n\n2015-08-19 08:40:51,228 [C EventLoop (2)] INFO  output                         - Exchange[ExchangePattern: InOnly, BodyType: null, Body: [Body is null]]\n\n2015-08-19 08:40:56,199 [0 - timer://foo] DEBUG SendProcessor                  - >>>> Endpoint[pgevent://localhost:5432/postgres/foobar?pass=mysecretpassword&user=postgres] Exchange[Message: hello]\n\n2015-08-19 08:40:56,200 [C EventLoop (2)] DEBUG SendProcessor                  - >>>> Endpoint[log://output] Exchange[Message: null]\n\n2015-08-19 08:40:56,201 [C EventLoop (2)] INFO  output                         - Exchange[ExchangePattern: InOnly, BodyType: null, Body: [Body is null]]\n\n2015-08-19 08:41:01,199 [0 - timer://foo] DEBUG SendProcessor                  - >>>> Endpoint[pgevent://localhost:5432/postgres/foobar?pass=mysecretpassword&user=postgres] Exchange[Message: hello]\n\n2015-08-19 08:41:01,201 [C EventLoop (2)] DEBUG SendProcessor                  - >>>> Endpoint[log://output] Exchange[Message: null]\n\n2015-08-19 08:41:01,201 [C EventLoop (2)] INFO  output                         - Exchange[ExchangePattern: InOnly, BodyType: null, Body: [Body is null]]\n\n\n\nIt seems that the camel-pgevent consumer uses \noutOnly.setOut(message);\ninstead of \noutOnly.setIn(message);\nand the producer\nexchange.getOut()\ninstead of \nexchange.getIn()", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.component.pgevent.PgEventEndpoint.java", "org.apache.camel.component.pgevent.PgEventProducer.java", "org.apache.camel.component.pgevent.PgEventConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 9092, "bug_title": "MQTT consumer receives duplicate messages after broker restart", "bug_description": "if clientId is specified, after ActiveMQ broker restart, camel-mqtt consumer starts to receive duplicate messages. Please see the testcase attached.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.component.mqtt.MQTTEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 8783, "bug_title": "Transacted not working correctly in scala", "bug_description": "When I try to use transacted in a scala route, I get an error and the camel context doesn&apos;t start.\nThe error states that there is no output in route.\nAfter some time watching at the code with different breakpoints, I noticed :\nIn org.apache.camel.scala.dsl.builder.RouteBuilder (l199) the transacted with URI doesn&apos;t call the stack.top.transacted with the URI parameter.\nEven with this done, it still gives the same error : \n\"Route myRoute has no output processors. You need to add outputs to the route such as to(\"log:foo\").\"\neven when  my route is as simple as :\n\"direct:something\" ==> \n{\n\n  routeId(\"log:foo\")\n\n  transacted(\"SomeStrategy\")\n\n  --> (\"log:foo\")\n\n}\n\nAfter some more debug, I saw that the output list for the transacted node stays empty, so my guess is that, in the org.apache.camel.scala.dsl.SAbstractDefinition class, the line def transacted(ref: String) = wrap(target.transacted(ref)) should return something else to handle outputs.\nSince I can mix java and scala routes, there are workarounds, but still, now that I&apos;ve gotten used to scala routes, I don&apos;t really want to go back to java routes \n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.model.RouteDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 9177, "bug_title": "combination of JPA-Component, loop and wiretap throws entitymanger cloesd exception", "bug_description": "Hi,\nI have found a problem regarding a wiretap and the camel-jpa component. I have a route with an async split through a wiretap. In each route, I use a jpa producer to persist an entity. The problem is, that the entitymanager was closed before the async routes can reach the end. The because is, that the same entitymanager is used in the async routes. The main route reaches the end and closes the entitymanager.\nE.g. like the following route:\n\n\n\n<route id=\"mainRoute>\n\n    <from uri=\"direct:restendpoint\"/>\n\n    ...\n\n    <to uri=\"jpa:MyEntity\"/>\n\n    ...\n\n    <loop copy=\"true\">\n\n        ...\n\n        <wireTap uri=\"direct:asyncroute\"/>\n\n    </loop>\n\n</route>\n\n\n\n<route id=\"asyncRoute\">\n\n    <from uri=\"direct:asyncroute\"/>\n\n    ...\n\n    <to uri=\"jpa:MyEntity\"/>\n\n    ...\n\n</route>\n\n\n\nI think a possible fix can be to check that the entitymanager is null or closed. If null or closed, create a new entitymanager.\n\n\n\npublic final class JpaHelper {\n\n    ...\n\n    public static EntityManager getTargetEntityManager(Exchange exchange, ...\n\n        ...\n\n        if (them == null || !them.isOpen()) {\n\n            // create a new entity manager\n\n            them = entityManagerFactory.createEntityManager();\n\n        ...\n\n        }\n\n    }\n\n    ...\n\n}\n\n\n\nAnother solution can be in the wiretap. There where the exchange is copied, the \"CamelEntityManager\" property could be removed.\nCurrently, my quickfix is: At the beginning of the async route I remove the entitymanager from the exchange.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.component.jpa.JpaProducer.java", "org.apache.camel.processor.idempotent.jpa.JpaMessageIdRepository.java", "org.apache.camel.component.jpa.JpaHelper.java", "org.apache.camel.component.jpa.JpaCloseEntityManagerOnCompletion.java"], "label": 1, "es_results": []}, {"bug_id": 9607, "bug_title": "chmod does not work for the File producer whitout setting a charset", "bug_description": "Hello\nhttps://github.com/apache/camel/blob/camel-2.15.2/camel-core/src/main/java/org/apache/camel/component/file/FileOperations.java\nPermissions are never updated without setting a charset.\nThe block code at line 280 seems to be at the wrong place.\nBest,\nNicolas.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.component.file.FileOperations.java"], "label": 1, "es_results": []}, {"bug_id": 10110, "bug_title": "Marshaling using CSV will insert escape char in header if using a pipe as separator", "bug_description": "This was an old bug that was supposedly resolved, but with version:2.15.2, the bug remains. The escape characters are still added into the Header Columns when marshaling back the object. \ne.g. \nSeparator = \\ \\ |\nOutput: \nHeader 1 \\ | Header2 \\ |...\nPls. advise when we can expect the fix.\nThanks.\nReply", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.2", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.dataformat.bindy.csv.BindyPipeDelimiterTest.java", "org.apache.camel.dataformat.bindy.BindyCsvFactory.java", "org.apache.camel.dataformat.bindy.model.simple.pipeline.MyData.java"], "label": 1, "es_results": []}, {"bug_id": 9124, "bug_title": "RedeliveryPattern should support property placeholders", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Can-t-configure-delayPattern-with-property-placeholders-tp5771356.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.model.RedeliveryPolicyDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 9142, "bug_title": "dropped support for multiple blueprint descriptors in unit tests", "bug_description": "Looks like update CAMEL-8948 dropped support for multiple blueprint descriptors within CamelBlueprintTestSupport file within camel-test-blueprint component. The symptom is a &apos;java.lang.RuntimeException: InputStream cannot be null&apos; for unit tests that have a getBlueprintDescriptor with multiple file references, i.e. a &apos;+&apos; concatenating two or more descriptor files.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.test.blueprint.CamelBlueprintHelper.java", "org.apache.camel.test.blueprint.CamelBlueprintTestSupport.java"], "label": 1, "es_results": []}, {"bug_id": 9150, "bug_title": "Seda suspend/resume should not trigger start/stop logic", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Suspend-Resume-Routes-Unreliable-tp5771723.html\nThe doResume method is calling doStart which may setup tasks. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.impl.RouteSedaSuspendResumeTest.java", "org.apache.camel.component.routebox.seda.RouteboxSedaConsumer.java", "org.apache.camel.component.kestrel.KestrelConsumer.java", "org.apache.camel.component.disruptor.DisruptorConsumer.java", "org.apache.camel.component.routebox.direct.RouteboxDirectConsumer.java", "org.apache.camel.spi.ShutdownPrepared.java", "org.apache.camel.impl.DefaultCamelContextSuspendResumeRouteStartupOrderTest.java", "org.apache.camel.component.seda.SedaConsumer.java", "org.apache.camel.processor.aggregate.AggregateProcessor.java", "org.apache.camel.impl.ScheduledBatchPollingConsumer.java", "org.apache.camel.component.direct.DirectConsumer.java", "org.apache.camel.impl.DefaultCamelContextSuspendResumeRouteTest.java", "org.apache.camel.impl.TwoRouteSuspendResumeTest.java", "org.apache.camel.impl.DefaultShutdownStrategy.java", "org.apache.camel.processor.RedeliveryErrorHandler.java"], "label": 1, "es_results": []}, {"bug_id": 9171, "bug_title": "camel-xmpp processes no messages when running in Karaf", "bug_description": "Camel XMPP communication is not working in version 2.15.3 when running in Karaf. The feature installation works fine but no messages are processed.\nI guess this is potentially based on changing the Smack version from 3 to 4. \nBased on the existing Camel Karaf tests I created a simple test which passes with Camel Version 2.14.3 and fails with Camel Version 2.15.3. The test can be found here https://github.com/achim86/karaf-xmpp (remember to start the XMPPServer before running the test as described in the readme).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.15.4", "fixed_files": ["org.apache.camel.component.xmpp.XmppEndpoint.java", "org.apache.camel.component.xmpp.XmppBinding.java"], "label": 1, "es_results": []}, {"bug_id": 9208, "bug_title": "camel-netty4-http does not resolve nettyHttpBinding option", "bug_description": "camel-netty4-http does not resolve or strip out the \"nettyHttpBinding\" endpoint option. For example, take the following route producer:\n<to uri=\"netty4-http:http://www.google.com:80?nettyHttpBinding=#myHttpBinding\" /> \nIt will issue the following HTTP GET request URI:\nGET http://www.google.com:80?nettyHttpBinding=%23myHttpBinding HTTP/1.1\nAssigning any of the other endpoint options, results in those options being removed from the GET request URI.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.15.4", "fixed_files": ["org.apache.camel.component.netty4.http.NettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 9230, "bug_title": "CXFRS NPE when response code not in Response.Status", "bug_description": "When used as a producer, the CXFRS component&apos;s org.apache.camel.component.cxf.jaxrs.CxfRsProducer#populateCxfRsProducerException method converts the received HTTP response status code to text:\nString statusText = Response.Status.fromStatusCode(responseCode).toString();\nIf the JAX-RS-supplied fromStatusCode method does not recognize the status code provided, it returns null, causing the CxfRsProducer to throw an NPE. \nNote that Response.Status does not encompass all response codes. I ran afoul of this when a REST service returned a 422 for a validation error.\nI will attach a unit test that illustrates this problem, as a patch to the current unit tests for the class.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.15.4", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.CxfRsProducerTest.java", "org.apache.camel.component.cxf.jaxrs.testbean.CustomerService.java", "org.apache.camel.component.cxf.jaxrs.CxfRsProducer.java"], "label": 1, "es_results": []}, {"bug_id": 9243, "bug_title": "Invocation of Bean fails when Bean extends and abstract which implements the actual method", "bug_description": "The issue described here does NOT exist in 2.15.2 and only manifests in 2.15.3.\nWith the following definition of a Bean:\n\n\n\n    public interface MyBaseInterface {\n\n        @Handler\n\n        String hello(@Body String hi);\n\n    }\n\n\n\n    public abstract static class MyAbstractBean implements MyBaseInterface {\n\n        public String hello(@Body String hi) {\n\n            return \"Hello \" + hi;\n\n        }\n\n        public String doCompute(String input) {\n\n            fail(\"Should not invoke me\");\n\n            return null;\n\n        }\n\n    }\n\n\n\n    public static class MyConcreteBean extends MyAbstractBean {\n\n    }\n\n\n\n\n\nThe following test case will fail to invoke the proper method:\n\n\n\npublic class BeanHandlerMethodTest extends ContextTestSupport {\n\n\n\n    public void testInterfaceBeanMethod() throws Exception {\n\n        BeanInfo info = new BeanInfo(context, MyConcreteBean.class);\n\n\n\n        Exchange exchange = new DefaultExchange(context);\n\n        MyConcreteBean pojo = new MyConcreteBean();\n\n        MethodInvocation mi = info.createInvocation(pojo, exchange);\n\n        assertNotNull(mi);\n\n        assertEquals(\"hello\", mi.getMethod().getName());\n\n    }\n\n\n\nThe issue is how BeanInfo.introspect determines which methods are available to be invoked.\nAt line 344, if the class is public, the interface methods are added to the list:\n\n\n\n        if (Modifier.isPublic(clazz.getModifiers())) {\n\n            // add additional interface methods\n\n            List<Method> extraMethods = getInterfaceMethods(clazz);\n\n            for (Method target : extraMethods) {\n\n                for (Method source : methods) {\n\n                    if (ObjectHelper.isOverridingMethod(source, target, false)) {\n\n                        overrides.add(target);\n\n                    }\n\n                }\n\n            }\n\n            // remove all the overrides methods\n\n            extraMethods.removeAll(overrides);\n\n            methods.addAll(extraMethods);\n\n        }\n\n\n\nHowever, all the methods from the interface are \"abstract\".  Later, when the real implementation is encountered as the code crawls up the tree, the abstract method is not replaced:\nLine 390:\n\n\n\n        MethodInfo existingMethodInfo = overridesExistingMethod(methodInfo);\n\n        if (existingMethodInfo != null) {\n\n            LOG.trace(\"This method is already overridden in a subclass, so the method from the sub class is preferred: {}\", existingMethodInfo);\n\n            return existingMethodInfo;\n\n        }\n\n\n\nFinally, during the invocation, the following was added as part of 2.15.3 release:\nLine 561:\n\n\n\n        removeAllAbstractMethods(localOperationsWithBody);\n\n        removeAllAbstractMethods(localOperationsWithNoBody);\n\n        removeAllAbstractMethods(localOperationsWithCustomAnnotation);\n\n        removeAllAbstractMethods(localOperationsWithHandlerAnnotation);\n\n\n\nAs a result, the abstract method is removed and not invoked.\nI think the fix should be to see if the existingMethodInfo references an \"abstract&apos; method and if it does and methodInfo does not, replace the existingMethodInfo with methodInfo in the collection.\nThis would preserve the preferences implied with the rest of the code while properly replacing the abstract method with their proper implementations.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.component.bean.BeanHandlerMethodTest.java", "org.apache.camel.component.bean.BeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 9259, "bug_title": "enableTrace of the Main class does not work", "bug_description": "The enableTrace() method of the Camel Main class doesn&apos;t work.\nWhen we setup the code as such\n\n\n\n    public static void main(String... args) throws Exception {\n\n        Main main = new Main();\n\n        main.enableHangupSupport();\n\n        main.addRouteBuilder(new MyRouteBuilder());\n\n        main.enableTrace();\n\n        main.run(args);\n\n\n\nand launch the Main class, than the messages reported by the route in the log are not traced at all.\nIf we debug, we can see that there is not CamelContext object when this method of the MainSupport class is called\n\n\n\n    public void enableTrace() {\n\n        this.trace = true;\n\n        for (CamelContext context : camelContexts) { // EMPTY\n\n            context.setTracing(true);\n\n        }\n\n    }\n\n\n\nThe workaround is to enable the tracing within the route definition\n\n\n\n    public void configure() {\n\n\n\n        getContext().setTracing(true);\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.main.MainTest.java", "org.apache.camel.main.MainSupport.java"], "label": 1, "es_results": []}, {"bug_id": 9316, "bug_title": "LevelDBAggregationRepository is logging warnings when exchange is already complete on first aggregation", "bug_description": "When using an aggregator with a LevelDBAggregationRepository and you are sending events, that are already complete by themselves (i.e. the fulfill the completionPredicate and they were never really aggregated), there is a lot of warning logging like\n\n\n\nNov 12, 2015 6:00:57 PM org.apache.camel.component.leveldb.LevelDBAggregationRepository confirm\n\nWARNUNG: Unable to confirm exchangeId [ID-LPNB2331-59378-1447347654170-0-939 from repository collectionNotifications: Not Found]\n\n\n\nMy guess is that calling confirm on the repository in such cases is unnecessary, but I&apos;m not sure, if it is easy to detect wether confirm should be called or not.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.15.5", "fixed_files": ["org.apache.camel.component.leveldb.LevelDBAggregationRepository.java"], "label": 1, "es_results": []}, {"bug_id": 9405, "bug_title": "Amazon SQS message deletion behaviour change on exception", "bug_description": "After upgrading applications from Camel 2.15.2 to Camel 2.15.3 we noticed a change in behaviour around the handling of a message from an SQS queue when a processing stage throws an exception.\nPreviously the message would not be deleted, and would become available to the SQS again.\nAfter the upgrade, messages are deleted even if an exception occurs during the processing stage.\nI will attach a unit test which demonstrates the behaviour by:\n\nincluding a mock process which throws an exception\nasserting that there is no call to delete a message.\n\nI believe that the commit which changed the behaviour is:\nhttps://github.com/apache/camel/commit/bd10c49bdcdbe6181e35461c449ea61db9a13bf1\nThis issue may be specific to the inclusion of a seda stage.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.component.aws.sqs.SqsProducer.java", "org.apache.camel.component.aws.sqs.SqsConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 9106, "bug_title": "URI option mapMailMessage does not obey peek=true option", "bug_description": "URI option mapMailMessage=true as is the default with Mail Component fetches IMAP-messages without peek=true option. This results to faulty rollback logic since in case of an exception and rollback, messages are already marked with flag SEEN and won&apos;t be rolled back to UNSEEN.\nMessages are marked with peek-option in processBatch-method but mapping mail messages to Camel messages happens before that method call in createExchanges-method.\nI&apos;ve attached a patch where peek option is set to the messages already in the poll-method which resolves the issue. Unfortunately I couldn&apos;t write a proper jUnit test for this scenario since org.jvnet.mock_javamail.Mailbox doesn&apos;t seem to support marking messages as SEEN even if they have been fetched.  However, I have attached traces of faulty and fixed run with a test route (that route is also included).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.3", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.component.mail.MailConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 9319, "bug_title": "SshClient resource leak when used from ProducerTemplate", "bug_description": "When using ProducerTemplate for execution of an ssh command (i.e. producerTemplate.requestBody(\"ssh://...\", String.class)), 11 threads are being created, that are never killed. Any subsequent calls create new threads, eventually this exhausts memory and thread resources on the machine.\nThese threads are not being killed even when explicitly stopping the ProducerTemplate.\nHere is a more detailed discussion, together with profiler results and code to reproduce the issue:\nhttp://stackoverflow.com/questions/33671567/spring-boot-camel-producertemplate-thousands-of-threads\nhttp://camel.465427.n5.nabble.com/Spring-Boot-Camel-producerTemplate-ssh-spawning-thousands-of-threads-td5773741.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.4", "fixed_version": "camel-2.15.5", "fixed_files": ["org.apache.camel.component.ssh.SshConsumer.java", "org.apache.camel.component.ssh.SshProducer.java", "org.apache.camel.component.ssh.SshEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 9631, "bug_title": "The OsgiServiceRegistry class caches service references", "bug_description": "As discussed in http://camel.465427.n5.nabble.com/OsgiServiceRegistry-caching-service-references-why-td5777410.html the OsgiServiceRegistry class caches service references hence limits the ability to dynamically install and use a new version of the same service", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.4", "fixed_version": "camel-2.18.0", "fixed_files": ["org.apache.camel.core.osgi.OsgiServiceRegistry.java"], "label": 1, "es_results": []}, {"bug_id": 9606, "bug_title": "SJMS Consumer-Producer in transaciton", "bug_description": "I&apos;m not 100% sure this is a bug but it feels that way from conversation I had via mailing lists.\nI&apos;m trying to ensure transactional processing between SJMS consumer and producer (e.g. using same JMS session). \nIn other words this simple case:\n1. prepare higher amount of JMS messages in broker (e.g. ActiveMQ with 1000 messages) \n2. use Camel route from input queue to output queue using trasacted=true \n3. start context (starts consuming messages) and in any time kill java process \nWhen I kill process, I would expect that sum of messages in input and output queue will be 1000 - so the transaction works. But what happens is that I always end up with 1001+ messages. Maybe it is misconfiguration of routes or misunderstanding how SJMS can work.\nI feel this is critical because JMS is generally used because it its transactional capabilities.\nHere is the sample code I used for reproduction (using ActiveMQ):\n\n\n\npublic class SjmsTransaction {\n\n\n\n    public static void main(String[] args) throws Exception {\n\n        RouteBuilder rb = new RouteBuilder() {\n\n            @Override\n\n            public void configure() throws Exception {\n\n                onException(Exception.class)\n\n                        .process(systemOut(\"Exception!!\"));\n\n\n\n                from(\"sjms:queue:test-in?transacted=true&consumerCount=5\")\n\n                        .process(systemOut(\"Processing\"))\n\n                        .to(\"sjms:queue:test-out?transacted=true\")\n\n                        .process(systemOut(\"Processed\"));\n\n            }\n\n        };\n\n\n\n        CamelContext context = new DefaultCamelContext();\n\n        addJmsComponent(context);\n\n        context.addRoutes(rb);\n\n\n\n        System.out.println(\"=====> Starting context\");\n\n        context.start();\n\n        // Now the context will run and consume messages, when I kill application by force in any time\n\n        // I expect this to be true: <#messagesInInputAtBeginning> == <#messagesInInputNow> + <#messagesInOutputNow>\n\n        // What happens is that there is always < (e.g. I submitted 1000 messages, out has 500, in has 501)\n\n    }\n\n\n\n    private static void addJmsComponent(CamelContext context) {\n\n        ConnectionFactory factory = new ActiveMQConnectionFactory(\"tcp://localhost:61616\");\n\n        ConnectionFactoryResource connResource = new ConnectionFactoryResource(5, factory);\n\n        SjmsComponent comp = new SjmsComponent();\n\n        comp.setConnectionResource(connResource);\n\n        context.addComponent(\"sjms\", comp);\n\n    }\n\n\n\n    private static Processor systemOut(final String message) {\n\n        return new Processor() {\n\n            @Override\n\n            public void process(Exchange exchange) throws Exception {\n\n                System.out.println(exchange.getExchangeId() + \": \" + message);\n\n            }\n\n        };\n\n    }\n\n}\n\n\n\nNote that I tried to use it with various combinations of acknowledgeMode and In/InOut exchange pattern - but without luck.\nI&apos;m not that much oriented in Camel source code but I found that JMS session is held within the exchange so probably when producer finds in an exchange existing JMS session and is configured to be transacted, then maybe it can participate this session? Or maybe there are other hooks (like Synchronization objects) in some registry that take care of this issue?\nHere is the link to the previous mailing list conversation: http://camel.465427.n5.nabble.com/SJMS-transaction-td5777522.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.4", "fixed_version": "camel-2.19.0", "fixed_files": ["org.apache.camel.component.sjms.consumer.AbstractMessageHandler.java", "org.apache.camel.component.sjms.SjmsProducer.java", "org.apache.camel.component.sjms.SjmsConstants.java", "org.apache.camel.component.sjms.producer.InOutProducer.java", "org.apache.camel.component.sjms.SjmsConsumer.java", "org.apache.camel.component.sjms.SjmsEndpoint.java", "org.apache.camel.component.sjms.producer.InOnlyProducer.java"], "label": 1, "es_results": []}, {"bug_id": 9480, "bug_title": "IdempotentConsumer - If exception from repo it should be able to handle by onException", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Exception-from-idempotentConsumer-not-propagating-to-onException-tp5775779.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.5", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.processor.idempotent.IdempotentConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 9543, "bug_title": "Discovering new type converters in OSGi wipes out those manually added", "bug_description": "When adding a type converter manually like:\n\n\n\ngetContext().getTypeConverterRegistry().addTypeConverter(A.class, B.class, new ABConverter()); \n\n\n\nIt gets removed from the type converter registry when a new type converter is discovered in a newly installed bundle. (Like when you install camel-hl7 feature in Karaf say).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.5", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.core.osgi.OsgiTypeConverter.java"], "label": 1, "es_results": []}, {"bug_id": 9569, "bug_title": "Idempotent Consumer EIP - Memory leak when add/remove routes", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Leak-of-Memory-using-JdbcMessageIdRepository-tp5777242.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.5", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.processor.idempotent.IdempotentConsumer.java", "org.apache.camel.model.IdempotentConsumerDefinition.java", "org.apache.camel.spi.IdempotentRepository.java"], "label": 1, "es_results": []}, {"bug_id": 9896, "bug_title": "Deadletter Failure processor is invoked even if error handling strategy defines to continue routing", "bug_description": "When a DeadLetterChannel is used as context scoped error handling strategy with FailureProcessor set and a route scoped error handling strategy is defined to continue routing in case of certain exception classes, then it is observed that even in case of exceptions which belong to exceptions defined in OnException clause, the dead letter&apos;s failure processor is still triggered. \nA test case to simulate this behaviour is pasted. \nhttp://pastebin.com/raw/cfQhsJj0 - contains test case.\nhttp://pastebin.com/raw/K6wZc9Yh - test output", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.15.6", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.issues.OnExceptionContinuedNoFailureProcessorTest.java", "org.apache.camel.processor.RedeliveryErrorHandler.java"], "label": 1, "es_results": []}, {"bug_id": 9199, "bug_title": "RabbitMQ Consumer threads crash when sending partially serializable objects", "bug_description": "The RabbitMQ consumer thread crashes when sending an object that implements Serializable but contains non Serializable objects. \nI will have PR very soon to fix this and add tests.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.component.rabbitmq.RabbitMQInOutIntTest.java", "org.apache.camel.component.rabbitmq.RabbitMQConsumer.java", "org.apache.camel.component.rabbitmq.RabbitMQEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 9215, "bug_title": "Missing .handled(true) in Camel 2.16.0 when using wiretap and newExchangeBody", "bug_description": "This worked in 2.15.x: \nRoutesDefinition routeDef = someRouteDef(); \nrouteDef.onException(someException) \n   .wireTap(someUri) \n   .newExchange(someProcessor) \n   .end() \n   .handled(true) \n   .bean(someBean); \nBut in 2.16.0 it seems like .handled(..) isn&apos;t available anymore.  Any \nsuggestions on how to mark this as handled now? ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.model.WireTapDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 9220, "bug_title": "swagger - Model schema not including nested objects", "bug_description": "This issue relates to the new camel-swagger-java component and not the camel-swagger component. JIRA is not yet updated for the new camel-swagger-java component.\nWhen there are nested objects in the Api model for swagger, they are not being represented correctly in the model schema. See below for an example:\nSomeRequest.java\n\n\n@ApiModel\n\npublic class SomeRequest {\n\n    @ApiModelProperty\n\n    private List<SomeObject> someObjects;\n\n    ...\n\n}\n\n\n\nSomeObject.java\n\n\n@ApiModel\n\npublic class SomeObject {\n\n    @ApiModelProperty\n\n    private String somePropertyOne;\n\n    @ApiModelProperty\n\n    private String somePropertyTwo;\n\n    ...\n\n}\n\n\n\nShould have a model in swagger of:\n\n\n\n{ \n\n  \"someObjects\": [ \n\n    { \n\n      \"somePropertyOne\": \"\", \n\n      \"somePropertyTwo\": \"\" \n\n    } \n\n  ] \n\n}\n\n\n\nBut instead has:\n\n\n\n{ \n\n  \"someObjects\": [ \n\n    \"SomeObject\" \n\n  ] \n\n}\n\n\n\nThis model is invalid and makes the swagger documentation useless as no one knows what the request model is supposed to be.\nNote: this worked correctly on camel 2.15.3 with the scala based camel-swagger component.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.swagger.RestModelConverters.java", "org.apache.camel.swagger.RestSwaggerReader.java"], "label": 1, "es_results": []}, {"bug_id": 9233, "bug_title": "ZipFileDataFormat does not take exchange's charset into account", "bug_description": "ZIP streams are always encoded with the default UTF-8 charset, even if in the Exchange header a different charset is set. (Exchange.CHARSET_NAME)\n(Please note that this is my first bugreport and -fix for the camel project, I hope i do it right)", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.dataformat.zipfile.ZipFileDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 9248, "bug_title": "Exception is thrown when receiving a message where JMSDestination is null", "bug_description": "The change linked below causes camel-jms throws an exception when a message is received where JMSDestination is null (when used with WebSphere MQ). \nThis works as expected in 2.15.3.\nThe message looks like this (anonymized)\n  JMSMessage class: jms_text\n  JMSType:          null\n  JMSDeliveryMode:  2\n  JMSExpiration:    0\n  JMSPriority:      5\n  JMSMessageID:     ID:c3e2d840e3d4d8d44040404040404040cfbd668a5f4f4261\n  JMSTimestamp:     1445609217800\n  JMSCorrelationID: null\n  JMSDestination:   null\n  JMSReplyTo:       queue://TEST/INPUT.QUEUE?targetClient=1\n  JMSRedelivered:   false\n    JMSXAppID: ilities\\RFHUtil\\rfhutilc.exe\n    JMSXDeliveryCount: 1\n    JMSXUserID: MQXPLO      \n    JMS_IBM_Character_Set: IBM277\n    JMS_IBM_Encoding: 785\n    JMS_IBM_Format: MQSTR   \n    JMS_IBM_MsgType: 8\n    JMS_IBM_PutApplType: 11\n    JMS_IBM_PutDate: 20151023\n    JMS_IBM_PutTime: 14065780\nThe following is a link to the troublesome patch included in 2.16.0:\nhttps://fisheye6.atlassian.com/browse/camel-git/components/camel-jms/src/main/java/org/apache/camel/component/jms/EndpointMessageListener.java?r2=812fa060bfeac5b320624b5d6d4833ac441d42c9&r1=e00e0d6599b01733c270f3053e23118d35ea0881\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.component.jms.JmsMessageHelper.java", "org.apache.camel.component.jms.EndpointMessageListener.java"], "label": 1, "es_results": []}, {"bug_id": 9247, "bug_title": "rest-dsl with api-doc should allow multiple rest's", "bug_description": "See SO\nhttp://stackoverflow.com/questions/33291657/how-to-have-multiple-camel-rest-dsl-definitions-with-swagger\nThe api-doc endpoint should merge multiple rest&apos;s together.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.component.netty4.http.NettyHttpComponent.java", "org.apache.camel.component.netty.http.BaseNettyTest.java", "org.apache.camel.component.netty.http.NettyHttpComponent.java", "org.apache.camel.builder.RouteBuilder.java", "org.apache.camel.model.rest.RestDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 9245, "bug_title": "camel-paho - Endpoint should allow a flexible naming.", "bug_description": "If endpoint doesn&apos;t match exactly paho: it fails to publish to the correct topic.\nOnce should be able in Spring to @Autowired two different PahoComponent pointing to different Application Context defined PahoComponent bean id&apos;S.\nCurrently if bean id is not named exactly paho it fails to publish to the correct topic and for example instead of input/1 publishes to t://input/1 when id is mypaho instead of paho\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.component.paho.PahoEndpoint.java", "org.apache.camel.component.paho.PahoComponent.java", "org.apache.camel.component.paho.PahoComponentTest.java"], "label": 1, "es_results": []}, {"bug_id": 9269, "bug_title": "NotifyBuilder.fromRoute() does not work for some endpoint types", "bug_description": "NotifyBuilder.fromRoute() does not work if the endpoint uri in the from() clause for a route does not match the actual endpoint uri the exchange was sent to. Because we also have the route id itself available in the exchange, we can use that as a fallback when the match on from endpoint uri doesn&apos;t work.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.builder.NotifyBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 9290, "bug_title": "netty4 consumer in clientMode only reconnects once", "bug_description": "Currently, when the server goes down the first time, it will reconnect fine. But if the server goes down again later, it will not attempt a reconnect.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.1", "fixed_files": ["org.apache.camel.component.netty4.ClientModeTCPNettyServerBootstrapFactory.java"], "label": 1, "es_results": []}, {"bug_id": 9331, "bug_title": "Thread leak in Http4Endpoint, doStop() does not close() the httpClient", "bug_description": "Thread leak in Http4Endpoint, doStop() does not close() the httpClient\nAlso when using a shared client connection manager the builder must be told of this so it does not close() a shared connection manager when the http client is closed.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.2", "fixed_files": ["org.apache.camel.component.http4.HttpEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 9277, "bug_title": "FTP shutdown whole context on startup if throwExceptionOnConnectFailed true", "bug_description": "I recently upgrade from camel 2.14.1 to 2.16.1 and discover following issue: Unable to catch the Exception on startup if throwExceptionOnConnectFailed=true and autoCreate=true or autoCreate not declared.\nIf at the time of startup, the FTP site was down or unreachable, the Exception cannot be handled by any means I currently know which are onException route and doTry-doCatch method.\nFollowing are my FTP parameters:\n&maximumReconnectAttempts=0&stepwise=false&disconnect=true&throwExceptionOnConnectFailed=true&consumer.bridgeErrorHandler=true\"\nThe only workaround without compromising error log functionality is by adding parameter \"autoCreate=false\".\nBased on Camel-FTP documentation: \"\nFor Camel 2.16, autoCreate option is supported. When consumer starts, before polling is scheduled, there&apos;s additional FTP operation performed to create the directory configured for endpoint. The default value for autoCreate is true.\n\"\nIn my opinion, the exception should be able to be caught, instead shutdown the whole Camel context. And description for throwExceptionOnConnectFailed need to be updated so unaware developer does not suffer the same as I did.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.2", "fixed_files": ["org.apache.camel.component.file.remote.SftpConsumer.java", "org.apache.camel.component.file.remote.FtpConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 9386, "bug_title": "Camel-git: Support credentials in clone operation", "bug_description": "As noted here: https://github.com/oscerd/camel-git/issues/1\nCurrently we don&apos;t support credentials for the clone operation.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.0", "fixed_version": "camel-2.16.2", "fixed_files": ["org.apache.camel.component.git.producer.GitProducer.java"], "label": 1, "es_results": []}, {"bug_id": 9366, "bug_title": "CXFRS \"skipFaultLogging\" attribute does not work in Spring", "bug_description": "When defining a camel-cxf cxf:rsClient bean with skipFaultLogging enabled, like this\n\n\n\n<camelcxf:rsClient id=\"rsClient\" address=\"http://localhost:9081/CxfRsService/rest\"\n\n    serviceClass=\"org.apache.camel.component.cxf.jaxrs.testbean.CustomerService\"\n\n    skipFaultLogging=\"true\" />\n\n\n\n... the exception will still be logged. If you add the flag to the endpoint URI instead, skipFaultLogging works as expected.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.15.5", "fixed_files": ["org.apache.camel.component.cxf.spring.CxfRsClientFactoryBeanTest.java", "org.apache.camel.component.cxf.spring.SpringJAXRSClientFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 9381, "bug_title": "Upgrade camel-amqp to the latest qpid-jms-client", "bug_description": "We should upgrade camel-amqp to use the latest qpid-jms-client. That would mean that we should also drop support for AMQP < 1.0.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.17.0", "fixed_files": ["org.apache.camel.component.amqp.AMQPRouteTest.java", "org.apache.camel.component.amqp.AMQPComponent.java"], "label": 1, "es_results": []}, {"bug_id": 9417, "bug_title": "SOAP 1.2 Fault processing should use value of Reasontext to build exception message", "bug_description": "org.apache.camel.dataformat.soap.Soap12DataFormatAdapter#createExceptionFromFault uses org.w3._2003._05.soap_envelope.Reasontext#toString to generate the exception message, but Reasontext does not have a toString implementation, so it prints values like org.w3._2003._05.soap_envelope.Reasontext@2b6afea1 and the Reasontext value is lost. Reasontext#getValue which returns a human-readable string should be used instead.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.16.2", "fixed_files": ["org.apache.camel.dataformat.soap.Soap12DataFormatAdapter.java"], "label": 1, "es_results": []}, {"bug_id": 9426, "bug_title": "spring-boot with rest-dsl with api-doc registers multiple instances with multiple RouteBuilders", "bug_description": "Spring-boot + swagger throws exception when there is another RouteBuilder component present:\norg.apache.camel.spring.boot.CamelSpringBootInitializationException: org.apache.camel.FailedToStartRouteException: Failed to start route route4 because of Multiple consumers for the same endpoint is not allowed: Endpointhttp://0.0.0.0:10000/api-doc?httpMethodRestrict=GET\nI used the same example as CAMEL-9247 but added an empty RouteBuilder component (AnotherRouter.java)\nThis makes it so we cannot have more than one RouteBuilder in our app.\nExample here:\nhttps://github.com/jmandawg/camel-spring-boot-swagger-problem", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.16.2", "fixed_files": ["org.apache.camel.builder.RouteBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 9375, "bug_title": "camel-tarfile - TarSplitter includes one extra empty entry at the end", "bug_description": "This unit test demonstrates the bug\norg.apache.camel.dataformat.tarfile.TarSplitterRouteIssueTest\nThere is only 3 files in the tar file that it uses, but the splitter has 4 entries where the last is empty.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.16.2", "fixed_files": ["org.apache.camel.dataformat.tarfile.TarIterator.java", "org.apache.camel.dataformat.tarfile.TarFileDataFormat.java", "org.apache.camel.dataformat.tarfile.TarSplitter.java", "org.apache.camel.dataformat.tarfile.TarSplitterRouteIssueTest.java"], "label": 1, "es_results": []}, {"bug_id": 9406, "bug_title": "Request Reply via RabbitMQ not handling reply correctly", "bug_description": "The reply will be a empty message.\nWhen \norg.apache.camel.component.rabbitmq.reply.ReplyManagerSupport#processReply\ncalls\norg.apache.camel.component.rabbitmq.RabbitMQEndpoint#setRabbitExchange\nThe method setRabbitExchange will get `In` message and setBody for reply message while the expected message is `Out`.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.17.0", "fixed_files": ["org.apache.camel.component.rabbitmq.reply.ReplyManagerSupport.java", "org.apache.camel.component.rabbitmq.RabbitMQEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 9468, "bug_title": "Bindy fails to marshal objects in Spring Boot", "bug_description": "When using Spring Boot or when manually registering an ObjectToList-converter with Camel, Bindy fails to marshal objects.\nPR submitted with fix.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.16.2", "fixed_files": ["org.apache.camel.dataformat.bindy.fixed.BindyFixedLengthDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 9503, "bug_title": "OnCompletion - restores the rollback only last in the wrong key", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.processor.OnCompletionProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 9505, "bug_title": "RabbitMQConsumer do not use Camel ExceptionHandler BEFORE requeing message", "bug_description": "The use case is :\nonException(NotHandledException.class)\n   .handled(false)\n   .log(\"Exception not handled\");\nonException(HandledException.class)\n   .handled(true)\n   .log(\"Exception handled\");\nfrom(\"rabbitmq://...&autoAck=false\")\n   .setHeader(RabbitMQConstants.REQUEUE, constant(true))\n   .to(...);\nIf the route generate a NotHandledException, the message is requeue in RabbitMQ, it works fine.\nIf the route generate a HandledException, the message is requeue in RabbitMQ before the execution of Camel ExceptionHandler wich should handle the exception and should not propagate it. \nThe message handled by Camel ExceptionHandler should not be requeue in RabbitMQ since the exception is handled.\nThe related code is in :\norg.apache.camel.component.rabbitmq.RabbitConsumer.handleDelivery\nMaybe this line :\ngetExceptionHandler().handleException(\"Error processing exchange\", exchange, exchange.getException());\nshould be before :\nif (deliveryTag != 0 && !consumer.endpoint.isAutoAck()) {\n   log.trace(\"Rejecting receipt [delivery_tag={}] with requeue={}\", deliveryTag, isRequeueHeaderSet);\n   if (isRequeueHeaderSet) \n{\n\n      channel.basicReject(deliveryTag, true);\n\n   }\n else \n{\n\n      channel.basicReject(deliveryTag, false);\n\n   }\n}\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.component.rabbitmq.RabbitMQConsumer.java", "org.apache.camel.component.rabbitmq.RabbitMQRequeueIntTest.java"], "label": 1, "es_results": []}, {"bug_id": 9431, "bug_title": "camel-spring-boot - TypeConverter autoconfiguration leads to invalid shutdown sequence", "bug_description": "When using autoconfiguration provided by the camel-spring-boot artifact,  a TypeConverter bean is automatically registered into the Spring context: \nTypeConversionConfiguration.java\n\n\n@Bean\n\nTypeConverter typeConverter(CamelContext camelContext) {\n\n    return camelContext.getTypeConverter();\n\n}\n\n\n\nThe returned bean is an instance of DefaultTypeConverter, which in turn implements ServiceSupport and its method public void shutdown(). This method is infered as a destroy-method by Spring, and called during the shutdown of the ApplicationContext. \nAs a consequence, the TypeConverter will be destroyed before the CamelContext, effectively preventing any type conversion support for the inflight messages that have still to be processed during the graceful shutdown period of Camel. \nAFAIK the simple fix would be to disable the destroy-method inference using @Bean(destroyMethod=\"\"). This will let Camel have a chance to perform a clean shutdown in the right sequence. \nAs a workaround, it is possible to entirely disable the registration of the type converter in Spring using the property\ncamel.springboot.type-conversion = false", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.spring.boot.TypeConversionConfiguration.java", "org.apache.camel.spring.boot.CamelAutoConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 9432, "bug_title": "Bindy CSV separator not treated as regex but fixed character in all cases", "bug_description": "In the camel-bindy documentation (http://camel.apache.org/bindy.html) it says that the @CsvRecord separator parameter is interpreted as a regex. While that does seem to be the case when the record is being parsed, it seems to be treated as a literal string when autospanLine is true.\nFor example, if we have \n\n\n\n@CsvRecord(separator=\"\\\\s+\", autospanLine=true)\n\n\n\n and we have defined three string @DataField fields, and we have a line of input like this:\n\n\n\n     field1 field2 field3a field3b\n\n\n\nthen the third field value will be\n\n\n\n    \"field3a\\s+field3b\"\n\n\n\nLooking at the code in BindyCsvDataFormat.java it seems that this is because in some cases the separator string is treated as a literal string, not a regex. For example, this also seems to be true in some cases in the unquoteTokens method.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.dataformat.bindy.csv.BindyCsvDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 9665, "bug_title": "camel-ahc-ws Consumer does not connect", "bug_description": "WsEndpoint.connect(WsConsumer) should connect to the WebSocket server in case this is not already done. \nThe test case (WsProducerConsumerTest.java) only works, since the producer is connecting first and the consumer is using the same connection. If no producer is used, then no connection is created and the consumer will not receive any messages.\nSuggestion:\n\n\n\n  void connect(final WsConsumer wsConsumer)\n\n  {\n\n    this.consumers.add(wsConsumer);\n\n\n\n    if (this.websocket == null || !this.websocket.isOpen())\n\n    {\n\n      this.connect();\n\n    }\n\n  }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.component.ahc.ws.WsProducer.java", "org.apache.camel.component.ahc.ws.WsEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 9506, "bug_title": "STOMP component does not handle stream objects", "bug_description": "The STOMP component only performs a very simple toString() on the body of the incoming exchange when it creates a STOMP message. This does not work when the body does not support a toString() which gives some reasonable human readable representation of the object. This is exactly the case if you try to do the following:\n\n\n\nfrom(\"direct:a\").marshal().json(JsonLibrary.Gson).to(\"stomp:topic:foobar\")\n\n\n\nThe result of the JSON serialization is a Stream, and the toString() on a Stream results in an object hash (e.g. \"[B@30479402\"). A more appropriate thing to do here is similar to what the File component does. (Basically, read the stream for the data.)", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.16.2", "fixed_files": ["org.apache.camel.component.stomp.StompProducerTest.java", "org.apache.camel.component.stomp.StompEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 9566, "bug_title": "camel-asf-ws component does not reconnect to the web socket ", "bug_description": "Create a web socket consumer route \nEnsure that the web socket provider is available\nCreate connection to the web socket provider from the consumer\nSend a message from the provider (send to all)\nMessage gets consumed in the consumer route\nBring down the producer\nBring up the producer \nCreate connection to the web socket provider from the consumer\nSend a message from the provider (send to all)\nMessage is not printed as reconnect was not done\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.ahc.ws.WsConsumer.java", "org.apache.camel.component.ahc.ws.WsProducerConsumerTest.java", "org.apache.camel.component.ahc.ws.WsEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 9820, "bug_title": "SFTP readLock=changed does not work with readLockMinAge option", "bug_description": "For the feature I am building, I need to use SFTP to fetch files to process. When it comes to large files, I want to be able to pick up a file for processing only when its been completely written. In order to achieve this, I tried the following combination suggesting the file age should be 10 minutes or more.\nreadLock=changed\nreadLockMinAge=10m \nThis did not work as expected and kept picking up the files in the next poll as soon as the write has started. I found the following while debugging.\nIn class SftpChangedExclusiveReadLockStrategy,  \nlong startTime = (new Date()).getTime();  ==> returns time in milliseconds as long\nnewLastModified = f.getAttrs().getMTime();  ==> returns time in seconds as int casted to long.\nHence when the following check is made, the comparison becomes to be between newLastModified which is in seconds and newOlderThan in milliseconds hence never quite honoring the readLockMinAge specified.\n(minAge != 0 && newLastModified < newOlderThan)\nIt looks like there should be \nnewLastModified = f.getAttrs().getMTime() * 1000; \nto convert this to milliseconds.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.1", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 9553, "bug_title": "Twitter consumer does not respect the delay parameter", "bug_description": "I&apos;ve configured a polling Twitter endpoint with nearly default Spring configuration as follows:\n\n\n\nfrom(\"twitter://search?type=polling&keywords=searchterms&delay=60&consumerKey=[s]&consumerSecret=[s]&accessToken=[s]&accessTokenSecret=[s]\")\n\n\n\nHowever, it doesn&apos;t seem to respect the delay parameter.\nOn debugging, the delay parameter is set properly:\n\n\n\n    public TwitterConsumerPolling(TwitterEndpoint endpoint, Processor processor,\n\n                                  Twitter4JConsumer twitter4jConsumer) {\n\n        super(endpoint, processor);\n\n\n\n        this.twitter4jConsumer = twitter4jConsumer;\n\n\n\n        int delay = endpoint.getProperties().getDelay();\n\n        setInitialDelay(1);\n\n        setDelay(delay);  // delay is set properly to 60 here\n\n        setTimeUnit(TimeUnit.SECONDS);\n\n    }\n\n\n\nbut when the run() method of the ScheduledPollConsumer runs, the delay parameter is still the default of 500.\n\n\n\npublic void run() {\n\n        // avoid this thread to throw exceptions because the thread pool will not re-schedule a new thread\n\n        try {\n\n            // log starting\n\n            if (LoggingLevel.ERROR == runLoggingLevel) {\n\n                LOG.error(\"Scheduled task started on:   {}\", this.getEndpoint());\n\n            } else if (LoggingLevel.WARN == runLoggingLevel) {\n\n                LOG.warn(\"Scheduled task started on:   {}\", this.getEndpoint());\n\n            } else if (LoggingLevel.INFO == runLoggingLevel) {\n\n                LOG.info(\"Scheduled task started on:   {}\", this.getEndpoint());\n\n            } else if (LoggingLevel.DEBUG == runLoggingLevel) {\n\n                LOG.debug(\"Scheduled task started on:   {}\", this.getEndpoint());\n\n            } else {\n\n                LOG.trace(\"Scheduled task started on:   {}\", this.getEndpoint());\n\n            }\n\n\n\n            // execute scheduled task\n\n            doRun();\n\n\n\n            // log completed\n\n            if (LoggingLevel.ERROR == runLoggingLevel) {\n\n                LOG.error(\"Scheduled task completed on: {}\", this.getEndpoint());\n\n            } else if (LoggingLevel.WARN == runLoggingLevel) {\n\n                LOG.warn(\"Scheduled task completed on: {}\", this.getEndpoint());\n\n            } else if (LoggingLevel.INFO == runLoggingLevel) {\n\n                LOG.info(\"Scheduled task completed on: {}\", this.getEndpoint());\n\n            } else if (LoggingLevel.DEBUG == runLoggingLevel) {\n\n                LOG.debug(\"Scheduled task completed on: {}\", this.getEndpoint());\n\n            } else {\n\n                LOG.trace(\"Scheduled task completed on: {}\", this.getEndpoint());\n\n            }\n\n\n\n        } catch (Error e) {\n\n            // must catch Error, to ensure the task is re-scheduled\n\n            LOG.error(\"Error occurred during running scheduled task on: \" + this.getEndpoint() + \", due: \" + e.getMessage(), e);\n\n        }\n\n    }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.17.0", "fixed_files": ["org.apache.camel.component.twitter.consumer.TwitterConsumerPolling.java", "org.apache.camel.component.twitter.SearchPollingTest.java", "org.apache.camel.component.twitter.TwitterEndpointPolling.java", "org.apache.camel.component.twitter.TwitterConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 9557, "bug_title": "Facebook consumer throws IllegalArgumentException with reading parameters", "bug_description": "I&apos;ve configured a polling Facebook consumer with nearly default Spring configuration as follows:\n\n\n\n<camel:from uri=\"facebook://getFeed?\n\n    reading.since=2016-01-01T00:00:00Z&amp;\n\n    userId=myUserId&amp;\n\n    consumer.delay=60000&amp;\n\n    oAuthAppId=[s]&amp;\n\n    oAuthAppSecret=[s]&amp;\n\n    oAuthAccessToken=[s]\n\n    \"/>\n\n\n\nHowever, an IllegalArgument is thrown with any reading.* parameter regardless of endpoint:\n\n\n\njava.lang.IllegalArgumentException: No matching operation for getFeed, with arguments [readingOptions, reading, userId]\n\n\n\nOn debugging, the method FacebookEndpoint.configureProperties sets two properties on the configuration object: (readingOptions, reading).\n\n\n\nFacebookPropertiesHelper.configureReadingProperties(configuration, options);\n\n\n\nreadingOptions is a Map which is finally size 0 after all the relevant properties are extracted into the reading parameter. However, it isn&apos;t nullified.\nIn the subsequent method initState(), it tries to look for a method in FacebookMethodTypes with the signature:\n\n\n\ngetFeed(String userId, Reading reading)\n\n\n\nBecause readingOptions is non null, it looks for a readingOptions parameter as well which doesn&apos;t exist, throwing the Exception.\nIs there a workaround I can use for the moment? Thanks!", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.facebook.data.FacebookPropertiesHelper.java"], "label": 1, "es_results": []}, {"bug_id": 9560, "bug_title": "RabbitMQ InOut Producer generates a null body response when a correlationId is used", "bug_description": "The use case uses a correlationId with RabbitMQ InOut Producer :\n\n\n\nfrom(\"direct:route\")\n\n   .setHeader(RabbitMQConstants.EXCHANGE_NAME, constant(\"exchange\"))\n\n   .setHeader(RabbitMQConstants.CORRELATIONID, constant(\"123\"))\n\n   .inOut(\"rabbitmq://...\")\n\n\n\nThe response always contains a null out body message.\nThe problem is located when restoring the original correlation id in org.apache.camel.component.rabbitmq.reply.ReplyManagerSupport : \n\n\n\npublic void processReply(ReplyHolder holder)\n\n\n\n\n\n\n// restore correlation id in case the remote server messed with it\n\nif (holder.getOriginalCorrelationId() != null) {\n\n   if (exchange.getOut() != null) {\n\n      exchange.getOut().setHeader(RabbitMQConstants.CORRELATIONID, \n\n         holder.getOriginalCorrelationId());\n\n   } else {\n\n      exchange.getIn().setHeader(RabbitMQConstants.CORRELATIONID, \n\n         holder.getOriginalCorrelationId());\n\n   }\n\n}\n\n\n\nThe test exchange.getOut() is always successful because of the lazy creation in DefaultExchange.getOut\n\n\n\npublic Message getOut() {\n\n   // lazy create\n\n   if (out == null) {\n\n      out = (in != null && in instanceof MessageSupport)\n\n      ? ((MessageSupport)in).newInstance() : new DefaultMessage();\n\n      configureMessage(out);\n\n   }\n\n   return out;\n\n}\n\n\n\nThe in body message contains the correct response from RabbitMQ. However, since the out message will be always created with a null body, the null body response will be returned in the pipeline", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.rabbitmq.reply.ReplyManagerSupport.java"], "label": 1, "es_results": []}, {"bug_id": 9558, "bug_title": "DefaultErrorHandler logs despite \"handled true\"", "bug_description": "The assumption is that handled exceptions will not be propagated any further.\nIn version 2.15.5 the test code (from the camel-jetty page) runs fine - but since 2.16.0 the handled exception is propagated to the DefaultErrorHandler which causes ERROR logs for every handled exception.\nI could not find a hint in the release notes - so I think it&apos;s a bug.\n\n\n\npublic class MyTest extends CamelTestSupport {\n\n\n\n    @Test\n\n    public void test() throws Exception {\n\n        // using httpclient 4.3.5\n\n        CloseableHttpClient httpclient = HttpClients.createDefault();\n\n        HttpGet httRequest = new HttpGet(\"http://127.0.0.1:7890/myserver\");\n\n        HttpResponse response = httpclient.execute(httRequest);\n\n        assertThat(response.getStatusLine().getStatusCode(), is(500));\n\n    }\n\n\n\n    @Override\n\n    protected RouteBuilder createRouteBuilder() throws Exception {\n\n        return new RouteBuilder() {\n\n\n\n            @Override\n\n            public void configure() throws Exception {\n\n                from(\"jetty://http://localhost:7890/myserver\").tracing()\n\n                        // use onException to catch all exceptions and return a custom reply message\n\n                        .onException(Exception.class).handled(true)\n\n                        // create a custom failure response\n\n                        .transform(constant(\"Dude something went wrong\"))\n\n                        // we must remember to set error code 500 as handled(true)\n\n                        // otherwise would let Camel thing its a OK response (200)\n\n                        .setHeader(Exchange.HTTP_RESPONSE_CODE, constant(500)).end()\n\n                        // now just force an exception immediately\n\n                        .throwException(new IllegalArgumentException(\"I cannot do this\"));\n\n            }\n\n        };\n\n    }\n\n}\n\n\n\nReferences from the user-list\nhttp://camel.465427.n5.nabble.com/onException-block-in-camel-2-16-1-td5777019.html#a5777064\nhttp://camel.465427.n5.nabble.com/jetty-handled-true-not-working-as-before-td5776774.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.processor.DeadLetterChannelLogExhaustedMessageHistoryTest.java", "org.apache.camel.builder.DeadLetterChannelBuilder.java", "org.apache.camel.processor.RedeliveryErrorHandler.java"], "label": 1, "es_results": []}, {"bug_id": 9576, "bug_title": "ClassLoadingAwareObjectInputStream cannot handle deserializing proxies with mixed JDK & non-JDK interfaces", "bug_description": "The camel-sql component contains a class called ClassLoadingAwareObjectInputStream, which is a copy from a class from the apache activeMQ project.\nThere was a bug in this class that was fixed in the activeMQ project but was not updated in the camel-sql component.\nI face the same issue as in: https://issues.apache.org/jira/browse/AMQ-3537\nwhere I cannot deserialize an object that contains a mix of jdk and non-jdk classes.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.processor.aggregate.jdbc.ClassLoadingAwareObjectInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 9582, "bug_title": "swagger-api docs not working in only using xml without any java route", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Swagger-not-working-tp5777255.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.core.xml.AbstractCamelContextFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 9593, "bug_title": "camel-example-swagger-cdi is not producing valid swagger definition", "bug_description": "http://camel.465427.n5.nabble.com/Camel-Swagger-cdi-Example-td5777632.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.example.cdi.UserRouteBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 9599, "bug_title": "camel-cxfrs - When responding then do not use content-length from input", "bug_description": "The simple binding of cxfrs uses the content-length header from the input as the response content-length. It should not do that.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.SimpleCxfRsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 9611, "bug_title": "Restlet GET request should not trying to stringify the exchange body", "bug_description": "When sending a restlet GET request, the body should not be looked at at all.  \nCurrently In my exchange i have a java object in the body, and i send a restlet GET request, i expect the restlet response to replace whatever is in the body.  But i get a type conversion error because it is trying to stringify the java object before sending the get request.\nI think the body should only be used in PUT and POST request.\nHere is the fix in org.apache.camel.component.restlet.DefaultRestletBinding.java  line 151:\n   public void populateRestletRequestFromExchange(Request request, Exchange exchange) {\n        request.setReferrerRef(\"camel-restlet\");\n        String body = null;\n        if(request.getMethod() == Method.POST || request.getMethod() == Method.PUT)\n        {\n\n        \tbody = exchange.getIn().getBody(String.class);\n\n    \t}\n        Form form = new Form();\n        // add the body as the key in the form with null value\n        form.add(body, null);", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.restlet.DefaultRestletBinding.java"], "label": 1, "es_results": []}, {"bug_id": 9641, "bug_title": "Simple backwards parser bug if using file", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Unknown-File-Language-Syntax-tp5778208.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.language.simple.SimpleTest.java", "org.apache.camel.language.simple.ast.SimpleFunctionExpression.java"], "label": 1, "es_results": []}, {"bug_id": 9640, "bug_title": "Query string gets decoded when bridging from netty*-http to netty*-http", "bug_description": "The same problem as CAMEL-9442 but for camel-netty*-http instead of camel-http4.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.netty4.http.NettyHttpHelper.java", "org.apache.camel.component.netty4.http.NettyHttpProducerBridgeTest.java", "org.apache.camel.http.common.HttpHelper.java", "org.apache.camel.component.netty.http.NettyHttpHelper.java", "org.apache.camel.component.netty.http.NettyHttpProducerBridgeTest.java"], "label": 1, "es_results": []}, {"bug_id": 9666, "bug_title": "Safe copy of DefaultExchange does not propagate 'fault' property ", "bug_description": "fault property should be copied in the following places:\nhttps://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/impl/DefaultExchange.java#L100\nhttps://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/impl/DefaultExchange.java#L107\nConsequences:\nDefaultExchange#isFault() does not work if exception property is not set.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.impl.DefaultExchange.java", "org.apache.camel.impl.DefaultExchangeTest.java"], "label": 1, "es_results": []}, {"bug_id": 9658, "bug_title": "Path gets decoded when bridging HTTP endpoints", "bug_description": "When bridging HTTP endpoints like this, the HTTP path gets decoded at the TO endpoint:\n\n\n\nfrom(\"netty-http:http://localhost:9000/camel?matchOnUriPrefix=true\")\n\n        .to(\"http4://host1:9000/fred?bridgeEndpoint=true\");\n\n\n\nFor instance, requesting to http://localhost:9000/camel/%3B will result in a call to http://host1:9000/fred/;, thus because several issues downstream.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.jetty.CamelContinuationServlet.java", "org.apache.camel.component.netty4.http.NettyHttpBridgeEncodedPathTest.java", "org.apache.camel.component.netty.http.DefaultNettyHttpBinding.java", "org.apache.camel.http.common.DefaultHttpBinding.java", "org.apache.camel.component.jetty.HttpBridgeEncodedPathTest.java", "org.apache.camel.component.netty.http.NettyHttpBridgeEncodedPathTest.java", "org.apache.camel.component.netty4.http.DefaultNettyHttpBinding.java"], "label": 1, "es_results": []}, {"bug_id": 9664, "bug_title": "DefaultRestletBinding.populateRestletRequestFromExchange uses wrong mediaType comparison", "bug_description": "We are trying to bind an incoming POST REST call with Content-Type application/x-www-form-urlencoded to an outgoing one. The incoming request body containing the url encoded form does not get transferred to the outgoing one. We investigated and saw there&apos;s a bug in \nDefaultRestletBinding.populateRestletRequestFromExchange:180 (2.16.2):\nif (request.getMethod() == Method.GET || (request.getMethod() == Method.POST && mediaType == MediaType.APPLICATION_WWW_FORM)) {\nThe mediaType gets compared using ==. This is wrong. The condition always evaluates to false although the configured Content-Type within the header is \"application/x-www-form-urlencoded\" (or in our case \"application/x-www-form-urlencoded; charset=ISO-8859-1\").", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.restlet.DefaultRestletBinding.java"], "label": 1, "es_results": []}, {"bug_id": 9673, "bug_title": "doTry .. doFinally should run the finally block for fault messages also", "bug_description": "If a message has fault flag, then a doFinally block is only executed the first processor. We should ensure the entire block is processed like we do if an exception was thrown. The same kind of logic should apply for fault.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.processor.FinallyProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 9672, "bug_title": "ClassCastException with interceptFrom", "bug_description": "If a statement like\n\n\n\ninterceptFrom().when(simple(\"${header.foo} == &apos;bar&apos;\")).to(\"mock:intercepted\");\n\n\n\nis available in a route builder with JMX enabled the startup will fail in Camel 2.16.2 (and the current 2.17-SNAPSHOT) with a ClassCastException in line 310 of DefaultManagementObjectStrategy.\nThe generated processor is a FilterProcessor, but the resulting definition is a WhenDefinition not a FilterDefinition.\nThe reason is that CAMEL-8992 introduced a too precise class check for this.\nThe attached patch relexes the class constraint on the definition.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.management.mbean.ManagedFilter.java", "org.apache.camel.management.DefaultManagementObjectStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 9686, "bug_title": "camel-aws - Using cron scheduler on aws-s3 do not work", "bug_description": "See SO\nhttp://stackoverflow.com/questions/35865863/camel-aws-s3-cron-schedule", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.aws.s3.S3Component.java"], "label": 1, "es_results": []}, {"bug_id": 9687, "bug_title": "camel-swagger - Should use resolved placeholders in output", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/camel-swagger-java-not-parse-property-tp5778734.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.management.mbean.ManagedCamelContext.java", "org.apache.camel.management.mbean.ManagedRoute.java", "org.apache.camel.model.RouteDefinitionHelper.java", "org.apache.camel.swagger.RestSwaggerSupport.java", "org.apache.camel.api.management.mbean.ManagedRouteMBean.java", "org.apache.camel.api.management.mbean.ManagedCamelContextMBean.java"], "label": 1, "es_results": []}, {"bug_id": 9700, "bug_title": "seda - discardIfNoConsumers=true do not call on completions ", "bug_description": "See SO\nhttp://stackoverflow.com/questions/35938139/how-to-release-file-lock-with-camel-when-not-consuming-from-seda-queue/35940850#35940850", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.seda.SedaProducer.java", "org.apache.camel.component.seda.SedaDiscardIfNoConsumerTest.java"], "label": 1, "es_results": []}, {"bug_id": 9714, "bug_title": "camel-boon - Unmarshal to Map does not work", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.boon.BoonDataFormatTest.java", "org.apache.camel.component.boon.BoonDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 9732, "bug_title": "camel-swagger-java - Issue in appendModels in the reader", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Re-Bug-in-RestSwaggerReader-appendModels-need-confirmation-td5779271.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.swagger.RestSwaggerReader.java"], "label": 1, "es_results": []}, {"bug_id": 9656, "bug_title": "Using SpringBoot HealthEndpoint bean throws AmbiguousMethodCallException", "bug_description": "I&apos;m trying to re-use the Springboot Actuator HealthEndpoint and InfoEndpoint beans and exposing them via rest:\n\n\n\n@Autowired\n\nHealthEndpoint healthEndpoint;\n\n...\n\n//build the route\n\nfrom(\"rest:get:health\").routeId(\"REST-healthcheck\").bean(healthEndpoint, \"invoke\");\n\n\n\nHowever when that gets invoked, it throws a AmbiguousMethodCallException and its other candidate for \"invoke()\" is on the abstract class that HealthEndpoint is extending\nIt looks like the issue is here:\nhttps://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/component/bean/BeanInfo.java#L1020\nIn this case HealthEndpoint extends AbstractHealthEndpoint, which is abstract, but invoke() is declared on the Endpoint interface. So this logic: \n\n !isFromInterface && Modifier.isAbstract(info.getMethod().getModifiers()) \n\nWill not evaluate to true, and the abstract method will remain a candidate.\nHere are the sources for the Endpoint/HealthEndpoint/AbstractEndpoint classes:\nhttps://github.com/spring-projects/spring-boot/blob/master/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/endpoint/Endpoint.java#L56\nhttps://github.com/spring-projects/spring-boot/blob/master/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/endpoint/AbstractEndpoint.java#L32\nhttps://github.com/spring-projects/spring-boot/blob/master/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/endpoint/HealthEndpoint.java#L36\nIt seems this is intentional, due to the \"if the class is an interface then keep the method\" comment in BeanInfo.\nI&apos;m curious as to why that is.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.bean.issues.BeanInfoSingleMethodServiceTest.java", "org.apache.camel.example.spring.boot.MySpringBootRouter.java", "org.apache.camel.util.ObjectHelper.java", "org.apache.camel.example.spring.boot.MySpringBootRouterTest.java", "org.apache.camel.component.bean.issues.SingleMethodServiceImpl.java", "org.apache.camel.component.bean.BeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 9738, "bug_title": "Thread leak for camel-mina2 consumers", "bug_description": "The camel-mina2 consumer (org.apache.camel.component.mina2.Mina2Consumer.doStart()) internally start a Mina2 IoAcceptor (org.apache.mina.core.service.IoAcceptor), but the org.apache.camel.component.mina2.Mina2Consumer.doStop() do not invoke acceptor.dispose(true); to release the threadpool started by Mina2.\nCurrent behavior: While stopping the web application, the threadpool started by Mina2 IoAcceptor will not be stopped, so we have several thread which do not properly shutdown.\nCurrent workaround: I create custom CamelShutdownStrategy and collect all Mina2Consumer and invoke Mina2consumer.getAcceptor.dispose(true); OR as an alternative create a dedicated consumer and overwrite the stop method of the Mina2 consumer as seen here: https://github.com/oehf/ipf/commit/12fdde8df7ebbbb7cd9966aadeab3ea3bed8fe75\nSuggested bugfix: In org.apache.camel.component.mina2.Mina2Consumer.doShutdown() call the IoAcceptor dispose(true) method after unbind from the address.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.component.mina2.Mina2Consumer.java"], "label": 1, "es_results": []}, {"bug_id": 9739, "bug_title": "Mina2Consumer exception handler do close session also for IOException", "bug_description": "The camel-mina2 consumer Exception handler (org.apache.camel.component.mina2.Mina2Consumer.ReceiveHandler.exceptionCaught(IoSession, Throwable)\nIs an implementation of the Mina2 interface org.apache.mina.core.service.IoHandler. The Javadoc Mina2 documentation of exceptionCaught explicitly mention, that Mina2 will close the connection for all IOExceptions automatically. But camel-mina2 seems to close the connection for any kind of exception.\nCurrent behavior: If an error occurs on the transport layer (e.g. secure TCP connection where the certificate was not trusted and a SSLException occure), I saw the following warning:\n\n\n\norg.apache.camel.component.mina2.Mina2Consumer#exceptionCaught(376) - Closing session as an exception was thrown from MINA\n\n\n\nAnd also an error from mina2:\n\n\n\norg.apache.mina.filter.ssl.SslHandler#destroy(210) - Unexpected exception from SSLEngine.closeInbound().\n\n\" javax.net.ssl.SSLException: Inbound closed before receiving peer&apos;s close_notify: possible truncation attack?\n\n\n\nFrom a functional perspective, beside the annoying log entries, it looks like the SSL error details are sometime not send properly back to the client.\nCurrent workaround: I do not have one.\nSuggested bugfix: Implement the Mina2Consumer exception handling similar to org.apache.mina.handler.stream.StreamIoHandler.exceptionCaught(IoSession, Throwable) and do not handle IoException and simply rethrow them. E.g.\n\n\n\nif (because instanceof IOException) {\n\n\tLOG.info(\"IOException will be closed by Mina itself\")\n\n\treturn;\n\n}\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.15.6", "fixed_files": ["org.apache.camel.component.mina2.Mina2Consumer.java"], "label": 1, "es_results": []}, {"bug_id": 9774, "bug_title": "CXFPayload may lose CDATA sections under stream caching", "bug_description": "CAMEL-8410 introduced CachedCXFPayload to add stream-caching support to CXFPayload. This may use during its sequence of conversions the default java.xml.transform.Transformer to convert javax.xml.transform.stax.StAXSource to SAX, which will then drop CDATA sections because its CDATA handling is empty.\nhttp://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7u40-b43/com/sun/org/apache/xalan/internal/xsltc/trax/StAXStream2SAX.java#StAXStream2SAX.handleCDATA%28%29\nTo avoid this, CachedCXFPayload can use the conversion utility already included in CXF.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.component.cxf.converter.CachedCxfPayloadTest.java", "org.apache.camel.component.cxf.converter.CachedCxfPayload.java"], "label": 1, "es_results": []}, {"bug_id": 9807, "bug_title": "Blocking of CXF consumer endpoint by http GET request", "bug_description": "Hi,\nby chance we found a problem that can create a security risk. The scenario is a CXF WS consumer endpoint configured without WSDL and right after that a step, e.g. a setHeader that uses an XPath to access the CXF payload. When you now create a http GET request on the endpoint (normally it should be POST) then the error No type converter available to convert from type: org.apache.camel.component.cxf.converter.CachedCxfPayload to the required type: javax.xml.transform.sax.SAXSource with value org.apache.camel.component.cxf.converter.CachedCxfPayload@45812dad\nis thrown.\nFirst of all it is surprising that CXF accepts the request. Second a type conversion like that is possible. The reason for this error message is within the type converter coding. The converter returns a null object for the not existing payload of the GET request but this is interpreted as there is no type converter available. One could say that this is just a bad error message but more over the type converter is now kind of black-listed internally. That means the next calls being correct or wrong will not look for a type converter but return immedeately the above error. That means the endpoint is blocked until the scenario is restarted. This seems to be more efficient than a DOS attack (but of course one has to have access rights). I provide a simple patch that raises a runtime exception in the CXF type converter when there is no payload. I am not 100% sure whether this is the best way to fix it. Maybe GET requests should be blocked already in CXF. I also could imagine that somethin like that is also possible in other components. \nWe use Camel 2.16.2 and I tested it in CXF 2.16.3 and it is not fixed.  ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.cxf.converter.CxfPayloadConverterTest.java", "org.apache.camel.component.cxf.converter.CxfPayloadConverter.java"], "label": 1, "es_results": []}, {"bug_id": 10048, "bug_title": "Memory leak in RoutingSlip", "bug_description": "RoutingSlip has a cache of error handlers implemented as a ConcurrentHashMap. This map stores error handlers as values, and uses some synthetic objects as keys. For some kind of destinations provided in routing slip, map lookup operation does not work. Hence, new error handlers are always added to the map and existing error handlers never reused. Finally, the program runs out of memory.\nThe synthetic keys are actually instances of class RoutingSlip.PreparedErrorHandler. Such key is based on two objects: RouteContext and destination Processor. Neither RouteContext nor Processor do not require their implementations to provide equals() and hashCode() methods. Strictly speaking, caching implementation in RoutingSlip is incorrect, because it uses hash map in the discouraged way. However, for some cases it works.\nThe problem occurs when routing slip contains a &apos;sync&apos; destination, in other words - destination is a Processor that does not implement AsyncProcessor interface. RoutingSlip determines destination producer via ProducerCache.doInAsyncProducer(), and the latter uses AsyncProcessorConverterHelper.convert() method. This method creates new instance of Processor for every processor that is not an instance of AsyncProcessor. This is where the problem hides: new object has different hash code (defined by Object.hashCode()) and new object isn&apos;t equal to the object used as a key in the hash map (well, Object.equals()). Finally, new key for the hash map is calculated, lookup operation cannot find this key in the hash map, new key-value pair is put into the hash map.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.2", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.processor.RoutingSlip.java", "org.apache.camel.util.AsyncProcessorConverterHelper.java"], "label": 1, "es_results": []}, {"bug_id": 9862, "bug_title": "Potential NPE in UndertowComponent.unregisterConsumer", "bug_description": "There&apos;s a potential NPE in the UndertowComponent class when Undertow consumers are stopped. Here&apos;s a snippet from the unregisterConsumer method:\n\n\n\n    public void unregisterConsumer(UndertowConsumer consumer) {\n\n        int port = consumer.getEndpoint().getHttpURI().getPort();\n\n        if (serversRegistry.containsKey(port)) {\n\n            serversRegistry.get(port).unregisterConsumer(consumer);\n\n        }\n\n        if (serversRegistry.get(port).isEmpty()) {\n\n          // stuff happens here\n\n        }\n\n    }\n\n\n\nIf serversRegistry.containsKey returns false for the given port, then we should not be proceeding to call methods like isEmpty afterwards.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.3", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.undertow.UndertowComponent.java"], "label": 1, "es_results": []}, {"bug_id": 9854, "bug_title": "CXF Stream Cache contains duplicate namespace definition", "bug_description": "If stream caching is enabled for incoming messages in Camel-CXF and a namespace defined on the SOAP envelope is also defined on the root tag of the payload document the XML contained within the stream cache contains a duplicate namespace definition.\nSee attached unit test for details", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.3", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.cxf.converter.DelegatingXMLStreamReader.java"], "label": 1, "es_results": []}, {"bug_id": 9933, "bug_title": "Camel-CSV marshalling breaks characters not in default charset", "bug_description": "Marshalling data with camel-csv will use the JVM default encoding instead of the one configured for the message. This will break non-ascii characters, especially if the platform default encoding does not support them (e.g. because the platform default encoding is ASCII)", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.3", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.dataformat.csv.CsvMarshaller.java"], "label": 1, "es_results": []}, {"bug_id": 9970, "bug_title": "CamelFileLength header is wrong for long write file", "bug_description": "Given a long write file slowfile.dat. Camel polls the file and stores its parameters (length, lastModified...). Camel tries to get exclusive read lock and fails as file is writing by someone, next time Camel gets lock. But file was changed and its parameters are wrong.\nThe solution is pretty simple - check the file length and lastModified when exclusive read lock is granted and update them.\nHere is a patch: http://pastebin.com/2vyF8BTU", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.3", "fixed_version": "camel-2.18.0", "fixed_files": ["org.apache.camel.component.file.FileConsumer.java", "org.apache.camel.component.file.GenericFileConsumer.java", "org.apache.camel.component.file.remote.FtpConsumer.java", "org.apache.camel.component.file.remote.RemoteFileIgnoreDoPollErrorTest.java", "org.apache.camel.component.file.remote.SftpConsumer.java", "org.apache.camel.component.file.strategy.FileChangedReadLockTest.java"], "label": 1, "es_results": []}, {"bug_id": 10104, "bug_title": "Mail consumer does not work with quartz scheduler", "bug_description": "Using the mail consumer with the quartz scheduler like\nimap://myhost?scheduler=quartz2&scheduler.cron=0%2F5+0-23%3F+&scheduler.timeZone=Europe%2FBerlin\nleads to an error:\n\"There are 2 scheduler parameters that couldn&apos;t be set on the endpoint. Check the uri if the parameters are spelt correctly and that they are properties of the endpoint.\"", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.3", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.mail.MailComponent.java", "org.apache.camel.component.mail.MailComponentTest.java"], "label": 1, "es_results": []}, {"bug_id": 10157, "bug_title": "Values in KafkaConstants do not fit their variable name", "bug_description": "Value of KafkaConstants.KEY = \"kafka.CONTENT_TYPE\" and KafkaConstants.PARTITION = \"kafka.EXCHANGE_NAME\" should match their variable name.\n\n\n\n    public static final String PARTITION = \"kafka.PARTITION\";\n\n    public static final String KEY = \"kafka.KEY\";\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.3", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.kafka.KafkaConstants.java"], "label": 1, "es_results": []}, {"bug_id": 10064, "bug_title": "Extra request parameter sent by the camel-jetty component", "bug_description": "Trying to build a very simple HTTP proxy, exactly like described in the first code sample from http://camel.apache.org/how-to-use-camel-as-a-http-proxy-between-a-client-and-server.html.\nUsing wireshark to see exactly what is going on.\nWhen i do a request on http://localhost:8080/myapp, the request that is sended to real server is http://realserverhostname:8090/myapp?bridgeEndpoint=true&throwExceptionOnFailure=false.\nwhen i do a parameterized request http://localhost:8080/myapp?toto=tata, the request that is sended to real server is http://realserverhostname:8090/myapp?toto=tata.\nSo when the when there is no request param, the endpoint parameter bridgeEndpoint=true&throwExceptionOnFailure=false are sended to the realserver, and if there is param they are not sended.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.16.3", "fixed_version": "camel-2.17.4", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpComponent.java", "org.apache.camel.component.jetty.jettyproducer.JettyHttpProducerBridgeTest.java"], "label": 1, "es_results": []}, {"bug_id": 9777, "bug_title": "camel-zipfile - Using zip iterator with dataformat may fail", "bug_description": "The zip entry may say the current size of the entry is -1 while there is still data.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.16.3", "fixed_files": ["org.apache.camel.dataformat.zipfile.ZipIterator.java"], "label": 1, "es_results": []}, {"bug_id": 9768, "bug_title": "HTTP[4] component disableStreamCache issue: java.io.IOException: Attempted read from closed stream.", "bug_description": "This issue is related to CAMEL-7638 which was recently fixed/released in 2.17.0\nI was doing some testing with disableStreamCache=true on a http4 producer and am getting \"java.io.IOException: Attempted read from closed stream\"\nThe stack trace shows the error occurring when trying to copy the input stream to an output stream inside the DefaultHttpBinding copyStream methods (i am using camel as a proxy from a sevlet component (consumer) to an http4 component (producer)).\nI think though, I see the root because of this issue.  Inside the HttpProducer process method.\n\n\n\n// let us store the result in the output message.\n\n        HttpResponse httpResponse = null;\n\n        try {\n\n            if (LOG.isDebugEnabled()) {\n\n                LOG.debug(\"Executing http {} method: {}\", httpRequest.getMethod(), httpRequest.getURI().toString());\n\n            }\n\n            httpResponse = executeMethod(httpRequest);\n\n            int responseCode = httpResponse.getStatusLine().getStatusCode();\n\n            LOG.debug(\"Http responseCode: {}\", responseCode);\n\n\n\n            if (!throwException) {\n\n                // if we do not use failed exception then populate response for all response codes\n\n                populateResponse(exchange, httpRequest, httpResponse, in, strategy, responseCode);\n\n            } else {\n\n                boolean ok = HttpHelper.isStatusCodeOk(responseCode, getEndpoint().getOkStatusCodeRange());\n\n                if (ok) {\n\n                    // only populate response for OK response\n\n                    populateResponse(exchange, httpRequest, httpResponse, in, strategy, responseCode);\n\n                } else {\n\n                    // operation failed so populate exception to throw\n\n                    throw populateHttpOperationFailedException(exchange, httpRequest, httpResponse, responseCode);\n\n                }\n\n            }\n\n        } finally {\n\n            if (httpResponse != null) {\n\n                try {\n\n                    EntityUtils.consume(httpResponse.getEntity());\n\n                } catch (IOException e) {\n\n                    // nothing we could do\n\n                }\n\n            }\n\n        }\n\n\n\nSpecifically, that finally block at the end.\nWhen disableStreamCache=true is set on the Producer, the raw input stream is put in the exchange body, which was the change fixed by CAMEL-7638\nHowever, the finally block is consuming and closing that input stream making it unusable later when we try to copy it to the servlet output stream to send back to the caller.\nI think the fix for this would be to check the endpoint to see if disableStreamCaching is set prior to consuming the entity in the finally block, perhaps something like this:\n\n\n\n    ...\n\n    } finally {\n\n            if (httpResponse != null && !getEndpoint().isDisableStreamCache()) {\n\n                try {\n\n                    EntityUtils.consume(httpResponse.getEntity());\n\n                } catch (IOException e) {\n\n                    // nothing we could do\n\n                }\n\n            }\n\n        }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.1", "fixed_files": ["org.apache.camel.component.http4.HttpProducer.java"], "label": 1, "es_results": []}, {"bug_id": 9828, "bug_title": "Swagger seems to inject empty headers", "bug_description": "See http://camel.465427.n5.nabble.com/swagger-injects-empty-headers-in-2-17-0-td5780620.html\nkey part copied below.\nI have routes built using REST DSL, and this includes swagger definitions.\nWith 2.1.6.2 all was good.\nOn switching to 2.17.0 I find that having a swagger query parameter definition causes a header property to be defined as an empty string even when there is no query parameter defined.\nAs an example, the REST DSL snippet looks like this:\n.post(\"/\n{notebookid}\n/e\").description(\"Description ...\")\n.bindingMode(RestBindingMode.json).produces(\"application/json\")\n.outType(Foo.class)\n.param().name(\"notebookid\").type(path).description(\"Notebook ID\").dataType(\"long\").endParam()\n.param().name(\"parent\").type(query).description(\"The parent\").dataType(\"long\").required(false).endParam()\n.route()\n.process((Exchange exch) -> \n{\n\n    Long parent = exch.getIn().getHeader(\"parent\", Long.class);\n\n    ...\n\n}\n)\n.endRest()\n\"parent\" is an optional query param.\nWith 2.16.2 there was no \"parent\" header.\nWith 2.17.0 a \"parent\" header is magically added with the value being the empty string, which causes the TypeConversion to Long to blow up. \nIf I remove the swagger params then it works OK again.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.1", "fixed_files": ["org.apache.camel.model.rest.RestDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 9841, "bug_title": "NPE in MIME-Multipart Data Format if no file name is defined on attachment", "bug_description": "If an MIME-Multipart message is unmarshalled into a Camel Message with attachments and the attachment parts do not have a file name defined, the MIME-Multipart data format will throw a NullPointerException (because the file name is used as the key for the map where the DataHandler is stored in the Camel Message).\nThe provided patch checks whether there is a file name defined on the data handler. If this is not the case the Content-ID header of the attachment is used, if that is also not defined a unique id is generated for the key.\nSee the two attached MIME-Multipart documents used in the unit tests for details.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.1", "fixed_files": ["org.apache.camel.dataformat.mime.multipart.MimeMultipartDataFormatTest.java", "org.apache.camel.dataformat.mime.multipart.MimeMultipartDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 9863, "bug_title": "loopDoWhile will loop forever if using ahc component in the loop.", "bug_description": "I have tested several times. The loop condition will not be checked if ahc component is in the loop.\n        from(\"quartz2://jf-log/trigger3?trigger.repeatInterval=2&trigger.repeatCount=0\")\n            .loopDoWhile(body().isNotEqualTo(\"done\"))\n              .setHeader(Exchange.HTTP_METHOD, constant(\"GET\"))\n              .to(\"ahc:https://www.baidu.com/\")\n              .choice()\n              .when(exchangeProperty(\"CamelLoopIndex\").isEqualTo(3))\n                .setBody(constant(\"done\"))\n                .endChoice()\n              .end()\n            .end()\n            .to(\"mock:result\");\nThe loop will be infinite, the workarround is to use http component instead of ahc.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.1", "fixed_files": ["org.apache.camel.processor.LoopProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 9898, "bug_title": "SimpleBuilder throws NullPointerException when replacing string using regexAll method and the regex contains }", "bug_description": "\nAdd this unit test in org.apache.camel.builder.SimpleBuilderTest to reproduce the issue. Only fails when the regex contains }\n\n\n\npublic  void testRegexAllWithPlaceHolders() {\n\n        exchange.getIn().setHeader(\"activateUrl\", \"http://some/rest/api/(id)/activate\");\n\n        assertEquals(\"http://some/rest/api/12/activate\",SimpleBuilder.simple(\"${header.activateUrl.replaceAll(\\\"\\\\(id\\\\)\\\",\\\"12\\\")}\").evaluate(exchange,String.class));\n\n\n\n        //passes when contains { only\n\n        exchange.getIn().setHeader(\"activateUrl\", \"http://some/rest/api/{id/activate\");\n\n        assertEquals(\"http://some/rest/api/12/activate\",SimpleBuilder.simple(\"${header.activateUrl.replaceAll(\\\"\\\\{id\\\",\\\"12\\\")}\").evaluate(exchange,String.class));\n\n\n\n\n\n        String replaced  = \"http://some/rest/api/{id}/activate\".replaceAll(\"\\\\{id\\\\}\",\"12\");\n\n        assertEquals( \"http://some/rest/api/12/activate\", replaced);\n\n        /// But throws throws NullPointerException when regexALl inside a simple expression\n\n        exchange.getIn().setHeader(\"activateUrl\", \"http://some/rest/api/{id}/activate\");\n\n        assertEquals(\"http://some/rest/api/12/activate\",SimpleBuilder.simple(\"${header.activateUrl.replaceAll(\\\"\\\\{id\\\\}\\\",\\\"12\\\")}\").evaluate(exchange,String.class));\n\n    }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.18.0", "fixed_files": ["org.apache.camel.language.simple.SimpleTest.java", "org.apache.camel.builder.SimpleBuilderTest.java", "org.apache.camel.language.simple.SimpleTokenizer.java"], "label": 1, "es_results": []}, {"bug_id": 9903, "bug_title": "DumpRouteStatsAsXml do not work when jmx domain is customized", "bug_description": "According to the documentation (Here) we can customize the jmx domain name.\nWhen the domain name is different from org.apache.camel all the dumpRouteStatsAsXml are empty and does not work anymore.\nAfter some investigation it is because the MBean operation does not take into account the runtime domain name and only use \"org.apache.camel\" (here) \nI&apos;ve done a unit test to reproduce and tried to do a fix,", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.1", "fixed_files": ["org.apache.camel.management.mbean.ManagedCamelContext.java", "org.apache.camel.management.mbean.ManagedRoute.java"], "label": 1, "es_results": []}, {"bug_id": 9906, "bug_title": "camel-sql - Should allow null values as a valid value", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/2-17-0-NULLs-are-not-allowed-with-named-parameter-in-SQL-component-tp5781552.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.1", "fixed_files": ["org.apache.camel.component.sql.DefaultSqlPrepareStatementStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 9887, "bug_title": "onCompletion not called on Splitter configured with CompletionAwareAggregationStrategy and shareUnitOfWork=true", "bug_description": "In 2.17.0, if a Splitter is configured with shareUnitOfWork=true, then its AggregationStrategy is now wrapped by a ShareUnitOfWorkAggregationStrategy. This causes the following code from MulticastProcessor.doDone() to fail to invoke onCompletion when the configured strategy implements CompletionAwareAggregationStrategy.\n\nif (strategy instanceof CompletionAwareAggregationStrategy) {\n\n    ((CompletionAwareAggregationStrategy) strategy).onCompletion(subExchange);\n\n}\n\n\n\nIt appears the change was a part of CAMEL-9573.\nI haven&apos;t completely analyzed the shared unit of work changes but one possible approach to fix this would be to have the ShareUnitOfWorkAggregationStrategy implement CompletionAwareAggregationStrategy and then have its onCompletionMethod delegate to the wrapped strategy iff it also implements CompletionAwareAggregationStrategy.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.1", "fixed_files": ["org.apache.camel.processor.aggregate.AggregateProcessor.java", "org.apache.camel.processor.aggregate.ShareUnitOfWorkAggregationStrategy.java", "org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 9911, "bug_title": "RestBindingMode auto not honored", "bug_description": "The usage of rest binding mode is confusing/doesn&apos;t work consistently.\nIf you look at the field definition:\n\n\n\n    @XmlAttribute @Metadata(defaultValue = \"auto\")\n\n    private RestBindingMode bindingMode;\n\n\n\nYou would expect that the default value is \"auto\" but in fact there is no default when running.\nYou need to explicitly set it in configuration, like this:\n\n\n\nrestConfiguration()\n\n                .component(\"jetty\")\n\n                .port(9097)\n\n                .bindingMode(RestBindingMode.auto)\n\n        ;\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.1", "fixed_files": ["org.apache.camel.model.rest.RestBindingDefinition.java", "org.apache.camel.model.rest.RestConfigurationDefinition.java", "org.apache.camel.processor.binding.RestBindingProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 9917, "bug_title": "Route stopped events are sent in inconsistent order", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.18.0", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 9784, "bug_title": "Camel polling the files from S3 only once if deleteAfterRead is false", "bug_description": "if deleteAfterRead option is kept false while fetching all the files in the S3 bucket to linux machine, Camel polls all the files only once. I tried to debug org.apache.camel.component.aws.s3.S3Consumer.java class inside method poll() there is check for filesConsumed boolean variable which seems to be always true after the first polling and then it never access any files.\nSame issue was reported http://stackoverflow.com/questions/34193738/apache-camel-s3-only-do-polling-once but the suggestion was to downgrade to 2.16.0 which has many different issues which got fixed in 2.17.0", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.aws.s3.S3Consumer.java", "org.apache.camel.component.aws.s3.S3Configuration.java"], "label": 1, "es_results": []}, {"bug_id": 9920, "bug_title": "Handle SocketTimeoutException on accept", "bug_description": "The MLLP receiver logs and error when a SocketTimeoutException is encountered while waiting for a connection.  It will not successfully accept connections after that.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.1", "fixed_files": ["org.apache.camel.component.mllp.MllpTcpServerConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 9805, "bug_title": "camel-sql - body not copied from in to out when useing outputHeader and outputType=SelectOne when sql does not return a result", "bug_description": "When using the options outputHeader and outputType=SelectOne the body of the Exchange is not copied from in to out when the select returns no data..\nProbably root is line 175-185 of SqlProducer.java that is missing an else for the above mentioned case.\n\n\n\nif (data != null) { // <--- Missing the else\n\n// for noop=true we still want to enrich with the row count header\n\n  if (getEndpoint().isNoop()) {\n\n    exchange.getOut().setBody(exchange.getIn().getBody());\n\n  } else if (getEndpoint().getOutputHeader() != null) {\n\n    exchange.getOut().setBody(exchange.getIn().getBody());\n\n    exchange.getOut().setHeader(getEndpoint().getOutputHeader(), data);\n\n  } else {\n\n    exchange.getOut().setBody(data); \n\n  }\n\n  exchange.getOut().setHeader(SqlConstants.SQL_ROW_COUNT, 1);\n\n}\n\n\n\nfollowing could be added (untested)\n\n\n\nelse { // if data == null\n\nif (getEndpoint().isNoop()) {\n\n                                        exchange.getOut().setBody(exchange.getIn().getBody());\n\n                                    } else if (getEndpoint().getOutputHeader() != null) {\n\n                                        exchange.getOut().setBody(exchange.getIn().getBody());\n\n                                    }\n\n exchange.getOut().setHeader(SqlConstants.SQL_ROW_COUNT, 0);\n\n}\n\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.elsql.ElsqlProducer.java", "org.apache.camel.component.sql.SqlProducer.java"], "label": 1, "es_results": []}, {"bug_id": 9929, "bug_title": "camel-restlet - Using synchronous=false with no error handler leak inflight exchange", "bug_description": "invoking restlet component within a recipientList EIP with noErrorHandler configured on route is causing DefaultExchange&apos;s to build up in org.apache.camel.impl.DefaultInflightRepository.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.restlet.RestletProducer.java"], "label": 1, "es_results": []}, {"bug_id": 9941, "bug_title": "Blueprint bug ARIES-1544 causes issues in Olingo2 configuration", "bug_description": "Due to Blueprint bug ARIES-1544, the setConfiguration() method overridden in Olingo2Component for Camel documentation, MUST have a matching getConfiguration() method. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.google.drive.GoogleDriveComponent.java", "org.apache.camel.component.box.BoxComponent.java", "org.apache.camel.component.google.mail.GoogleMailComponent.java", "org.apache.camel.component.linkedin.LinkedInComponent.java", "org.apache.camel.component.olingo2.Olingo2Component.java"], "label": 1, "es_results": []}, {"bug_id": 10017, "bug_title": "Fix syntax for ironmq component", "bug_description": "replace ironmq:queue by ironmq:queueName", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.ironmq.IronMQEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 10015, "bug_title": "Fix syntax for braintree component", "bug_description": "replace braintree:name by braintree:apiName/methodName", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.braintree.BraintreeEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 10014, "bug_title": "Fix syntax for kubernetes component", "bug_description": "replace kubernetes:master by kubernetes:masterUrl", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.kubernetes.KubernetesEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 10016, "bug_title": "Fix syntax for crypto component", "bug_description": "replace crypto:cryptoOperation:name by crypto:cryptoOperation", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.0", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.crypto.DigitalSignatureConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 9968, "bug_title": "camel restlet not populating body form parameters correctly for x-www-form-urlencoded", "bug_description": "Currently for x-www-form-urlencoded post request camel puts the body into a form key with a null value:\n\n\n\nif ((Method.PUT == method || Method.POST == method) && MediaType.APPLICATION_WWW_FORM.equals(mediaType, true)) {\n\n            form = new Form();\n\n            // must use string based for forms\n\n            String body = exchange.getIn().getBody(String.class);\n\n            if (body != null) {\n\n                form.add(body, null);\n\n            }\n\n        }\n\n\n\nWhich results in a body like this:\nname=jay&password=secret\nending up with a form parameter looking like this: \nname%3Djay%26password%3Dsecret=null\nI think something like this should be used to correctly set the key values.\n\n\n\nif ((Method.PUT == method || Method.POST == method) && MediaType.APPLICATION_WWW_FORM.equals(mediaType, true)) {\n\n            form = new Form();\n\n            // must use string based for forms\n\n            String body = exchange.getIn().getBody(String.class);\n\n            if (body != null) {\n\n                List<NameValuePair> pairs = URLEncodedUtils.parse(body, Charset.forName(\"UTF-8\"));\n\n                for(NameValuePair p : pairs){\n\n                \tform.add(p.getName(), p.getValue());\n\n                }\n\n            }\n\n        }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.restlet.DefaultRestletBinding.java"], "label": 1, "es_results": []}, {"bug_id": 9982, "bug_title": "Marshalling fixed length record with links fails", "bug_description": "When a fixed length record contains a \"Link\" marshalling doesn&apos;t take this field into account while with unmarshalling it is unmarshalled.\nIssue comes from a difference in building the model in BindyCsvDataFormat and BindyFixedLengthDataFormat. In the csv-format all field are taken into account that have the \"Link\"  annotation.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.18.0", "fixed_files": ["org.apache.camel.dataformat.bindy.fixed.BindyFixedLengthDataFormat.java", "org.apache.camel.dataformat.bindy.fixed.link.BindySimpleFixedLengthWithLinkTest.java", "org.apache.camel.dataformat.bindy.csv.BindyCsvDataFormat.java", "org.apache.camel.dataformat.bindy.BindyAbstractDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 9926, "bug_title": "HTTP Proxy support in Salesforce component is broken with upgrade to Jetty9", "bug_description": "HTTP Proxy support in Jetty9 Client is broken, which causes HTTP Proxy tests in the Salesforce components to fail. \nThey have been marked as ignored for now, but it MUST be fixed in Jetty9 AS SOON AS POSSIBLE and the component updated to use the new version of Jetty9 client with working support for Proxy authentication. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.salesforce.HttpProxyIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 9986, "bug_title": "MIME-Multipart Data Format is inconsistent if trying to unmarshal non-MIME data", "bug_description": "If data that is not a MIME-Multipart is tried to marshal with the headersInline option set to false, no error is thrown and the message is left as the original message.\nIn case the headersInline option is set to true, an empty message is returned.\nThe provided patch changes the behavior that it also tries to return the original message in case the message is not a mime-multipart and headersInline is set to true. However there are limits to this: If the message body is a stream and stream caching is not enabled it is necessary to read the stream in order to find out whether this is a MIME-Multipart or not. Afterwards the message body will be empty (as with the previous implementation). ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.dataformat.mime.multipart.MimeMultipartDataFormatTest.java", "org.apache.camel.dataformat.mime.multipart.MimeMultipartDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 10011, "bug_title": "Overlap in management name for multiple contexts in OSGi bundle", "bug_description": "Problem is that OsgiManagementNameStrategy uses only the bundle symbolic name for the management name used for each CamelContext. So if you have multiple CamelContexts per bundle this creates overlap in the naming. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.core.osgi.OsgiManagementNameStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 10012, "bug_title": "Add a Path Home option in camel-elasticsearch configuration", "bug_description": "Otherwise we get errors.\nhttps://github.com/elastic/elasticsearch/blob/v2.2.0/core/src/main/java/org/elasticsearch/env/Environment.java#L101\nhttps://github.com/elastic/elasticsearch/blob/v2.3.3/core/src/main/java/org/elasticsearch/env/Environment.java#L101", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.elasticsearch.ElasticsearchGetSearchDeleteExistsUpdateTest.java", "org.apache.camel.component.elasticsearch.ElasticsearchEndpoint.java", "org.apache.camel.component.elasticsearch.ElasticsearchConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 10039, "bug_title": "LinkedIn broke login in LinnkedIn component by adding a redundant reference to the callback url", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.box.internal.LoginAuthFlowUI.java", "org.apache.camel.component.linkedin.api.LinkedInOAuthRequestFilter.java"], "label": 1, "es_results": []}, {"bug_id": 10051, "bug_title": "netty4 reuseChannel not working as expected", "bug_description": "NettyProducer returns the channel to the pool every time a message is sent when the reuseChannel is set to true. It should instead return the channel to the pool only if the exchange is completed, so that other exchanges will not use the same channel.\nI have the following problem:\nI need to send messages to a legacy tcp server through a REST server.\nThe tcp server requires messages from one connection to be send in a specific order, for example in order to send COMMAND1 I have to first send PRE1 and PRE2 and then COMMAND1. The component is set to reuseChannel=true and be sync=true and i have the following exchange:\n\n\n\nfrom(\"direct:command1\")\n\n    .setBody(constant(\"PRE1\"))\n\n    .to(nettyEndpoint)\n\n    .setBody(constant(\"PRE2\"))\n\n    .to(nettyEndpoint)\n\n    .setBody(constant(\"COMMAND1\"))\n\n    .to(nettyEndpoint);\n\n\n\nThe problem is that the NettyProducer returns the channel to the pool every time i send a message. So if concurrent requests are sent to the REST server things get messy. \nNettyProducer supports this behavior, if the reuseChannel is set to true the producer adds an onCompletion listener on the exchange that returns the channel to the pool. The problem is that it also sets the callback of the NettyCamelState to a NettyProducerCallback which runs every time a message is send by the ClientChannelHandler and returns the channel to the pool.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.netty4.NettyProducer.java"], "label": 1, "es_results": []}, {"bug_id": 10082, "bug_title": "camel-api-component-maven-plugin does not handle inner class names in Javadoc", "bug_description": "ApiMethodParser.forName() doesn&apos;t handle inner class names of the format package.OutClass.InnerClass from camel-api-component-maven-plugin&apos;s Javadoc parser. It needs to convert class names in this human readable format to package.OutClass$InnerClass form as expected by Class.forName().", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.3", "fixed_files": ["org.apache.camel.util.component.ApiMethodParser.java", "org.apache.camel.util.component.ArgumentSubstitutionParserTest.java", "org.apache.camel.util.component.TestProxy.java", "org.apache.camel.util.component.ApiMethodHelperTest.java", "org.apache.camel.component.test.TestProxy.java"], "label": 1, "es_results": []}, {"bug_id": 10024, "bug_title": "Race condition in Mina2Producer/Mina2Consumer when closing connections with disconnect=true", "bug_description": "There is a race condition in the Mina2Producer when trying to close connections after use by setting disconnect=true or\nsetting CamelMina2CloseSessionWhenComplete=true. \nConnections will not be fully closed in the method maybeDisconnectOnDone. \nThe call to session.close(true) returns a CloseFuture - one must await this to ensure the session is really closed.\nIn the current implementation, there is no await on the CloseFuture. This means that the producer will be returned to the pool before the session is closed. If the next call comes right after, it is very likely that it will get the same producer and that the session will suddenly be closed while in use, leading to errors like \nExchangeTimedOutException: The OUT message was not received within 30000 ms\nor \njava.lang.IllegalStateException: handler cannot be set while the service is active.\nThe fix is trivial - just change line 221 in Mina2Producer.java from\n\n\n\nsession.close(true);\n\n\n\nto \n\n\n\nlong timeout = getEndpoint().getConfiguration().getTimeout();          \n\nCloseFuture closeFuture = session.close(true);  \n\ncloseFuture.awaitUninterruptibly(timeout, TimeUnit.MILLISECONDS);\n\n\n\nBut the unit testing might be more complex.\nThere might be a similar issue in Mina2Consumer - but I could not provoke it as easily.\nHere is a small program demonstrating the problem - on my system it will fail within the first 50 iterations. When looking at the debug log, it is clear that the connections are closed too late - after the next iteration has started.\n\n\n\n\n\nimport org.apache.camel.*;\n\nimport org.apache.camel.builder.RouteBuilder;\n\nimport org.apache.camel.impl.DefaultCamelContext;\n\nimport org.slf4j.*;\n\n\n\n/**\n\n * Demonstrating race condition\n\n */\n\npublic class Main {\n\n    public static void main(String[] args) throws Exception {\n\n\n\n        System.setProperty(\"org.slf4j.simpleLogger.log.org.apache.camel.component.mina2.Mina2Producer\", \"trace\");\n\n        System.setProperty(\"org.slf4j.simpleLogger.log.org.apache.mina.filter.logging.LoggingFilter\", \"trace\");\n\n        \n\n        Logger logger = LoggerFactory.getLogger(Main.class);\n\n        CamelContext context = new DefaultCamelContext();\n\n        context.addRoutes(new RouteBuilder() {\n\n            @Override\n\n            public void configure() throws Exception {\n\n                from(\"mina2:tcp://localhost:20000?sync=true\").setBody(simple(\"Hello ${in.body}\"));\n\n            }\n\n        });\n\n\n\n        ProducerTemplate producerTemplate = context.createProducerTemplate();\n\n        context.start();\n\n        try {\n\n            for (int i = 0; i < 10000; i++) {\n\n                logger.info(\"---- Call # \" + i);\n\n                String result = (String) producerTemplate.requestBody(\"mina2:tcp://localhost:20000?disconnect=true&timeout=1000&sync=true&minaLogger=true\", \"world \" + i);\n\n                logger.info(\"---- End call # \" + i + \": \" + result);\n\n            }\n\n        } finally {\n\n            context.stop();\n\n        }\n\n    }\n\n}\n\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.mina2.Mina2Producer.java", "org.apache.camel.component.mina2.Mina2TextLineDelimiter.java", "org.apache.camel.component.mina2.Mina2NoResponseFromServerTest.java", "org.apache.camel.component.mina2.Mina2Consumer.java", "org.apache.camel.component.mina2.Mina2ProducerShutdownMockTest.java", "org.apache.camel.component.mina2.Mina2ExchangeTimeOutTest.java", "org.apache.camel.component.mina2.Mina2ClientModeTcpTextlineDelimiterTest.java", "org.apache.camel.component.mina2.Mina2EncodingTest.java", "org.apache.camel.component.mina2.Mina2TransferExchangeOptionTest.java", "org.apache.camel.component.mina2.Mina2ReverseProtocolHandler.java"], "label": 1, "es_results": []}, {"bug_id": 10091, "bug_title": "Camel-Git: Always check if Git instance is null in GitProducer before closing", "bug_description": "We need to double check JGit Git class instance each time we want to close it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.git.producer.GitProducer.java", "org.apache.camel.component.git.producer.GitProducerTest.java"], "label": 1, "es_results": []}, {"bug_id": 10087, "bug_title": "camel-kafka does not work in OSGI container", "bug_description": "Currently this component is broken in OSGI environment due to this:\nhttps://issues.apache.org/jira/browse/KAFKA-3218\nWe can leave it broken until they get their bugs worked out.  Or i can submit a PR with an intermediate fix that should work, as described in the jira noted above.\nhttps://github.com/apache/camel/pull/1053", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.3", "fixed_files": ["org.apache.camel.component.kafka.KafkaEndpoint.java", "org.apache.camel.component.kafka.KafkaProducer.java", "org.apache.camel.component.kafka.KafkaConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 10115, "bug_title": "Kafka consumer stays running if no messages were received after shutdown start", "bug_description": "After triggering CamelContext#close() method the execution will reach org.apache.camel.component.kafka.KafkaConsumer#doStop where the shutdown of the executor instance will be triggered and where in it&apos;s turn the interruption of the submitted to the executor threads should happen (by reaching the native implementation of Thread#interrupt())\nAccording to https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#interrupt-- interrupt method will only set a corresponding status to the thread, but will not terminate it. \nProblem is in the line KafkaConsumer.java:108:\nConsumerRecords<Object, Object> records = consumer.poll(Long.MAX_VALUE);\nIn the Kafka implementation of the poll method this will lead to almost infinite while loop which is not checking the thread status and this loop will exit only in case of receiving a message from a broker. Only after exciting the loop the interrupted status of the thread will be discovered and the thread will be terminated.\nThis leads to a couple of problems:\n1. The KafkaConsumers remain alive until receiving at least one more message from the broker.\n2. As the CamelContext at this point of time is most likely already shut down, the received message is not going to be processed, but will be acknowledged to the broker. So effectively the message gets lost.\nA potential fix would be to either make the poll timeout reasonably small or configurable.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.18.0", "fixed_files": ["org.apache.camel.component.kafka.KafkaConfiguration.java", "org.apache.camel.component.kafka.KafkaConsumerTest.java", "org.apache.camel.component.kafka.KafkaConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 10143, "bug_title": "Camel Salesforce component field LastActivityDate is typed with java.lang.String, which is not consistent with the Salesforce SOAP type \"xsd:date\"", "bug_description": "This field was not a valid date time field in older versions of the Salesforce APIs, hence it was left as String. \nIt should now be converted into DateTime to work with latest Salesforce API and be mappable to dates in Hibernate for e.g.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.3", "fixed_files": ["org.apache.camel.component.salesforce.api.dto.AbstractSObjectBase.java"], "label": 1, "es_results": []}, {"bug_id": 10147, "bug_title": "MesssageHistory will take very long time for large expressions", "bug_description": "If a route contains a large expressen (a few MB) and an error occurs the message history feature will take a very long time.\nThe following code is a slightly modified unit test from camel-core (the only change is the String used in the constant expression).\n\n\n\nimport org.apache.camel.CamelExecutionException;\n\nimport org.apache.camel.ContextTestSupport;\n\nimport org.apache.camel.builder.RouteBuilder;\n\n\n\n/**\n\n * @version \n\n */\n\npublic class SedaInOutWithErrorTest extends ContextTestSupport {\n\n\n\n    public void testInOutWithError() throws Exception {\n\n        getMockEndpoint(\"mock:result\").expectedMessageCount(0);\n\n\n\n        try {\n\n            template.requestBody(\"direct:start\", \"Hello World\", String.class);\n\n            fail(\"Should have thrown an exception\");\n\n        } catch (CamelExecutionException e) {\n\n            assertIsInstanceOf(IllegalArgumentException.class, e.getCause());\n\n            assertEquals(\"Damn I cannot do this\", e.getCause().getMessage());\n\n        }\n\n\n\n        assertMockEndpointsSatisfied();\n\n    }\n\n\n\n    @Override\n\n    protected RouteBuilder createRouteBuilder() throws Exception {\n\n        return new RouteBuilder() {\n\n            @Override\n\n            public void configure() throws Exception {\n\n                StringBuilder sb = new StringBuilder();\n\n                sb.append(\"Something \");\n\n                for (int i=0; i<1000000; i++) {\n\n                    sb.append(\"very \");\n\n                }\n\n                sb.append(\"long\");\n\n                \n\n                from(\"direct:start\").to(\"seda:foo\");\n\n\n\n                from(\"seda:foo\").transform(constant(sb.toString()))\n\n                    .throwException(new IllegalArgumentException(\"Damn I cannot do this\"))\n\n                    .to(\"mock:result\");\n\n            }\n\n        };\n\n    }\n\n}\n\n\n\nThis test will set the body to a 5MB test and then run into an error. This will run for a very long time, because MessageHelper.doDumpMessageHistoryStacktrace() will first run a URISupport.sanitizeUri() on the expression and then cut it off to 78 characters.\nIf we cut the expression of (e.g. to 100 characters) before doing the sanitizeUri() this will run much faster in this case (and not slower for smaller expressions).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.18.0", "fixed_files": ["org.apache.camel.util.MessageHelper.java", "org.apache.camel.util.StringHelper.java"], "label": 1, "es_results": []}, {"bug_id": 10142, "bug_title": "ScheduledPollingConsumer properties", "bug_description": "It appears that support for a named scheduled polling consumer is there, but you have to have at least one scheduler.xxx property set before it utilises it. e.g. \nI have this configured: \n<bean id=\"nps-scheduler\" class=\"org.apache.camel.pollconsumer.quartz2.QuartzScheduledPollConsumerScheduler\">\n    <property name=\"cron\" value=\"0 * * * * ?\"/>\n</bean>\nIn my route, I reference it like this, but it doesn&apos;t use it: \n<from uri=\"ftp://ftp.somewhere.com/path?scheduler=nps-scheduler\" />\nIf I add a scheduler property, it then uses it, e.g. \n<from uri=\"ftp://ftp.somewhere.com/path?scheduler=nps-scheduler&scheduler.triggerId=?\" />\nDiscussion with Claus Ibsen here: http://camel.465427.n5.nabble.com/ScheduledPollingConsumer-properties-td5785071.html\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.3", "fixed_files": ["org.apache.camel.impl.DefaultComponent.java"], "label": 1, "es_results": []}, {"bug_id": 10341, "bug_title": "When using SSL, a NettyConsumer set to Client Mode does not initiate a handshake", "bug_description": "When used as a Consumer Netty can be put into clientMode, which will cause it to act as a client rather than a server.  However when SSL is enabled on the endpoint the SSL Handshake does not occur.\nDefaultServerInitializerFactory creates a new SslHandler on-demand during channel initialisation, but forces the SSLEngine to not use client mode, regardless of the setting in the NettyConfiguration instance.\nTo cause handshakes to happen when in client mode, set the section in DefaultServerInitializerFactory.configureServerSSLOnDemand() to:\n\n\n\nSSLEngine engine = sslContext.createSSLEngine();\n\nengine.setUseClientMode(consumer.getConfiguration().isClientMode());\n\nengine.setNeedClientAuth(consumer.getConfiguration().isNeedClientAuth());\n\n\n\nFor reference see StackOverflow.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.1", "fixed_version": "camel-2.17.4", "fixed_files": ["org.apache.camel.component.netty4.DefaultServerInitializerFactory.java"], "label": 1, "es_results": []}, {"bug_id": 10128, "bug_title": "camel-jt400 - Need to call configure consumer", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Problem-with-JT400-DataQueue-tp5784876.html\nWhen creating the consumer we must remember to call configureConsumer like all other component does.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.2", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.jt400.Jt400Endpoint.java"], "label": 1, "es_results": []}, {"bug_id": 10144, "bug_title": "Salesforce keeps breaking backward compatibility by adding fields to older API versions", "bug_description": "Salesforce adds fields to org.apache.camel.component.salesforce.api.dto.RestResources even after an API has been released. This needs to be handled in the component by ignoring unknown properties for that type. \nXStream doesn&apos;t support doing this for an single DTO, but it can be done for all types by using XStream.ignoreUknownProperties(). It will make it ignore all unknown properties for all inbuilt DTOs as wells user generated ones. But since the XML payload is less popular, hopefully this behavior won&apos;t be an issue. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.2", "fixed_version": "camel-2.17.3", "fixed_files": ["org.apache.camel.component.salesforce.internal.processor.XmlRestProcessor.java", "org.apache.camel.component.salesforce.api.dto.AbstractDTOBase.java", "org.apache.camel.maven.CamelSalesforceMojo.java", "org.apache.camel.component.salesforce.internal.client.DefaultRestClient.java"], "label": 1, "es_results": []}, {"bug_id": 10145, "bug_title": "Camel-Git: Pull and Push operations require the remote Name and not the remote Path to git repository", "bug_description": "Using the remote Path to git Repository is causing errors during Pull and Push operations.\nI&apos;ll add a new option remoteName in GitEndpoint to avoid this situation.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.2", "fixed_version": "camel-2.17.3", "fixed_files": ["org.apache.camel.component.git.GitEndpoint.java", "org.apache.camel.component.git.producer.GitProducer.java", "org.apache.camel.component.git.producer.GitProducerTest.java"], "label": 1, "es_results": []}, {"bug_id": 10151, "bug_title": "camel-sql - Query parameter count mismatch when using IN and other names in SQL", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/camel-sql-IN-query-number-of-parameters-mismatch-tp5785054.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.2", "fixed_version": "camel-2.17.3", "fixed_files": ["org.apache.camel.component.sql.DefaultSqlPrepareStatementStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 10150, "bug_title": "Camel-Apt: Check for empty lines in parseAsMap method of EndpointAnnotationProcessor", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.2", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.tools.apt.EndpointAnnotationProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 10174, "bug_title": "weaveByToString throws UnsupportedOperationException on CBR", "bug_description": "weaveByToString throws UnsupportedOperationException on CBR.\nSee the attached test case.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.2", "fixed_version": "camel-2.17.3", "fixed_files": ["org.apache.camel.builder.AdviceWithTasks.java", "org.apache.camel.model.ChoiceDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 10185, "bug_title": "camel-ftp - fastExistsCheck issue", "bug_description": "See SO\nhttps://github.com/apache/camel/commit/91c086b7af22b6cfaf2f66b2c872b752dc10a19c#commitcomment-18363186", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.2", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.file.remote.strategy.FtpChangedExclusiveReadLockStrategy.java", "org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 10195, "bug_title": "rest-dsl - automatic binding failure with waitForTaskToComplete=Never", "bug_description": "Hello,\nInto my project i am using rest dsl.\nwhen insert into my seda waitForTaskToComplete=Never attribute, automatic binding (json) is disabled!!!!\nI create a simple camel example (see attachment)\nTo simulate it:\n1. extract zip file\n2. run \"mvn clean camel:run\"\nTEST SUCCESS\ncurl -X GET -H \"Content-type: application/json\" -H \"Accept: application/json\"  localhost:8080/test/ok\"\nOUTPUT (JSON):\n\n{\n\n  \"id\" : 100.0\n\n}\n\nTEST FAILURE\ncurl -X GET -H \"Content-type: application/json\" -H \"Accept: application/json\"localhost:8080/test/failure\"\nOUTPUT (HashMap.toString()):\n\n{id=100.0}\n\nBest regards\nFabryProg", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.2", "fixed_version": "camel-2.18.0", "fixed_files": ["org.apache.camel.component.seda.SedaProducer.java", "org.apache.camel.util.ExchangeHelper.java", "org.apache.camel.spi.UnitOfWork.java", "org.apache.camel.impl.DefaultUnitOfWork.java"], "label": 1, "es_results": []}, {"bug_id": 10253, "bug_title": "NullPointer in ThrowExceptionProcessor.getTraceLabel", "bug_description": "When using\nProcessorDefinition.java\n\n\n throwException(Class<? extends Exception> type, String message) \n\n\n\nto define a Exception and having the tracing set to True in Camel context, if  an exception occurred, it will call \nThrowExceptionProcessor.java\n\n\n  public String getTraceLabel() {\n\n        return \"throwException[\" + this.exception.getClass().getSimpleName() + \"]\";\n\n    }\n\n\n\nBut this.exception is null.\nA patch could be:\nThrowExceptionProcessor.java\n\n\n    public String getTraceLabel() {\n\n        String className= this.exception==null?this.type.getSimpleName():this.exception.getClass().getSimpleName();\n\n        return \"throwException[\" + className + \"]\";\n\n    }\n\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.3", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.processor.ThrowExceptionProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 10282, "bug_title": "[Avro] Issue on OSGi due to static cache", "bug_description": "Avro holds a cache of classloaders and other reflection related classes in a static map here:\nhttps://github.com/apache/avro/blob/master/lang/java/avro/src/main/java/org/apache/avro/specific/SpecificData.java#L52-L57\nKeys of that map are {{String}}s with classname.\nThe cache is never updated, and this breaks OSGi ability to update a new class definition, since avro will keep pointing to the old version.\nThis will lead to a situation where, on an OSGi platform, you are exposed to ClassCastException since the rest of the platform might see a different varsion than the old one cached by avro.\nThe attached fix is a simple way to entirely bypass this caching behavior with just public subclasses that turn off just the caching behavior.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.3", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.dataformat.avro.AvroDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 10291, "bug_title": "Camel RabbitMQ invalid handling of message timestamp", "bug_description": "At the moment the RabbitMQ component is does not map the timestamp of a message appropriately. The outbound mapping (producer) expects the timestamp of the camel message is of type String whereas the String is just the long value representing the timestamp. However the timestamp is already a java.util.Date when the producer just forwards a message from a rabbitmq consumer as the timestamp is already a java.util.date as define in AMQP.BasicProperties.\nThe provided pull request provides a compatible change. So it still keeps the old behaviour as fallback so that the long value is evaluated if the provided data is not a java.util.Date", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.3", "fixed_version": "camel-2.18.0", "fixed_files": ["org.apache.camel.component.rabbitmq.RabbitMQProducerTest.java", "org.apache.camel.component.rabbitmq.RabbitMQMessageConverter.java", "org.apache.camel.component.rabbitmq.RabbitMQConsumerIntTest.java"], "label": 1, "es_results": []}, {"bug_id": 10293, "bug_title": "[camel-maven-plugin] When blueprint detected, plugin ignores useBlueprint, fileApplicationContextUri tags", "bug_description": "It seems not to be possible to force camel:run to use Spring DSL file if OSGi bluepring is available. Following configuration is completely ignored if there is a blueprint in OSGI-INF folder:\n\n\n\n            <plugin>\n\n                <groupId>org.apache.camel</groupId>\n\n                <artifactId>camel-maven-plugin</artifactId>\n\n                <version>2.17.3</version>\n\n                <configuration>\n\n                    <useBlueprint>false</useBlueprint>\n\n                    <fileApplicationContextUri>\n\n                        META-INF/spring/camel-context.xml\n\n                    </fileApplicationContextUri>\n\n                </configuration>\n\n            </plugin>\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.3", "fixed_version": "camel-2.17.4", "fixed_files": ["org.apache.camel.maven.RunMojo.java"], "label": 1, "es_results": []}, {"bug_id": 10340, "bug_title": "camel-aws - SQS option deleteAfterRead not work if set deleteIfFiltered=false", "bug_description": "I&apos;m using aws-sqs 2.17.3, if I set deleteAfterRead=true and deleteIfFiltered=false in my DSL, the message will not be deleted. If I want to delete the message after read it, I have to set deleteAfterRead and deleteIfFiltered both with true when I use the two options in one DSL, but in fact there is no filter in my route, the message should be removed whatever the deleteIfFiltered option set to ture or false.\nSqsConsumerDeleteTest.java\n\n\nfrom(\"aws-sqs:my-quque\"\n\n    + \"?amazonSQSClient=#conn_cAWSConnection_1\"\n\n    + \"&deleteAfterRead=\" + true + \"&deleteIfFiltered=\"\n\n    + false).to(\"log:qs_route.cLog_1\" + \"?level=DEBUG\").to(\"mock:mock_1\");\n\n\n\nI attached my test file, after run the test method, the sqs message still exists in the sqs queue after 30 seconds.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.3", "fixed_version": "camel-2.17.4", "fixed_files": ["org.apache.camel.component.aws.sqs.SqsConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 10370, "bug_title": "Conversion to CxfPayload throws Exception for Non-XML payload", "bug_description": "The CxfPayloadConverter throws a runtime Exception for (optional) conversion from byte[] to CxfPayload instead of returning null when the body is not valid XML.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.3", "fixed_version": "camel-2.17.4", "fixed_files": ["org.apache.camel.component.cxf.converter.CxfPayloadConverterTest.java", "org.apache.camel.component.cxf.converter.CxfPayloadConverter.java"], "label": 1, "es_results": []}, {"bug_id": 10376, "bug_title": "BeanInfo#introspect does not work correctly with bridge methods", "bug_description": "Instead of selecting implementation method, bridge method is used. We faced an issue with conversion of parameter when bean implements generic interface.\nFor example having bean implementation like this:\n\n\n\n    public interface Service<R> {\n\n\n\n        int process(R request);\n\n    }\n\n\n\n    public static class MyService implements Service<Request> {\n\n\n\n        public int process(Request request) {\n\n            return request.x + 1;\n\n        }\n\n    }\n\n\n\nwould lead to beanInfo containing Method with signature \n\n\n\npublic abstract int process(Object request)\n\n\n\nin methodMap\nThis is not correct as conversion of parameter is not possible in this case.\nI could find an issue with the same problem that was previously fixed (CAMEL-8940), but later on it was broken with CAMEL-9656 (commit 5639b78).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.3", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 10380, "bug_title": "JettyHttpEndpoint9 ignores eagerCheckContentAvailable so Jetty builds a reuqest with \"Transfer-Encoding: chunked\"", "bug_description": "My original problem is the fact, that in a Jetty based proxy the consumer sets the header \"Transfer-Encoding: chunked\" to the backend request for a GET request without body. This is not necessary, since there is no body, but the http component puts that null body into a InputStreamCache/HttpInputOverHTTP. This happens, because there is no content-length header set (which means it&apos;s \"-1\"):\nsee https://github.com/apache/camel/blob/master/components/camel-http-common/src/main/java/org/apache/camel/http/common/DefaultHttpBinding.java#L564\nThe only workaround I found, is to set \"eagerCheckContentAvailable=true\". Unfortunately the JettyHttpEndpoint9 does not transfer this setting to the http binding and therefor my problem can&apos;t be fixed. (Ok, I found another workaround, but it&apos;s really ugly.)\nI found that problem using camel-2.17. In the current master branch, there is an open TODO to transfer this option:\nhttps://github.com/apache/camel/blame/master/components/camel-jetty9/src/main/java/org/apache/camel/component/jetty9/JettyHttpEndpoint9.java#L52\nFor me this fixed the problem in my \"test\".\nI didn&apos;t manage to build a good test, since that header is set deeply inside \"org.eclipse.jetty.client.HttpConnection.normalizeRequest(Request)\", but it&apos;s easy to reproduce it, by running a simple proxy and enable DEBUG log for \"org.eclipse.jetty.client.HttpSender\":\nInside the log a http header like will be logged:\n\nAccept-Encoding: gzip\n\nUser-Agent: Jetty/9.2.15.v20160210\n\nsendDirect: true\n\nHost: 127.0.0.1\n\nTransfer-Encoding: chunked\n\n\n\nThis is the test I used to reproduce and debug that problen\n\n\n\npublic class JettyEndpointsChuckedFalseTest extends BaseJettyTest {\n\n    \n\n    @Test\n\n    public void runningTest() throws Exception {\n\n        Exchange exchange = template.request(\"http://localhost:{{port}}/test\", new Processor() {\n\n            @Override\n\n            public void process(Exchange exchange) throws Exception {\n\n            \texchange.getIn().getBody();\n\n            }\n\n        });\n\n        assertNotNull(exchange);\n\n    }\n\n    \n\n    @Override\n\n    protected RouteBuilder createRouteBuilder() throws Exception {\n\n        return new RouteBuilder() {\n\n            @Override\n\n            public void configure() throws Exception {\n\n\n\n            \tfrom(\"jetty:http://localhost:{{port}}/test?matchOnUriPrefix=true&chunked=false&disableStreamCache=true\"\n\n            \t\t\t+ \"&eagerCheckContentAvailable=true\")\n\n\t            \t.to(\"log:request-debug?showHeaders=true&showBody=false&level=INFO\")\n\n\t            \t.to(\"jetty:http://localhost:{{port2}}/test?bridgeEndpoint=true&chunked=false\");\n\n            \t\n\n                from(\"jetty:http://localhost:{{port2}}/test\")\n\n                \t.to(\"mock:dead.end\");\n\n            }\n\n        };\n\n    }\n\n}\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.3", "fixed_version": "camel-2.17.4", "fixed_files": ["org.apache.camel.component.jetty9.JettyHttpEndpoint9.java"], "label": 1, "es_results": []}, {"bug_id": 10384, "bug_title": "Shutdown broken when using Spring Boot", "bug_description": "Camel doesn&apos;t shutdown properly in a Spring Boot environment. Calling the /shutdown endpoint causes Camel to hang-up after stopping the CamelContext. The reason for this is the count-down latch in the CamelSpringBootApplicationController. The overriden Main class modifies the latch in the Main class itself, not the one in CamelSpringBootApplicationController. As a result, the latch in CamelSpringBootApplicationController is waiting forever preventing a proper shutdown.\nCamelSpringBootApplicationController .java\n\n\n    public CamelSpringBootApplicationController(final ApplicationContext applicationContext, final CamelContext camelContext) {\n\n        this.main = new Main() {\n\n             \n\n            ...\n\n\n\n            @Override\n\n            protected void doStop() throws Exception {\n\n                LOG.debug(\"Controller is shutting down CamelContext\");\n\n                try {\n\n                    super.doStop();\n\n                } finally {\n\n                    // Should be CamelSpringBootApplicationController.this.latch.countDown();\n\n                    latch.countDown();\n\n                }\n\n            }\n\n        };\n\n    }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.3", "fixed_version": "camel-2.17.4", "fixed_files": ["org.apache.camel.spring.boot.CamelSpringBootApplicationController.java"], "label": 1, "es_results": []}, {"bug_id": 10480, "bug_title": "MemoryLeak in the DatagramPacketObjectEncoder", "bug_description": "Just found a memory leak in the camel-netty4 UDP encoding when working on CAMEL-10409. \n\n\n\nFailed tests:\n\n  NettyUDPAsyncTest>BaseNettyTest.verifyNoLeaks:89 Leaks detected while running tests: [org.apache.logging.log4j.core.impl.MutableLogEvent@7c96c85]\n\n  NettyUDPMessageLargerThanDefaultBufferSizeTest>BaseNettyTest.verifyNoLeaks:89 Leaks detected while running tests: [org.apache.logging.log4j.core.impl.MutableLogEvent@73bb1337]\n\n  NettyUDPObjectSyncTest>BaseNettyTest.verifyNoLeaks:89 Leaks detected while running tests: [org.apache.logging.log4j.core.impl.MutableLogEvent@5eed6dfb, org.apache.logging.log4j.core.impl.MutableLogEvent@7c96c85, org.apache.logging.log4j.core.impl.MutableLogEvent@7c96c85]\n\n  NettyUDPSyncTest>BaseNettyTest.verifyNoLeaks:89 Leaks detected while running tests: [org.apache.logging.log4j.core.impl.MutableLogEvent@7c96c85, org.apache.logging.log4j.core.impl.MutableLogEvent@7c96c85]\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.17.3", "fixed_version": "camel-2.17.4", "fixed_files": ["org.apache.camel.component.netty4.codec.DatagramPacketObjectEncoder.java"], "label": 1, "es_results": []}, {"bug_id": 9953, "bug_title": "Camel-ssh: Review logic in doStart and doStop in the SshConsumer and SshProducer", "bug_description": "We need to create the client before calling super start and in the doStop method we need to stop the scheduler before stopping and nulling the client.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.component.ssh.SshConsumer.java", "org.apache.camel.component.ssh.SshProducer.java"], "label": 1, "es_results": []}, {"bug_id": 10032, "bug_title": "camel-braintree - Cannot install in Karaf", "bug_description": "karaf@root()> feature:install camel-braintree\nError executing command: Unable to resolve root: missing requirement [root] osgi.identity; osgi.identity=camel-braintree; type=karaf.feature; version=\"[2.18.0.SNAPSHOT,2.18.0.SNAPSHOT]\"; filter:=\"(&(osgi.identity=camel-braintree)(type=karaf.feature)(version>=2.18.0.SNAPSHOT)(version<=2.18.0.SNAPSHOT))\" [caused by: Unable to resolve camel-braintree/2.18.0.SNAPSHOT: missing requirement [camel-braintree/2.18.0.SNAPSHOT] osgi.identity; osgi.identity=org.apache.camel.camel-braintree; type=osgi.bundle; version=\"[2.18.0.SNAPSHOT,2.18.0.SNAPSHOT]\"; resolution:=mandatory [caused by: Unable to resolve org.apache.camel.camel-braintree/2.18.0.SNAPSHOT: missing requirement [org.apache.camel.camel-braintree/2.18.0.SNAPSHOT] osgi.wiring.package; filter:=\"(&(osgi.wiring.package=org.slf4j.bridge)(version>=1.6.0)(!(version>=2.0.0)))\" [caused by: Unable to resolve jul.to.slf4j/1.7.21: missing requirement [jul.to.slf4j/1.7.21] osgi.wiring.package; filter:=\"(&(osgi.wiring.package=org.slf4j)(version>=1.7.21))\"]]]", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.17.2", "fixed_files": ["org.apache.camel.component.braintree.AbstractBraintreeTestSupport.java", "org.apache.camel.component.braintree.MerchantAccountGatewayIntegrationTest.java", "org.apache.camel.component.braintree.BraintreeConfiguration.java", "org.apache.camel.component.braintree.PaymentMethodGatewayIntegrationTest.java", "org.apache.camel.component.braintree.ClientTokenGatewayIntegrationTest.java", "org.apache.camel.component.braintree.CustomerGatewayIntegrationTest.java", "org.apache.camel.component.braintree.WebhookNotificationGatewayIntegrationTest.java", "org.apache.camel.component.braintree.TransactionGatewayIntegrationTest.java", "org.apache.camel.component.braintree.AddressGatewayIntegrationTest.java"], "label": 1, "es_results": []}, {"bug_id": 10215, "bug_title": "EventDrivenPollingConsumer is not thread safe when used with ConsumerCache ", "bug_description": "For static endpoints, if DefaultConsumerTemplate#receive() is used,  ConsumerCache will return the cached instance of PollingConsumer for every thread part of parallel processing of split/aggregate.\nNow, if EventDrivenPollingConsumer is used, returning same cache instance causes race condition in receive and receive(long) as the methods are not thread safe. The ordering of beforePoll, poll and afterPoll needs to be synchronized, otherwise the consumer might end up being suspended in afterPoll and the client whose beforePoll couldn&apos;t wake up the suspended thread will get no data.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.16.4", "fixed_files": ["org.apache.camel.impl.EventDrivenPollingConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 10366, "bug_title": "Missing input/output values in camel-catalog for several eips", "bug_description": "In camel-catalog jar in org.apache.camel.catalog.models/*.json,\nsome of the files does not define the input/output values\nlist of elements for which it is missing:\nbeanPostProcessor, errorHandler, endpoint, streamCaching, propertiesFunction, export, restContext, propertyPlaceholder, fluentTemplate, template, consumerTemplate, camelContext, routeContext, jmxAgent, redeliveryPolicyProfile, threadPool, proxy", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.19.0", "fixed_files": ["org.apache.camel.tools.apt.SpringAnnotationProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 10372, "bug_title": "camel-stream - Component doc issue", "bug_description": "A little mistake", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.component.stream.StreamEndpoint.java", "org.apache.camel.component.stream.StreamComponent.java"], "label": 1, "es_results": []}, {"bug_id": 10381, "bug_title": "camel-google-mail getting NPE from component configuration", "bug_description": "From forums:\nWhen i run the application and start the route i get an NPE which points to\norg.apache.camel.component.google.mail.GoogleMailComponent.getClient(GoogleMailComponent.java:50\nas the culprit.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.17.3", "fixed_files": ["org.apache.camel.component.google.calendar.GoogleCalendarEndpoint.java", "org.apache.camel.component.google.calendar.GoogleCalendarComponent.java", "org.apache.camel.component.google.drive.GoogleDriveComponent.java", "org.apache.camel.component.google.drive.GoogleDriveFilesConverter.java", "org.apache.camel.component.google.drive.GoogleDriveEndpoint.java", "org.apache.camel.component.google.mail.GoogleMailEndpoint.java", "org.apache.camel.component.google.mail.GoogleMailComponent.java"], "label": 1, "es_results": []}, {"bug_id": 10383, "bug_title": "activemq-camel - Issue with parsing uri to determine queue vs topic", "bug_description": "See screenshot\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.util.EndpointHelper.java", "org.apache.camel.util.URISupport.java", "org.apache.camel.catalog.CamelCatalogTest.java", "org.apache.camel.catalog.URISupport.java", "org.apache.camel.util.JsonSchemaHelper.java"], "label": 1, "es_results": []}, {"bug_id": 10411, "bug_title": "Camel-Blueprint - failed container gets restarted automatically", "bug_description": "In case of a wrong endpoint definition, a CamelContext could be automatically restarted even if the start operation was correctly aborted.\nThis happens because during BlueprintCamelContext, the CamelContext instance is registered as a ServiceListener\nhttps://github.com/apache/camel/blob/master/components/camel-blueprint/src/main/java/org/apache/camel/blueprint/BlueprintCamelContext.java#L102\nDue to that reason, even after a stop() call is invoked by the error execution branch of start(), that instance is receptive to external service events that result in the invocation of this method:\nhttps://github.com/apache/camel/blob/master/components/camel-blueprint/src/main/java/org/apache/camel/blueprint/BlueprintCamelContext.java#L155\nThat method has the freedom to start again the CamelContext instance.\nThe proposed solution is to use a BlueprintCamelContext local flag to keep track of the successful initialization of the Context, to be able to ignore events in case activation wasn&apos;t successful.\nNote that the BlueprintCamelContext instance, un-registers itself as a service listener, only in its destroy() method.\nAttached to JBoss-Fuse linked case, there is a reproducer that shows the issue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.17.4", "fixed_files": ["org.apache.camel.blueprint.BlueprintCamelContext.java"], "label": 1, "es_results": []}, {"bug_id": 10414, "bug_title": "Query is ignore if field filter header is set", "bug_description": "If the field filter header (CamelMongoDbFieldsFilter) is set the query is ignored.\nAs can be seen here:\nhttps://github.com/apache/camel/blob/camel-2.18.x/components/camel-mongodb/src/main/java/org/apache/camel/component/mongodb/MongoDbProducer.java#L314-L320", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.component.mongodb.MongoDbFindOperationTest.java", "org.apache.camel.component.mongodb.MongoDbProducer.java"], "label": 1, "es_results": []}, {"bug_id": 10430, "bug_title": "camel-hystrix - Should also execute fallback if exception not from Camel", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Hystrix-Fallback-not-executed-on-Thread-Pool-Semaphore-rejection-tp5789521.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.component.hystrix.processor.HystrixProcessorCommand.java"], "label": 1, "es_results": []}, {"bug_id": 10431, "bug_title": "camel-elsql - Does not read named parameter from header properties", "bug_description": "Camel-elsql could not lookup for parameters from message headers. Problem is here https://github.com/apache/camel/blob/camel-2.18.0/components/camel-elsql/src/main/java/org/apache/camel/component/elsql/ElsqlSqlMapSource.java#L70\nSample route:\n\n\n\nfrom(\"direct:projects\")\n\n  .setHeader(\"lic\", constant(\"ASF\"))\n\n  .setHeader(\"min\", constant(123))\n\n  .to(\"elsql:projects:com/foo/projects.elsql\")\n\n\n\nElSql:\n\n\n\n@NAME(projects)\n\n  SELECT *\n\n  FROM projects\n\n  WHERE license = :lic AND id > :min\n\n  ORDER BY id\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.component.elsql.ElsqlSqlMapSource.java", "org.apache.camel.component.elsql.ElSqlProducerBodySimpleTest.java"], "label": 1, "es_results": []}, {"bug_id": 10394, "bug_title": "BlueprintCamelContext cannot find components created in RouteBuilder.configure method", "bug_description": "When a simple java RouteBuilder that creates a component and adds it to the context in the configure method is used in a blueprint, the context cannot find the component.\nExample Builder:\npublic class TimerRouteBuilder extends RouteBuilder {\n    @Override\n    public void configure() throws Exception \n{\n\n        TimerComponent timerComponent = new TimerComponent();\n\n\n\n        getContext().addComponent(\"my-timer\", timerComponent);\n\n\n\n        from( \"my-timer://test-timer\")\n\n                .log(\"Timer Fired\")\n\n                .to(\"mock://result\");\n\n    }\n}\nExample Blueprint:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<blueprint xmlns=\"http://www.osgi.org/xmlns/blueprint/v1.0.0\"\n           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n           xsi:schemaLocation=\"\nhttp://www.osgi.org/xmlns/blueprint/v1.0.0 https://www.osgi.org/xmlns/blueprint/v1.0.0/blueprint.xsd\nhttp://camel.apache.org/schema/blueprint http://camel.apache.org/schema/blueprint/camel-blueprint.xsd\">\n    <bean id=\"timer-route-builder\" class=\"com.pronoia.camel.builder.TimerRouteBuilder\"/>\n    <camelContext id=\"blueprint-context\" xmlns=\"http://camel.apache.org/schema/blueprint\">\n        <routeBuilder ref=\"timer-route-builder\"/>\n    </camelContext>\n</blueprint>\nThis test fails:\npublic class BlueprintTest extends CamelBlueprintTestSupport {\n    @EndpointInject(uri = \"mock://result\")\n    MockEndpoint result;\n    @Override\n    protected String getBlueprintDescriptor() \n{\n\n        return \"/OSGI-INF/blueprint/blueprint.xml\";\n\n    }\n\n    @Test\n    public void testRoute() throws Exception \n{\n\n        result.expectedMessageCount(5);\n\n\n\n        assertMockEndpointsSatisfied(10, TimeUnit.SECONDS);\n\n    }\n\n}\n\nBut this test passes\npublic class CamelTest extends CamelTestSupport {\n\n    @EndpointInject(uri = \"mock://result\")\n    MockEndpoint result;\n\n    @Override\n    protected RoutesBuilder createRouteBuilder() throws Exception {\n\n        return new TimerRouteBuilder();\n\n    }\n\n    @Test\n    public void testRoute() throws Exception {\n        result.expectedMessageCount(5);\n\n        assertMockEndpointsSatisfied(10, TimeUnit.SECONDS);\n    }\n}", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.blueprint.handler.CamelNamespaceHandler.java"], "label": 1, "es_results": []}, {"bug_id": 10443, "bug_title": "findById does not work with ObjectId", "bug_description": "When using Camel MongoDB component with \"operation=findById\" and [_id] field is happened to be a standard ObjectId(), it doesn&apos;t work anymore in v.2.18 (worked 2.17.3 and before). \nBasically, to test you&apos;d need to send org.bson.types.ObjectId in the message body but it never finds a document. \nThe problem lies in MongoDbProducer.createDoFindById(). \nCan be fixed by replacing line: \n                String id = exchange1.getIn().getMandatoryBody(String.class); \nwith \n                Object id = exchange1.getIn().getMandatoryBody(); ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.component.mongodb.MongoDbFindOperationTest.java", "org.apache.camel.component.mongodb.MongoDbProducer.java"], "label": 1, "es_results": []}, {"bug_id": 10449, "bug_title": "Set CXF SoapAction header correctly", "bug_description": "As of CXF-6732, CXF will by default always set the SoapAction header with the value found at key SoapBindingConstants.SOAP_ACTION. See comment for more details. Workaround can be either adding a custom CXF OutInterceptor or use a different Camel/CXF binding.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.component.cxf.DefaultCxfBinding.java", "org.apache.camel.component.cxf.DefaultCxfBindingTest.java"], "label": 1, "es_results": []}, {"bug_id": 10460, "bug_title": "MetricsMessageHistoryFactory.java:138 Generate a NPE", "bug_description": "I&apos;m trying to play with Spring-boot camel and metrics to expose dashboard. \nHawtio is a great tools to check what it&apos;s going on at runtime but no persistence is done. We would like to persist statistics in prometheus by example. \nI configure camel context with MetricsMessageHistoryFactory as: \n    MetricsMessageHistoryFactory metricsMessageHistoryFactory = new MetricsMessageHistoryFactory(); \n    metricsMessageHistoryFactory.setMetricsRegistry(metricRegistry); \n    camelContext.setMessageHistoryFactory(metricsMessageHistoryFactory); \nwhere metricRegistry is injected with: \n  /** \n\nSrping Boot metrics\n   */ \n  @Autowired \n  private MetricRegistry metricRegistry; \n\nEverything is ok until I passed in OnExceptionRoute. It seems that the routeId is set to Null in this case and generate a NPE (line MetricsMessageHistoryFactory:138) in the exception route. Route causing the NPE:\nonException(NoAttachmentToProcessException.class) \n        .routeId(RouteIds.TECHNICAL_ERROR_HANDLING.routeId()) \n        .log(LoggingLevel.ERROR,\"TECHNICAL ERROR: $\n{exchangeProperty.CamelExceptionCaught}\n\") ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.processor.CamelInternalProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 10453, "bug_title": "camel-elsql does not set CamelSqlUpdateCount header on update operation", "bug_description": "Camel ElSql should store number of rows updated for update operation into CamelSqlUpdateCount header.\nRoute:\n\n\n\n<route>\n\n  <from uri=\"direct:updateLicense\"/>\n\n  <to uri=\"elsql:updateLicense\"/>\n\n  <to uri=\"mock:updateLicense\"/>\n\n</route>\n\n\n\nElSQL\n\n\n\n@NAME(updateLicense)\n\n    UPDATE projects\n\n    SET license = :lic\n\n    WHERE id = :id\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.18.0", "fixed_version": "camel-2.18.1", "fixed_files": ["org.apache.camel.component.elsql.ElsqlProducer.java", "org.apache.camel.component.elsql.ElSqlProducerBodySimpleTest.java"], "label": 1, "es_results": []}, {"bug_id": 2237, "bug_title": "Improper ordering of MINA filters", "bug_description": "When we want to use SSL on the consumer side, MINA&apos;s SSLFilter must be inserted as the very first filter into the chain (or, at least, as the second one after an ExecutorFilter), but it is currently not possible, because MinaComponent#createSocketEndpoint() always inserts a ProtocolCodecFilter at the very beginning by calling configureCodecFactory().\nA proposed workaround is to introduce an additional URL parameter noDefaultCodec which prohibits the installation of the default protocol codec filter. The protocol codec must be then configured manually, e.g. using Spring:\n\n<bean id=\"sslFilter\" class=\"org.apache.mina.filter.SSLFilter\">\n    <constructor-arg>\n        <bean class=\"javax.net.ssl.SSLContext\" factory-method=\"getDefault\" />\n    </constructor-arg>\n</bean>\n<bean id=\"codecFilter\" class=\"org.apache.mina.filter.codec.ProtocolCodecFilter\">\n    <constructor-arg ref=\"hl7codec\" />\n</bean>\n<bean id=\"hl7codec\" class=\"org.apache.camel.component.hl7.HL7MLLPCodec\" />\n<bean id=\"minaFilters\" class=\"java.util.ArrayList\">\n    <constructor-arg>\n        <list value-type=\"org.apache.mina.common.IoFilter\">\n             <ref bean=\"sslFilter\" /> \n             <ref bean=\"codecFilter\" /> \n        </list>\n    </constructor-arg>\n</bean>\nAfter that, the consumer endpoint URL can look like\nfrom(\"mina:tcp://0.0.0.0:8888?sync=true&lazySessionCreation=true&noDefaultCodec=true&filters=#minaFilters\").\nI am not sure whether this approach is optimal, therefore I call it \"workaround\" and not \"solution\".  The corresponding patch is attached.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.mina.MinaFiltersTest.java", "org.apache.camel.component.mina.MinaConfiguration.java", "org.apache.camel.component.mina.MinaComponent.java"], "label": 1, "es_results": []}, {"bug_id": 2274, "bug_title": "Camel proxy - Detect the proxied method returned null and allow that as a valid answer", "bug_description": "See nabble\nhttp://old.nabble.com/Can%27t-return-null-with-Camel-remoting-ts26682177.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.bean.CamelInvocationHandler.java"], "label": 1, "es_results": []}, {"bug_id": 2289, "bug_title": "camel-bindy - csv - Make it easier to marshal with POJO out of the box", "bug_description": "Its to ugly that you must use List<Map<String, Object>> to wrap you POJO to let Bindy be able to marhal my POJO to a CSV record.\nIt should be able to detect that its not a List but a plain POJO and construct the List itself and based on defaults.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.dataformat.bindy.csv.BindyCsvDataFormat.java", "org.apache.camel.dataformat.bindy.BindyCsvFactory.java"], "label": 1, "es_results": []}, {"bug_id": 2306, "bug_title": "IOConverter - File to byte[] should close input stream", "bug_description": "See nabble\nhttp://old.nabble.com/Problem-with-JMS-and-file-ts26857645.html\nMost likely a problem on Windows as it tend to lock files for much longer duration than needed.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.converter.IOConverter.java"], "label": 1, "es_results": []}, {"bug_id": 2320, "bug_title": "JDBC component does not preserve headers", "bug_description": "JDBC component doesn&apos;t preserve any of the headers that are sent into it", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.jdbc.JdbcProducer.java"], "label": 1, "es_results": []}, {"bug_id": 2353, "bug_title": "Simple Language - Parsing complex date patterns fails if they contain additional colons", "bug_description": "See nabble\nhttp://old.nabble.com/Camel-%3A-Date-Pattern-ts27131137.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.language.simple.SimpleLanguage.java", "org.apache.camel.language.SimpleTest.java"], "label": 1, "es_results": []}, {"bug_id": 2345, "bug_title": "One should not have to explicitly provide the service/port for cxf is there is only one", "bug_description": "This is due to the following code in Client createClient() in CxfEnpoint\n\n        } else {\n            ObjectHelper.notNull(portName, \"Please provide endpoint/port name\");\n            ObjectHelper.notNull(serviceName, \"Please provide service name\");\n            ClientFactoryBean factoryBean = createClientFactoryBean();\n\n\nWhen there&apos;s only one service/port, we should just use it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.cxf.CxfEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2385, "bug_title": "BindingOperationInfos that are stored in exchange are inconsistent between CxfProducer and CxfConsumer", "bug_description": "CxfProducer could store a unwrapped version of the BindingOperationInfo in the exchange which is inconsistent with CxfConsumer and it puts burden on the application to revert back to the wrapped version.  CxfProducer should store the original version before calling.\n\n                boi = boi.getUnwrappedOperation();\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.cxf.CxfProducer.java"], "label": 1, "es_results": []}, {"bug_id": 2420, "bug_title": "In camel-cache CacheConfiguration \"diskExpiryThreadIntervalSeconds\" not set correctly", "bug_description": "See CacheConfiguration.java\nCacheConfiguration.java\n\n if (cacheSettings.containsKey(\"diskExpiryThreadIntervalSeconds\")) {\n\tsetTimeToLiveSeconds(Long.valueOf((String) cacheSettings.get(\"diskExpiryThreadIntervalSeconds\")).longValue());\n} \n\n\n ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.cache.CacheConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 2428, "bug_title": "In camel-cache \"diskStorePath\" property not efficient", "bug_description": "If you want to change path to persisent cache with \"diskStorePath\", it has no effect  because cacheManager erase this value.\nIn \"addCacheNoCheck\" method, \"setDiskStorePath\" call with \"diskStorePath\" CacheManager property value.\nCacheManager.java\n\n    public void addCache(Ehcache cache) throws IllegalStateException,\n            ObjectExistsException, CacheException {\n        checkStatus();\n        if (cache == null) {\n            return;\n        }\n        addCacheNoCheck(cache);\n    }\n\n    private void addCacheNoCheck(Ehcache cache) throws IllegalStateException,\n            ObjectExistsException, CacheException {\n        if (ehcaches.get(cache.getName()) != null) {\n            throw new ObjectExistsException(\"Cache \" + cache.getName() + \" already exists\");\n        }\n        cache.setCacheManager(this);\n        cache.setDiskStorePath(diskStorePath);\n        cache.initialise();\n        try {\n            cache.bootstrap();\n        } catch (CacheException e) {\n            LOG.log(Level.WARNING, \"Cache \" + cache.getName() + \"requested bootstrap but a CacheException occured. \" + e.getMessage(), e);\n        }\n        ehcaches.put(cache.getName(), cache);\n        if (cache instanceof Cache) {\n            caches.put(cache.getName(), cache);\n        }\n\n        //Don&apos;t notify initial config. The init method of each listener should take care of this.\n        if (status.equals(Status.STATUS_ALIVE)) {\n            cacheManagerEventListenerRegistry.notifyCacheAdded(cache.getName());\n        }\n    }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.cache.CacheConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 2426, "bug_title": "CXF Header \"ResponseContext\" cannot be filtered by HeaderFilterStrategy", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.cxf.DefaultCxfBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2439, "bug_title": "File consumer - Consuming from absolute paths can cause issue on Windows when moving file when done", "bug_description": "When using a file consumer with an absolute path, eg from(\"file:/data\") then when the process is done and it wants to move the file to .camel sub dir of /data it may not do that correctly on Windows.\nThe issue is the logic in Camel is based upon that a path starting with \\ is considered absolute, as it is on Unix OS and other platforms. Where as on Windows java.io.File returns false for such files. The other return true. So we need to cater for this and make Windows return true as well so the logic is consistent across OS.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.file.FileConsumer.java", "org.apache.camel.component.file.GenericFileProducer.java", "org.apache.camel.component.file.FileEndpoint.java", "org.apache.camel.util.FileUtil.java", "org.apache.camel.component.file.GenericFile.java", "org.apache.camel.language.FileLanguageTest.java"], "label": 1, "es_results": []}, {"bug_id": 2444, "bug_title": "Log Component documentation for \"showAll\" option is misleading", "bug_description": "Log Component documentation for \"showAll\" option is misleading or the option does not work as expected.\n\"showAll \t false \t Quick option for turning all options on.\"\nThere appear to be several options that are not turned as expected such as \"multiline\" and \"showStackTrace\", etc..  \n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.log.LogFormatter.java"], "label": 1, "es_results": []}, {"bug_id": 2451, "bug_title": "HL7MLLPDecoder fails if message length is exactly 1022", "bug_description": "When an HL7 message length is exactly 1022 then the two end control charcters are not read from the same packet/buffer. This causes HL7MLLPDecoder.scan() method to fail.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.2.0", "fixed_files": ["org.apache.camel.component.hl7.HL7MLLPDecoder.java"], "label": 1, "es_results": []}, {"bug_id": 2456, "bug_title": "WARNING log of JmsTemporaryTopicEndpoint , JmsTemporaryQueueEndpoint do not have the ManagedResource annotation", "bug_description": "Here is the mail thread which discusses about this issue.\nhttp://old.nabble.com/Attemp-to-send-message-to-activemq-temporary-queue-using-producerTemplate%3A-InvalidMetadataException-tp27520096p27520096.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.jms.JmsTemporaryTopicEndpoint.java", "org.apache.camel.component.jms.JmsTemporaryQueueEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2476, "bug_title": "Camel Velocity: change the case of all fields properties Header", "bug_description": "Using the velocity component doesn&apos;t respect the case matching of all fields of the header.\nFor example: the exchange of properties CorrelationID change the case matching, after using velocity component, it puts the property in lowercase correlationid.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.velocity.VelocityEndpoint.java", "org.apache.camel.component.velocity.VelocityTemplateInHeaderTest.java"], "label": 1, "es_results": []}, {"bug_id": 2478, "bug_title": "Camel Stringtemplate : lose all fields properties", "bug_description": "the component Camel-stringtemplate loses all fields properties after the generation of Result.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.stringtemplate.StringTemplateEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2477, "bug_title": "Camel Freemarker: change the case of all fields properties Header", "bug_description": "Using the Freemarker component doesn&apos;t respect the case matching of all fields of the header.\nFor example: the exchange of properties CorrelationID change the case matching, after using Freemarker component, it puts the property in lowercase correlationid.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.freemarker.FreemarkerEndpoint.java", "org.apache.camel.component.freemarker.FreemarkerTemplateInHeaderTest.java"], "label": 1, "es_results": []}, {"bug_id": 2484, "bug_title": "camel-mina - Using close session could potentially because memory to be not released", "bug_description": "See nabble\nhttp://old.nabble.com/SocketSessionImpl-in-Mina-component-retained-in-memory-indefinitely-ts27624487.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.impl.DefaultProducerCacheTest.java", "org.apache.camel.util.LRUCache.java", "org.apache.camel.processor.RecipientList.java", "org.apache.camel.management.JmxInstrumentationUsingDefaultsTest.java", "org.apache.camel.impl.ProducerCache.java", "org.apache.camel.management.ManagedUnregisterProducerTest.java"], "label": 1, "es_results": []}, {"bug_id": 2510, "bug_title": "Mixing jetty/http in a route screws up the URI used by HttpClient", "bug_description": "Below test shows the Http producer can&apos;t build up right HttpRequest URI as a bridgeEndpoint.\n\n   public class JettyHttpTest extends CamelTestSupport {\n\n    private String targetProducerUri = \"http://localhost:8542/someservice?bridgeEndpoint=true&throwExceptionOnFailure=false\";\n    private String targetConsumerUri = \"jetty:http://localhost:8542/someservice?matchOnUriPrefix=true\";\n    private String sourceUri = \"jetty:http://localhost:6323/myservice?matchOnUriPrefix=true\";\n    private String sourceProducerUri = \"http://localhost:6323/myservice\";\n\n    @Test\n    public void testGetRootPath() throws Exception {\n        MockEndpoint mock = getMockEndpoint(\"mock:result\");\n        mock.expectedBodiesReceived(\"Hi! /someservice\");\n\n        template.sendBody(\"direct:root\", \"\");\n\n        assertMockEndpointsSatisfied();\n    }\n    \n    @Test\n    public void testGetWithRelativePath() throws Exception {\n        MockEndpoint mock = getMockEndpoint(\"mock:result\");\n        mock.expectedBodiesReceived(\"Hi! /someservice/relative\");\n        \n        template.sendBody(\"direct:relative\", \"\");\n        assertMockEndpointsSatisfied();\n        \n    }\n\n    @Override\n    protected RouteBuilder createRouteBuilder() throws Exception {\n        return new RouteBuilder() {\n            @Override\n            public void configure() throws Exception {\n\n                from(targetConsumerUri)\n                    .process(new Processor() {\n                        public void process(Exchange exchange) throws Exception {\n                            String path = exchange.getIn().getHeader(Exchange.HTTP_PATH, String.class);\n                            exchange.getOut().setBody(\"Hi! \" + path);\n                        }   \n                    });\n\n                from(sourceUri)\n                    .to(targetProducerUri);\n\n                from(\"direct:root\")\n                    .to(sourceProducerUri)\n                    .to(\"mock:result\");\n                \n                from(\"direct:relative\")\n                    .to(sourceProducerUri + \"/relative\")\n                    .to(\"mock:result\");\n\n            }\n        };\n    }\n}\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.http.helper.HttpProducerHelper.java", "org.apache.camel.component.jetty.HttpBridgeRouteTest.java"], "label": 1, "es_results": []}, {"bug_id": 2638, "bug_title": "Restlet component is URL encoding the POST message body.  It should encode it based on content-type request header.", "bug_description": "I attempted to POST a JSON document to couchdb via restlet.  The post fails with and \"Invalid JSON format\" error from couchdb.  This is because the POST data was being URL encoded which substitutes %XX sequences for all the curly braces.  I believe the encoding should be done based on the content-type header, or possibly not at all for POST requests.  ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.restlet.DefaultRestletBinding.java", "org.apache.camel.component.restlet.RestletRouteBuilderTest.java"], "label": 1, "es_results": []}, {"bug_id": 2817, "bug_title": "renames (to .processed) are sometimes done even if the download failed.", "bug_description": "I have an ftp consumer endpoint URI like: \nftp://conaxTest@localhost:2121/autreq/ok?passiveMode=true&amp;password=conaxTest&move=.processed&delay=5000\nFrom my ftp server logs, I can see files being downloaded, and then renamed...\n\n[org.apache.ftpserver.command.impl.RETR:pool-2-thread-56] - <File downloaded /autreq/ok/vp006331.emm>\n[org.apache.ftpserver.command.impl.RETR:pool-2-thread-56] - <File downloaded /autreq/ok/vp006332.emm>\n[org.apache.ftpserver.impl.DefaultFtpHandler:pool-2-thread-57] - <Session idle, closing>\n[org.apache.ftpserver.command.impl.RNTO:pool-2-thread-56] - <File rename from \"/autreq/ok/vp006331.emm\" to \"/autreq/ok/.processed/vp006331.emm\">\n[org.apache.ftpserver.command.impl.RNTO:pool-2-thread-54] - <File rename from \"/autreq/ok/vp006332.emm\" to \"/autreq/ok/.processed/vp006332.emm\">\n[org.apache.ftpserver.command.impl.RNTO:pool-2-thread-54] - <File rename from \"/autreq/ok/vp006333.emm\" to \"/autreq/ok/.processed/vp006333.emm\">\nNote, that vp006333.emm is renamed, but was never downloaded.  There&apos;s no other mention of the file in my logs.  On the camel ftp consumer side, I see that camel attempted to download the file, but ended up with a null...\n\n[is.vf.conan.conax.FileParser:Camel thread 7: seda://updateOk] - <handling: GenericFile[vp006331.emm]>\n[is.vf.conan.ConanCore:Camel thread 7: seda://updateOk] - <updating operation txid:006331 with the results: OK>\n[is.vf.conan.conax.FileParser:Camel thread 7: seda://updateOk] - <handling: GenericFile[vp006332.emm]>\n[is.vf.conan.ConanCore:Camel thread 7: seda://updateOk] - <updating operation txid:006332 with the results: OK>\n[is.vf.conan.conax.FileParser:Camel thread 7: seda://updateOk] - <handling: GenericFile[vp006333.emm]>\n[is.vf.conan.conax.FileParser:Camel thread 7: seda://updateOk] - <Requested parse of an empty file!>\nThe \"requested parse of an empty file\" is logged when ex.getIn().getBody(String.class) is empty or blank for the file object.\nI had a look through the bugs fixed for 2.3.0, but I don&apos;t see anything that would be related to this at all.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.file.GenericFileConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 2922, "bug_title": "XMPPConsumer does not remove the message which causes OOME with XMPP", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.xmpp.XmppConsumer.java", "org.apache.camel.component.xmpp.XmppComponent.java", "org.apache.camel.component.xmpp.XmppGroupChatProducer.java", "org.apache.camel.component.xmpp.XmppPrivateChatProducer.java", "org.apache.camel.component.xmpp.XmppBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2452, "bug_title": "HttpHeaderFilterStrategy dosn't filters out 'Cache-Control', 'Connection', 'Pragma', 'Trailer', 'Transfer-Encoding', 'Upgrade', 'Via' and 'Warning' in method applyFilterToCamelHeaders", "bug_description": "HttpHeaderFilterStrategy uses the HTTP headers with upper case characters on the beginning (e. g. &apos;Transfer-Encoding&apos; instead of &apos;transfer-encoding&apos;).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.1.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.http.HttpHeaderFilterStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 2360, "bug_title": "recipientList retryUntil not working", "bug_description": "summary:\nthe bean gets intialized but it looks like the method retryUntil is never called, could it be an error of implementation? am I doing something wrong?\nusing following route:\n\nfrom(\"jms-test:queue:queue.delivery.notification.test\") \n.process(processor) \n.onException(Exception.class).retryUntil(bean(\"myRetryBean\")).end() \n.recipientList(header(\"recipientListHeader\").tokenize(\",\")) \n.parallelProcessing().executorService(customThreadPoolExecutor) \n.aggregationStrategy(new RecipientAggregationStrategy()) \n.to(\"direct:chunk.completed\"); \n\n\nbean is registered in such way: \n\nJndiRegistry jndi = new JndiRegistry(new JndiContext()); \njndi.bind(\"myRetryBean\", new RetryBean()); \n\n\nbean class is: \n\npublic class RetryBean { \n\n        private int _invoked; \n        private Logger _logger; \n        \n    public RetryBean() { \n    this._logger = Logger.getLogger(RetryBean.class); \n    this._invoked = 0; \n    _logger.debug(\"BEAN INITIALIZED \" + _invoked); \n    } \n        \n    // using bean binding we can bind the information from the exchange to the types we have in our method signature \n    public boolean retryUntil(@Header(Exchange.REDELIVERY_COUNTER) Integer counter, @Body String body, @ExchangeException Exception causedBy) { \n        // NOTE: counter is the redelivery attempt, will start from 1 \n    _invoked++; \n    \n    \n    _logger.debug(\"invoked\" + _invoked); \n    _logger.debug(\"counter\" + counter); \n    _logger.debug(\"result\" + (counter < 2)); \n    \n        // we can of course do what ever we want to determine the result but this is a unit test so we end after 3 attempts \n        return counter < 7; \n    } \n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.issues.RetryRouteScopedUntilRecipientListIssueTest.java", "org.apache.camel.processor.Splitter.java", "org.apache.camel.processor.MulticastProcessor.java", "org.apache.camel.processor.DefaultChannel.java", "org.apache.camel.impl.DefaultUnitOfWork.java", "org.apache.camel.processor.ShutdownDeferTest.java", "org.apache.camel.spi.UnitOfWork.java"], "label": 1, "es_results": []}, {"bug_id": 2551, "bug_title": "File component does not correctly handle PipedInputStream in message body.", "bug_description": "Streams that do not have their contents length at immediate disposal, like PipedInputStream, are not processed correctly by the file component.\n\n\n    private void writeFileByStream(InputStream in, File target) throws IOException {\n        FileChannel out = null;\n        try {\n            out = prepareOutputFileChannel(target, out);\n\n            if (LOG.isTraceEnabled()) {\n                LOG.trace(\"Using InputStream to transfer from: \" + in + \" to: \" + out);\n            }\n            int size = endpoint.getBufferSize();\n            byte[] buffer = new byte[size];\n            ByteBuffer byteBuffer = ByteBuffer.wrap(buffer);\n            while (true) {\n                int count = in.read(buffer);\n                if (count <= 0) {\n                    break;\n                } else if (count < size) {\n                    byteBuffer = ByteBuffer.wrap(buffer, 0, count);\n                    out.write(byteBuffer);\n                    break;\n                } else {\n                    out.write(byteBuffer);\n                    byteBuffer.clear();\n                }\n            }\n        } finally {\n            ObjectHelper.close(in, target.getName(), LOG);\n            ObjectHelper.close(out, target.getName(), LOG);\n        }\n    }\n\n\n\nThe code \n\n                } else if (count < size) {\n                    byteBuffer = ByteBuffer.wrap(buffer, 0, count);\n                    out.write(byteBuffer);\n                    break;\n                } else {\n\n\ndoes not take into account that bytes read can be less than the size of the buffer passed into the InputStream.read method and stream can still have more content. The only indication that EOF was reached is -1 returned from the read method according to Java API.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.file.FileOperations.java"], "label": 1, "es_results": []}, {"bug_id": 2545, "bug_title": "Camel-Mail: Alternative body part does not handle charset", "bug_description": "Mail component does not set charset for alternative body part. See - http://old.nabble.com/Camel-Mail:-Alternative-part-does-not-handle-charset--td27882178.html for more information.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.mail.MailBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2566, "bug_title": "camel-http component should set Transfer-Encoding as chunked header for response message when checkChunked is true", "bug_description": "so that the client side which send request will know this is a chunked message.\nIt&apos;s important when the response http headers is more than 4096 and we want to use chunked response message", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.Exchange.java", "org.apache.camel.component.http.DefaultHttpBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2575, "bug_title": "CXFRS Routing in 2.2.0 does not behave like in 2.0.0", "bug_description": "CXFRS Routing in Camel 2.0.0 works  well with these endPoints:\n<cxf:rsServer id=\"restRouter\" address=\"/restRouter/\"\t\n      serviceClass=\"com.project.service.impl.ServiceManagerImpl\"   />\n     <cxf:rsClient id=\"restEndpoint\" address=\"http://localhost:8080/services/rest\"\n      serviceClass=\"com.project.service.impl.ServiceManagerImpl\" />\nIn Camel 2.2.0, Routing fails with error causing the CXF Client to invoke a 404 not found Rest Service which is &apos;http://localhost:8080/services/rest/restRouter/Path&apos;.\nThe address of cxf:rsClient is being appended by the cxf:rsServer&apos;s address", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.DefaultCxfRsBinding.java", "org.apache.camel.component.cxf.jaxrs.CxfRsRouterTest.java"], "label": 1, "es_results": []}, {"bug_id": 2622, "bug_title": "Invocation of hasNext() on org.apache.camel.util.ObjectHelper.createIterator(...).new Iterator<Node>() {...} return different results", "bug_description": "\n                    public boolean hasNext() {\n                        // empty string should not be regarded as having next\n                        return ++idx == 0 && ObjectHelper.isNotEmpty(s);\n                    }\n\n                    public String next() {\n                        return s;\n                    }\n\n\nThis is incorrect. Should be:\n\n                    public boolean hasNext() {\n                        // empty string should not be regarded as having next\n                        return idx+1 == 0 && ObjectHelper.isNotEmpty(s);\n                    }\n\n                    public String next() {\n\t\t\t\t\t\tidx++;\n                        return s;\n                    }\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.util.ObjectHelper.java", "org.apache.camel.impl.CustomProducerServicePoolTest.java", "org.apache.camel.converter.ObjectHelperTest.java"], "label": 1, "es_results": []}, {"bug_id": 2640, "bug_title": "file component - Fix recursive and noop not picking up files with similar name in sibling folders", "bug_description": "See nabble\nhttp://old.nabble.com/File-consumer-with-noop%3Dtrue-recursive%3Dtrue-ts28229501.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.file.GenericFileConsumer.java", "org.apache.camel.component.file.GenericFileOnCompletion.java", "org.apache.camel.spring.processor.idempotent.FileConsumerIdempotentTest.java", "org.apache.camel.component.file.stress.FileAsyncStressTest.java"], "label": 1, "es_results": []}, {"bug_id": 2649, "bug_title": "FactoryBeans for ProducerTemplate and ConsumerTemplate should be singleton to avoid", "bug_description": "You want the ProducerTemplate with the assigned id to be a singleton scoped instance, so its shared.\nCurrently it creates a new instance which it should not.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.spring.CamelProducerTemplateFactoryBean.java", "org.apache.camel.spring.remoting.CamelProxyFactoryBean.java", "org.apache.camel.spring.CamelConsumerTemplateFactoryBean.java", "org.apache.camel.spring.config.ConsumerTemplateAlreadyExistTest.java", "org.apache.camel.spring.config.ProducerTemplateAlreadyExistTest.java"], "label": 1, "es_results": []}, {"bug_id": 2708, "bug_title": "File name lost when it starts with the same characters as the relative directory on the endpoint", "bug_description": "When polling file from a directory using a relative file URI, the file name gets lost when it starts with the same characters as the directory name.\nE.g. a directory &apos;orders&apos; containing &apos;orders-1719.xml&apos; and &apos;orders-1819.xml&apos;\n\nfrom(\"file:orders\").process(new Processor() {\n  public void process(Exchange exchange) {\n    // there&apos;s no file name on the message here (exchange.getIn().getHeader(Exchange.FILE_NAME) returns null)\n  }\n});\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.file.stress.FileAsyncStressReadLockNoneTest.java", "org.apache.camel.component.file.stress.FileAsyncStressTest.java", "org.apache.camel.component.file.stress.FileAsyncStressReadLockRenameTest.java", "org.apache.camel.component.file.GenericFileEndpoint.java", "org.apache.camel.component.file.stress.FileAsyncStressReadLockLockFileTest.java"], "label": 1, "es_results": []}, {"bug_id": 2732, "bug_title": "SMPP component should set the final status header for delivery notifications", "bug_description": "The camel-smpp comonent should set the final status that is provided by the delivery receipt from the SMSC.\nCurrently the status header is not set, but can easily be added by making a call to smscDeliveryReceipt.getFinalStatus() and setting the header CamelSmppStatus on the camel message (as documented on the site http://camel.apache.org/smpp.html.\nPatch provided. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.smpp.SmppBindingTest.java", "org.apache.camel.component.smpp.SmppBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2636, "bug_title": "IOException: Bad file descriptor and FileNotFoundException", "bug_description": "When I try to stream BINARY (pdf) file using camel-http I get the java.io.IOException: Bad file descriptor\nThe pdf isn&apos;t recieved succesfully by reciever (0kb)\nThis seems to be caused by a bug in java (on linux systems), closing inputstream twice causes problems. It seemed to me this is exactly what is happening, see also link:\nhttp://256.com/gray/docs/misc/java_bad_file_descriptor_close_bug.shtml\nI fixed this by (checking out apache camel-core and camel-http 2.2.0):\nIn FileInputStreamCache.java:\nIn method close() wrapped getInputStream().close() in if:\nif (stream != null && stream instanceof FileInputStream && ((FileInputStream) stream).getChannel().isOpen()) {\ngetInputStream().close() ;\n}\nIn method reset() also:\nif (stream != null && stream instanceof FileInputStream && ((FileInputStream) stream).getChannel().isOpen()) {\ngetInputStream().close() ;\n}\nSecond I needed to fix a filenotfoundexception, the tempfile created by camel was deleted to early.\nI changed CachedOutputStream.java\n\nReimplemented constructor:\npublic CachedOutputStream(Exchange exchange) {\n        String hold = exchange.getContext().getProperties().get(THRESHOLD);\n        String dir = exchange.getContext().getProperties().get(TEMP_DIR);\n        if (hold != null) \n{\n            this.threshold = exchange.getContext().getTypeConverter().convertTo(Long.class, hold);\n        }\n        if (dir != null) \n{\n            this.outputDir = exchange.getContext().getTypeConverter().convertTo(File.class, dir);\n        }\n\n        // add on completion so we can cleanup after the exchange is done such\n        // as deleting temporary files\n        exchange.addOnCompletion(new SynchronizationAdapter() {\n            @Override\n            public void onDone(Exchange exchange) {\n                try {\n                    // close the stream and FileInputStreamCache\n                    // close();\n                    // for (FileInputStreamCache cache : fileInputStreamCaches)\n                    // \n{\n                    // cache.close();\n                    // }\n                    // cleanup temporary file\n                    if (tempFile != null) {\n                        System.err.println(\"####################################################\");\n                        System.err.println(\"DISABLED tempFile.delete:89\");\n                        System.err.println(\"####################################################\");\n                        // boolean deleted = tempFile.delete();\n                        // if (!deleted) \n{\n                        // LOG.warn(\"Cannot delete temporary cache file: \" +\n                        // tempFile);\n                        // }\n else if (LOG.isTraceEnabled()) \n{\n                        // LOG.trace(\"Deleted temporary cache file: \" +\n                        // tempFile);\n                        // }\n                        tempFile = null;\n                    }\n                } catch (Exception e) \n{\n                    LOG.warn(\"Error deleting temporary cache file: \" + tempFile, e);\n                }\n            }\n            @Override\n            public String toString() \n{\n                return \"OnCompletion[CachedOutputStream]\";\n            }\n        });\n    }\nReimplemented close():\npublic void close() throws IOException {\n        System.err.println(\"####################################################\");\n        System.err.println(\"outputStream.close:119 -> delete tempFile\");\n        System.err.println(\"####################################################\");\n        new Exception().printStackTrace();\n        currentStream.close();\n        boolean deleted = tempFile.delete();\n        if (!deleted) \n{\n            LOG.warn(\"Cannot delete temporary cache file: \" + tempFile);\n        }\n else if (LOG.isTraceEnabled()) \n{\n            LOG.trace(\"Deleted temporary cache file: \" + tempFile);\n        }\n    }", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.converter.stream.CachedOutputStreamTest.java", "org.apache.camel.converter.stream.CachedOutputStream.java", "org.apache.camel.converter.stream.FileInputStreamCache.java"], "label": 1, "es_results": []}, {"bug_id": 2742, "bug_title": "camel-jms - Sending to WebSphereMQ must use specific setBooleanProperty methods to set JMS properties", "bug_description": "This code in JMSBinding\n\n                // must encode to safe JMS header name before setting property on jmsMessage\n                String key = jmsKeyFormatStrategy.encodeKey(headerName);\n                jmsMessage.setObjectProperty(key, value);\n\n\nShould detect the value type and use the jmsMessage.setBooleanProperty() and so on. Otherwise IBM thrown an exception.\nSee nabble\nhttp://old.nabble.com/jmsbinding-problem-ts28620489.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.3.0", "fixed_files": ["org.apache.camel.component.jms.JmsMessageHelper.java", "org.apache.camel.component.jms.JmsBinding.java"], "label": 1, "es_results": []}, {"bug_id": 2751, "bug_title": "Timer Component is not Restartable", "bug_description": "When the TimerComponent is stopped, it cancels all of the Timer instances it has created, which is good. However, TimerEndpoint keeps a local reference to the Timer instance, so if the TimerComponent is restarted, the TimerEndpoint will throw an exception as its timer is no longer usable as it has been canceled.\nThis patch provides a unit test, TimerRestartTest, and an update to TimerComponent that fixes this issue. The TimerComponent fix is to keep a list of all TimerEndpoints that have a reference to a Timer instance created by the TimerComponent. When TimerComponent.doStop is called, those TimerEndpoint references to the now canceled Timer instances are cleared.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.timer.TimerEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2750, "bug_title": "org.apache.camel.component.bean.BeanInfo not working properly with mocked Interfaces", "bug_description": "The BeanInfo-Class shows the same behaviour as ClassUtils-Class from Spring 3.0.x (see Spring Issue 7066, https://jira.springsource.org/browse/SPR-7066).\nWhen working with mock objects created by mockito using an interface the getSuperclass()-Method returns java.lang.Object and the method to called on the mock can&apos;t be found. \nWhile looking around for a solution I found the path applied in Spring (see https://fisheye.springsource.org/browse/spring-framework/trunk/org.springframework.core/src/main/java/org/springframework/util/ClassUtils.java?content-type=text/vnd.viewcvs-markup&r1=3227&r2=3228).\nIn method \npublic static Class<?> getUserClass(Class<?> clazz) \nthe code\nreturn (clazz != null && clazz.getName().contains(CGLIB_CLASS_SEPARATOR) ?  clazz.getSuperclass() : clazz);\nis replaced by \nif (clazz != null && clazz.getName().contains(CGLIB_CLASS_SEPARATOR)) {\n    Class<?> superClass = clazz.getSuperclass();\n    if (superClass != null && !Object.class.equals(superClass)) \n{\n         return superClass;\n    }\n}\nreturn clazz;\nWhile waiting for a fix in BeanInfo class a workaround is to mock the concrete class not the interface, but this makes the test code more complicated if several classes implement the same interface.\nA Wor", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java"], "label": 1, "es_results": []}, {"bug_id": 2755, "bug_title": "VM endpoints with same name do not communicate if args do not match", "bug_description": "If you send to a VM endpoint from one route and consume from the same endpoint in another route, but include an argument on only one of the routes, Camel sees them as two different routes.  Therefore, the messages are never consumed.  For example:\n<camelContext id=\"sendNotifyContext\"\n\txmlns=\"http://camel.apache.org/schema/spring\"\n\terrorHandlerRef=\"errorHandler\">\n      <route id=\"sendToNotify\">\n            <from uri=\"...\" />\n            ....\n<to uri=\"vm:myNotify\" />\n      </route>\n</camelContext>\n<camelContext id=\"receiveNotifyContext\"\n\txmlns=\"http://camel.apache.org/schema/spring\"\n\terrorHandlerRef=\"errorHandler\">\n      <route id=\"receiveNotify\">\n<from uri=\"vm:myNotify?size=2500\" />\n            ....\n            <to uri=\"...\" />\n      </route>\n</camelContext>\nThe producer appears to send to vm:myNotify while the consumer is listening to a separate endpoint named vm:myNotify?size=2500, so the messages build up and are never received.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.seda.SedaComponent.java", "org.apache.camel.component.vm.VmComponent.java"], "label": 1, "es_results": []}, {"bug_id": 2881, "bug_title": "jms consumer should handle markRollbackOnly", "bug_description": "If using markRollbackOnly to dente the route should rollback the Spring DMLC does not rollback despite its status has been told so. We are forced to thrown runtime exceptions.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.jms.tx.JmsToJmsTransactedTest.java", "org.apache.camel.component.jms.EndpointMessageListener.java", "org.apache.camel.spring.spi.TransactionErrorHandler.java"], "label": 1, "es_results": []}, {"bug_id": 3113, "bug_title": "@QueryParam does not work for CxfBeans", "bug_description": "Charle report an issue[1], after tracing the code I found the DefaultCxfBeanBinding doesn&apos;t put the http query string into the cxf message.\nSo the @QueryParam will not take effect on the resource beans.\n[1]http://camel.465427.n5.nabble.com/camel-cxfbean-JAX-Rs-QueryParam-td2827252.html#a2827252", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.cxf.spring.CxfEndpointBean.java", "org.apache.camel.component.cxf.feature.AbstractDataFormatFeature.java", "org.apache.camel.component.cxf.cxfbean.DefaultCxfBeanBinding.java", "org.apache.camel.component.cxf.jaxrs.testbean.CustomerService.java", "org.apache.camel.component.cxf.cxfbean.CxfBeanTest.java"], "label": 1, "es_results": []}, {"bug_id": 3349, "bug_title": "Race condition found in CxfRsEndpoint while getting the endpoint binding under load and performing sync and async invocation", "bug_description": "The CxfRsEndpoint&apos;s getBinding method is not thread safe. At a customer site, I ran into an issue at startup if 2 threads raced to perform sync and async invocation, the code for getBinding (given below) would react in the following way.\n\nThread 1 would proceed to create a binding object\nThread 2 would mean while still find the  binding to be null and proceed to create a new binding\nMeanwhile thread one would have its binding and set the Atomic boolean for binding initialized and proceed to set the HeaderStrategy.\nThread 2 meanwhile would overwrite the original binding object and find that Atomic boolean already set and would have no way to associate a HeaderFilterStrategy object since the flag is up.\nIn the absence of a HeaderFilterStrategy, copying of ProtocolHeaders etc will throw exceptions on every following request/invocation.\n\n--------------------------------------------------\n    public CxfRsBinding getBinding() {\n        if (binding == null) {\n            binding = new DefaultCxfRsBinding();\n            if (LOG.isDebugEnabled()) \n{\n                LOG.debug(\"Create default CXF Binding \" + binding);\n            }\n        } \n        if (!bindingInitialized.getAndSet(true) && binding instanceof HeaderFilterStrategyAware) \n{\n            ((HeaderFilterStrategyAware)binding).setHeaderFilterStrategy(getHeaderFilterStrategy());\n        }\n\n        return binding;\n    }\n------------------------------------------------", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.2.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.CxfRsEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2758, "bug_title": "OnCompletion - Should use pipes and filters to ensure IN is OUT from last step", "bug_description": "OnCompletion will route the Exchange directly as is which means if you have set an OUT the first step in the onCompletion route may not use this OUT but the IN instead.\nAnd also add option useOriginalBody so you can do work based on the original input instead.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.processor.OnCompletionProcessor.java", "org.apache.camel.model.OnCompletionDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 2760, "bug_title": "@Consume should run in an unit of work", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.impl.CamelPostProcessorHelperTest.java", "org.apache.camel.impl.CamelPostProcessorHelper.java"], "label": 1, "es_results": []}, {"bug_id": 2778, "bug_title": "Escaped characters in http4 URLs do not work", "bug_description": "There is a bug in http4 HttpProducer that is unescaping URL query and path components too early. Patch fix attached.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.http.helper.HttpProducerHelper.java", "org.apache.camel.component.jetty.HttpClientRouteTest.java", "org.apache.camel.component.http.HttpProducer.java", "org.apache.camel.component.http4.HttpProducer.java", "org.apache.camel.component.http4.helper.HttpProducerHelper.java", "org.apache.camel.component.http4.HttpPathTest.java", "org.apache.camel.component.http4.HttpQueryTest.java", "org.apache.camel.component.http4.handler.BasicValidationHandler.java"], "label": 1, "es_results": []}, {"bug_id": 2772, "bug_title": "camel-jetty cannot deal with multiform data rightly", "bug_description": "You can&apos;t get the inputStream from the attachment when camel-jetty handle the request of MultiPartForm.\nHere is the mail thread[1] which discusses about it.\n[1] http://old.nabble.com/Unsupported-data-type-exception-with-Jetty-component-tp28730373p28731758.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.http4.DefaultHttpBinding.java", "org.apache.camel.component.http.DefaultHttpBinding.java", "org.apache.camel.component.jetty.MultiPartFormTest.java"], "label": 1, "es_results": []}, {"bug_id": 2642, "bug_title": "Inconsistency between IntrospectionSupport.getProperties() and IntrospectionSupport.getProperty()", "bug_description": "IntrospectionSupport.getProperties() and IntrospectionSupport.getProperty() work in an inconsistency way:\n\nExampleBean bean = new ExampleBean();\nDate date = new Date(0);\nbean.setDate(date);\n\nassertSame(date, IntrospectionSupport.getProperty(bean, \"date\")); // succeed\n\nMap<String, Object> map = new HashMap<String, Object>();\nIntrospectionSupport.getProperties(bean, map, null);\nassertSame(date, map.get(\"date\")); // fails\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.bean.BeanInfo.java", "org.apache.camel.util.IntrospectionSupport.java", "org.apache.camel.util.IntrospectionSupportTest.java"], "label": 1, "es_results": []}, {"bug_id": 2798, "bug_title": "Aggregation raises NullPointerException if last file in batch is not to be aggregated.", "bug_description": "When trying to aggregate files from a directory, it seems that camel-core raises a NullPointerException if the last file of the batch is a single file. That is it doesn&apos;t match the correlationExpression with any other files and therefore should just pass through. If such a file is the first file or in the middle of the files (alphabetically) this issue is not present.\nSee:\nhttp://old.nabble.com/Aggregator-problem-with-files-(Camel-2.3)-td28778641.html#a28780522", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.processor.aggregate.AggregateProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 2773, "bug_title": "Bindy - No @Section causes a null key being generated which causes a NumberFormatException", "bug_description": "See nabble\nhttp://old.nabble.com/Bindy-CSV-not-Marshaling-ts28719942.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.dataformat.bindy.BindyKeyValuePairFactory.java", "org.apache.camel.dataformat.bindy.CommonBindyTest.java", "org.apache.camel.dataformat.bindy.BindyAbstractFactory.java", "org.apache.camel.dataformat.bindy.fix.BindySimpleKeyValuePairMarshallDslTest.java", "org.apache.camel.dataformat.bindy.csv.BindySimpleCsvUnmarshallBadIntegerTest.java"], "label": 1, "es_results": []}, {"bug_id": 2806, "bug_title": "camel-jetty  cannot config the temp directory for the multi part form support rightly", "bug_description": "You will get a ClassCastException if you try to set the temp directory from the camel property.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.jetty.MultiPartFormTest.java", "org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 2821, "bug_title": "camel-ftp - SFTP in fileExists should handle exception being thrown with id code stating no such file", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/camel-ftp-cannot-create-directory-using-sftp-tp479092p479092.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.file.remote.SftpOperations.java"], "label": 1, "es_results": []}, {"bug_id": 2805, "bug_title": "It is impossible to put # sign in SQL statement in camel-sql", "bug_description": "SqlProducer implementation turns all # characters into ? making it impossible to have # in the SQL statement.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.sql.SqlRouteTest.java", "org.apache.camel.component.sql.SqlComponent.java"], "label": 1, "es_results": []}, {"bug_id": 2829, "bug_title": "\"Unconnected sockets not implemented\" exception in camel-ftp when using ftps", "bug_description": "When using ftps with a secure data channel, camel-ftp cannot recover from a lost connection. This is due to a bug/flaw in the SFTPClient class in commons-net. Once a secure data channel has been established, SFTPClient replaces the connection factory with one that does not provide support for creating \"unconnected sockets\".\nWhile waiting for a fix from the commons-net team (should they chosse to create one), a work-around in camel-ftp is to always create a new instance of SFTPClient for every connect attempt.\nAttached are patches containing an attempt to implement the mentioned work-around. The classes, FtpsOperations, FtpsEndpoint and FtpOperations are involved.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.file.remote.RemoteFileConsumer.java", "org.apache.camel.component.file.GenericFileProducer.java", "org.apache.camel.component.file.GenericFileConsumer.java", "org.apache.camel.component.file.GenericFileConverter.java", "org.apache.camel.component.file.remote.RemoteFileProducer.java", "org.apache.camel.component.file.remote.RemoteFileEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2843, "bug_title": "camel-groovy - setting header causes exchange to lose message details", "bug_description": "See SMX4-417\nThis is the ticket to fix this issue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.util.ExchangeHelper.java", "org.apache.camel.util.ExchangeHelperTest.java"], "label": 1, "es_results": []}, {"bug_id": 2851, "bug_title": "typo: ManagedBrowsableEndpoint qeue should be queue", "bug_description": "\n    @ManagedOperation(description = \"Current number of Exchanges in Queue\")\n    public long qeueSize() {\n        return endpoint.getExchanges().size();\n    }\n\n\nShould be \n\n    @ManagedOperation(description = \"Current number of Exchanges in Queue\")\n    public long queueSize() {\n        return endpoint.getExchanges().size();\n    }\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.management.ManagedBrowseableEndpointTest.java", "org.apache.camel.management.mbean.ManagedBrowsableEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2858, "bug_title": "useOriginalBody() still appears in fluent API; should be useOriginalMessage()", "bug_description": "The org.apache.camel.model.OnExceptionDefinition class defines useOriginalBody() in the fluent API. This should actually be useOriginalMessage(), in accordance with CAMEL-1820.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.model.OnExceptionDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 2893, "bug_title": "(FromDefintion|ToDefinition).getUriOrRef tend to return null when they should return a URI", "bug_description": "looks like a simple logic bug to me. Have a fix locally - just checking it works...", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.model.FromDefinition.java", "org.apache.camel.view.GraphSupport.java", "org.apache.camel.model.SendDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 2897, "bug_title": "Splitting file using tokenizer should close the Scanner to avoid files not being able to be moved thereafter", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.processor.Splitter.java", "org.apache.camel.issues.FileSplitStreamingWithChoiceTest.java"], "label": 1, "es_results": []}, {"bug_id": 2899, "bug_title": "out of heap space if remote FTP site has too many files to pick up", "bug_description": "2010-07-02 11:38:07,439 FATAL [org.apache.camel.component.file.remote.FtpConsumer:CamelThread 10]   - <Consumer Consumer[my_ftp_URI_here caused by: Java heap space>\njava.lang.OutOfMemoryError: Java heap space\nMy remote FTP server has ~60k 100 byte files, and the camel endpoint consumer falls over and doesn&apos;t start again.  I can use JMX to stop/start the consumer, (it still has status \"started\") and it will log in to the remote server again, but then fall over with the out of heap space.\nI can work around this by increasing the heap, or by moving some of the files aside,  but I don&apos;t think camel should care how many files there are, or at least, I think it should deal with it more gracefully.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.file.FileConsumer.java", "org.apache.camel.component.file.remote.SftpConsumer.java", "org.apache.camel.component.file.remote.FtpConsumer.java", "org.apache.camel.component.file.GenericFileConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 2909, "bug_title": "Oracle AQ does not support the JMSReplyTo property and therefore throws a \"JMS-102: Feature not supported\" exception when the JMS provider tries to get it.", "bug_description": "Oracle AQ does not support the JMSReplyTo property and therefore throws a \"JMS-102: Feature not supported\" exception when the JMS provider tries to get it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.jms.JmsBinding.java", "org.apache.camel.component.jms.EndpointMessageListener.java"], "label": 1, "es_results": []}, {"bug_id": 2912, "bug_title": "SFTP throws ResolveEndpointFailedException when \"ftpClient.connectionTimeout\" option is provided", "bug_description": "Currently, SFTP throws an ResolveEndpointFailed exception if the option \"ftpClient.connectionTimeout\" is provided as a part of the URI. This could be fixed by calling the Session.connect(int timeout) method of JSCH. Some investigation reveals that this method eventually calls Socket.setSoTimeout(int timeout), which implies that the time unit of the timeout is milliseconds.\nhttp://grepcode.com/file/repo1.maven.org/maven2/com.jcraft/jsch/0.1.42/com/jcraft/jsch/Session.java#Session.connect%28int%29\nExample:\nsftp://user@host/dir?password=secret&ftpClient.connectionTimeout=30000&disconnect=true&passiveMode=true\nResolveEndpointFailedException:\n[...]\nThere are 1 parameters that couldn&apos;t be set on the endpoint. Check the uri if the parameters are spelt correctly and that they are properties of the endpoint. Unknown parameters=[\n{ftpClient.connectionTimeout=30000}\n] ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.file.remote.RemoteFileConfiguration.java", "org.apache.camel.component.file.remote.SftpOperations.java", "org.apache.camel.component.file.remote.FtpOperations.java", "org.apache.camel.component.file.remote.SftpEndpoint.java", "org.apache.camel.component.file.remote.FtpEndpoint.java", "org.apache.camel.component.file.remote.RemoteFileEndpoint.java", "org.apache.camel.component.file.remote.FtpsEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 2935, "bug_title": "Broken failure reporting via DefaultProucerTemplate.asyncCallback and Synchronization.onFailure", "bug_description": "The recent change in ProducerCache.send\nProducerCache.java\n    public void send(Endpoint endpoint, Exchange exchange) {\n        try {\n            sendExchange(endpoint, null, null, exchange);\n            // RECENT CHANGE HERE:\n            // ensure that CamelExecutionException is always thrown\n            if (exchange.getException() != null) {\n                exchange.setException(wrapCamelExecutionException(exchange, exchange.getException()));\n            }\n        } catch (Exception e) {\n            throw wrapCamelExecutionException(exchange, e);\n        }\n    }\n\n\nthat throws a CamelExecutionException if exchange.getException is not null, makes it impossible for DefaultProducerTemplate.asyncCallback to report failures (other than fault messages) asynchronously via Synchronization.onFailure\nDefaultProducerTemplate.java\n    public Future<Exchange> asyncCallback(final Endpoint endpoint, final Exchange exchange, final Synchronization onCompletion) {\n        Callable<Exchange> task = new Callable<Exchange>() {\n            public Exchange call() throws Exception {\n\n                // FIXME: exception is thrown in Camel 2.4 where a normal return with answer.getException != null was done in Camel 2.3\n                Exchange answer = send(endpoint, exchange);\n\n                if (answer.isFailed()) {\n                    onCompletion.onFailure(answer);\n                } else {\n                    // ...\n                }\n                return answer;\n            }\n        };\n        // ...\n    }\n\n\nThis was working in Camel 2.3 (but unfortunately there wasn&apos;t any test case for it). I attached a patch for DefaultProducerTemplateAsyncTest that demonstrates the problem. I didn&apos;t commit a fix yet because I&apos;m unsure at the moment about the best way to fix that. Of course I tried a naive fix in the DefaultProducerTemplate.asyncCallback methods which causes the test (in the patch) to pass but I&apos;d like to hear other opinions before I continue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.impl.DefaultProducerTemplateAsyncTest.java", "org.apache.camel.impl.DefaultConsumerTemplateTest.java", "org.apache.camel.ProducerTemplate.java", "org.apache.camel.impl.DefaultProducerTemplateTest.java", "org.apache.camel.component.jetty.HttpAuthMethodPriorityTest.java", "org.apache.camel.impl.ProducerCache.java", "org.apache.camel.processor.BeanInvocationThrowsExceptionTest.java", "org.apache.camel.impl.DefaultProducerTemplate.java", "org.apache.camel.impl.DefaultMessage.java", "org.apache.camel.issues.Issue3Test.java"], "label": 1, "es_results": []}, {"bug_id": 2937, "bug_title": "StreamProducer does not close stream in camel-stream", "bug_description": "Since the stream could be System.out or System.err, the producer does not close the stream ever. It should close all streams with the noted exceptions.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.4.0", "fixed_files": ["org.apache.camel.component.printer.PrinterPrintTest.java", "org.apache.camel.component.stream.StreamConsumer.java", "org.apache.camel.component.stream.StreamProducer.java"], "label": 1, "es_results": []}, {"bug_id": 2952, "bug_title": "camel-ftp - Support polling from MVS file system", "bug_description": "The MVS file system requires to change directory to starting path and use CD to traverse file path instead of using listFile(path) as currently done in camel-ftp.\nThis means we should walk the path using code like:\n0. remember path\n1. cd path\n2. list files()\n3. loop files\n4. if dir then goto 1\n5. if file add file\n6. when done cd back to \"remember path\"\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.file.remote.FtpOperations.java", "org.apache.camel.component.file.remote.FromFtpRecursiveNoopTest.java", "org.apache.camel.component.file.remote.FtpConsumerMultipleDirectoriesTest.java", "org.apache.camel.component.file.remote.FtpConsumer.java", "org.apache.camel.component.file.remote.SftpOperations.java"], "label": 1, "es_results": []}, {"bug_id": 2995, "bug_title": "charset parser should cater for quotes, both single and double quotes", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/issue-with-encoding-when-using-HTTP-component-td2227887.html#a2227887\nI bet many systems may report charset in different ways such as\n\nContent-Type:text/xml;charset=\"utf-8\" \nContent-Type:text/xml;charset=&apos;utf-8&apos; \nContent-Type:text/xml;charset=utf-8 \n\n\nWe should ensure that we support all ways of setting this. And there may also be spaces between so we should trim and whatnot.\nThe code in 2.4 may have been improved. Just creating a ticket to be sure.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.mina.MinaProducer.java", "org.apache.camel.component.http.HttpProducer.java", "org.apache.camel.converter.IOConverter.java", "org.apache.camel.converter.IOConverterTest.java", "org.apache.camel.dataformat.xstream.XStreamDataFormat.java", "org.apache.camel.component.netty.handlers.ServerChannelHandler.java", "org.apache.camel.component.netty.NettyProducer.java", "org.apache.camel.processor.ConvertBodyProcessor.java", "org.apache.camel.component.mina.MinaConsumer.java", "org.apache.camel.component.http4.HttpProducer.java", "org.apache.camel.component.mail.MailBinding.java"], "label": 1, "es_results": []}, {"bug_id": 3060, "bug_title": "Out of Heap memory issues with Camel-File processing large files", "bug_description": "Camel-File component throws heap memory issue when processing csv file which is about 45MB with 218k Lines. \nhttp://camel.465427.n5.nabble.com/Java-heap-space-issue-with-reading-large-CSV-file-tt2638903.html#a2638903", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.file.GenericFileMessage.java"], "label": 1, "es_results": []}, {"bug_id": 3188, "bug_title": "Concurrent consumers on seda endpoint can cause content routing to mismatch", "bug_description": "When consuming concurrently from a seda endpoint, when the route contains a content router based on the header, it will randomly route through the wrong choice.\nIn my specific case, I was consuming from an activemq queue, which would receive messages with a header that would then determine which route it would follow. It would randomly send messages down the wrong path. When I turned on tracing, it would behave itself. It also behaved itself when I limited it to only a single consumer. I was, however, able to duplicate it with the unit test below. Due to the concurrency issue, the test can occasionally pass, but run it a couple times and it should fail. It&apos;ll either receive 2 messages when it should have only gotten 1, or it will get no messages when it should have gotten 1.\nConcurrencyTest.java\nimport org.apache.camel.EndpointInject;\nimport org.apache.camel.Produce;\nimport org.apache.camel.ProducerTemplate;\nimport org.apache.camel.builder.RouteBuilder;\nimport org.apache.camel.component.mock.MockEndpoint;\nimport org.apache.camel.test.CamelTestSupport;\n\npublic class ConcurrencyTest extends CamelTestSupport {\n\n\t@EndpointInject(uri = \"mock:result\")\n\tprotected MockEndpoint resultEndpoint;\n\t\n\t@EndpointInject(uri = \"mock:otherResult\")\n\tprotected MockEndpoint otherResultEndpoint;\n\n\t@Produce(uri = \"seda:start\")\n\tprotected ProducerTemplate template;\n\n\tpublic void testSendMatchingMessage() throws Exception {\n\t\tString expectedBody = \"<matched/>\";\n\t\t\n\t\tresultEndpoint.expectedBodiesReceived(expectedBody);\n\t\totherResultEndpoint.expectedBodiesReceived(expectedBody);\n\n\t\ttemplate.sendBodyAndHeader(expectedBody, \"myDirection\", \"send\");\n\t\ttemplate.sendBodyAndHeader(expectedBody, \"myDirection\", \"received\");\n\n\t\tresultEndpoint.assertIsSatisfied();\n\t}\n\n\t@Override\n    protected RouteBuilder createRouteBuilder() {\n        return new RouteBuilder() {\n            public void configure() {\n                from(\"seda:start?concurrentConsumers=10\")\n//                from(\"seda:start?concurrentConsumers=1\")\n                \t.choice()\n                \t\t.when(header(\"myDirection\").isEqualTo(\"send\")).to(\"mock:result\")\n                \t\t.when(header(\"myDirection\").isEqualTo(\"received\")).to(\"mock:otherResult\");\n            }\n        };\n    }\n}\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.3.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.util.ObjectHelper.java", "org.apache.camel.BinaryPredicate.java", "org.apache.camel.processor.ChoiceProcessor.java", "org.apache.camel.builder.BinaryPredicateSupport.java", "org.apache.camel.builder.PredicateBuilderTest.java"], "label": 1, "es_results": []}, {"bug_id": 2962, "bug_title": "camel-jms - disableReplyTo is not used in JmsProducer", "bug_description": "The JmsProducer does not check the disableReplyTo option when sending the message.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.jms.JmsProducer.java"], "label": 1, "es_results": []}, {"bug_id": 2979, "bug_title": "FtpComponent: If login fails and disconnect=true another connection is opened.", "bug_description": "In a route such as below a second connection to the ftp server is opened if the login fails.\n<route> \n        <from uri=\"ftp:localhost/inbox/?username=usr&password=pwd&disconnect=true&consumer.delay=60s&maximumReconnectAttempts=0\" /> \n        <to uri=\"file:test_data\" /> \n</route> \nFurther description: http://camel.465427.n5.nabble.com/FTP-Try-login-once-and-disconnect-if-failure-td1692660.html#a1692660\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.file.remote.RemoteFileEndpoint.java", "org.apache.camel.component.file.remote.RemoteFileConsumer.java", "org.apache.camel.component.file.GenericFileConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 2978, "bug_title": "java.net.SocketException: Too many open files with Apache Camel(Netty TCP) 2.4.0.", "bug_description": "I&apos;ve got a unit test that works fine with Apache Camel 2.3.0, but as soon as I upgraded to 2.4.0 it consistently started to fail.\nIt performs a number of concurrent requests using this url:\nnetty:tcp://localhost:2048?sync=true\nIn both the client and server side of the unit test.\nThere&apos;s also a sister test which does the same thing with Netty directly and that works in isolation, so it would appear something has been broken in the transition to 2.4.0.  Previously this code was also using a beta version of Netty, but even updating that specific dependency has made no difference.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.netty.ClientPipelineFactory.java", "org.apache.camel.component.netty.NettyEndpoint.java", "org.apache.camel.component.netty.NettyComponent.java"], "label": 1, "es_results": []}, {"bug_id": 3015, "bug_title": "MailConsumer - Use OnCompletion for commit/rollback", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Mail-component-velocity-and-NullpointerException-td2256742.html#a2259340\nThis ensure the mail message is kept and we can do commit/rollback without impact of mail message being lost during the routing", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.mail.MailConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 3008, "bug_title": "Starting CamelContext with autoStartup=false should startup JMX connector", "bug_description": "I recall some user saying something about JMX appears to not be loaded if he has autoStartup=false on the CamelContext.\nIt should only be the routes which are not started. The other stuff should start.\nYou may need to use JMX to start the routes there after ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.spring.config.CamelContextFactoryBeanTest.java", "org.apache.camel.spring.config.CamelContextAutoStartupTest.java", "org.apache.camel.impl.GracefulShutdownNoAutoStartOrderClashTest.java", "org.apache.camel.impl.DefaultCamelContextAutoStartupTest.java", "org.apache.camel.management.DefaultManagementAgent.java", "org.apache.camel.impl.RouteService.java", "org.apache.camel.processor.RouteServicesStartupOrderTest.java", "org.apache.camel.impl.ServiceSupport.java"], "label": 1, "es_results": []}, {"bug_id": 3007, "bug_title": "Adding route from XML should honor its autoStartup flag", "bug_description": "If a route is adding after CamelContext has been started, then it&apos;s autoStartup flag should be honored. In case the flag is autoStartup=false, the route should not be auto started.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.CamelContext.java", "org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.model.RouteDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 3036, "bug_title": "camel-web seems a bit borked viewing an endpoint in tomcat", "bug_description": "e.g. try \"mvn tomcat:run\" then try navigate to an endpoint to try send a message to it.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.web.model.EndpointLink.java", "org.apache.camel.web.resources.EndpointsResource.java"], "label": 1, "es_results": []}, {"bug_id": 3050, "bug_title": "RouteBuilderRef should work out of the box with Spring 3 and dependency injection", "bug_description": "Spring 3 changed internally how dependency resolution works. Its now worse as we have to play tricks in the Camel namespace handler to tell Camel the various pieces you can ref, should depend on Camel. Otherwise the dependency resolution in Spring is not working properly.\nThis used to work like a charm in Spring 2.5.6.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.spring.handler.CamelNamespaceHandler.java"], "label": 1, "es_results": []}, {"bug_id": 3049, "bug_title": "Using custom global interceptor can cause routes to not entirely warmup", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Camel-2-4-InterceptStrategy-error-tp2473088p2473088.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.processor.InterceptorToAsyncProcessorBridge.java"], "label": 1, "es_results": []}, {"bug_id": 3047, "bug_title": "JettyHttpComponent.doStop() shuts down all servers in the VM, not just those associated with the component", "bug_description": "We are running several bundles in Karaf with separate Camel contexts, each of which uses the camel-jetty component to expose services over HTTP.  Each bundle has an assigned port and may listen on multiple URIs.  We noticed that when we updated or shut down one of these bundles, all of the Jetty servers in the other bundles would stop listening on their respective ports.\nThe problem is that the map of ConnectorRef objects in JettyHttpComponent is static, and therefore shared across the entire VM.  Changing this from static to an instance variable fixed the issue for us.  ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.jetty.SpringJettyNoConnectionTest.java", "org.apache.camel.itest.osgi.OSGiIntegrationTestSupport.java", "org.apache.camel.component.jetty.SpringJettyNoConnectionRedeliveryTest.java", "org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 3069, "bug_title": "JMX statistics is incomplete for processors", "bug_description": "The statistics for routes is correct. However for processors the stats is wrong. For example ExchangesCompleted appears to be doubled.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.model.ProcessorDefinition.java", "org.apache.camel.management.mbean.ManagedPerformanceCounter.java"], "label": 1, "es_results": []}, {"bug_id": 3077, "bug_title": "Cache Component needs to check for null values during GET operations", "bug_description": "EhCache often has a bit of lag time when invalidating expired cache elements, first setting the Element value to null and then removing the key. If you are hitting a cache rapidly one often will run across a key that is present in the cache that still has a null element entry.\nThe logic for successfully key retrieval just needs to be slightly tweaked to check for null values.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.processor.cache.CacheBasedXPathReplacer.java", "org.apache.camel.component.cache.CacheProducer.java"], "label": 1, "es_results": []}, {"bug_id": 3078, "bug_title": "Cache Component configuration requires MemoryStoreEvictionPolicy instance, will not accept parameter as part of URI", "bug_description": "In Spring, if one attempts to specify a cache endpoint as:\n<endpoint id=\"myCache\" uri=\"cache://MyCache?memoryStoreEvictionPolicy=MemoryStoreEvictionPolicy.FIFO\"/>\nan exception will be thrown that the String \"MemoryStoreEvictionPolicy.FIFO\" was not a proper Java Object and no TypeConverter is available. This can be worked-around by manually creating a type converter that performs:\n    String policyName = evictionPolicy.replace(\"MemoryStoreEvictionPolicy.\", \"\");\n    return MemoryStoreEvictionPolicy.fromString(policyName);\nOr one could just try to create a new instance from reflection. Above way is a bit more manageable however, since EhCache is taking care of the conversion for you.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.cache.CacheConfiguration.java", "org.apache.camel.component.cache.CacheConsumerTest.java"], "label": 1, "es_results": []}, {"bug_id": 3079, "bug_title": "job rescheduling and clustering does not work properly ", "bug_description": "Currently stateful cron jobs are identified by their group name, job name and the cron expression. This prevents an easy rescheduling of cron jobs. For instance, stopping a camel context, rescheduling the cron job by editing the cron expressing and restart will end up in an exception. This will happen because the rescheduled job will be added as an additional job. The already existing job will produce an exception because the corresponding endpoint doesn&apos;t exist anymore. The previous solution deleting all triggers on shutdown doesn&apos;t work in a cluster scenario. \nI suggest to identify cron jobs only by their group and job name. On startup it will check if a trigger already exists and check if the cron expression has changed. If so it will be rescheduled.\nAlso the current explicit resuming of stateful jobs will produce an exception during startup, because the scheduler automatically finds and resumes stored triggers. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.quartz.QuartzEndpoint.java", "org.apache.camel.component.quartz.CamelJob.java", "org.apache.camel.component.quartz.StatefulCamelJob.java", "org.apache.camel.component.quartz.QuartzComponent.java"], "label": 1, "es_results": []}, {"bug_id": 3093, "bug_title": "camel-smpp shoud honor the configured encoding", "bug_description": "Currently camel-smpp doesn&apos;t honor the encoding option by creating the consumer/producer.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.smpp.SmppProducerTest.java", "org.apache.camel.component.smpp.SmppProducer.java", "org.apache.camel.component.smpp.SmppBinding.java", "org.apache.camel.component.smpp.SmppBindingTest.java", "org.apache.camel.component.smpp.SmppConfigurationTest.java", "org.apache.camel.component.smpp.SmppConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 3107, "bug_title": "SmppBinding set the destination address npi instead of the source address npi", "bug_description": "See discussion on the user@list", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.smpp.SmppBinding.java"], "label": 1, "es_results": []}, {"bug_id": 3118, "bug_title": "camel-spring causes wrong initialization-order of dependent beans", "bug_description": "Attached is a patch with a test that demonstrates the problem. The test uses a custom RouteBuilder (SampleIninitalizingRouteBuilder) and another bean (SampleIninitalizingBean) that both implement InitializingBean. When the beans&apos; afterPropertiesSet() methods are called, these beans add their names to a shared list. When the SampleIninitalizingRouteBuilder.configure() method is called then \"configured\" is added to the shared list.\n\npackage  org.apache.camel.spring.issues\n\n// imports omitted ...\n\npublic class SampleInitializingBean implements InitializingBean {\n    private String name;\n    private List<String> entries;\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public void setEntries(List<String> entries) {\n        this.entries = entries;\n    }\n\n    public void afterPropertiesSet() {\n        entries.add(name);\n    }\n}\n\npublic class SampleInitializingRouteBuilder extends RouteBuilder implements InitializingBean {\n    private String name;\n    private List<String> entries;\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public void setEntries(List<String> entries) {\n        this.entries = entries;\n    }\n\n    public void afterPropertiesSet() {\n        entries.add(name);\n    }\n\n    @Override\n    public void configure() throws Exception {\n        entries.add(\"configured\");\n    }\n}\n\n\nThese beans are wired as follows:\n\n    <bean id=\"entries1\" class=\"java.util.ArrayList\"/>\n\n    <bean id=\"sampleBean1\"\n          class=\"org.apache.camel.spring.issues.SampleInitializingBean\">\n        <property name=\"name\" value=\"test1a\"/>\n        <property name=\"entries\" ref=\"entries1\"/>\n    </bean>\n\n    <bean id=\"sampleRouteBuilder1\"\n          class=\"org.apache.camel.spring.issues.SampleInitializingRouteBuilder\" depends-on=\"sampleBean1\">\n        <property name=\"name\" value=\"test1b\"/>\n        <property name=\"entries\" ref=\"entries1\"/>\n    </bean>\n\n    <camelContext xmlns=\"http://camel.apache.org/schema/spring\">\n        <routeBuilder ref=\"sampleRouteBuilder1\"/>\n    </camelContext>\n\n\nNote the depends-on attribute on the sampleRouteBuilder1 bean: it should ensure that sampleBean1 is being initialized before sampleRouteBuilder1 and the camelContext. \nActual behaviour, however, is that the beans are initialized in the following order:\n\nsampleRouteBuilder1\ncamelContext\nsampleBean1\n\nwhich is definitely wrong. The shared list contains the entries\n\ntest1b\nconfigured\ntest1a\n\nThis differs from the expected order\n\ntest1a\ntest1b\nconfigured\n\nwhich cannot be observed. After some debugging, it seems the problem is related to the CamelBeanPostProcessor.postProcessBeforeInitialization() method. It does a lookup of the camelContext (i.e. applicationContext.getBean(camelId))) before the application context finished initialization of dependent beans. The problem is that this lookup already triggers a SampleInitializingRouteBuilder.configure() method call.\nEven worse, this behaviour depends on the declaration order of the beans in the application context XML file. When the camelContext bean is moved to the top, the bean initialization are done in the correct order.\nTo demonstrate that this is not a Spring-related problem, the attached test also contains another bean (SampleRouteBuilderContainer) that plays the role of the camelContext but does nothing else than calling configure() on the injected route builder within (afterPropertiesSet()). In this case, the bean initialization occur in the expected, correct order.\nI didn&apos;t find a solution to this problem so far and need to dig in further (hope to find some time next week for that). If any of the committers (who are more familiar with camel-spring than I am) have already an idea how to solve that, I appreciate any hints.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.spring.handler.CamelNamespaceHandler.java"], "label": 1, "es_results": []}, {"bug_id": 3108, "bug_title": "ConsumerTemplate return body should complete UoW beforehand", "bug_description": "Issue is discussed here: http://camel.465427.n5.nabble.com/ConsumerTemplate-not-finishing-td2642233.html#a2642233", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.Exchange.java", "org.apache.camel.impl.DefaultExchange.java", "org.apache.camel.impl.DefaultConsumerTemplate.java", "org.apache.camel.impl.DefaultUnitOfWork.java"], "label": 1, "es_results": []}, {"bug_id": 3128, "bug_title": "Using $ in endpoint uri causes thread name parser to fail", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/How-to-specify-route-to-folder-with-in-actual-name-tp2839895p2839895.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.impl.DefaultExecutorServiceStrategyTest.java", "org.apache.camel.util.concurrent.ExecutorServiceHelper.java"], "label": 1, "es_results": []}, {"bug_id": 3130, "bug_title": "Ref component causes consumer parameters to be cleared, such as delay and initialDelay", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/The-delay-option-seems-not-to-be-working-in-CAMEL-Java-DSL-mode-tp2840369p2840369.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.impl.DefaultComponent.java", "org.apache.camel.component.xslt.XsltOutputStringTest.java"], "label": 1, "es_results": []}, {"bug_id": 3124, "bug_title": "polling of feeds in FeedEntryPollingConsumer is broken.", "bug_description": "The FeedEntryPollingConsumer class implements the poll() method for the &apos;splitEntries&apos; mode of the RssEndpoint is broken.\nYou can think of two ways that polling feeds could work:\n1) A feed is created, then one item is processed, then the delay, then process another item. This way the feed is kept between calls to poll().\n2) A feed is created, then all the items are processed, the feed is cleared, and then the delay.\nBut the way it presently works:\nA feed is created, one items is processed, and the feed is cleared, then the delay, and again the feed is created and the next item is cleared.\nThis is clearly wrong. Feed entries can be missed, because the index of the next item to process is stored over polls but the list isn&apos;t. Also this creates a big network overhead when polling very active feeds such as twitter search...\nThis is easy to fix. In the below code:\n\npublic void poll() throws Exception {\n        Object feed = createFeed();\n        populateList(feed);   \n\n        while (hasNextEntry()) {\n            Object entry = list.get(entryIndex--);\n\n            boolean valid = true;\n            if (entryFilter != null) {\n                valid = entryFilter.isValidEntry(endpoint, feed, entry);\n            }\n            if (valid) {\n                Exchange exchange = endpoint.createExchange(feed, entry);\n                getProcessor().process(exchange);\n                // return and wait for the next poll to continue from last time (this consumer is stateful)\n                return;\n            }\n        }\n\n\nThe return (at line 56 of org.apache.camel.component.feed.FeedEntryPollingConsumer) should be deleted.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.feed.FeedEntryPollingConsumer.java", "org.apache.camel.component.atom.AtomEntryPollingConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 3114, "bug_title": "url encoding goes wrong in org.apache.camel.component.rss.RssComponent#afterConfiguration", "bug_description": "The method org.apache.camel.component.rss.RssComponent#afterConfiguration creates the Url that will be used to fetch the rss feed. We find that with some url&apos;s url encoding goes wrong.\nconsider this url: http://api.flickr.com/services/feeds/photos_public.gne?id=23353282@N05&tags=lowlands&lang=en-us&format=rss_200\nAfterConfiguration() calls org.apache.camel.util.URISupport#createRemainingURI This method first calls org.apache.camel.util.URISupport#createQueryString, which is a method that will iterate over a map of request parameters, escape each param name and value using java.net.URLEncoder#encode, and put them together with all the & and = stuff to form the query string.\nThen it calls org.apache.camel.util.URISupport#createURIWithQuery Which is a method that takes a URI (the base url) and the constructed query string, and simply creates a new URI with that, returning the toString() output from that.\nSo this is what the output of this procedure looks like: http://api.flickr.com/services/feeds/photos_public.gne?format=rss_200&id=23353282%2540N05&lang=en-us&tags=lowlands\n1 the @ sign was escaped by org.apache.camel.util.URISupport#createQueryString, creating a query string like: id=23353282%40N05&tags=lowlands&lang=en-us&format=rss_200 (which is good)\n2 the URI constructor then finds the % in %40 and escapes that again! creating a url like: http://api.flickr.com/services/feeds/photos_public.gne?format=rss_200&id=23353282%2540N05&lang=en-us&tags=lowlands \nWhich predictably fails...\nI did some tests with the URI constructor, and it seems it only escapes % chars, everything else is left alone.\nI attach a groovy script that demonstrates the problem\nregards,\nErnst Bunders", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.rss.RssComponent.java"], "label": 1, "es_results": []}, {"bug_id": 3136, "bug_title": "cxfbean creates another instance of the class instead of directly using the referenced bean", "bug_description": "... this makes it impossible to inject properties to the @WebService class\n<camelContext>\n        <route>\n            <from uri=\"....\" />\n            <to uri=\"cxfbean:handler\" />\n        </route>\n</camelContext>\n<bean id=\"handler\" class=\"a.b.c.Handler\">\n        <property name=\"prop1\" value=\"5\" />\n</bean>\nwhen \"handler\" is created by Spring, its &apos;prop1&apos; is set to &apos;5&apos;\nwhen the cxfbean:handler is triggered, the &apos;prop1&apos; is null, because CXF created another instance of Handler instead of using the one created by Spring", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.cxf.cxfbean.CxfBeanEndpoint.java", "org.apache.camel.wsdl_first.PersonImplWithWsdl.java"], "label": 1, "es_results": []}, {"bug_id": 3137, "bug_title": "ftp login anonymous should send empty string as password instead of null parameter", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/anonymous-FTP-login-fails-tp2846235p2846235.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.file.remote.FtpOperations.java"], "label": 1, "es_results": []}, {"bug_id": 3144, "bug_title": "camel-ftp: fileExist=Fail not honored when producer works in FTP root path", "bug_description": "when using a route with ftp producer such as :\n <to uri=\"ftp://user@host/?fileExist=Fail&fileName=test&password=mypass\" />\nThe route will not correctly fail if the fileName already exists.\nIt will work in any sub-directory:\n <to uri=\"ftp://user@host/mydir?fileExist=Fail&fileName=test&password=mypass\" />\nThe root because of the bug is an incorrect test in org.apache.camel.util.FileUtil.onlyPath() in component camel-core.\nThis method returns null when the parameter string is \"/\". It should return \"/\".\nThe attached patch fixes the issue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.file.GenericFile.java", "org.apache.camel.component.file.remote.RemoteFile.java", "org.apache.camel.component.file.remote.FtpOperations.java", "org.apache.camel.component.file.remote.SftpOperations.java", "org.apache.camel.util.FileUtilTest.java", "org.apache.camel.util.FileUtil.java"], "label": 1, "es_results": []}, {"bug_id": 3143, "bug_title": "OsgiDefaultCamelContext.getTypeConverterRegistry() returns null ", "bug_description": "The showed up when using dozer as shown in http://camel.apache.org/dozer-type-conversion.html, whose DozerTypeConverterLoader tries to get the type converter registry using: {{\n{TypeConverterRegistry registry = camelContext.getTypeConverterRegistry();}\n}}\nPlausible error:\nOsgiDefaultCamelContext.java\n   @Override\n    protected TypeConverter createTypeConverter() {\n        return new OsgiTypeConverter(bundleContext, getInjector());\n    }\n\n\nDefaultCamelContext.java\npublic TypeConverterRegistry getTypeConverterRegistry() {\n        if (typeConverterRegistry == null) {\n            // init type converter as its lazy\n            if (typeConverter == null) {\n                getTypeConverter();\n            }\n            // type converter is usually the default one that also is the registry\n            if (typeConverter instanceof DefaultTypeConverter) {\n                typeConverterRegistry = (DefaultTypeConverter) typeConverter;\n            }\n        }\n        return typeConverterRegistry;\n    }\n\n\nError:\ngetTypeConverter() returns an OsgiTypeConverter \nOsgiTypeConverter does not inherit from DefaultTypeConverter, thus the instanceof returns false\n=> null is returned\nSolution:\nLots of different ways to do this, and it&apos;s getting late here. In this case, maybe it&apos;s OsgiDefaultCamelContext&apos;s responsibility to also override getTypeConverterRegistry with something along the lines of:\n\n@Override\npublic TypeConverterRegistry getTypeConverterRegistry() {\n        if (typeConverterRegistry == null) {\n            // init type converter as its lazy\n            if (typeConverter == null) {\n                getTypeConverter();\n            }\n            // type converter is usually the default one that also is the registry\n            if (typeConverter instanceof OsgiDefaultTypeConverter) {\n                typeConverterRegistry = ((OsgiDefaultTypeConverter) typeConverter).getRegistry();\n            }\n        }\n        return typeConverterRegistry;\n    }\n\n\nWe&apos;ve employed an (ugly) workaround in a local version of DozerTypeConverterLoader.java:\n\n        TypeConverter typeConverter = camelContext.getTypeConverter();\n        DefaultTypeConverter registry = null;\n        if (typeConverter instanceof DefaultTypeConverter) {\n            registry = (DefaultTypeConverter)typeConverter;\n        } else if (typeConverter instanceof OsgiTypeConverter) {\n            OsgiTypeConverter osgiTypeConverter = (OsgiTypeConverter)typeConverter;\n            registry = osgiTypeConverter.getRegistry();\n        }\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.converter.dozer.model.Address.java", "org.apache.camel.converter.dozer.DozerTypeConverterTest.java", "org.apache.camel.core.osgi.OsgiTypeConverter.java", "org.apache.camel.converter.dozer.model.Customer.java", "org.apache.camel.converter.dozer.DozerTypeConverterLoader.java", "org.apache.camel.converter.dozer.SpringDozerTypeConverterTest.java"], "label": 1, "es_results": []}, {"bug_id": 3158, "bug_title": "PollingConsumerSupport.start() do not get called", "bug_description": "I have a subclass of PollingConsumerSupport and create it in a subclass of DefaultPollingEndpoint. The problem is that DefaultPollingEndpoint wraps PollingConsumer into DefaultScheduledPollConsumer and the latter does not call PollingConsumerSupport.start:\nDefaultScheduledPollConsumer.java:\n    @Override\n    protected void doStart() throws Exception \n{\n        pollingConsumer = getEndpoint().createPollingConsumer();\n        super.doStart();\n    }", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.impl.DefaultScheduledPollConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 3170, "bug_title": "camel-ftp - Setting password and username using UserInfo on uri does not work", "bug_description": "You should be able to set ftp endpoint uri as:\n\n\"ftp://scott@localhost:\" + getPort() + \"/deletefile?password=tiger&binary=false&delete=true\"\n\n\nAnd when using password and username in the userinfo part of the uri:\n\n\"ftp://tiger:scott@localhost:\" + getPort() + \"/deletefile?binary=false&delete=true\"\n\n\nThe latter didn&apos;t work", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.file.remote.RemoteFileConfiguration.java", "org.apache.camel.component.file.remote.FromFtpToBinarySampleTest.java", "org.apache.camel.component.file.remote.FromFtpDeleteFileTest.java"], "label": 1, "es_results": []}, {"bug_id": 3185, "bug_title": "restlet producer - Should set status code", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Restlet-HTTP-status-and-message-tp3047023p3047023.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.restlet.DefaultRestletBinding.java"], "label": 1, "es_results": []}, {"bug_id": 3187, "bug_title": "PublishEventNotifier - Should not emit events during startup/shutdown and not spawn new events when processing event", "bug_description": "When an event is being send to an endpoint using PublishEventNotifier it may create new events, and so forth. This causes a flood of events.\nAlso further complications occur during start/shutdown of Camel when you send events to routes, which are then being graceful shutdown. Therefore this event notifier should only publish if camel is fully started and running.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.Exchange.java", "org.apache.camel.util.EventHelper.java", "org.apache.camel.management.PublishEventNotifierTest.java", "org.apache.camel.management.PublishEventNotifier.java"], "label": 1, "es_results": []}, {"bug_id": 3199, "bug_title": "Allow : and , inside quoted names for addresses", "bug_description": "\"Snell, Tracy\" <tjs@juicelabs.com> breaks with the current address parsing.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.mail.MailMultipleRecipientsTest.java", "org.apache.camel.component.mail.MailBinding.java"], "label": 1, "es_results": []}, {"bug_id": 3198, "bug_title": " DefaultCamelContext throws NPE in getRoute(String id)", "bug_description": " DefaultCamelContext throws NPE in getRoute(String id) if no routes are deployed.\nThis happens because the \"route LinkedHashSet\" is not initialized.\nFix:\n don&apos;t do Lazy init of route  or add this to getRoute(String id):\n if (routes == null) \n{\n\troutes = new LinkedHashSet<Route>();\n }\n \n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.impl.DefaultCamelContextTest.java"], "label": 1, "es_results": []}, {"bug_id": 3203, "bug_title": "Quartz routes are not started if quartz component is referenced after context was started", "bug_description": "Quartz routes are not active if added after camel context was already started.\nHere is an elaborate description of the problem and a sample project that reproduces it: http://anydoby.com/jblog/en/java/1955\nSorry, no patch this time because I may not know enough about the internals of QuartzComponent, hesitate to offer anything but a boolean flag somewhere.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.quartz.QuartzComponent.java"], "label": 1, "es_results": []}, {"bug_id": 3216, "bug_title": "ScheduledPollConsumer should avoid runnable thread to die to ensure its kept being scheduled", "bug_description": "scheduled consumers such as file / ftp uses ScheduledPollConsumer to poll. If a throwable is thrown then the thread may die due it throws that to the JDK.\nWe should avoid this and ensure to catch all exceptions, otherwise the thread may die, and the JDK will not re-schedule a new thread.\nThis because it to stop polling.\nSee\nhttp://fusesource.com/forums/thread.jspa?threadID=2320&tstart=0", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.impl.ScheduledPollConsumer.java", "org.apache.camel.component.file.FileConsumerPollStrategyRollbackThrowExceptionTest.java"], "label": 1, "es_results": []}, {"bug_id": 3219, "bug_title": "Bindy should not trim separate in case end users use tab separators or the likes", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Big-problem-with-csv-tab-separator-file-and-bindy-data-format-tp3207520p3207520.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.dataformat.bindy.csv.BindySimpleCsvMarshallTest.java", "org.apache.camel.dataformat.bindy.csv.BindySimpleCsvUnmarshallPositionModifiedTest.java", "org.apache.camel.dataformat.bindy.csv.BindySimpleCsvMarshallPositionModifiedTest.java", "org.apache.camel.dataformat.bindy.csv.BindyPojoSimpleCsvMarshallTest.java", "org.apache.camel.dataformat.bindy.csv.BindyCsvDataFormat.java", "org.apache.camel.dataformat.bindy.csv.BindySimpleCsvUnmarshallTest.java", "org.apache.camel.dataformat.bindy.kvp.BindyKeyValuePairDataFormat.java", "org.apache.camel.dataformat.bindy.fixed.BindyFixedLengthDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 3223, "bug_title": "GenericFileProducer.writeFile method creates instance of un-used InputStream", "bug_description": "org.apache.camel.component.file.GenericFileProducer\nMethod: writeFile\nThis method has the following statement:\nInputStream payload = exchange.getIn().getBody(InputStream.class);\nThis internally results in calling a TypeConverter to convert an object into InputStream type. However this InputStream has not been used and is eventually closed in the finally block. \nIn the same method calling method storeFile on FileOperations (boolean success = operations.storeFile(fileName, exchange) also opens an InputStream on the same message.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.file.GenericFileProducer.java"], "label": 1, "es_results": []}, {"bug_id": 3281, "bug_title": "RouteBuilder - Let if fail if end user is configuring onException etc after routes", "bug_description": "All such cross cutting concerns must be defined before routes.\nWe should throw an exception if end user has configured them after routes, which is currently not supported in the DSL.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.mina.MinaTcpWithIoOutProcessorExceptionTest.java", "org.apache.camel.builder.RouteBuilder.java", "org.apache.camel.builder.RouteBuilderAddRoutesTest.java"], "label": 1, "es_results": []}, {"bug_id": 3269, "bug_title": "CXF CamelTransport and \"Cannot find input stream in message\"", "bug_description": "When using a JaxWS proxy with camel transport, to a route with only one processor, CXF fail with the message \"Can&apos;t find input stream in message\".\nCamelConduit call CxfMessageHelper.getCxfInMessage which is looking for an Out part in the exchange. If the processor of the route doesn&apos;t use the Out message, or doesn&apos;t copy the in part to the out part (like the Pipeline processor), the call to the JaxWS proxy fail with \"Can&apos;t find input stream in message\"", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.cxf.util.CxfMessageHelper.java"], "label": 1, "es_results": []}, {"bug_id": 3299, "bug_title": "BeanInvocation handling LinkedHashMap cannot be converted to java.util.Map", "bug_description": "I&apos;m calling a Camel proxy and passing it a single argument of type LinkedHashMap.\nOn the service side I&apos;m waiting for a java.util.Map, so the converter BeanInvocation -> java.util.Map is called.\nFinally it comes to BeanConverter:convertTo with type=java.util.Map and value class=BeanInvocation\nthen it goes to\n            // maybe from is already the type we want\n            if (from.isAssignableFrom(type)) \n{\n                return body;\n            }\nwhere from=LinkedHashMap\nand... the condition is false!\nThe LinkedHashMap is not assignable from java.util.Map,\nbut java.util.Map is assignable from LinkedHashMap and, I guess, that is what we want.\nPlease fix?\n\nif (from.isAssignableFrom(type)) {\n+ if (type.isAssignableFrom(from)) {\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.5.0", "fixed_files": ["org.apache.camel.component.bean.BeanConverter.java"], "label": 1, "es_results": []}, {"bug_id": 3321, "bug_title": "SmppBinding raises NullPointerException when an SMSC insert the Short Message Data inside the message_payload field", "bug_description": "When an SMSC sends the DeliverSm with the short message data inside the message_payload field (in the OptionalParameter) the method\ncreateSmppMessage(DeliverSm deliverSm) in the SmppBinding class raises a nullPointerException at the following line:\nelse {\n            smppMessage.setBody(String.valueOf(new String(deliverSm.getShortMessage(),  \nthis happen because deliverySm.getShortMessage return null\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.smpp.SmppBindingTest.java", "org.apache.camel.component.smpp.SmppBinding.java"], "label": 1, "es_results": []}, {"bug_id": 3237, "bug_title": "XmppEndPoint - setting login to false when creating an account results in no action", "bug_description": "\nXmppEndPoint myXmppEndPoint = new XmppEndPoint();\n(...)\nmyXmppEndPoint.setCreateAccount(true);\nmyXmppEndPoint .setLogin(false);\n(...)\n\n\nThis will result in \"no action\" in xmpp server, if setLogin true the account is created and the user stays online.\nI believe it&apos;s because of the logic used in createConnection() method of XmppEndPoint that could be changed to:\n\nif (!connection.isAuthenticated()) {\n            if (user != null) {\n                if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Logging in to XMPP as user: \" + user + \" on connection: \" + getConnectionMessage(connection));\n                }\n                if (password == null) {\n                    LOG.warn(\"No password configured for user: \" + user + \" on connection: \" + getConnectionMessage(connection));\n                }\n\n                if (createAccount) {\n                    AccountManager accountManager = new AccountManager(connection);\n                    accountManager.createAccount(user, password);\n                }\n                if(login){\n                \tif (resource != null) {\n                \t\tconnection.login(user, password, resource);\n                \t} else {\n                \t\tconnection.login(user, password);\n                \t}\n                }\n            } else {\n                if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Logging in anonymously to XMPP on connection: \"  + getConnectionMessage(connection));\n                }\n                connection.loginAnonymously();\n            }\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.xmpp.XmppEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 3271, "bug_title": "packageScan does not work with camel-blueprint", "bug_description": "Using the following xml code does not activate the Java Camel routes that can be found in the given package;\n<blueprint xmlns=\"http://www.osgi.org/xmlns/blueprint/v1.0.0\">\n\t<camelContext xmlns=\"http://camel.apache.org/schema/blueprint\">\n\t\t<packageScan>\n\t\t\t<package>eu.schuring.camel.blueprint.route</package>\n\t\t</packageScan>\n\t</camelContext>\n</blueprint>\nAttached is a usecase that should output messages from both a native blueprint DSL route and a Java DSL route activated by the xml section above. I&apos;ve attached both the bundle and a source jar.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.blueprint.CamelContextFactoryBean.java", "org.apache.camel.itest.osgi.blueprint.OSGiBlueprintTestSupport.java"], "label": 1, "es_results": []}, {"bug_id": 3479, "bug_title": "CamelContinuationServlet - May produce NPE under heavy load", "bug_description": "CAMEL-2986 fixes some issue with the CamelContinationServlet.\nHowever under extreme load and under some circumstances you can still get a NPE.\nThe Jetty guides for writing and using continuation at\nhttp://wiki.eclipse.org/Jetty/Feature/Continuations\nShows a different style for suspend/resume than we currently have implemented. \nI think it&apos;s best practice that we refactor the code in camel-jetty to be aligned with the Jetty guide.\nI will follow the Suspend Resume Pattern style listed on the Jetty guide.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.jetty.CamelContinuationServlet.java"], "label": 1, "es_results": []}, {"bug_id": 3133, "bug_title": "Camel soap dataformat does not work correctly if a method has no input or no output", "bug_description": "Currently the camel soap dataformat can not handle exachanges for a soap method that has no input or no output. This is a known limitation but we should support this.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.converter.soap.name.ServiceInterfaceStrategyTest.java", "org.apache.camel.dataformat.soap.name.MethodInfo.java", "org.apache.camel.dataformat.soap.SoapCxfClientTest.java", "org.apache.camel.dataformat.soap.SoapClientTest.java", "org.apache.camel.dataformat.soap.name.ServiceInterfaceStrategy.java", "org.apache.camel.dataformat.soap.CustomerServiceImpl.java", "org.apache.camel.dataformat.soap.SoapJaxbDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 3245, "bug_title": "Make Jetty continuation timeout configurable", "bug_description": "", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpEndpoint.java", "org.apache.camel.component.jetty.CamelContinuationServlet.java", "org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 3438, "bug_title": "JAXBDataFormatter should using Spring ApplicationContext's classLoader explicitly", "bug_description": "JAXBDataFormatter now using JAXBContext.newInstance(path) to create JAXBContext,\nbut this will using Thread&apos;s context classLoader.\nthis may causing un-expected class or resource not found exceptions;", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.4.0", "fixed_version": "camel-2.7.0", "fixed_files": ["org.apache.camel.converter.jaxb.JaxbDataFormat.java", "org.apache.camel.example.DataFormatTest.java", "org.apache.camel.processor.DefaultChannel.java", "org.apache.camel.processor.MarshalProcessor.java", "org.apache.camel.processor.UnmarshalProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 3302, "bug_title": "OsgiPackageScanClassResolver should have Non-OSGi classloader check fallback", "bug_description": "This is necessary when use JBI packaging for servicemix-camel ServiceUnit  so that we get chance to use SU classloader to scan packages in the ServiceUnit", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.core.osgi.CamelMockBundle.java", "org.apache.camel.core.osgi.OsgiPackageScanClassResolver.java", "org.apache.camel.core.osgi.OsgiPackageScanClassResolverTest.java"], "label": 1, "es_results": []}, {"bug_id": 3314, "bug_title": "Property resolve in EIP does not work when in a sub route.", "bug_description": "The 2.5 feature: \"The EIP now supports property placeholders in the String based options (a few spots in Java DSL where its not possible). For example: \n<convertBodyTo type=\"String\" charset=\"foo.myCharset\"/>\" does not work correctly when ie nested in a <choice> tag.\nSee discussion: http://camel.465427.n5.nabble.com/Camel-2-5-Propertyplaceholders-and-Spring-DSL-still-not-working-td3251608.html#a3251608\nExample route:\nThis works: \n<route> \n        <from uri=\"direct:in\" /> \n        <convertBodyTo type=\"String\" charset=\"charset.external\" />\t\n        <log message=\"Charset: charset.external\" /> \n        <to uri=\"mock:out\" /> \n</route> \nThis fails: \n<route> \n        <from uri=\"direct:in\" /> \n        <choice> \n                <when> \n                        <constant>true</constant> \n                        <convertBodyTo type=\"String\" charset=\"charset.external\" />\t\n                </when> \n        </choice> \n        <to uri=\"mock:out\" /> \n</route> ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.model.ProcessorDefinition.java"], "label": 1, "es_results": []}, {"bug_id": 3328, "bug_title": "NPE on Request-Reply InOut (Test attached)", "bug_description": "simply request reply with ActiveMQ 5.4.1 using a queue is failing in v2.5 instead is running in v.2.4\n\n \nimport static org.junit.Assert.assertTrue;\n\nimport org.apache.activemq.ActiveMQConnectionFactory;\nimport org.apache.camel.Endpoint;\nimport org.apache.camel.Exchange;\nimport org.apache.camel.ExchangePattern;\nimport org.apache.camel.Processor;\nimport org.apache.camel.Producer;\nimport org.apache.camel.builder.RouteBuilder;\nimport org.apache.camel.component.jms.JmsComponent;\nimport org.apache.camel.impl.DefaultCamelContext;\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\npublic class RequestReplyFailureTest\n{\n\tprivate Endpoint\t\t\tendpoint;\n\tprivate Producer\t\t\tproducer;\n\tprivate DefaultCamelContext\tcamel;\n\n\t@Before\n\tpublic void before() throws Exception\n\t{\n\t\tString brokerUrl = \"tcp://localhost:61616\";\n\t\tcamel = new DefaultCamelContext();\n\t\tcamel.addComponent(\"jms\", JmsComponent.jmsComponentAutoAcknowledge(new ActiveMQConnectionFactory(brokerUrl)));\n\n\t\tfinal String url = \"jms:queue:test\";\n\t\tendpoint = camel.getEndpoint(url);\n\n\t\tcamel.addRoutes(new RouteBuilder()\n\t\t{\n\t\t\t@Override\n\t\t\tpublic void configure() throws Exception\n\t\t\t{\n\t\t\t\tfrom(url).process(new Processor()\n\t\t\t\t{\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void process(Exchange exchange) throws Exception\n\t\t\t\t\t{\n\t\t\t\t\t\t// simply set &apos;pong&apos; as response\n\t\t\t\t\t\texchange.getOut().setBody(\"pong\");\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t}\n\t\t});\n\n\t\tSystem.out.println(camel.getVersion());\n\t\tcamel.start();\n\n\t\tproducer = endpoint.createProducer();\n\t}\n\n\t@After\n\tpublic void after() throws Exception\n\t{\n\t\tcamel.stop();\n\t}\n\n\t/**\n\t * @throws Exception\n\t */\n\t@Test\n\tpublic void testInOut() throws Exception\n\t{\n\t\tExchange exchange = endpoint.createExchange(ExchangePattern.InOut);\n\t\texchange.getIn().setBody(\"ping\");\n\t\tproducer.process(exchange);\n\n\t\tassertTrue(\"pong\".equals(exchange.getOut().getBody()));\n\t}\n\n ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.jms.JmsProducer.java"], "label": 1, "es_results": []}, {"bug_id": 3334, "bug_title": "BeanShell Expressions Do not Work", "bug_description": "BeanShell 2.0b5 has the JSR-223 integration, but it implements Compilable, when in fact it throws an exception if you call compile.\nCamel calls compile for any language that implements Compilable, therefore you get an exception every time your route has BeanShell in it.\nI notice the BeanShell tests are commented-out in camel-script.\nAlso, the Camel Wiki page for BeanShell is stupendously vague on actual usage.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.builder.script.BeanShellScriptRouteTest.java", "org.apache.camel.builder.script.ScriptBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 3333, "bug_title": "Nested multicast in splitter EIP and issue with UseOriginalAggregationStrategy", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Multicast-inside-splitter-tp3261288p3261288.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.processor.Splitter.java", "org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 3306, "bug_title": "Transfer-Encoding chunking implementation leaves a loop hole for error", "bug_description": "The description of this issue is at [1].\n[1] http://camel.465427.n5.nabble.com/CXF-http-conduit-AllowChunking-does-not-work-td3247495.html#a3248727", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.cxf.CxfHeaderFilterStrategy.java", "org.apache.camel.component.cxf.CxfProducerProtocalHeaderTest.java"], "label": 1, "es_results": []}, {"bug_id": 3344, "bug_title": "RedeliveryPolicy does not honor MaximumRedeliveryDelay", "bug_description": "When using exponential retry back-off with a maximumRedeliveryDelay, the delay is not honored.\nThe bug is in RedeliveryPolicy.java&apos;s calculateRedeliveryDelay method:\n        if (maximumRedeliveryDelay > 0 && redeliveryDelay > maximumRedeliveryDelay) \n{\n            redeliveryDelayResult = maximumRedeliveryDelay;\n        }\n\nredeliveryDelay is the initial delay and never increases, so the max is never applied. It needs to compare against redeliveryDelayResult instead.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.processor.RedeliveryPolicy.java"], "label": 1, "es_results": []}, {"bug_id": 3355, "bug_title": "ConcurrentModifictionException on UoW done, when under heavy load", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Errors-when-under-load-tp3276259p3276259.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.util.UnitOfWorkHelper.java"], "label": 1, "es_results": []}, {"bug_id": 3373, "bug_title": "markRollbackOnlyLast should remove caused exception to avoid it affecting outer transaction", "bug_description": "The markRollbackOnlyLast() should remove any caused exception because it should not affect outer transactions.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.spring.interceptor.MixedTransactionPropagationTest.java", "org.apache.camel.spring.spi.TransactionErrorHandler.java"], "label": 1, "es_results": []}, {"bug_id": 3389, "bug_title": "PackageHelper - issue with version numbers which contains non decimal", "bug_description": "See more here\nhttp://fusesource.com/forums/thread.jspa?threadID=2447&tstart=0", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.util.PackageHelper.java"], "label": 1, "es_results": []}, {"bug_id": 3394, "bug_title": "Splitter and Multicast EIP marks exchange as exhausted to early if exception was thrown from an evaluation", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Cannot-handle-Exception-thrown-from-Splitter-Expression-tp3286043p3286043.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 3395, "bug_title": "Splitter - Exchange.CORRELATION_ID should point back to parent Exchange id", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Splitted-exchange-has-incorrect-correlation-ID-tp3289354p3289354.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.processor.Splitter.java", "org.apache.camel.util.ExchangeHelper.java"], "label": 1, "es_results": []}, {"bug_id": 3295, "bug_title": "camel-blueprint - Dependency Injection seems not working", "bug_description": "This is just a placeholder - things like this should work.\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<blueprint xmlns=\"http://www.osgi.org/xmlns/blueprint/v1.0.0\"\n           xmlns:cm=\"http://aries.apache.org/blueprint/xmlns/blueprint-cm/v1.0.0\"\n           xmlns:ext=\"http://aries.apache.org/blueprint/xmlns/blueprint-ext/v1.0.0\"\n           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n           xsi:schemaLocation=\"http://www.osgi.org/xmlns/blueprint/v1.0.0\n            http://www.osgi.org/xmlns/blueprint/v1.0.0/blueprint.xsd\">\n\n\n    <camelContext xmlns=\"http://camel.apache.org/schema/blueprint\" id=\"camelBlueprint\">\n        <route>\n            <from uri=\"jms:queue\"/>\n            <to uri=\"mock:result\"/>\n        </route>\n    </camelContext>\n\n    <bean id=\"jms\" class=\"org.apache.camel.component.jms.JmsComponent\">\n        <property name=\"connectionFactory\" ref=\"jmsConnectionPool\"/>\n    </bean>\n\n    <reference id=\"jmsConnectionPool\" interface=\"javax.jms.ConnectionFactory\"/>\n\n</blueprint>\n\n\nCurrently the jmsConnectionPool is not at all passed to the JmsComponent.\nThe usage of JmsTemplate as debated on the mailinglists I think is of a (currently) much lesser concern.\nEspecially comparing a little to the ProducerCode in the servicemix-jms components.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.blueprint.BlueprintComponentResolver.java", "org.apache.camel.blueprint.BlueprintContainerRegistry.java", "org.apache.camel.blueprint.handler.CamelNamespaceHandler.java", "org.apache.camel.blueprint.CamelContextFactoryBean.java", "org.apache.camel.itest.osgi.blueprint.OSGiBlueprintTestSupport.java"], "label": 1, "es_results": []}, {"bug_id": 3390, "bug_title": "routeBuilder ref not working in camel 2.5.0", "bug_description": " I amended the camel-example-osgi project so as to use the Java RouteBuilder, the MyRouteBuilder class in that project\nI changed the blueprint.xml file to the following\n\t<bean id=\"routeBuilder\" class=\"org.apache.camel.example.osgi.MyRouteBuilder\" />\n\t<camelContext xmlns=\"http://camel.apache.org/schema/blueprint\">\n\t\t\t<routeBuilder ref=\"routeBuilder\"/>\n\t</camelContext>\nHowever nothing happens, no error message is displayed either.\nIf I leave the example in its original state, in other words using the xml-based dsl, then it works.\nMaybe this is somehow related to the fact that packageScan does not work???\nRegards \nIvanhoe\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.itest.osgi.blueprint.OSGiBlueprintTestSupport.java"], "label": 1, "es_results": []}, {"bug_id": 3351, "bug_title": "camel-irc component silently fails on nick collision", "bug_description": "When the camel-irc component connects to an irc server and there&apos;s a nick collision it silently fails. Also note there is no camel-irc component in Jira.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.irc.IrcConfigurationTest.java", "org.apache.camel.component.irc.IrcConfiguration.java", "org.apache.camel.component.irc.IrcEndpoint.java", "org.apache.camel.component.irc.IrcConsumer.java", "org.apache.camel.component.irc.IrcComponent.java", "org.apache.camel.component.irc.IrcProducer.java"], "label": 1, "es_results": []}, {"bug_id": 3419, "bug_title": "timestamp property in JpaTraceEventMessage does not specify a temporal type", "bug_description": "The timestamp property in the JpaTraceEventMessage does not specify a temporal type. This results in an exception using EclipseLink 2.1.0 (and maybe using other JPA frameworks, too). \nUsing the annotation  @Temporal(TemporalType.TIMESTAMP) on timestamp should solve this issue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.processor.interceptor.jpa.JpaTraceEventMessage.java"], "label": 1, "es_results": []}, {"bug_id": 3426, "bug_title": "CxfProducer does not callback.done when the operation is oneway.", "bug_description": "There is a mail thread[1] describes the whole story.\n[1]http://camel.465427.n5.nabble.com/file-to-oneway-cxf-service-scenario-does-not-work-with-camel-2-5-0-td3303263.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.itest.greeter.GreeterImpl.java", "org.apache.camel.component.cxf.CxfProducer.java"], "label": 1, "es_results": []}, {"bug_id": 3430, "bug_title": "InterceptSendToEndpoint has issues with interception http endpoints which has multiple parameters", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/interceptSendToEndpoint-with-dynamic-endpoint-tp3301978p3301978.html\nThe issue is when any endpoints have parameters which may be re-ordered when the endpoint is normalized.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.impl.DefaultCamelContext.java", "org.apache.camel.management.JmxInstrumentationCustomMBeanTest.java"], "label": 1, "es_results": []}, {"bug_id": 3442, "bug_title": "JBI ClassLoading issue in SMX 4.x in OsgiPackageScanClassResolver", "bug_description": "CAMEL-3302 introduced a fallback when using JBI in Apache ServiceMix 4.x.\nHowever it may lead to an issue with ConcurrentModificationException when traversing the list of classloaders.\n\n\n\n            for (ClassLoader classLoader : super.getClassLoaders()) {\n\n                if (!isOsgiClassloader(classLoader)) {\n\n                    find(test, packageName, classLoader, classes);\n\n                }\n\n            }  \n\n\n\nThe for loop is line 62 which causes the exception.\nIssue reported here\nhttp://camel.465427.n5.nabble.com/ServiceMix-4-Fuse-4-3-0-fuse-03-00-Problems-running-JBI-example-examples-camel-td3309088.html\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.impl.DefaultPackageScanClassResolver.java", "org.apache.camel.spring.scan.DefaultPackageScanClassResolverTest.java", "org.apache.camel.core.osgi.OsgiPackageScanClassResolver.java", "org.apache.camel.spi.PackageScanClassResolver.java"], "label": 1, "es_results": []}, {"bug_id": 3446, "bug_title": "NPE in camel-printer when not setting media size or omitting sides attribute", "bug_description": "When specifying a camel-printer configuration that does not include config properties for sides or mediaSize, the camel route will fail to start up with a NullPointerException. E.g. this route\n\n\n\nfrom(\"file://target/incoming?delete=true\")\n\n\t.to(\"lpr://localhost/default\");\n\n\n\nwill raise an NPE at route startup time.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.printer.PrinterPrintTest.java", "org.apache.camel.component.printer.PrinterConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 3458, "bug_title": "Bindy should support clipping fields if they exceed maximum length allowed when using fixed length", "bug_description": "Adding a new option to bindy annotation\n\n\n\n\n\n    /**\n\n     * Indicates to clip data in the field if it exceeds the allowed length when using fixed length.\n\n     */\n\n    boolean clip() default false;\n\n\n\nThen if enabled it will clip the data so it can fit the length.\nAlso now Camel throws an exception if the data is too long and you have clip set as false.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.dataformat.bindy.BindyKeyValuePairFactory.java", "org.apache.camel.dataformat.bindy.fix.BindySimpleKeyValuePairWithoutSectionMarshallDslTest.java", "org.apache.camel.dataformat.bindy.BindyFixedLengthFactory.java", "org.apache.camel.dataformat.bindy.BindyAbstractFactory.java", "org.apache.camel.dataformat.bindy.annotation.DataField.java", "org.apache.camel.dataformat.bindy.BindyCsvFactory.java"], "label": 1, "es_results": []}, {"bug_id": 3353, "bug_title": "CxfRsInvoker silently swallows exceptions", "bug_description": "If you have a route with a CXF consuming endpoint in the beginning and any component afterwards that can produce Exceptions that are not RuntimeCamelExceptions or WebApplicationException then the CxfRsInvoker will swallow the exception and return a HTTP 204 (all fine but no content to return) response.\nFor example in the following route:\n\n from(\"cxfrs://bean://fooServer\")\n            .convertBodyTo(Foo.class)\n            .to(\"bean-validator://x\")\n            .to(\"jms:queue:foos\").inOnly();\n\n\nThe bean validator component can throw BeanValidationException when the Foo instance has errors. This exception will be ignored by the CxfRsInvoker.\nThis causes important exceptions to become invisible by default which seems wrong to me. The docs and Camel in Action additionally talk about how the DefaultErrorHandler has a strategy of returning exceptions to the caller and this is also not happening here.\nMy local fix is a patched version of camel-cxf that converts any unknown exception (i.e. not CamelRuntimeException or WebApplicationException) to a WebApplicationException with the original exception as a constructor parameter. This is then effectively an HTTP 500 Exception and will be returned as such to the caller.\nHowever my knowledge of camel and camel-cxf is not sufficient to ascertain whether this is the right approach, it seems to me that the CamelRuntimeException should also be treated this way since in the current code that will also be swallowed (as far as I can tell).", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.cxf.jaxrs.CxfRsInvoker.java", "org.apache.camel.component.cxf.jaxrs.CxfRsConsumerTest.java"], "label": 1, "es_results": []}, {"bug_id": 3483, "bug_title": "csv unmarshal and maybe other components uses default encoding ", "bug_description": "See discussion in Nabble: http://camel.465427.n5.nabble.com/csv-unmarshal-uses-default-encoding-td3325474.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.dataformat.castor.AbstractCastorDataFormat.java", "org.apache.camel.component.flatpack.FlatpackDataFormat.java", "org.apache.camel.dataformat.csv.CsvDataFormat.java"], "label": 1, "es_results": []}, {"bug_id": 3489, "bug_title": " BindyCsvDataFormat broken for pipe delimited files", "bug_description": "Attempting to unmarshall a pipe delimited CSV file into a POJO using Bindy causese the first and last character the the line processed to be dropped.  It appears that the BindyCsvDataFormat class removes the first and the last character from the line read from the CSV if the seperator is > 1 characters in length (see below or line 162-165 in BindyCsvDataFormat).  For pipe delimited files, you need to specify | as the seperator, as | is not evaluated correctly as a java regex by the split fuction.  This leads to the first and last character for the line being parsed being dropped.  From the comments it appears a \"fix\" was added to remove the first and last character of the line when the seperator contains quotes or double quotes.  Making this determination using the length of the seperator, rather than evaluating using a regex seems to be a poor solution that breaks other CSV delimiters.\nSee Attached for an code example.\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.dataformat.bindy.csv.BindyCsvDataFormat.java", "org.apache.camel.dataformat.bindy.csv.BindyDoubleQuotesCsvUnmarshallTest.java"], "label": 1, "es_results": []}, {"bug_id": 3498, "bug_title": "Splitter Component: Setting 'streaming = \"true\"' breaks error handling", "bug_description": "Setting &apos;streaming = \"true\"&apos; breaks error handling:\nIf an exception is thrown in a processor, the exception in the subExchange is copied to the original exchange in MulticastProcessor line 554. In Splitter line 140 the original exchange is copied, including the exception that was thrown while processing the previous exchange. This prevents all subsequent exchanges from being processed successfully.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.processor.Splitter.java", "org.apache.camel.processor.SplitterNoStopOnExceptionTest.java", "org.apache.camel.processor.SplitterNoAggregationStrategyTest.java", "org.apache.camel.processor.SplitterParallelNoStopOnExceptionTest.java"], "label": 1, "es_results": []}, {"bug_id": 3524, "bug_title": "camel-jetty - Add option to set jetty continuation timeout", "bug_description": "By default Jetty uses 30 sec timeout for continuations.\nWe should allow end users to configure a value of choice, in case their system may take longer time to process an exchange.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.jetty.JettyHttpEndpoint.java", "org.apache.camel.component.jetty.CamelContinuationServlet.java", "org.apache.camel.component.jetty.JettyHttpComponent.java"], "label": 1, "es_results": []}, {"bug_id": 3537, "bug_title": "camel-snmp - Does not support tcp protocol", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/camel-snmp-2-5-problems-tp3339373p3339373.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.snmp.SnmpEndpoint.java", "org.apache.camel.component.snmp.SnmpTrapConsumer.java", "org.apache.camel.component.snmp.SnmpOIDPoller.java"], "label": 1, "es_results": []}, {"bug_id": 3540, "bug_title": "Jt400DataQueueConsumer incorrectly implements timeout semantics (jt400 component)", "bug_description": "Jt400DataQueueConsumer implementation of receive(long) passes the timeout argument directly to com.ibm.as400.access.DataQueue.read(int), not performing unit conversion. However, Jt400DataQueueConsumer.receive(long) accepts milliseconds, whereas DataQueue.read(int) accepts seconds as the time unit.\nAlso, invoking Jt400DataQueueConsumer.receive() results in a call to DataQueue.read(), which is not a blocking call; on the contrary, it will not wait for entries.\nCode snippet below.\nJt400DataQueueConsumer.java\n\n\n        DataQueue queue = endpoint.getDataQueue();\n\n        try {\n\n            DataQueueEntry entry;\n\n            if (timeout >= 0) {\n\n                entry = queue.read((int)timeout);\n\n            } else {\n\n                entry = queue.read();\n\n            }\n\n\n\nNote that the submitted patch floors the timeout value when converting to seconds, but different rounding might be desired, which should be specified in the class documentation.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.jt400.Jt400DataQueueConsumer.java", "org.apache.camel.impl.PollingConsumerSupport.java", "org.apache.camel.component.jt400.Jt400DataQueueProducer.java"], "label": 1, "es_results": []}, {"bug_id": 3545, "bug_title": "MethodCallExpression does not validate whether the method exists for all cases", "bug_description": "I tried to refactor\norg.apache.camel.model.language.MethodCallExpression.java\n\n\n    public Expression createExpression(CamelContext camelContext) {\n\n        Expression answer;\n\n\n\n        if (beanType != null) {            \n\n            instance = ObjectHelper.newInstance(beanType);\n\n            return new BeanExpression(instance, getMethod(), parameterType); // <--\n\n        } else if (instance != null) {\n\n            return new BeanExpression(instance, getMethod(), parameterType); // <--\n\n        } else {\n\n            String ref = beanName();\n\n            // if its a ref then check that the ref exists\n\n            BeanHolder holder = new RegistryBean(camelContext, ref);\n\n            // get the bean which will check that it exists\n\n            instance = holder.getBean();\n\n            answer = new BeanExpression(ref, getMethod(), parameterType);\n\n        }\n\n\n\n        // validate method\n\n        validateHasMethod(camelContext, instance, getMethod(), parameterType);\n\n\n\n        return answer;\n\n    }\n\n\n\nto\norg.apache.camel.model.language.MethodCallExpression.java\n\n\n    public Expression createExpression(CamelContext camelContext) {\n\n        Expression answer;\n\n\n\n        if (beanType != null) {            \n\n            instance = ObjectHelper.newInstance(beanType);\n\n            answer = new BeanExpression(instance, getMethod(), parameterType); // <--\n\n        } else if (instance != null) {\n\n            answer = new BeanExpression(instance, getMethod(), parameterType); // <--\n\n        } else {\n\n            String ref = beanName();\n\n            // if its a ref then check that the ref exists\n\n            BeanHolder holder = new RegistryBean(camelContext, ref);\n\n            // get the bean which will check that it exists\n\n            instance = holder.getBean();\n\n            answer = new BeanExpression(ref, getMethod(), parameterType);\n\n        }\n\n\n\n        // validate method\n\n        validateHasMethod(camelContext, instance, getMethod(), parameterType);\n\n\n\n        return answer;\n\n    }\n\n\n\nso that the created BeanExpression is also validate if you provide the bean type or an instance. With this change, some tests in org.apache.camel.language.SimpleTest fails.\nI&apos;m not sure whether the tests are faulty or if it&apos;s a bug.\nAlso not sure whether this should fixed in 2.6. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.bean.BeanWithMethodHeaderTest.java", "org.apache.camel.component.bean.MethodNotFoundException.java", "org.apache.camel.language.BeanLanguageInvalidOGNLTest.java", "org.apache.camel.model.language.MethodCallExpression.java"], "label": 1, "es_results": []}, {"bug_id": 3559, "bug_title": "Aggregator - The completionFromBatchConsumer option do not aggregate the last incoming exchange", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Last-Aggregated-Exchange-lost-Aggregator-with-a-Batch-Consumer-and-persistent-AggregationRepository-tp3346214p3346214.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.processor.aggregate.AggregateProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 3560, "bug_title": "Detect uncaught exceptions in UoWProcessor to ensure UoW is done even for those uncaught exceptions", "bug_description": "The Camel routing engines will handle this in 99.9% of the cases, but when you shutdown a Spring AC then Spring may stop beans in whatever order and this can cause those beans to fail operating during a graceful shutdown. And in worst case exceptions is thrown in situations where they are not normally done.\nTo cater for that and other situations the UoWProcessor should detect this and act accordingly.\nThis ensure the in flight registry will be tracked and we are not stuck with a missing inflight message, causing Camel to wait for the 300 sec timeout to shutdown.\nFor example just try hitting ctrl + c in that camel-example-management and you can see such an example.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.6.0", "fixed_files": ["org.apache.camel.component.bean.BeanProcessor.java", "org.apache.camel.processor.UnitOfWorkProcessor.java", "org.apache.camel.component.jms.EndpointMessageListener.java"], "label": 1, "es_results": []}, {"bug_id": 3531, "bug_title": "scala - xpath not working together with choice/when", "bug_description": "When using the Scala DSL, xpath expressions inside when() do not work as expected. As an example:\n\n\n\n     \"direct:a\" ==> {\n\n     choice {\n\n        when (xpath(\"//hello\")) to (\"mock:english\")\n\n        when (xpath(\"//hallo\")) {\n\n          to (\"mock:dutch\")\n\n          to (\"mock:german\")\n\n        } \n\n        otherwise to (\"mock:french\")\n\n      }\n\n    }\n\n\n\n// Send messages\n\n\"direct:a\" ! (\"<hello/>\", \"<hallo/>\", \"<hellos/>\")\n\n\n\nHere we should receive 1 message in each of the mocks. For whatever reason, all 3 messages go to mock:english. Similar routes work as expected with the Java DSL. ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.5.0", "fixed_version": "camel-2.7.3", "fixed_files": ["org.apache.camel.util.ObjectHelper.java", "org.apache.camel.converter.ObjectHelperTest.java"], "label": 1, "es_results": []}, {"bug_id": 3361, "bug_title": "Upgrade SpringIntegration to 2.0.0.RELEASE", "bug_description": "Spring Integration 2.0 just became GA. The camel-spring-integration component should be upgraded. I think it is best to do this in Camel 3.0 since Spring Integration 2.0 requires Spring 3.0.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.6.0", "fixed_version": "camel-2.7.0", "fixed_files": ["org.apache.camel.component.spring.integration.adapter.CamelTargetAdapter.java", "org.apache.camel.component.spring.integration.adapter.CamelSourceAdapterTest.java", "org.apache.camel.component.spring.integration.converter.SpringIntegrationConverter.java", "org.apache.camel.itest.karaf.AbstractFeatureTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationOneWayConsumerTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationProducer.java", "org.apache.camel.component.spring.integration.adapter.ConfigurationTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationConsumer.java", "org.apache.camel.component.spring.integration.SpringIntegrationBinding.java", "org.apache.camel.component.spring.integration.adapter.CamelSourceAdapter.java", "org.apache.camel.component.spring.integration.SpringIntegrationEndpoint.java", "org.apache.camel.component.spring.integration.SpringIntegrationTwoWayConsumerTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationMessage.java", "org.apache.camel.component.spring.integration.adapter.CamelTargetAdapterTest.java"], "label": 1, "es_results": []}, {"bug_id": 3630, "bug_title": "Upgrade to Spring Integration 2.0.x", "bug_description": "As part of the Spring 3.0x upgrade, we can upgrade to SI 2.x as it requires Spring 3.0", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.6.0", "fixed_version": "camel-2.7.0", "fixed_files": ["org.apache.camel.component.spring.integration.adapter.CamelTargetAdapter.java", "org.apache.camel.component.spring.integration.adapter.CamelSourceAdapterTest.java", "org.apache.camel.component.spring.integration.converter.SpringIntegrationConverter.java", "org.apache.camel.itest.karaf.AbstractFeatureTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationOneWayConsumerTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationProducer.java", "org.apache.camel.component.spring.integration.adapter.ConfigurationTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationConsumer.java", "org.apache.camel.component.spring.integration.SpringIntegrationBinding.java", "org.apache.camel.component.spring.integration.adapter.CamelSourceAdapter.java", "org.apache.camel.component.spring.integration.SpringIntegrationEndpoint.java", "org.apache.camel.component.spring.integration.SpringIntegrationTwoWayConsumerTest.java", "org.apache.camel.component.spring.integration.SpringIntegrationMessage.java", "org.apache.camel.component.spring.integration.adapter.CamelTargetAdapterTest.java"], "label": 1, "es_results": []}, {"bug_id": 3674, "bug_title": "Camel properties component - to leverage Blueprint properties", "bug_description": "The Camel properties component, should be able to ref to blueprint properties \"stuff\".\nWe can add a new scheme blueprint you can specify in the location url, or re-use the ref which would refer to the blueprint \"stuff\".\nWe may need some logic from camel-blueprint or whatever to type converter or help lookup and bridge the two of them.\nJust creating this ticket so we will look into this, sometime.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.6.0", "fixed_version": "camel-2.7.0", "fixed_files": ["org.apache.camel.component.jasypt.JasyptPropertiesParserTest.java", "org.apache.camel.core.xml.AbstractCamelContextFactoryBean.java", "org.apache.camel.component.jasypt.JasyptPropertiesParser.java", "org.apache.camel.component.properties.PropertiesParser.java", "org.apache.camel.component.properties.DefaultPropertiesParser.java", "org.apache.camel.blueprint.CamelContextFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 3774, "bug_title": "camel-manual generation fails during release process", "bug_description": "It works during a regular mvn install, but it fails during release:prepare. The bug is somewhere in the maven-html-to-pdf plugin. I hope a mock release using mvn -X will reveal the problem, but would take a frustrating amount of time. For now the available info is the output:\n\n\n\n[INFO] [INFO] [html-to-pdf:compile {execution: default}]\n\n[INFO] [INFO] Downloading: http://camel.apache.org/book-in-one-page.html\n\n[INFO] ERROR:  &apos;NOT_FOUND_ERR: An attempt is made to reference a node in a context where it does not exist.&apos;\n\n[INFO] [ERROR] Download or validation of &apos;http://camel.apache.org/book-in-one-page.html&apos; failed: org.apache.camel.CamelException: Failed to convert the HTML to tidy Markup\n\n[INFO] [INFO] Stored dummy file: /w1/apache/release/camel270/tooling/camel-manual/target/site/manual/camel-manual-2.7.0.html since download of http://camel.apache.org/book-in-one-page.html failed.\n\n\n\nThe error points to a DOM related issue, but since the downloaded manual gets overwritten with the dummy file, I have no idea if the download got interrupted, or in what way the source gets corrupted.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.6.0", "fixed_version": "camel-2.7.5", "fixed_files": ["org.apache.camel.maven.HtmlToPdfMojo.java"], "label": 1, "es_results": []}, {"bug_id": 5025, "bug_title": "The ErrorHandler does not work when Splitter working in Streaming model.", "bug_description": "When the Splitter working in Streaming model, the error handle does work if the exception is throw from body iterator.\n\n\n\nerrorHandler(deadLetterChannel(\"mock:error\"));\n\nfrom(\"direct:start\").\n\n\tsplit(body()).\t\t\t\t\t\t\n\n\t\tstreaming().\n\n\t\tto(\"mock:end\").\n\n\tend();\n\n\n\nHere is the mail thread[1] which discusses about it.\n[1]http://camel.465427.n5.nabble.com/Streaming-splitter-ignores-exception-handling-tt5501036.html#a5501601 ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.6.0", "fixed_version": "camel-2.8.5", "fixed_files": ["org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 3800, "bug_title": "Introduce a message header to let SqlProducer update the query string in the runtime", "bug_description": "Current we can only define a sql query in the SqlEndpoint URI, we need to a way to change the query per exchange.\nAn new camel header for this query could help use to do that.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.7.0", "fixed_version": "camel-2.8.0", "fixed_files": ["org.apache.camel.component.sql.SqlRouteTest.java", "org.apache.camel.component.sql.SqlProducer.java", "org.apache.camel.component.sql.SqlConstants.java"], "label": 1, "es_results": []}, {"bug_id": 3801, "bug_title": "Allow to send the Query as a Body + Header Message in exchange to the SQL component", "bug_description": "The existing SQL component requires that the query is attached to the endpoint and parameters provided as a List when we would like to replace the # symbol by those values. \nThe JDBC component adopts a different philosophy as the query can be build in a processor, bean or setBody and send to the JDBC endpoint.\nUsing the following syntax could be much more flexible to setup dynamically queries :\n\n\n\n<route id=\"insert-from-file\">\n\n           <from uri=\"file://target/datainsert?moveFailed=failed\"/>\n\n           <setBody>\n\n               <simple>INSERT INTO REPORT.T_INCIDENT (INCIDENT_REF,\n\n                   INCIDENT_DATE,GIVEN_NAME,FAMILY_NAME,SUMMARY,DETAILS,EMAIL,PHONE)\n\n                   VALUES (&apos;${body}&apos;,&apos;2011-03-21&apos;,&apos;Charles&apos;,&apos;Moulliard&apos;,&apos;Incident&apos;,\n\n                   &apos;This is a report incident for webinar-001&apos;,\n\n                   &apos;cmoulliard@fusesource.com&apos;,&apos;+111 10 20 300&apos;)\n\n               </simple>\n\n           </setBody>\n\n           <to uri=\"sql\"/>\n\n       </route>\n\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.7.0", "fixed_version": "camel-2.8.0", "fixed_files": ["org.apache.camel.component.sql.SqlRouteTest.java", "org.apache.camel.component.sql.SqlProducer.java", "org.apache.camel.component.sql.SqlConstants.java"], "label": 1, "es_results": []}, {"bug_id": 3907, "bug_title": "Component camel-sql needs the abiliity to process sql from the body of the exchange", "bug_description": "camel sql should also support the ability to add the sql statements in the exchange body for processing.  This is linked via 3803.  ", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.7.0", "fixed_version": "camel-2.16.0", "fixed_files": ["org.apache.camel.component.sql.SqlEndpoint.java", "org.apache.camel.component.sql.SqlProducer.java", "org.apache.camel.component.sql.SqlConstants.java"], "label": 1, "es_results": []}, {"bug_id": 4284, "bug_title": "missing documentation in binary distribution tarballs", "bug_description": "The documentation in the 2.8.0 binary distribution tarballs (both apache-camel-2.8.0.tar.gz and apache-camel-2.8.0.zip) is missing. The documentation file in doc/manual/camel-manual-2.8.0.html contains the following error message:\n\n\n\n<html>\n\n<body>Download of http://camel.apache.org/book-in-one-page.html failed</body>\n\n\n", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.8.0", "fixed_version": "camel-2.9.0", "fixed_files": ["org.apache.camel.maven.HtmlToPdfMojo.java"], "label": 1, "es_results": []}, {"bug_id": 4456, "bug_title": "camel-mail - Add option to control if mail folder should be closed after poll", "bug_description": "It should be possible to control if the mail folder should be closed after a poll or not.\nFor example if people is routing mails over SEDA or other async endpoints, then we should not close the mail folder before the mail message is being processed async.\nWe have a similar option on mina,netty etc. named disconnect. So the option could be named like that as well.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.8.0", "fixed_version": "camel-2.9.0", "fixed_files": ["org.apache.camel.component.mail.MailConfiguration.java", "org.apache.camel.component.mail.MailConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 4225, "bug_title": "camel-mail: add disconnect option", "bug_description": "A perpetually open connection to an email server can cause problems under various circumstances.  There should therefore be a \"disconnect\" option (default: false) that causes the connection to be closed after each action taken  mail read from a folder, a message flagged for deletion, a message sent, etc.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.8.0", "fixed_version": "camel-2.8.3", "fixed_files": ["org.apache.camel.component.mail.MailConfiguration.java", "org.apache.camel.component.mail.MailConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 4482, "bug_title": "Using custom expression in Splitter EIP which throws exception, is not triggering onException", "bug_description": "See nabble\nhttp://camel.465427.n5.nabble.com/Global-exception-not-invoked-in-case-of-Exception-fired-while-iterating-through-File-Splitter-td4826097.html\nWe should detect exceptions occurred during evaluation of the expression, and then because the splitter EIP to fail as soon as possible.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.8.1", "fixed_version": "camel-2.8.2", "fixed_files": ["org.apache.camel.processor.Splitter.java", "org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 4515, "bug_title": "Spring-WS should populate Camel Header with the SOAP Header", "bug_description": "Currently the Camel-Spring-WS component does not support the setting of SOAP Headers and has issues getting them. The current issue getting the SOAP Headers when receiving a message is that the resulting header key includes the namespace.\nChange the component so that a Camel header \"CamelSpringWebserviceSoapHeader\" can be populated with an intended SOAP Header for a request, and that this Header is also populated from the SOAP Header on a response.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.8.1", "fixed_version": "camel-2.11.1", "fixed_files": ["org.apache.camel.component.spring.ws.SpringWebserviceConsumer.java", "org.apache.camel.component.spring.ws.SpringWebserviceConstants.java", "org.apache.camel.component.spring.ws.SpringWebserviceProducer.java"], "label": 1, "es_results": []}, {"bug_id": 5669, "bug_title": "spring-ws - Adding custom soap headers", "bug_description": "Look into how it can be possible to add custom soap headers to using spring-ws.\nSee nabble\nhttp://camel.465427.n5.nabble.com/Adding-custom-soap-header-in-camel-spring-ws-tp5719609.html", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.8.1", "fixed_version": "camel-2.11.1", "fixed_files": ["org.apache.camel.component.spring.ws.SpringWebserviceConsumer.java", "org.apache.camel.component.spring.ws.SpringWebserviceConstants.java", "org.apache.camel.component.spring.ws.SpringWebserviceProducer.java"], "label": 1, "es_results": []}, {"bug_id": 4901, "bug_title": "SEDA/VM component does not remove the exchange from the producer BlockingQueue if the consumer timeout", "bug_description": "The fix has been made on trunk (Camel 2.9.1), but I raise this Jira to track it and merge it to 2.8.x branch.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.8.3", "fixed_version": "camel-2.8.4", "fixed_files": ["org.apache.camel.component.seda.SedaProducer.java", "org.apache.camel.component.seda.SedaTimeoutDisabledTest.java", "org.apache.camel.component.seda.SedaTimeoutTest.java"], "label": 1, "es_results": []}, {"bug_id": 5024, "bug_title": "Streaming splitter ignores exception handling", "bug_description": "If an exception occurs on the next() call of an Iterator in a streaming splitter, the exception is never propagated to the exception or the error handler. This will lead to improper route termination, and inifite rollback/retry cycles.\nI attached the minimal test case demonstrating the issue.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.8.3", "fixed_version": "camel-2.8.5", "fixed_files": ["org.apache.camel.processor.MulticastProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 6541, "bug_title": "DefaultUnitOfWork's headers for the original message are the same object as the in message headers", "bug_description": "For jms messages, the useOriginalMessage does not work when headers are changed during the route because the DefaultUnitOfWork&apos;s headers are the same object as the in message&apos;s headers.\nIn DefaultUnitOfWork, this.originalInMessage.setHeaders(exchange.getIn().getHeaders()); should be changed to create a copy of the headers instead of directly assigning.\nMaybe changed to this.originalInMessage.setHeaders((Map<String,Object>) new CaseInsensitiveMap(exchange.getIn().getHeaders()));", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.8.6", "fixed_version": "camel-2.10.7", "fixed_files": ["org.apache.camel.impl.DefaultUnitOfWork.java"], "label": 1, "es_results": []}, {"bug_id": 4882, "bug_title": "Timed out Exchanges should be removed from seda queues", "bug_description": "When the SedaProducer times out and stops waiting for an Exchange to be processed it should remove it from the queue as well. Same applies to the vm: component.", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.9.0", "fixed_version": "camel-2.7.5", "fixed_files": ["org.apache.camel.component.seda.SedaProducer.java", "org.apache.camel.component.seda.SedaTimeoutDisabledTest.java", "org.apache.camel.component.seda.SedaTimeoutTest.java"], "label": 1, "es_results": []}, {"bug_id": 5376, "bug_title": "Mail component does not work as expected (Email Deletion is partially broken et Disconnect does not work well)", "bug_description": "The way disconnect is implemented causes issues with some other options of the consumer. For instance \"disconnect\" option is not compatible with \"delete\" option.\nThe delete action is done in completion action (processCommit: line 185). On line 305, processCommit method checks if folder is open, but \"disconnect\" option force folder at null value at the end of poll method (Line 149).\nI guess disconnect method should be called on completion after any other completion actions occured: It is not possible to make completion actions if connection to mail server is closed.\nThe result of the usage of disconnect option and delete option is a NullPointerException on test: \"if (!folder.isOpen())\" statement on line 308.\nIssue should be always reproductible.\nI let you fix the priority of the issue, but it is an annoying issue even if there is a workaround by disabling disconnect option ...", "project": "Apache", "sub_project": "CAMEL", "version": "camel-2.9.5", "fixed_version": "camel-2.11.2", "fixed_files": ["org.apache.camel.component.mail.MailConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 552, "bug_title": "Bloom filter bugs", "bug_description": "There are some bugs in Bloom filters in the code that deals with initialization and (de)serialization.", "project": "Apache", "sub_project": "HBASE", "version": "0.1.0", "fixed_version": "0.1.1", "fixed_files": ["org.onelab.filter.DynamicBloomFilter.java", "org.onelab.filter.Key.java"], "label": 1, "es_results": []}, {"bug_id": 563, "bug_title": "TestRowFilterAfterWrite erroneously sets master address to 0.0.0.0:60100 rather than relying on conf", "bug_description": "TestRowFilterAfterWrite sets HConstants.MASTER_ADDRESS to 0.0.0.0:60100 rather than relying on the setting being in the configuration. Until the latest revision of hadoop-trunk this mysteriously worked. Removing this setting makes it work again.", "project": "Apache", "sub_project": "HBASE", "version": "0.1.0", "fixed_version": "0.1.1", "fixed_files": ["org.apache.hadoop.hbase.filter.TestRowFilterAfterWrite.java"], "label": 1, "es_results": []}, {"bug_id": 573, "bug_title": "HBase does not read hadoop-*.xml for dfs configuration after moving out hadoop/contrib", "bug_description": "When HBase was in hadoop/contrib, the hbase script set both HADOOP_CONF_DIR\nand HBASE_CONF_DIR to CLASSPATH, so that dfs&apos;s configuration can be loaded\ncorrectly. However, when moved out hadoop/contrib, it only sets HBASE_CONF_DIR.\nI can think of several possible solutions:\n1) set HADOOP_CONF_DIR in hbase-env.sh, then add HADOOP_CONF_DIR to CLASSPATH as before\n2) Instruct user to create links for hadoop-*.xml if they want to customize some dfs settings.\n3) If only a small set of dfs confs are related to dfs&apos;s client, maybe they can be set via  hbase-site.xml, then hbase sets these for us when create a FileSystem obj.\nPlease see the thread \"# of dfs replications when using hbase\" on hbase-user@.\n", "project": "Apache", "sub_project": "HBASE", "version": "0.1.0", "fixed_version": "0.1.2", "fixed_files": ["org.apache.hadoop.hbase.mapred.GroupingTableMap.java", "org.apache.hadoop.hbase.mapred.IdentityTableMap.java"], "label": 1, "es_results": []}, {"bug_id": 590, "bug_title": "HBase migration tool does not get correct FileSystem or root directory if configuration is not correct.", "bug_description": "The HBase migration tool does not validate hbase.rootdir as a valid URI that contains a scheme (e.g., file:// or hdfs://) and fails to find the root directory and the file system if hbase.rootdir is not a URI.", "project": "Apache", "sub_project": "HBASE", "version": "0.1.1", "fixed_version": "0.1.2", "fixed_files": ["org.apache.hadoop.hbase.util.Migrate.java"], "label": 1, "es_results": []}, {"bug_id": 805, "bug_title": "Remove unnecessary getRow overloads in HRS", "bug_description": "HRS currently contains:\n  public RowResult getRow(final byte [] regionName, final byte [] row, final long ts)\n  public RowResult getRow(final byte [] regionName, final byte [] row, final byte [][] columns)\n  public RowResult getRow(final byte [] regionName, final byte [] row, final byte [][] columns, final long ts)\nThe first two call the last one which calls HR.getFull.\nChanges will be made to HTable to map all getRow calls to a single getRow HRS method.", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegionServer.java", "org.apache.hadoop.hbase.ipc.HRegionInterface.java", "org.apache.hadoop.hbase.client.HTable.java"], "label": 1, "es_results": []}, {"bug_id": 811, "bug_title": "HTD is not fully copyable", "bug_description": "Part of my HBASE-62 patch was not applied. ", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.HColumnDescriptor.java", "org.apache.hadoop.hbase.client.UnmodifyableHTableDescriptor.java", "org.apache.hadoop.hbase.HTableDescriptor.java"], "label": 1, "es_results": []}, {"bug_id": 729, "bug_title": "client region/metadata cache should have a public method for invalidating entries", "bug_description": "While writing a testcase for HBASE-62, I observed that table metadata is cached as part of the region information cached  client side. This cached region information (and therefore table metadata) is not directly invalidated by disable/enable table, so to get up to date metadata the client may have to use a scanner over .META. directly using the meta visitor. Ideally other client code  for example the support for HBASE-62  should be able to invalidate entries as necessary, so then the next HTable.getTableDescriptor() would go to meta to return up to date information instead of incorrectly reusing outdated information from the cache.", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.client.HConnectionManager.java", "org.apache.hadoop.hbase.client.TestHTable.java"], "label": 1, "es_results": []}, {"bug_id": 819, "bug_title": "Remove DOS-style ^M carriage returns from all code where found", "bug_description": "There are a few files that contain DOS-style carriage returns.  This is leading to issues when applying patches.\nThe presence of these may also be causing a snowball effect as some IDEs/editors may see one and attempt to apply that LF/CR format to all lines or files.", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.ipc.HRegionInterface.java", "org.apache.hadoop.hbase.client.TestBatchUpdate.java", "org.apache.hadoop.hbase.client.HConnectionManager.java", "org.apache.hadoop.hbase.HConstants.java", "org.apache.hadoop.hbase.client.HBaseAdmin.java", "org.apache.hadoop.hbase.client.HTable.java"], "label": 1, "es_results": []}, {"bug_id": 812, "bug_title": "Compaction needs little better skip algo", "bug_description": "Looking at this section of one of my compaction&apos;s we have 3 files to compact the new algo is working great in my test but I see this below often we are skipping 2 out of the 3 files and compacting 1 file. 1 file is kind of a wast might as well just copy the file my suggestion is if there is only 1 file left after the new algo skips then just go on to the next column and skip the last file also. This will help improve compaction times a little more. \n\n2008-08-10 10:00:45,310 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 1339600874/size: 4.6m, skipped 2, 4851776\n2008-08-10 10:00:45,438 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 1 files into /hbase/webdata/compaction.dir/1339600874/size/mapfiles/8653208152776334891\n2008-08-10 10:00:46,838 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/1339600874/size/mapfiles/8653208152776334891 to /hbase/webdata/1339600874/size/mapfiles/7539342470259528578\n2008-08-10 10:00:47,166 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 1339600874/size store size is 4.6m\n\n", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HStore.java"], "label": 1, "es_results": []}, {"bug_id": 831, "bug_title": "committing BatchUpdate with no row should complain", "bug_description": "Running this code:\nBatchUpdate update = new BatchUpdate();\nupdate.put(key, value);\ntable.commit(update);\nDown in getRegionServer, this triggers an NPE because the row is null (which I saw because I was running in a debugger); this NPE gets retried somewhere in the bowels of IPC.  Instead, we should either remove the zero-arg BatchUpdate constructor, or have table.commit throw a runtimeexception if the row is null.", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegionServer.java", "org.apache.hadoop.hbase.client.HTable.java"], "label": 1, "es_results": []}, {"bug_id": 830, "bug_title": "Debugging HCM.locateRegionInMeta is painful", "bug_description": "I&apos;ve been debugging a case where a bunch of reduces were hanging for no apparent reason and then get killed because they did not do anything for 600 seconds. I figured that it&apos;s because we are stuck in a very long waiting time due to retry backoffs. \n\npublic static int RETRY_BACKOFF[] = { 1, 1, 1, 1, 2, 4, 8, 16, 32, 64 };\n\n\nThat means we wait 10 sec, 10 sec, 10, 10, ... then 640 sec. That&apos;s a long time, do we really need that much time to finally be warned that there&apos;s a bug in HBase? \nAlso, the places where we get this:\n\nLOG.debug(\"reloading table servers because: \" + t.getMessage());\n\n\nshould be more verbose. I my logs these are caused by a table not found but the only thing I see is \"reloading table servers because: tableName\".", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.client.HConnectionManager.java", "org.apache.hadoop.hbase.HConstants.java"], "label": 1, "es_results": []}, {"bug_id": 810, "bug_title": "Prevent temporary deadlocks when, during a scan with write operations, the region splits", "bug_description": "HBASE-804 was not about the good problem, this one is. Anyone that iterates through the results of a scanner and that rewrites data back into the row at each iteration will hit a UnknownScannerException if a split occurs. See the stack in the referred jira. Timeline :\nSplit occurs, acquires a write lock and waits for scanners to finish\nThe scanner in the custom code iterates and writes data until the write is blocked by the lock\ndeadlock\nThe scanner timeouts thus the region splits but the USE will be thrown when next() is called\nInside a Map, the task will simply be retried when the first one fails. Elsewhere, it becomes more complicated.", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.StoreFileScanner.java", "org.apache.hadoop.hbase.regionserver.TestSplit.java", "org.apache.hadoop.hbase.regionserver.HRegion.java"], "label": 1, "es_results": []}, {"bug_id": 768, "bug_title": "[Migration] This message 'java.io.IOException: Install 0.1.x of hbase and run its migration first' is useless", "bug_description": "You&apos;ll see above message after you&apos;ve committed to a new version of hadoop.  You won&apos;t be able to go back.", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.util.Migrate.java"], "label": 1, "es_results": []}, {"bug_id": 860, "bug_title": "IndexTableReduce does not write the column name as the lucene index field properly.", "bug_description": "Instead of using the table column name as the field in the lucene index, the byte array jvm object id is written to the lucene index.  i.e.  [B@234DE3 instead of \"myColFamily:myCol\"\nIn the class IndexTableReduce, essentially one line of code needs to be changed as far as i can see to fix this issue.  I will be submitting a patch here within the hour.", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.mapred.IndexTableReduce.java"], "label": 1, "es_results": []}, {"bug_id": 881, "bug_title": "If a server's lease times out or the server dies, All regions will get reassigned even split or offline ones.", "bug_description": "If a server&apos;s lease times out or a server dies (essentially the same thing), when the master tries to find the regions it was serving, it does not check to see if the region has been offlined or split.\nIn ProcessServerShutdown.scanMetaRegion, the code:\n\n        } else {\n          // Get region reassigned\n          regions.add(info);\n        }\n\n\nshould be:\n\n        } else {\n          if (!info.isOffline() && !info.isSplit()) {\n            // Get region reassigned\n            regions.add(info);\n          }\n        }\n\n", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "release-0.2.1", "fixed_files": ["org.apache.hadoop.hbase.master.ProcessServerShutdown.java"], "label": 1, "es_results": []}, {"bug_id": 891, "bug_title": "HRS.validateValuesLength throws IOE, gets caught in the retries", "bug_description": "When HRS.validateValuesLength throws a IOE, it gets caught in the retries because it does not use a DoNotRetryIOException.", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.1", "fixed_version": "0.18.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegionServer.java"], "label": 1, "es_results": []}, {"bug_id": 912, "bug_title": "PE is broken when other tables exist", "bug_description": "The iteration in checkTable is broken.\n\n      for (int i = 0; i < extantTables.length; i++) {\n        if (extantTables[0].equals(tableDescriptor)) {\n          LOG.warn(\"Table \" + tableDescriptor + \" already exists\");\n          tableExists = true;\n          break;\n        }\n      }\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.18.0", "fixed_version": "0.19.0", "fixed_files": ["org.apache.hadoop.hbase.PerformanceEvaluation.java"], "label": 1, "es_results": []}, {"bug_id": 1079, "bug_title": "Dumb NPE in ServerCallable hides the RetriesExhausted exception.", "bug_description": "From the list (and this is something I&apos;ve already seen) :\nNativeException: java.lang.NullPointerException: null\n       from org/apache/hadoop/hbase/client/ServerCallable.java:71:in `getRegio\nName&apos;\n       from org/apache/hadoop/hbase/client/HConnectionManager.java:863:in `get\negionServerWithRetries&apos;\n       from org/apache/hadoop/hbase/client/MetaScanner.java:56:in `metaScan&apos;\n       from org/apache/hadoop/hbase/client/MetaScanner.java:30:in `metaScan&apos;\n       from org/apache/hadoop/hbase/client/HConnectionManager.java:297:in `lis\nTables&apos;\n       from org/apache/hadoop/hbase/client/HBaseAdmin.java:117:in `listTables&apos;\nThis is ", "project": "Apache", "sub_project": "HBASE", "version": "0.18.1", "fixed_version": "0.19.0", "fixed_files": ["org.apache.hadoop.hbase.client.ServerCallable.java"], "label": 1, "es_results": []}, {"bug_id": 1148, "bug_title": "Always flush HLog on root or meta region updates", "bug_description": "Flushing an HLog does not currently guarantee that the updates will be visible (see HADOOP-4379), however in the case of root or meta region updates, this is critical.\nI was able to create a situation by killing both the root and meta region servers, from which the cluster recovered, but because of the missed edits, clients found the old parent region rather than the new child regions because the fact that the parent region had split was not in the HLog of the crashed region servers (the master knew because of the MSG_REGION_SPLIT message it received) but the clients read the meta table and because that change was lost, clients were trying to find the parent region.\nSo, when a SequenceFile.Writer.sync() guarantees that what has been written will be visible to new readers, we need to modify HLog so that if it is writing an update to the root or meta regions, that it immediately flushes (syncs) the log file so that the changes will be visible when the log file is recovered.\n", "project": "Apache", "sub_project": "HBASE", "version": "0.19.0", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HLog.java", "org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.regionserver.TestHLog.java"], "label": 1, "es_results": []}, {"bug_id": 1190, "bug_title": "TableInputFormatBase with row filters scan too far ", "bug_description": "When TableInputFormatBase has a non-null RowFilterInterface to apply, it creates combines the row filter with a StopRowFilter to get a scanner for each input split.  However, the StopRowFilter never indicates that fitlerAllRemaining is true, so each input split will end up scanning to the end of the table.  (Contrast with HTable.getScanner(byte[][] columns, byte[] starRow, byte[] stopRow, long timestamp) which uses a StopRowFilter wrapped in a WhileMatchRowFilter to ensure that scanning ends at the stop row.", "project": "Apache", "sub_project": "HBASE", "version": "0.19.0", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.mapred.TableInputFormatBase.java"], "label": 1, "es_results": []}, {"bug_id": 1142, "bug_title": "Cleanup thrift server; remove Text and profuse DEBUG messaging", "bug_description": "Ambiguous issue name.. sorry.\nThe thrift server has loads of getText(..) calls. Which is a local function that checks for utf8 compliance, we don&apos;t need them anywhere, because we don&apos;t use Text anymore.\nThere is probably other things we missed last time we updated the api, that we should also clean up while we&apos;re at it. Open to suggestions.", "project": "Apache", "sub_project": "HBASE", "version": "0.18.0", "fixed_version": "0.19.1", "fixed_files": ["org.apache.hadoop.hbase.thrift.ThriftServer.java", "org.apache.hadoop.hbase.thrift.ThriftUtilities.java"], "label": 1, "es_results": []}, {"bug_id": 1185, "bug_title": "wrong request/sec in the gui reporting wrong", "bug_description": "I am seeing lower number of request in the masters gui then I have seen in 0.18.0 while scanning.\nI thank part of it is we moved to report per sec request not per 3 secs so the request should be 1/3 of the old numbers I was getting.\nhbase.client.scanner.caching is not the reason the request are under reported.\nI set hbase.client.scanner.caching = 1 and still get about 2K request a sec in the gui\nbut when the job is done I take records / job time and get 36,324/ records /sec. So\nthere must be some caching out side of the hbase.client.scanner.caching making the\nrequest per sec lower then it should be. I know it running faster then reported just thought\nit might give some new users the wrong impression that request/sec = read/write /sec.", "project": "Apache", "sub_project": "HBASE", "version": "0.19.0", "fixed_version": "0.19.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegionServer.java"], "label": 1, "es_results": []}, {"bug_id": 1238, "bug_title": "Under upload, region servers are unable to compact when loaded with hundreds of regions", "bug_description": "We have a situation where each region server is loaded with 100+ regions, most of them in the same table. During a long upload of webpages, each memcache gets filled near equally fast so that the global memcache limit is usually triggered before the max memcache size. Since that emergency flush does not trigger compactions, the number of store files just keeps growing until it fails on all kinds of errors.\nWe need a better story for this as this is a \"normal\" situation.", "project": "Apache", "sub_project": "HBASE", "version": "0.19.0", "fixed_version": "0.19.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.MemcacheFlusher.java"], "label": 1, "es_results": []}, {"bug_id": 1251, "bug_title": "HConnectionManager.getConnection(HBaseConfiguration) returns same HConnection for different HBaseConfigurations ", "bug_description": "This occurs when the following happens:\n1. Consider a client that invokes HBaseAdmin.checkHBaseAvailable(config) before doing anything. Although this method copies the HBaseConfiguration object and sets hbase.client.retries.number to 1 (see HBaseAdmin, line 751), it creates an HBaseAdmin object, which invokes HConnectionManager.getConnection(conf). Please notice that this conf is that with hbase.client.retries.number equals to 1. \n2. HConnectionManager.getConnection then creates a HConnection using this conf and puts it into a static map (see HConnectionManager, line 93) indexed by hbase.rootdir. \n3. Then, if the same client now creates a HTable object (using, for instance, a HBaseConfiguration with  hbase.client.retries.number equals to 10 but the same hbase.rootdir), it will invoke HConnectionManager.getConnection(conf) again (see HTable, line 109). However, when it checks the static map for a HConnection it finds one - the one previously created by the HBaseAdmin object and using hbase.client.retries.number 1 - and returns it without creating a new one with the correct HBaseConfiguration.\nHowever, the expected behavior is: HConnectionManager must return different HConnections for different HBaseConfigurations.  ", "project": "Apache", "sub_project": "HBASE", "version": "0.19.0", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.client.HConnectionManager.java", "org.apache.hadoop.hbase.HBaseConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 1258, "bug_title": "ganglia metrics for 'requests' is confusing", "bug_description": "the &apos;requests&apos; metric is incremented for every request, but it is reset and published every interval.  Which means the number is actually &apos;requests per interval&apos; which is a config value in hbase.  \nHBase should export &apos;requests/second&apos; instead.\n", "project": "Apache", "sub_project": "HBASE", "version": "0.19.0", "fixed_version": "0.19.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java"], "label": 1, "es_results": []}, {"bug_id": 1277, "bug_title": "HStoreKey: Wrong comparator logic", "bug_description": "During fixing fail of TestCompaction JUnit was found error in removing of row Cells. Reason was a error in comparator logic of HStoreKey.\nFix of HStoreKey also fixed removing of row Cell and and TestCompaction.", "project": "Apache", "sub_project": "HBASE", "version": "0.19.1", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.master.HMaster.java", "org.apache.hadoop.hbase.HStoreKey.java"], "label": 1, "es_results": []}, {"bug_id": 1202, "bug_title": "getRow does not always work when specifying number of versions", "bug_description": "When a cell that exists is updated, getRow specifying number of versions does not work.\nWhat is returned is the original value at that timestamp, instead of the updated value.\nNote that this only applies when more than one version is specified. getRow with (implied) timestamp = latest does work.", "project": "Apache", "sub_project": "HBASE", "version": "0.19.0", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.io.Cell.java"], "label": 1, "es_results": []}, {"bug_id": 1301, "bug_title": "HTable.getRow() returns null if the row does no exist", "bug_description": "The HBase API docs says when the row does not exist, getRow() returns\n    RowResult is empty if row does not exist. \nHowever, in regionserver/HRegionServer.java&apos;s getRow():\n      if (result == null || result.isEmpty())\n        return null;\n      return new RowResult(row, result);\nIt actually returns null. Either fix the code or the document.", "project": "Apache", "sub_project": "HBASE", "version": "0.19.0", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.client.HTable.java"], "label": 1, "es_results": []}, {"bug_id": 1365, "bug_title": "Typo in TableInputFormatBase.setInputColums", "bug_description": "Typo in method name.  TableInputFormatBase.getInputColums should be .getInputColumns.", "project": "Apache", "sub_project": "HBASE", "version": "0.19.1", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.mapred.TableInputFormat.java", "org.apache.hadoop.hbase.mapred.TableInputFormatBase.java"], "label": 1, "es_results": []}, {"bug_id": 1279, "bug_title": "Fix the way hostnames and IPs are handled", "bug_description": "From the list by Yabo-Arber Xu,\n\nYes, I&apos;ve unlocked the port, and i am actually able to access from the web\nUI with a client not running on EC2 to HBase at example.com:60010. It shows\nall User Tables, but the Region Servers Address is the EC2 internal address:\ndomU-12-31-39-00-65-E5.compute-1.internal:60020.\nI guess the client fails because it can not connect region server, which\nserves only for an internal IP. However, in hbase-site.xml, I did configure\nwith region server explicitly in its external IP.\n<property>\n   <name>hbase.regionserver</name>\n   <value>ec2-67-202-57-127.compute-1.amazonaws.com:60020</value>\n   <description>The host and port a HBase region server runs at.\n   </description>\n </property>\nIn fact we completely bypass the hostname set in hbase.regionserver, also the hostnames in the web UI are not the good ones. We should do that part like hadoop does.", "project": "Apache", "sub_project": "HBASE", "version": "0.19.1", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.TestSerialization.java", "org.apache.hadoop.hbase.regionserver.HRegionServer.java", "org.apache.hadoop.hbase.HServerInfo.java", "org.apache.hadoop.hbase.master.HMaster.java"], "label": 1, "es_results": []}, {"bug_id": 889, "bug_title": "The current Thrift API does not allow a new scanner to be created without supplying a column list unlike the other APIs.", "bug_description": "The current Thrift API does not allow a new scanner to be created without supplying a column list, unlike the REST api. I posted this on the HBase-Users mailing list. Others concurred that it appears to have been an oversight in the Thrift API. \nIts quite significant as there is no easy work around, unless you already know which the column families names then list them all when you open the scanner.", "project": "Apache", "sub_project": "HBASE", "version": "release-0.2.0", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.thrift.ThriftServer.java"], "label": 1, "es_results": []}, {"bug_id": 1431, "bug_title": "NPE in HTable.checkAndSave when row does not exist", "bug_description": "To reproduce, just invoke htable.checkAndSave(batchUpdate, expectedValues, lock) using a batchUpdate of a row that doesn&apos;t exist. \n", "project": "Apache", "sub_project": "HBASE", "version": "0.19.2", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.client.TestHTable.java"], "label": 1, "es_results": []}, {"bug_id": 1644, "bug_title": "Result.row is cached in getRow; this breaks MapReduce", "bug_description": "In Result#getRow row field is computed (if row is null) and then is cached for further uses. But since MapReduce uses the same Result instance through different map()/reduce() calls, row field is not updated when Result instance changes.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.95.1", "fixed_files": ["org.apache.hadoop.hbase.client.Result.java", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java"], "label": 1, "es_results": []}, {"bug_id": 1604, "bug_title": "HBaseClient.getConnection() may return a broken connection without throwing an exception", "bug_description": "Consider the code of HBaseClient.getConnection():\n\n\n\n    connection.setupIOstreams();\n\n    return connection;\n\n  }\n\n\n\nNow consider the setupIOstreams() method:\n\n\n\n      } catch (IOException e) {\n\n        markClosed(e);\n\n        close(); // Removes the connection from pool\n\n      }\n\n\n\nSo, if something goes wrong inside of setupIOstreams, then after its invocation the connection will be broken (will have its .in and .out streams nulls, for example) and will not be in pool, but will still be returned from the getConnection method and because further harm (for example, because a NullPointerException in further calls such as sendCall, which use the in and out streams).\nSuggested fix: make the setupIOstreams method rethrow the IOException inside that catch block.\nReproduction: Restart the hbase master and/or regionserver while a client program is running, and put a breakpoint into that catch block.\nI actually observed a situation where the broken connection stayed in the pool, but I don&apos;t yet know how to reproduce it or what is the reason. I am investigating the issue, but for now at least the aforementioned bug should be fixed.", "project": "Apache", "sub_project": "HBASE", "version": "0.19.2", "fixed_version": "0.20.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.HBaseClient.java"], "label": 1, "es_results": []}, {"bug_id": 1791, "bug_title": "Timeout in IndexRecordWriter", "bug_description": "A MapReduce job to generate Lucene Indexes from HBase will fail on sufficiently large tables. After the indexing finished, the close() method of IndexRecordWriter is called.  The  writer.optimize() call in this method can take many minutes, forcing most MapReduce tasks to timeout. There is a HeartBeatsThread, but it does not seem to send progress updates. \nA suggested fix may be to add context.progress(); in the HeardbeatsThread run() method, after the context.setStatus call. Not sure why context.setStatus is not \"good enough\". ", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.IndexRecordWriter.java"], "label": 1, "es_results": []}, {"bug_id": 1795, "bug_title": "log recovery does not reset the max sequence id, new logfiles can get tossed as 'duplicates'", "bug_description": "during log recovery, we do not reset Store.maxSeqId, thus new log entries are stamped starting off from the old files.  This can cause a problem if we fail and recover again, since the new mutations are deemed \"old\" and should not be applied in a subsequent recovery scenario.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.Store.java"], "label": 1, "es_results": []}, {"bug_id": 1794, "bug_title": "recovered log files are not inserted into the storefile map", "bug_description": "after a log recovery, the resulting flushed file is not introduced into the store.storefiles map. The new data is not available until the region is closed or compacted.\n", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.Store.java"], "label": 1, "es_results": []}, {"bug_id": 1740, "bug_title": "ICV has a subtle race condition only visible under high load", "bug_description": "ICV demonstrates a race condition under high load.  The result is a duplicate KeyValue with the same timestamp, at first in the memcache, and in hfile, then both in hfile.  The get/scan code does not know which one to read, and picks one arbitrarily.  One of the keyvalues is correct, one is incorrect.\nWhat happens at a deeper level:\n\nwe start an ICV\na snapshot happens and moves the memstore to the snapshot\nthe ICV code puts a key-value into memstore that has the same timestamp as a keyvalue in the snapshot.\n\nThis is a deep race condition and several attempts to fix it failed in production here at SU.  This issue is about a more permanent fix.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestHRegion.java", "org.apache.hadoop.hbase.regionserver.TestStore.java", "org.apache.hadoop.hbase.regionserver.MemStore.java", "org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.regionserver.Store.java"], "label": 1, "es_results": []}, {"bug_id": 1828, "bug_title": "CompareFilters are broken from client-side", "bug_description": "Some filters pass region-level tests but seem to freeze client-side.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.client.TestClient.java", "org.apache.hadoop.hbase.filter.CompareFilter.java"], "label": 1, "es_results": []}, {"bug_id": 1574, "bug_title": "Client and server APIs to do batch deletes.", "bug_description": "in 880 there is no way to do a batch delete (anymore?).  We should add one back in.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.ipc.HRegionInterface.java", "org.apache.hadoop.hbase.client.TestClient.java", "org.apache.hadoop.hbase.client.HConnection.java", "org.apache.hadoop.hbase.client.Delete.java", "org.apache.hadoop.hbase.io.HbaseObjectWritable.java", "org.apache.hadoop.hbase.client.HConnectionManager.java", "org.apache.hadoop.hbase.client.Put.java", "org.apache.hadoop.hbase.regionserver.HRegionServer.java", "org.apache.hadoop.hbase.client.HTable.java"], "label": 1, "es_results": []}, {"bug_id": 1847, "bug_title": "Delete latest of a null qualifier when non-null qualifiers exist throws a RuntimeException", "bug_description": "Bug in delete latest code when deleting the null qualifier column when other columns also exist.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.client.TestClient.java", "org.apache.hadoop.hbase.regionserver.HRegion.java"], "label": 1, "es_results": []}, {"bug_id": 1815, "bug_title": "HBaseClient can get stuck in an infinite loop while attempting to contact a failed regionserver", "bug_description": "While using HBase Thrift server, if a regionserver goes down due to shutdown or failure clients will timeout because the thrift server cannot contact the dead regionserver.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.client.HConnectionManager.java", "org.apache.hadoop.hbase.client.RetriesExhaustedException.java", "org.apache.hadoop.hbase.ipc.HBaseClient.java", "org.apache.hadoop.hbase.regionserver.HRegionServer.java", "org.apache.hadoop.hbase.ipc.HBaseRPC.java"], "label": 1, "es_results": []}, {"bug_id": 1857, "bug_title": "WrongRegionException when setting region online after .META. split", "bug_description": "After splitting .META. when updating region information in .META. (e.g. ProcessRegionOpen) the wrong .META. region was retrieved in RegionManager from onlineMetaRegions map. \nThis is due to a bug in RegionManager.getFirstMetaRegionForRegion that was using the wrong key to get data out of the map (the table name instead of the region name) \nreturn onlineMetaRegions.get(onlineMetaRegions.headMap(newRegion.getTableDesc().getName()).lastKey());\nand when adding the region to the map it was added with \nonlineMetaRegions.put(metaRegion.getStartKey(), metaRegion);\nso it&apos;s supposed to be taken out with: \nreturn onlineMetaRegions.get(onlineMetaRegions.headMap(newRegion.getRegionName()).lastKey());", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.master.RegionManager.java"], "label": 1, "es_results": []}, {"bug_id": 1865, "bug_title": "0.20.0 TableInputFormatBase NPE", "bug_description": "Spot the bug in this code:\npublic List<InputSplit> getSplits(JobContext context) throws IOException {\n    byte [][] startKeys = table.getStartKeys();\n    if (startKeys == null || startKeys.length == 0) \n{\n\n      throw new IOException(\"Expecting at least one region.\");\n\n    }\n    if (table == null) \n{\n\n      throw new IOException(\"No table was provided.\");\n\n    }\n...\n}\nShould check if the table is null before calling a method on it.\nAdmittedly, this isn&apos;t the worst bug in the world, it&apos;s really just more of a nuisance in that the \"No table was provided\" message becomes an NPE\nThis bug is in both\norg.apache.hadoop.hbase.mapred.TableInputFormatBase\norg.apache.hadoop.hbase.mapreduce.TableInputFormatBase", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java"], "label": 1, "es_results": []}, {"bug_id": 1866, "bug_title": "Scan(Scan) copy constructor does not copy value of cacheBlocks", "bug_description": "", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.client.Scan.java"], "label": 1, "es_results": []}, {"bug_id": 1871, "bug_title": "Wrong type used in TableMapReduceUtil.initTableReduceJob()", "bug_description": "Since we changed it so that TableOutputFormat can handle Put and Delete it is necessary to set the output value class to Writable.\n\n\n\n  public static void initTableReducerJob(String table,\n\n    Class<? extends TableReducer> reducer, Job job, Class partitioner)\n\n  throws IOException {\n\n    job.setOutputFormatClass(TableOutputFormat.class);\n\n    if (reducer != null) job.setReducerClass(reducer);\n\n    job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, table);\n\n    job.setOutputKeyClass(ImmutableBytesWritable.class);\n\n    job.setOutputValueClass(Put.class);\n\n   ....\n\n\n\nThe last line should be \n\n\n\n    job.setOutputValueClass(Writable.class);\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java"], "label": 1, "es_results": []}, {"bug_id": 1883, "bug_title": "HRegion passes the wrong minSequenceNumber to doReconstructionLog", "bug_description": "HRegion initializes by opens up all store files which may recover from the WAL. It then calls protected doReconstructionLog which THBase uses to go through the log and look for pending transactions that may need to be recovered. Currently HRegion is passing down the minSequenceNumber after WAL recovery. What we want is the lowest sequence number before the wal recovery.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.Store.java", "org.apache.hadoop.hbase.regionserver.HRegion.java"], "label": 1, "es_results": []}, {"bug_id": 1879, "bug_title": "ReadOnly transactions generate WAL activity.", "bug_description": "Currently we write a start entry in the WAL each time a transaction starts, and a commit/abort at the end of the transaction. This means read-only transactions unnecessarily generate two WAL entries per region.\nCan avoid this by removing the start entry from the WAL (this is implicit in the first trx OP entry we see), and only writing commit/abort when the transaction has a write.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion.java", "org.apache.hadoop.hbase.regionserver.transactional.TestTHLog.java", "org.apache.hadoop.hbase.regionserver.transactional.THLog.java", "org.apache.hadoop.hbase.regionserver.transactional.THLogRecoveryManager.java", "org.apache.hadoop.hbase.regionserver.transactional.THLogKey.java"], "label": 1, "es_results": []}, {"bug_id": 1831, "bug_title": "Scanning API must be reworked to allow for fully functional Filters client-side", "bug_description": "Right now, a client replays part of the Filter locally by calling filterRowKey() and filterAllRemaining() to determine whether it should continue to the next region.\nA number of new filters rely on filterKeyValue() and other calls to alter state.  It&apos;s also a false assumption that all rows/keys affecting a filter returning true for FAR will be seen client-side (what about those that failed the filter).\nThis issue is about dealing with Filters properly from the client-side.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.20.1", "fixed_files": ["org.apache.hadoop.hbase.ipc.HRegionInterface.java", "org.apache.hadoop.hbase.regionserver.HRegionServer.java", "org.apache.hadoop.hbase.client.TestClient.java", "org.apache.hadoop.hbase.regionserver.TestHRegion.java", "org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.filter.BinaryComparator.java", "org.apache.hadoop.hbase.client.HConnectionManager.java", "org.apache.hadoop.hbase.filter.package-info.java", "org.apache.hadoop.hbase.client.ScannerCallable.java", "org.apache.hadoop.hbase.client.HBaseAdmin.java", "org.apache.hadoop.hbase.client.HTable.java"], "label": 1, "es_results": []}, {"bug_id": 1896, "bug_title": "WhileMatchFilter.reset should call encapsulated filter reset  ", "bug_description": "Bumped into this when trying to encapsulate a SingleValueColumnFilter in a WhileMatchFilter. \nA scanner would grab all the rows after the first matched row in the table ", "project": "Apache", "sub_project": "HBASE", "version": "0.20.1", "fixed_version": "0.90.0", "fixed_files": ["org.apache.hadoop.hbase.filter.WhileMatchFilter.java"], "label": 1, "es_results": []}, {"bug_id": 1927, "bug_title": "Scanners not closed properly in certain circumstances (memory leak)", "bug_description": "Scanners are sometimes leaked by the KeyValueHeap class. The constructor adds each scanner to a heap, but only if the scanner&apos;s peek() method returns not null (line 58). Otherwise the scanner is dropped without being closed.\nUnfortunately some scanners (like StoreScanner and MemStoreScanner) register themselves to some global list when constructed and only deregister on close(). This can cause a memory leak, for example with MemStoreScanners on an empty memory store.\nThe quick fix is to add an else clause to the if on line 58:\n} else \n{\n\n  scanner.close()\n\n}\n\nThe root because is that ownership of the scanners is transferred from the caller to the KeyValueHeap on construction. Maybe this should be made clear in the documentation or changed.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.1", "fixed_version": "0.90.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.KeyValueHeap.java", "org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.java"], "label": 1, "es_results": []}, {"bug_id": 1930, "bug_title": "Put.setTimeStamp misleading (does not change timestamp on existing KeyValues, not copied in copy constructor)", "bug_description": "In the process of migrating some code from 0.19, and was changing BatchUpdate&apos;s to Put&apos;s.  I was hit by a bit of a gotcha.  In the old code, I populated the BatchUpdate, then set the timestamp.  However, this doesn&apos;t wotk for Put, because Put creates KeyValue&apos;s with the currently set timestamp when adding values.  Setting the timestamp at the end has no effect.  Also, the copy constructor doesn&apos;t copy the timestamp (or writeToWAL) setting.\nOne option would be to simply update the javadoc to make it clear that the timestamp needs to be set prior to adding values.  I&apos;m attaching a proposed patch which moves the timestamp setting to constructor only so that it isn&apos;t possible to trigger the confusing case at all.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.90.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestScanner.java", "org.apache.hadoop.hbase.HBaseTestCase.java", "org.apache.hadoop.hbase.TimestampTestBase.java", "org.apache.hadoop.hbase.client.TestGetRowVersions.java", "org.apache.hadoop.hbase.thrift.ThriftServer.java", "org.apache.hadoop.hbase.client.Put.java", "org.apache.hadoop.hbase.TestScanMultipleVersions.java"], "label": 1, "es_results": []}, {"bug_id": 1919, "bug_title": "code: HRS.delete seems to ignore exceptions it should not", "bug_description": "the code is:\n      region.delete(delete, lid, writeToWAL);\n      this.hlog.sync(region.getRegionInfo().isMetaRegion());\n    } catch (WrongRegionException ex) {\n    } catch (NotServingRegionException ex) \n{\n\n      // ignore                                                                                                                                                                          \n\n    }\n catch (Throwable t) \n{\n\n      throw convertThrowableToIOE(cleanup(t));\n\n    }\n\nwe ignore those 2 exceptions... weird... should not be!", "project": "Apache", "sub_project": "HBASE", "version": "0.20.1", "fixed_version": "0.90.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegionServer.java"], "label": 1, "es_results": []}, {"bug_id": 1957, "bug_title": "Get-s cannot set a Filter", "bug_description": "This is an issue directly related to HBASE-1646. Get#write and Get#readFields both use  HbaseObjectWritable to write filters and when it comes to custom filters or filters in general that are not hardcoded in HbaseObjectWritable , an exception is thrown. \nIt has been fixed in the issue noted above for Scan. Attached patch fixes it fot Get too.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.90.0", "fixed_files": ["org.apache.hadoop.hbase.client.Get.java"], "label": 1, "es_results": []}, {"bug_id": 1979, "bug_title": "MurmurHash does not yield the same results as the reference C++ implementation when size % 4 >= 2", "bug_description": "Last rounds of MurmurHash are done in reverse order. data[length - 3], data[length - 2] and data[length - 1] in the block processing the remaining bytes should be data[len_m +2], data[len_m + 1], data[len_m].", "project": "Apache", "sub_project": "HBASE", "version": "0.20.1", "fixed_version": "0.90.0", "fixed_files": ["org.apache.hadoop.hbase.util.MurmurHash.java"], "label": 1, "es_results": []}, {"bug_id": 2027, "bug_title": "HConnectionManager.HBASE_INSTANCES leaks TableServers", "bug_description": "HConnectionManager.HBASE_INSTANCES is a WeakHashMap from HBaseConfiguration to TableServers.  However, each TableServers has a strong reference back to the HBaseConfiguration key so they are never freed.  (See note at http://java.sun.com/javase/6/docs/api/java/util/WeakHashMap.html : \"Implementation note: The value objects in a WeakHashMap are held by ordinary strong references. Thus care should be taken to ensure that value objects do not strongly refer to their own keys, either directly or indirectly, since that will prevent the keys from being discarded.\")\nMoreover, HBaseConfiguration implements hashCode() but not equals() so identical HBaseConfiguration objects each get their own TableServers object.\nWe had a long running HBase client process that was creating new HTable() objects, each creating a new HBaseConfiguration() and thus a new TableServers object.  It eventually went OOM, and gave a heap dump indicating 360 MB of data retained by HBASE_INSTANCES.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.90.0", "fixed_files": ["org.apache.hadoop.hbase.client.HConnectionManager.java", "org.apache.hadoop.hbase.HBaseConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 2048, "bug_title": "Small inconsistency in the \"Example API Usage\"", "bug_description": "The example uses \"myLittleRow\" but refers to \"myRow\" in one of the comments.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.2", "fixed_version": "0.90.0", "fixed_files": ["org.apache.hadoop.hbase.client.package-info.java"], "label": 1, "es_results": []}, {"bug_id": 2280, "bug_title": "HFileOutputFormat writes output to \"unsafe\" directory", "bug_description": "HFileOutputFormat writes data direct to output folder. It&apos;s incorrect as failed (or killed, or interrupted) reducers leaves inconsistent files in output folder.\nThe convinient way to ouput data from OutputFormat is to use \"working directory\". The content of this directory is being moved to output directory at the end of reducer process if only reducer succeeded (this process is called \"output commit\" or \"atomic commit\").\nIf means that instead of\n final Path outputdir = FileOutputFormat.getOutputPath(context);\nhbase should use\n final Path outputdir = FileOutputFormat.getWorkOutputPath(context);", "project": "Apache", "sub_project": "HBASE", "version": "0.20.3", "fixed_version": "0.90.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 2023, "bug_title": "Client sync block can cause 1 thread of a multi-threaded client to block all others", "bug_description": "Take a highly multithreaded client, processing a few thousand requests a second.  If a table goes offline, one thread will get stuck in \"locateRegionInMeta\" which is located inside the following sync block:\n        synchronized(userRegionLock)\n{\n\n          return locateRegionInMeta(META_TABLE_NAME, tableName, row, useCache);\n\n        }\n\nSo when other threads need to find a region (EVEN IF ITS CACHED!!!) it will encounter this sync and wait. \nThis can become an issue on a busy thrift server (where I first noticed the problem), one region offline can prevent access to all other regions!\nPotential solution: narrow this lock, or perhaps just get rid of it completely.", "project": "Apache", "sub_project": "HBASE", "version": "0.20.2", "fixed_version": "0.90.0", "fixed_files": ["org.apache.hadoop.hbase.client.HConnectionManager.java"], "label": 1, "es_results": []}, {"bug_id": 1888, "bug_title": "KeyValue methods throw NullPointerException instead of IllegalArgumentException during parameter sanity check", "bug_description": "Methods of org.apache.hadoop.hbase.KeyValue\npublic static int getDelimiter(final byte [] b, int offset, final int length, final int delimiter)\npublic static int getDelimiterInReverse(final byte [] b, final int offset, final int length, final int delimiter)\nthrow NullPointerException instead of IllegalArgumentException when byte array b is check for null  - which is very bad practice!\nPlease refactor this because this can be very misleading.  ", "project": "Apache", "sub_project": "HBASE", "version": "0.20.0", "fixed_version": "0.92.0", "fixed_files": ["org.apache.hadoop.hbase.KeyValue.java"], "label": 1, "es_results": []}, {"bug_id": 3636, "bug_title": "a bug about deciding whether this key is a new key for the ROWCOL bloomfilter", "bug_description": "When ROWCOL bloomfilter needs to decide whether this key is a new key or not,\nit will call the matchingRowColumn function, which will compare the timestamp offset between this kv and last kv.\nBut when checking the timestamp offset, it didn&apos;t deduct the original offset of the keyvalue itself.\nFor example, when 2 keyvalue objects have the same row key and col key, but from different storefiles. It is highly likely that these 2 keyvalue objects have different offset value. So the timestamp offset of these 2 objects are totally different. They will be regard as new keys to add into bloomfilters.\nSo after compaction, the key count of bloomfilter will increase immediately, which is almost equal to the number of entries.\nThe solution is straightforward. Just compare the relevant timestamp offset, which is the timestamp offset - key_value offset.\nThis also may explain this jira: https://issues.apache.org/jira/browse/HBASE-3007", "project": "Apache", "sub_project": "HBASE", "version": "0.89.20100621", "fixed_version": "0.90.2", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestHRegion.java", "org.apache.hadoop.hbase.KeyValue.java"], "label": 1, "es_results": []}, {"bug_id": 3497, "bug_title": "TableMapReduceUtil.initTableReducerJob broken due to setConf method in TableOutputFormat", "bug_description": "setConf() method in TableOutputFormat gets called and it replaces the hbase.zookeeper.quorum address in the job conf xml when you run a CopyTable job from one cluster to another. The conf gets set to the peer.addr that is specified, which makes the job read and write from/to the peer cluster instead of reading from the original cluster and writing to the peer.\nPossibly caused due to the change in https://issues.apache.org/jira/browse/HBASE-3111", "project": "Apache", "sub_project": "HBASE", "version": "0.90.0", "fixed_version": "0.90.2", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.TableOutputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 4931, "bug_title": "CopyTable instructions could be improved.", "bug_description": "The book and the usage instructions could be improved to include more details, things caveats and to better explain usage.\nOne example in particular, could be updated to refer to ReplicationRegionInterface and ReplicationRegionServer in thier current locations (o.a.h.h.client.replication and o.a.h.h.replication.regionserver), and better explain why one would use particular arguments.\n\n\n\n$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable\n\n--rs.class=org.apache.hadoop.hbase.ipc.ReplicationRegionInterface\n\n--rs.impl=org.apache.hadoop.hbase.regionserver.replication.ReplicationRegionServer\n\n--starttime=1265875194289 --endtime=1265878794289\n\n--peer.adr=server1,server2,server3:2181:/hbase TestTable\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.90.4", "fixed_version": "0.99.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.CopyTable.java"], "label": 1, "es_results": []}, {"bug_id": 5128, "bug_title": "[uber hbck] Online automated repair of table integrity and region consistency problems", "bug_description": "The current (0.90.5, 0.92.0rc2) versions of hbck detects most of region consistency and table integrity invariant violations.  However with &apos;-fix&apos; it can only automatically repair region consistency cases having to do with deployment problems.  This updated version should be able to handle all cases (including a new orphan regiondir case).  When complete will likely deprecate the OfflineMetaRepair tool and subsume several open META-hole related issue.\nHere&apos;s the approach (from the comment of at the top of the new version of the file).\n\n\n\n/**\n\n * HBaseFsck (hbck) is a tool for checking and repairing region consistency and\n\n * table integrity.  \n\n * \n\n * Region consistency checks verify that META, region deployment on\n\n * region servers and the state of data in HDFS (.regioninfo files) all are in\n\n * accordance. \n\n * \n\n * Table integrity checks verify that that all possible row keys can resolve to\n\n * exactly one region of a table.  This means there are no individual degenerate\n\n * or backwards regions; no holes between regions; and that there no overlapping\n\n * regions. \n\n * \n\n * The general repair strategy works in these steps.\n\n * 1) Repair Table Integrity on HDFS. (merge or fabricate regions)\n\n * 2) Repair Region Consistency with META and assignments\n\n * \n\n * For table integrity repairs, the tables their region directories are scanned\n\n * for .regioninfo files.  Each table&apos;s integrity is then verified.  If there \n\n * are any orphan regions (regions with no .regioninfo files), or holes, new \n\n * regions are fabricated.  Backwards regions are sidelined as well as empty\n\n * degenerate (endkey==startkey) regions.  If there are any overlapping regions,\n\n * a new region is created and all data is merged into the new region.  \n\n * \n\n * Table integrity repairs deal solely with HDFS and can be done offline -- the\n\n * hbase region servers or master do not need to be running.  These phase can be\n\n * use to completely reconstruct the META table in an offline fashion. \n\n * \n\n * Region consistency requires three conditions -- 1) valid .regioninfo file \n\n * present in an hdfs region dir,  2) valid row with .regioninfo data in META,\n\n * and 3) a region is deployed only at the regionserver that is was assigned to.\n\n * \n\n * Region consistency requires hbck to contact the HBase master and region\n\n * servers, so the connect() must first be called successfully.  Much of the\n\n * region consistency information is transient and less risky to repair.\n\n */\n\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.90.5", "fixed_version": "0.94.0", "fixed_files": ["org.apache.hadoop.hbase.util.hbck.TestOfflineMetaRebuildHole.java", "org.apache.hadoop.hbase.master.AssignmentManager.java", "org.apache.hadoop.hbase.master.HMaster.java", "org.apache.hadoop.hbase.util.TestHBaseFsck.java", "org.apache.hadoop.hbase.io.hfile.HFile.java", "org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair.java", "org.apache.hadoop.hbase.HBaseTestingUtility.java", "org.apache.hadoop.hbase.util.hbck.TestOfflineMetaRebuildBase.java", "org.apache.hadoop.hbase.ipc.HMasterInterface.java", "org.apache.hadoop.hbase.util.HBaseFsck.java", "org.apache.hadoop.hbase.util.HBaseFsckRepair.java", "org.apache.hadoop.hbase.util.TestHBaseFsckComparator.java", "org.apache.hadoop.hbase.util.hbck.TestOfflineMetaRebuildOverlap.java", "org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.java"], "label": 1, "es_results": []}, {"bug_id": 5970, "bug_title": "Improve the AssignmentManager#updateTimer and speed up handling opened event", "bug_description": "We found handing opened event very slow in the environment with lots of regions.\nThe problem is the slow AssignmentManager#updateTimer.\nWe do the test for bulk assigning 10w (i.e. 100k) regions, the whole process of bulk assigning took 1 hours.\n2012-05-06 20:31:49,201 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 100000 region(s) round-robin across 5 server(s)\n2012-05-06 21:26:32,103 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning done\nI think we could do the improvement for the AssignmentManager#updateTimer: Make a thread do this work.\nAfter the improvement, it took only 4.5mins\n2012-05-07 11:03:36,581 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 100000 region(s) across 5 server(s), retainAssignment=true \n2012-05-07 11:07:57,073 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning done ", "project": "Apache", "sub_project": "HBASE", "version": "0.90.5", "fixed_version": "0.95.0", "fixed_files": ["org.apache.hadoop.hbase.master.AssignmentManager.java"], "label": 1, "es_results": []}, {"bug_id": 5714, "bug_title": "Add write permissions check before any hbck run that modifies hdfs.", "bug_description": "We encoutered a situation where hbck was run by an under-privileged user that was unable to write/modify/merge regions due to hdfs perms.  Unfortunately, this user was alerted of this  after several minutes of read-only operations.  hbck should fail early by having a write perm check and providing actionable advice to the hbase admin.\nMaybe something like: \"Current user yy does not have write perms to <hbase home>. Please run hbck as hdfs user xxx\"", "project": "Apache", "sub_project": "HBASE", "version": "0.90.6", "fixed_version": "0.94.2", "fixed_files": ["org.apache.hadoop.hbase.util.FSUtils.java", "org.apache.hadoop.hbase.util.HBaseFsck.java"], "label": 1, "es_results": []}, {"bug_id": 4536, "bug_title": "Allow CF to retain deleted rows", "bug_description": "Parent allows for a cluster to retain rows for a TTL or keep a minimum number of versions.\nHowever, if a client deletes a row all version older than the delete tomb stone will be remove at the next major compaction (and even at memstore flush - see HBASE-4241).\nThere should be a way to retain those version to guard against software error.\nI see two options here:\n1. Add a new flag HColumnDescriptor. Something like \"RETAIN_DELETED\".\n2. Folds this into the parent change. I.e. keep minimum-number-of-versions of versions even past the delete marker.\n#1 would allow for more flexibility. #2 comes somewhat naturally with parent (from a user viewpoint)\nComments? Any other options?", "project": "Apache", "sub_project": "HBASE", "version": "0.92.0", "fixed_version": "0.94.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestMinVersions.java", "org.apache.hadoop.hbase.regionserver.TestQueryMatcher.java", "org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.java", "org.apache.hadoop.hbase.regionserver.Store.java", "org.apache.hadoop.hbase.regionserver.TestStoreScanner.java", "org.apache.hadoop.hbase.client.Attributes.java", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java", "org.apache.hadoop.hbase.regionserver.StoreFile.java", "org.apache.hadoop.hbase.client.TestFromClientSide.java", "org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.java", "org.apache.hadoop.hbase.client.Scan.java", "org.apache.hadoop.hbase.regionserver.TestExplicitColumnTracker.java", "org.apache.hadoop.hbase.regionserver.ColumnTracker.java", "org.apache.hadoop.hbase.KeyValue.java", "org.apache.hadoop.hbase.regionserver.TestScanWildcardColumnTracker.java", "org.apache.hadoop.hbase.regionserver.TestCompaction.java", "org.apache.hadoop.hbase.regionserver.TestMemStore.java", "org.apache.hadoop.hbase.HColumnDescriptor.java", "org.apache.hadoop.hbase.regionserver.StoreScanner.java", "org.apache.hadoop.hbase.HBaseTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 6265, "bug_title": "Calling getTimestamp() on a KV in cp.prePut() causes KV not to be flushed", "bug_description": "There is an issue when you call getTimestamp() on any KV handed into a Coprocessor&apos;s prePut(). It initializes the internal \"timestampCache\" variable. \nWhen you then pass it to the normal processing, the region server sets the time to the server time in case you have left it unset from the client side (updateLatestStamp() call). \nThe TimeRangeTracker then calls getTimestamp() later on to see if it has to include the KV, but instead of getting the proper time it sees the cached timestamp from the prePut() call.", "project": "Apache", "sub_project": "HBASE", "version": "0.92.0", "fixed_version": "0.95.0", "fixed_files": ["org.apache.hadoop.hbase.TestKeyValue.java", "org.apache.hadoop.hbase.KeyValue.java"], "label": 1, "es_results": []}, {"bug_id": 6565, "bug_title": "Coprocessor exec result Map is not thread safe", "bug_description": "I develop a coprocessor program ,but found some different results in repeated tests.for example,normally,the result&apos;s size is 10.but sometimes it appears 9.\nI read the HTable.java code,found a TreeMap(thread-unsafe) be used in multithreading environment.It because the bug happened", "project": "Apache", "sub_project": "HBASE", "version": "0.92.2", "fixed_version": "0.94.2", "fixed_files": ["org.apache.hadoop.hbase.client.HTable.java"], "label": 1, "es_results": []}, {"bug_id": 5372, "bug_title": "Table mutation operations should check table level rights, not global rights ", "bug_description": "drop/modify/disable/enable etc table operations should not check for global CREATE/ADMIN rights, but table CREATE/ADMIN rights. Since we check for global permissions first for table permissions, configuring table access using global permissions will continue to work. ", "project": "Apache", "sub_project": "HBASE", "version": "0.94.0", "fixed_version": "0.95.0", "fixed_files": ["org.apache.hadoop.hbase.security.access.AccessController.java", "org.apache.hadoop.hbase.security.access.TestAccessController.java"], "label": 1, "es_results": []}, {"bug_id": 5876, "bug_title": "TestImportExport has been failing against hadoop 0.23 profile", "bug_description": "TestImportExport has been failing against hadoop 0.23 profile", "project": "Apache", "sub_project": "HBASE", "version": "0.94.0", "fixed_version": "0.95.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim.java", "org.apache.hadoop.hbase.HBaseTestingUtility.java", "org.apache.hadoop.hbase.mapreduce.TestImportExport.java"], "label": 1, "es_results": []}, {"bug_id": 6529, "bug_title": "With HFile v2, the region server will always perform an extra copy of source files", "bug_description": "With HFile v2 implementation in HBase 0.94 & 0.96, the region server will use HFileSystem as its fs. When it performs bulk load in Store.bulkLoadHFile(), it checks if its fs is the same as srcFs, which however will be DistributedFileSystem. Consequently, it will always perform an extra copy of source files.", "project": "Apache", "sub_project": "HBASE", "version": "0.94.0", "fixed_version": "0.94.2", "fixed_files": ["org.apache.hadoop.hbase.regionserver.Store.java"], "label": 1, "es_results": []}, {"bug_id": 6330, "bug_title": "TestImportExport has been failing against hadoop 0.23/2.0 profile", "bug_description": "See HBASE-5876.  I&apos;m going to commit the v3 patches under this name since there has been two months (my bad) since the first half was committed and found to be incomplte.\n\n4/9/13 Updated - this will take the patch from HBASE-8258 to fix this specific problem.  The umbrella that used to be HBASE-8258 is now handled with HBASE-6891.", "project": "Apache", "sub_project": "HBASE", "version": "0.94.1", "fixed_version": "0.95.1", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.Export.java", "org.apache.hadoop.hbase.HBaseTestingUtility.java", "org.apache.hadoop.hbase.mapreduce.TestImportExport.java", "org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim.java"], "label": 1, "es_results": []}, {"bug_id": 6870, "bug_title": "HTable#coprocessorExec always scan the whole table ", "bug_description": "In current logic, HTable#coprocessorExec always scans the entire META table, loading it into memory and then filters the keys to return only those that fall in specified range.  The version after the patch only scans the portions of meta that are in the specified key range, and returns them.  Put simply  before we did a load-all-then-filter; afterwards we only-scan-what-is-needed.\nThe former has low efficiency and greatly impacts the Regionserver carrying .META. when there are many coprocessorExec requests.\n", "project": "Apache", "sub_project": "HBASE", "version": "0.94.1", "fixed_version": "0.95.1", "fixed_files": ["org.apache.hadoop.hbase.client.HTable.java"], "label": 1, "es_results": []}, {"bug_id": 11052, "bug_title": "Sending random data crashes thrift service", "bug_description": "Upstream thrift library has a know issue (THRIFT-601) causing the thrift server to crash with an Out-of-Memory Error when bogus requests are sent.\nThis reproduces when a very large request size is sent in the request header, making the thrift server to allocate a large memory segment leading to OOM.\nLoadBalancer health checks are the first \"candidate\" for bogus requests\nThrift developers admit this is a known issue with TBinaryProtocol and their recommandation is to use TCompactProtocol/TFramedTransport but this requires all thrift clients to be updated (might not be feasible atm)\nSo we need a fix similar to CASSANDRA-475.", "project": "Apache", "sub_project": "HBASE", "version": "0.94.18", "fixed_version": "0.99.0", "fixed_files": ["org.apache.hadoop.hbase.thrift2.ThriftServer.java", "org.apache.hadoop.hbase.thrift.ThriftServerRunner.java"], "label": 1, "es_results": []}, {"bug_id": 11625, "bug_title": "Reading datablock throws \"Invalid HFile block magic\" and can not switch to hdfs checksum ", "bug_description": "when using hbase checksum,call readBlockDataInternal() in hfileblock.java, it could happen file corruption but it only can switch to hdfs checksum inputstream till validateBlockChecksum(). If the datablock&apos;s header corrupted when b = new HFileBlock(),it throws the exception \"Invalid HFile block magic\" and the rpc call fail", "project": "Apache", "sub_project": "HBASE", "version": "0.94.21", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.io.hfile.ChecksumUtil.java", "org.apache.hadoop.hbase.io.hfile.HFileBlock.java", "org.apache.hadoop.hbase.io.hfile.TestChecksum.java"], "label": 1, "es_results": []}, {"bug_id": 13651, "bug_title": "Handle StoreFileScanner FileNotFoundException", "bug_description": "Example:\n\nMachine-1 is serving Region-X and start compaction\nMachine-1 goes in GC pause\nRegion-X gets reassigned to Machine-2\nMachine-1 exit from the GC pause\nMachine-1 (re)moves the compacted files\nMachine-1 get the lease expired and shutdown\n\nMachine-2 has now tons of FileNotFoundException on scan. If we reassign the region everything is ok, because we pickup the files compacted by Machine-1.\nThis problem doesn&apos;t happen in the new code 1.0+  (i think but I haven&apos;t checked, it may be 1.1) where we write on the WAL the compaction event before (re)moving the files.\nA workaround is handling FileNotFoundException and refresh the store files, or shutdown the region and reassign. the first one is easy in 1.0+ the second one requires more work because at the moment we don&apos;t have the code to notify the master that the RS is closing the region, alternatively we can shutdown the entire RS (it is not a good solution but the case is rare enough)", "project": "Apache", "sub_project": "HBASE", "version": "0.94.27", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.java"], "label": 1, "es_results": []}, {"bug_id": 7693, "bug_title": "Hostname returned by TableInputFormatBase.reverseDNS contains trailing period", "bug_description": "TableInputFormatBase.reverseDNS makes a call to org.apache.hadoop.net.DNS.reverseDns which (at least in Hadoop 1.0.4) returns a PTR record.  PTR records contain a trailing period, which then shows up in the input split location causing the JobTracker to incorrectly match map jobs to data-local map slots.", "project": "Apache", "sub_project": "HBASE", "version": "0.94.3", "fixed_version": "0.95.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java"], "label": 1, "es_results": []}, {"bug_id": 8044, "bug_title": "split/flush/compact/major_compact from hbase she will does not work for region key with \\x format", "bug_description": "the conversion between bytes and string is incorrect", "project": "Apache", "sub_project": "HBASE", "version": "0.94.5", "fixed_version": "0.95.0", "fixed_files": ["org.apache.hadoop.hbase.client.HBaseAdmin.java"], "label": 1, "es_results": []}, {"bug_id": 8212, "bug_title": "Introduce a new separator instead of hyphen('-') for renaming recovered queues' znodes", "bug_description": "hyphen is frequently used in the HostName. Likes we have one regionserver named \"160-172-0-1\", so under this scenario, 160-172-0-1 will be splited to 4 Strings and will be considered for 4 possible dead servers.\nIt won&apos;t find all the logs for \"160-172-0-1\" any more, so causes data-loss.", "project": "Apache", "sub_project": "HBASE", "version": "0.94.6", "fixed_version": "0.94.7", "fixed_files": ["org.apache.hadoop.hbase.util.FSHDFSUtils.java", "org.apache.hadoop.hbase.regionserver.wal.TestHLog.java", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java", "org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java", "org.apache.hadoop.hbase.replication.ReplicationZookeeper.java"], "label": 1, "es_results": []}, {"bug_id": 8781, "bug_title": "ImmutableBytesWritable constructor with another IBW as param need to consider the offset of the passed IBW", "bug_description": "\n\n\n/**\n\n   * Set the new ImmutableBytesWritable to the contents of the passed\n\n   * <code>ibw</code>.\n\n   * @param ibw the value to set this ImmutableBytesWritable to.\n\n   */\n\n  public ImmutableBytesWritable(final ImmutableBytesWritable ibw) {\n\n    this(ibw.get(), 0, ibw.getSize());\n\n  }\n\n\n\nIt should be this(ibw.get(), ibw.getOffset(), ibw.getSize());", "project": "Apache", "sub_project": "HBASE", "version": "0.94.8", "fixed_version": "0.98.0", "fixed_files": ["org.apache.hadoop.hbase.io.ImmutableBytesWritable.java"], "label": 1, "es_results": []}, {"bug_id": 7770, "bug_title": "minor integration test framework fixes", "bug_description": "\nmade FileSystem on HBaseTestingUtil.createMulti() not expect mini cluster\nadded check if server is not running before deciding to restore a server\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.94.9", "fixed_version": "0.98.0", "fixed_files": ["org.apache.hadoop.hbase.HBaseTestingUtility.java", "org.apache.hadoop.hbase.DistributedHBaseCluster.java"], "label": 1, "es_results": []}, {"bug_id": 7581, "bug_title": "TestAccessController depends on the execution order", "bug_description": "", "project": "Apache", "sub_project": "HBASE", "version": "0.95.2", "fixed_version": "0.95.0", "fixed_files": ["org.apache.hadoop.hbase.security.access.TestAccessController.java", "org.apache.hadoop.hbase.client.RetriesExhaustedException.java"], "label": 1, "es_results": []}, {"bug_id": 10020, "bug_title": "Add maven compile-protobuf profile", "bug_description": "Ad a maven profile to compile protobufs instead of the dev-support script which is only for hbase-protocol module. \n\n\n\nmvn test-compile -Dcompile-protobuf \n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.95.2", "fixed_version": "0.98.0", "fixed_files": ["org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.java", "org.apache.hadoop.hbase.coprocessor.example.generated.BulkDeleteProtos.java", "org.apache.hadoop.hbase.coprocessor.example.generated.ExampleProtos.java", "org.apache.hadoop.hbase.rest.protobuf.generated.VersionMessage.java", "org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatusMessage.java", "org.apache.hadoop.hbase.coprocessor.protobuf.generated.PingProtos.java", "org.apache.hadoop.hbase.rest.protobuf.generated.ScannerMessage.java", "org.apache.hadoop.hbase.ipc.protobuf.generated.TestProtos.java", "org.apache.hadoop.hbase.rest.protobuf.generated.TableSchemaMessage.java", "org.apache.hadoop.hbase.rest.protobuf.generated.TableInfoMessage.java"], "label": 1, "es_results": []}, {"bug_id": 10448, "bug_title": "ZKUtil create and watch methods do not set watch in some cases", "bug_description": "While using the ZKUtil methods during testing, I found that watch was not set when it should be set based on the methods and method comments:\ncreateNodeIfNotExistsAndWatch\ncreateEphemeralNodeAndWatch\nFor example, in createNodeIfNotExistsAndWatch():\n\n\n\n public static boolean createNodeIfNotExistsAndWatch(\n\n      ZooKeeperWatcher zkw, String znode, byte [] data)\n\n  throws KeeperException {\n\n    try {\n\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n\n          CreateMode.PERSISTENT);\n\n    } catch (KeeperException.NodeExistsException nee) {\n\n      try {\n\n        zkw.getRecoverableZooKeeper().exists(znode, zkw);\n\n      } catch (InterruptedException e) {\n\n        zkw.interruptedException(e);\n\n        return false;\n\n      }\n\n      return false;\n\n    } catch (InterruptedException e) {\n\n      zkw.interruptedException(e);\n\n      return false;\n\n    }\n\n    return true;\n\n  }\n\n\n\nThe watch is only set via exists() call when the node already exists.\nSimilarly in createEphemeralNodeAndWatch():\n\n\n\n  public static boolean createEphemeralNodeAndWatch(ZooKeeperWatcher zkw,\n\n      String znode, byte [] data)\n\n  throws KeeperException {\n\n    try {\n\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n\n          CreateMode.EPHEMERAL);\n\n    } catch (KeeperException.NodeExistsException nee) {\n\n      if(!watchAndCheckExists(zkw, znode)) {\n\n        // It did exist but now it doesn&apos;t, try again\n\n        return createEphemeralNodeAndWatch(zkw, znode, data);\n\n      }\n\n      return false;\n\n    } catch (InterruptedException e) {\n\n      LOG.info(\"Interrupted\", e);\n\n      Thread.currentThread().interrupt();\n\n    }\n\n    return true;\n\n  }\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "hbase-0.96.0", "fixed_version": "0.98.0", "fixed_files": ["org.apache.hadoop.hbase.zookeeper.ZKUtil.java"], "label": 1, "es_results": []}, {"bug_id": 10622, "bug_title": "Improve log and Exceptions in Export Snapshot ", "bug_description": "from the logs of export snapshot is not really clear what&apos;s going on,\nadding some extra information useful to debug, and in some places the real exception can be thrown", "project": "Apache", "sub_project": "HBASE", "version": "hbase-0.96.0", "fixed_version": "0.96.2", "fixed_files": ["org.apache.hadoop.hbase.snapshot.ExportSnapshot.java"], "label": 1, "es_results": []}, {"bug_id": 11039, "bug_title": "[VisibilityController] Integration test for labeled data set mixing and filtered excise", "bug_description": "Create an integration test for the VisibilityController that:\n1. Create several tables of test data\n2. Assign a set of auths to each table. Label all entries in the table with appropriate visibility expressions. Insure that some data in every table overlaps with data in other tables at common row/family/qualifier coordinates. Generate data like ITBLL so we can verify all data present later.\n3. Mix the data from the different tables into a new common table\n4. Verify for each set of auths defined in step #2 that all entries found in the source table can be found in the common table. Like the ITBLL verification step but done N times for each set of auths defined in step #2.\n5. Choose one of the source tables. Get its set of auths. Perform a deletion with visibility expression from the common table using those auths.\n6. Verify that no data in the common table with the auth set chosen in #5 remains. A simple row count with the set of auths chosen in #5 that should return 0.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.1", "fixed_version": "0.98.4", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.Import.java", "org.apache.hadoop.hbase.security.access.AccessControlClient.java", "org.apache.hadoop.hbase.security.visibility.CellVisibility.java", "org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java"], "label": 1, "es_results": []}, {"bug_id": 13694, "bug_title": "CallQueueSize is incorrectly decremented until the response is sent", "bug_description": "We should decrement the CallQueueSize as soon as we no longer need the call around, e.g. after RpcServer.CurCall.set(null) otherwise we will be only pushing back other client requests while we send the response back to the client that originated the call.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.12", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.CallRunner.java"], "label": 1, "es_results": []}, {"bug_id": 13779, "bug_title": "Calling table.exists() before table.get() end up with an empty Result", "bug_description": "If we call exists() before a get() the result returned by the get() will be empty.\nsimple test to verify it:\n\n\n\nPut put = new Put(ROW);\n\nput.add(FAMILY, QUALIFIER, VALUE);\n\ntable.put(put);\n\n\n\nGet get = new Get(ROW);\n\n\n\nboolean exist = table.exists(get);\n\nexist = table.exists(get);\n\nassertEquals(true, exist);\n\n\n\nResult result = table.get(get);\n\n// this will fail saying that the Result is empty\n\n// if we remove the exist everything is fine\n\nassertEquals(false, result.isEmpty()); \n\nassertTrue(Bytes.equals(VALUE, result.getValue(FAMILY, QUALIFIER)));\n\n\n\nif we use a different Get instance for the get everything works\n\n\n\n...\n\nget = new Get(ROW);\n\nResult result = table.get(get);\n\nassertEquals(false, result.isEmpty()); \n\n\n\nHTable.exists() set the checkExistenceOnly flag in the Get so that object is not reusable by a table.get()\n\n\n\n  public boolean exists(final Get get) throws IOException {\n\n    get.setCheckExistenceOnly(true);\n\n    Result are = get(get);\n\n    assert r.getExists() != null;\n\n    return r.getExists();\n\n  }\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.98.12.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.HTable.java", "org.apache.hadoop.hbase.client.Get.java", "org.apache.hadoop.hbase.client.TestGet.java", "org.apache.hadoop.hbase.client.TestFromClientSide3.java", "org.apache.hadoop.hbase.client.Scan.java"], "label": 1, "es_results": []}, {"bug_id": 13995, "bug_title": "ServerName is not fully case insensitive", "bug_description": "we ended up with two ServerName with different cases, AAA and aaa.\nTrying to create a table, every once in a while, we ended up with the region lost and not assigned. \nBaseLoadBalancer.roundRobinAssignment() goes through each server and create a map with what to assign to them.\nWe had to server on the list AAA and aaa which are the same machine, the problem is that the round robin now is assigning an empty list to one of the two. so depending on the order we ended up with a region not assigned.\nServerName equals() does the case insensitive comparison but the hashCode() is done on a case sensitive server name, so the Map in ServerManager will never hit the item and compare it using equals, so we end up with two entries that are the same server. similar thing for ServerName.isSameHostnameAndPort() where we don&apos;t check for cases", "project": "Apache", "sub_project": "HBASE", "version": "0.98.12.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ServerName.java", "org.apache.hadoop.hbase.TestServerName.java"], "label": 1, "es_results": []}, {"bug_id": 13706, "bug_title": "CoprocessorClassLoader should not exempt Hive classes", "bug_description": "CoprocessorClassLoader is used to load classes from the coprocessor jar.\nCertain classes are exempt from being loaded by this ClassLoader, which means they will be ignored in the coprocessor jar, but loaded from parent classpath instead.\nOne problem is that we categorically exempt \"org.apache.hadoop\".\nBut it happens that Hive packages start with \"org.apache.hadoop\".\nThere is no reason to exclude hive classes from theCoprocessorClassLoader.\nHBase does not even include Hive jars.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.12", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.util.CoprocessorClassLoader.java"], "label": 1, "es_results": []}, {"bug_id": 14598, "bug_title": "ByteBufferOutputStream grows its HeapByteBuffer beyond JVM limitations", "bug_description": "We noticed that in returning a Scan against a region containing particularly large (wide) rows that it is possible during ByteBufferOutputStream.checkSizeAndGrow() to attempt to create a new ByteBuffer larger than the JVM allows which then throws a OutOfMemoryError. The code currently caps it at Integer.MAX_VALUE which is actually larger than the JVM allows. This lead to us dealing with cascading region server death as the RegionServer hosting the region died, opened on a new server, the client retried the scan, and the new RS died as well. \nI believe ByteBufferOutputStream should not try to create ByteBuffers that large and instead throw an exception back up if it needs to grow any bigger. The limit should probably be something like Integer.MAX_VALUE-8, as that is what ArrayList uses. ref: http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/8-b132/java/util/ArrayList.java#221", "project": "Apache", "sub_project": "HBASE", "version": "0.98.12", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.IPCUtil.java", "org.apache.hadoop.hbase.io.ByteBufferOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 13668, "bug_title": "TestFlushRegionEntry is flaky", "bug_description": "\nFlaked tests: \n\norg.apache.hadoop.hbase.regionserver.TestFlushRegionEntry.test\n\n    (org.apache.hadoop.hbase.regionserver.TestFlushRegionEntry)\n\n  Run 1: TestFlushRegionEntry.test:41 expected:\n\n       org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry<[flush region null]>\n\n     but was:\n\n       org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry<[flush region null]>\n\n  Run 2: PASS\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestFlushRegionEntry.java"], "label": 1, "es_results": []}, {"bug_id": 13647, "bug_title": "Default value for hbase.client.operation.timeout is too high", "bug_description": "Default value for hbase.client.operation.timeout is too high, it is LONG.Max.\nThat value will block any service calls to coprocessor endpoints indefinitely.\nShould we introduce better default value for that?", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.HConstants.java"], "label": 1, "es_results": []}, {"bug_id": 13826, "bug_title": "Unable to create table when group acls are appropriately set.", "bug_description": "Steps for reproducing the issue.\n\nCreate user &apos;test&apos; and group &apos;hbase-admin&apos;.\nGrant global create permissions to &apos;hbase-admin&apos;.\nAdd user &apos;test&apos; to &apos;hbase-admin&apos; group.\nCreate table operation for &apos;test&apos; user will throw ADE.\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.security.access.TestAccessController2.java", "org.apache.hadoop.hbase.security.access.TableAuthManager.java"], "label": 1, "es_results": []}, {"bug_id": 13933, "bug_title": "DBE's seekBefore with tags corrupts the tag's offset information thus leading to incorrect results", "bug_description": "The problem occurs with moveToPrevious() case and incase of tags we copy the previous pointer&apos;s tag info to the current because already decoded the tags.\nWill check once again before I post other details.  I have a test case to reproduce the problem. Found this while working with MultibyteBuffers and verified if this is present in trunk - it is in all branches where we have tags compression (I suppose) will verify", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.io.hfile.TestSeekTo.java", "org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java"], "label": 1, "es_results": []}, {"bug_id": 13969, "bug_title": "AuthenticationTokenSecretManager is never stopped in RPCServer", "bug_description": "AuthenticationTokenSecretManager is never stopped in RPCServer.\n\n\n\n    AuthenticationTokenSecretManager mgr = createSecretManager();\n\n    if (mgr != null) {\n\n      setSecretManager(mgr);\n\n      mgr.start();\n\n    }\n\n\n\nIt should be stopped during exit.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.RpcServer.java"], "label": 1, "es_results": []}, {"bug_id": 14042, "bug_title": "Fix FATAL level logging in FSHLog where logged for non fatal exceptions", "bug_description": "We have FATAL level logging in FSHLog where an IOException causes a log roll to be requested. It isn&apos;t a fatal event. Drop the log level to WARN. (Could even be INFO.)", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.wal.FSHLog.java"], "label": 1, "es_results": []}, {"bug_id": 13897, "bug_title": "OOM may occur when Import imports a row with too many KeyValues", "bug_description": "When importing a row with too many KeyValues (may have too many columns or versions)KeyValueReducer will incur OOM.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.Import.java"], "label": 1, "es_results": []}, {"bug_id": 14196, "bug_title": "Thrift server idle connection timeout issue", "bug_description": "When idle connection get cleaned from Thrift server, underlying table instances are still cached in a thread local cache.\nThis is the antipattern. Table objects are lightweight and should not be cached, besides this, underlying connections can be closed by periodic connection cleaner chore (ConnectionCache) and cached table instances may become invalid. This is Thrift1 specific issue. Thrift2 is OK.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.thrift.IncrementCoalescer.java", "org.apache.hadoop.hbase.thrift.ThriftServerRunner.java"], "label": 1, "es_results": []}, {"bug_id": 14229, "bug_title": "Flushing canceled by coprocessor still leads to memstoreSize set down", "bug_description": "A Coprocessor override \"public InternalScanner preFlush(final Store store, final InternalScanner scanner)\" and return NULL when calling this method, will cancel flush request, leaving snapshot un-flushed, and no new storefile created. But the HRegion.internalFlushCache still set down memstoreSize to 0 by totalFlushableSize. \nIf there&apos;s no write requests anymore, the memstoreSize will remaining as 0, and no more flush quests will be processed because of the checking of memstoreSize.get() <=0 at the beginning of internalFlushCache.\nThis issue may not because data loss, but it will confuse coprocessor users. If we argree with this, I&apos;ll apply a patch later.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.regionserver.TestHRegion.java"], "label": 1, "es_results": []}, {"bug_id": 13770, "bug_title": "Programmatic JAAS configuration option for secure zookeeper may be broken", "bug_description": "While verifying the patch fix for HBASE-13768 we were unable to successfully test the programmatic JAAS configuration option for secure ZooKeeper integration. Unclear if that was due to a bug or incorrect test configuration.\nUpdate the security section of the online book with clear instructions for setting up the programmatic JAAS configuration option for secure ZooKeeper integration.\nVerify it works.\nFix as necessary.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegionServer.java", "org.apache.hadoop.hbase.HConstants.java", "org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java", "org.apache.hadoop.hbase.zookeeper.ZKUtil.java", "org.apache.hadoop.hbase.zookeeper.TestZooKeeperACL.java", "org.apache.hadoop.hbase.master.HMasterCommandLine.java"], "label": 1, "es_results": []}, {"bug_id": 15218, "bug_title": "On RS crash and replay of WAL, loosing all Tags in Cells", "bug_description": "The KeyValueCodec&apos;s Decoder makes NoTagsKeyValue. The WalCellCodec also makes this Decoder and so while reading Cells after RS crash, we will loose all tags in Cells.  (While writing to WAL using WALCellCodec, we are preserving Cell tags.)\nIn case of SecureWALCellCodec, we are not even writing Cell tags to WAL.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithDefaultVisLabelService.java", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec.java", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec.java"], "label": 1, "es_results": []}, {"bug_id": 14192, "bug_title": "Fix REST Cluster constructor with String List", "bug_description": "The HBase REST Cluster which takes a list of hostname colon port numbers is not setting the internal list of nodes correctly.\nExisting method:\npublic Cluster(List<String> nodes) \n{\n\n   nodes.addAll(nodes)\n\n}\n\nCorrected method:\npublic Cluster(List<String> nodes) \n{\n\n   this.nodes.addAll(nodes)\n\n}\n", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.rest.client.Cluster.java"], "label": 1, "es_results": []}, {"bug_id": 15676, "bug_title": "FuzzyRowFilter fails and matches all the rows in the table if the mask consists of all 0s", "bug_description": "While using FuzzyRowFilter we noticed that if the mask array consists of all 0s (fixed) the FuzzyRowFilter matches all the rows in the table. We noticed this on HBase 1.1, 1.2 and higher.\nAfter some digging we suspect that this is because of isPreprocessedMask() check which is used in preprocessMask() which was added here: https://issues.apache.org/jira/browse/HBASE-13761\nIf the mask consists of all 0s then the isPreprocessedMask() returns true and the preprocessing which responsible for changing 0s to -1 doesn&apos;t happen and hence all rows are matched in scan.\nThis scenario can be tested in TestFuzzyRowFilterEndToEnd#testHBASE14782() If we change the \nbyte[] fuzzyKey = Bytes.toBytesBinary(\"\\\\x00\\\\x00x044\");\nbyte[] mask = new byte[] \n{1,0,0,0}\n;\nto \nbyte[] fuzzyKey = Bytes.toBytesBinary(\"\\\\x9B\\\\x00x044e\");\nbyte[] mask = new byte[] \n{0,0,0,0,0}\n;\nWe expect one match but this will match all the rows in the table. ", "project": "Apache", "sub_project": "HBASE", "version": "0.98.13", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.filter.FuzzyRowFilter.java", "org.apache.hadoop.hbase.filter.TestFuzzyRowFilterEndToEnd.java"], "label": 1, "es_results": []}, {"bug_id": 13776, "bug_title": "Setting illegal versions for HColumnDescriptor does not throw IllegalArgumentException ", "bug_description": "HColumnDescriptor hcd = new HColumnDescriptor(\n        new HColumnDescriptor(HConstants.CATALOG_FAMILY)\n            .setInMemory(true)\n            .setScope(HConstants.REPLICATION_SCOPE_LOCAL)\n            .setBloomFilterType(BloomType.NONE)\n            .setCacheDataInL1(true));\n    final int minVersions = 123;\n    final int maxVersions = 234;\n    hcd.setMaxVersions(minVersions);\n    hcd.setMinVersions(maxVersions);\n//no exception throw", "project": "Apache", "sub_project": "HBASE", "version": "0.98.14", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.HMaster.java", "org.apache.hadoop.hbase.client.TestFromClientSide.java"], "label": 1, "es_results": []}, {"bug_id": 14209, "bug_title": "TestShell visibility tests failing", "bug_description": "This is after HBASE-14105  but we&apos;ve seen this earlier where adding ruby units to the she will tests can cause the visibility tests to fail inexplicably. We can&apos;t just avoid adding ruby units to TestShell in 0.98 so figure out the root because and fix or disable these. \n\n  1) Error:\n\ntest_The_get/put_methods_should_work_for_data_written_with_Visibility(Hbase::VisibilityLabelsAdminMethodsTest):\n\nNativeException: junit.framework.AssertionFailedError: Waiting timed out after [10,000] msec\n\n    junit/framework/Assert.java:57:in `fail&apos;\n\n    org/apache/hadoop/hbase/Waiter.java:193:in `waitFor&apos;\n\n    org/apache/hadoop/hbase/Waiter.java:128:in `waitFor&apos;\n\n    org/apache/hadoop/hbase/HBaseTestingUtility.java:3514:in `waitFor&apos;\n\n    org/apache/hadoop/hbase/HBaseTestingUtility.java:3576:in `waitLabelAvailable&apos;\n\n    ./src/test/ruby/hbase/visibility_labels_admin_test.rb:73:in `test_The_get/put_methods_should_work_for_data_written_with_Visibility&apos;\n\n    org/jruby/RubyProc.java:270:in `call&apos;\n\n    org/jruby/RubyKernel.java:2105:in `send&apos;\n\n    org/jruby/RubyArray.java:1620:in `each&apos;\n\n    org/jruby/RubyArray.java:1620:in `each&apos;\n\n\n\n  2) Error:\n\ntest_The_set/clear_methods_should_work_with_authorizations(Hbase::VisibilityLabelsAdminMethodsTest):\n\nNativeException: junit.framework.AssertionFailedError: Waiting timed out after [10,000] msec\n\n    junit/framework/Assert.java:57:in `fail&apos;\n\n    org/apache/hadoop/hbase/Waiter.java:193:in `waitFor&apos;\n\n    org/apache/hadoop/hbase/Waiter.java:128:in `waitFor&apos;\n\n    org/apache/hadoop/hbase/HBaseTestingUtility.java:3514:in `waitFor&apos;\n\n    org/apache/hadoop/hbase/HBaseTestingUtility.java:3576:in `waitLabelAvailable&apos;\n\n    ./src/test/ruby/hbase/visibility_labels_admin_test.rb:52:in `test_The_set/clear_methods_should_work_with_authorizations&apos;\n\n    org/jruby/RubyProc.java:270:in `call&apos;\n\n    org/jruby/RubyKernel.java:2105:in `send&apos;\n\n    org/jruby/RubyArray.java:1620:in `each&apos;\n\n    org/jruby/RubyArray.java:1620:in `each&apos;\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.98.14", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.java"], "label": 1, "es_results": []}, {"bug_id": 14359, "bug_title": "HTable#close will hang forever if unchecked error/exception thrown in AsyncProcess#sendMultiAction", "bug_description": "Currently in AsyncProcess#sendMultiAction, we only catch the RejectedExecutionException and let other error/exception go, which will cause decTaskCounter not invoked. Meanwhile, the recommendation for using HTable is to close the table in the finally clause, and HTable#close will call flushCommits and wait until all task done.\nThe problem is when unchecked error/exception like OutOfMemoryError thrown, taskSent will never be equal to taskDone, so AsyncProcess#waitUntilDone will never return. Especially, if autoflush is set thus no data to flush during table close, there would be no rpc call so rpcTimeOut will not break the call, and thread will wait there forever.\nIn our product env, the unchecked error we observed is \"java.lang.OutOfMemoryError: unable to create new native thread\", and we observed the client thread hang for hours", "project": "Apache", "sub_project": "HBASE", "version": "0.98.14", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.AsyncProcess.java"], "label": 1, "es_results": []}, {"bug_id": 14510, "bug_title": "Can not set coprocessor from She Will after HBASE-14224", "bug_description": "I was able to set coprocessors for table by she will normally, but it didn&apos;t worked now.\nHere&apos;s the she will output (omit really jar path and coprocessor classname)\n\n\n\nHBase She Will; enter &apos;help<RETURN>&apos; for list of supported commands.\n\nType \"exit<RETURN>\" to leave the HBase She Will\n\nVersion 1.3.0-SNAPSHOT, 642273bc2a5a415eba6f1592a439a6b2b53a70a9, Tue Sep 29 15:58:28 CST 2015\n\n\n\nhbase(main):001:0> describe &apos;test&apos;\n\nTable test is ENABLED\n\ntest\n\nCOLUMN FAMILIES DESCRIPTION\n\n{NAME => &apos;f&apos;, DATA_BLOCK_ENCODING => &apos;NONE&apos;, BLOOMFILTER => &apos;ROW&apos;, REPLICATION_SCOPE => &apos;0&apos;, VERSIONS => &apos;1&apos;, COMPRESSION => &apos;NONE&apos;, MIN_VERSIONS => &apos;0&apos;, TTL => &apos;FOREVER&apos;, KEEP_DELETED_CELLS => &apos;FALSE&apos;, B\n\nLOCKSIZE => &apos;65536&apos;, IN_MEMORY => &apos;false&apos;, BLOCKCACHE => &apos;true&apos;}\n\n1 row(s) in 0.4370 seconds\n\n\n\nhbase(main):002:0> alter &apos;test&apos;, &apos;coprocessor&apos;=>&apos;hdfs:///some.jar|com.somepackage.SomeObserver|1001&apos;\n\nUpdating all regions with the new schema...\n\n1/1 regions updated.\n\nDone.\n\n0 row(s) in 1.9760 seconds\n\n\n\nhbase(main):003:0> describe &apos;test&apos;\n\nTable test is ENABLED\n\ntest, {TABLE_ATTRIBUTES => {coprocessor$1 => &apos;|hdfs:///some.jar|com.somepackage.SomeObserver|1001|1073741823|&apos;}\n\nCOLUMN FAMILIES DESCRIPTION\n\n{NAME => &apos;f&apos;, DATA_BLOCK_ENCODING => &apos;NONE&apos;, BLOOMFILTER => &apos;ROW&apos;, REPLICATION_SCOPE => &apos;0&apos;, VERSIONS => &apos;1&apos;, COMPRESSION => &apos;NONE&apos;, MIN_VERSIONS => &apos;0&apos;, TTL => &apos;FOREVER&apos;, KEEP_DELETED_CELLS => &apos;FALSE&apos;, B\n\nLOCKSIZE => &apos;65536&apos;, IN_MEMORY => &apos;false&apos;, BLOCKCACHE => &apos;true&apos;}\n\n1 row(s) in 0.0180 seconds\n\n\n\nI checked the recent commits and found HBASE-14224 is related to. It&apos;s an important improvement, but made a mistake in alter() of admin.rb, line 587. As the value is coprocess spec string but not only class name, here should use htd.setCoprocessorWithSpec instead of htd.setCoprocessor.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.14", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.HTableDescriptor.java"], "label": 1, "es_results": []}, {"bug_id": 14463, "bug_title": "Severe performance downgrade when parallel reading a single key from BucketCache", "bug_description": "We store feature data of online items in HBase, do machine learning on these features, and supply the outputs to our online search engine. In such scenario we will launch hundreds of yarn workers and each worker will read all features of one item(i.e. single rowkey in HBase), so there&apos;ll be heavy parallel reading on a single rowkey.\nWe were using LruCache but start to try BucketCache recently to resolve gc issue, and just as titled we have observed severe performance downgrade. After some analytics we found the root because is the lock in BucketCache#getBlock, as shown below\n\n\n\n      try {\n\n        lockEntry = offsetLock.getLockEntry(bucketEntry.offset());\n\n        // ...\n\n        if (bucketEntry.equals(backingMap.get(key))) {\n\n          // ...\n\n          int len = bucketEntry.getLength();\n\n          Cacheable cachedBlock = ioEngine.read(bucketEntry.offset(), len,\n\n              bucketEntry.deserializerReference(this.deserialiserMap));\n\n\n\nSince ioEnging.read involves array copy, it&apos;s much more time-costed than the operation in LruCache. And since we&apos;re using synchronized in IdLock#getLockEntry, parallel read dropping on the same bucket would be executed in serial, which causes a really bad performance.\nTo resolve the problem, we propose to use ReentranceReadWriteLock in BucketCache, and introduce a new class called IdReadWriteLock to implement it.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.14", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java", "org.apache.hadoop.hbase.io.hfile.CacheTestUtils.java", "org.apache.hadoop.hbase.io.hfile.bucket.TestBucketCache.java"], "label": 1, "es_results": []}, {"bug_id": 14347, "bug_title": "Add a switch to DynamicClassLoader to disable it", "bug_description": "Since HBASE-1936 we have the option to load jars dynamically by default from HDFS or the local filesystem, however hbase.dynamic.jars.dir points to a directory that could be world writable it potentially opens a security problem in both the client side and the RS. We should consider to have a switch to enable or disable this option and it should be off by default.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.15", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.util.DynamicClassLoader.java", "org.apache.hadoop.hbase.util.TestDynamicClassLoader.java"], "label": 1, "es_results": []}, {"bug_id": 14592, "bug_title": "BatchRestartRsAction always restarts 0 RS when running SlowDeterministicMonkey", "bug_description": "When running ITBLL, observed below log and found the ratio of batch restarting is always zero:\n\n15/10/12 17:05:37 INFO actions.Action: Performing action: Batch restarting 0% of region servers\n\n\n\nChecking code, found batchRestartRSRatio never got initialized in SlowDeterministicMonkeyFactory, and current ITBLL never check the case of batch RS restarting in actual.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.15", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.chaos.factories.SlowDeterministicMonkeyFactory.java"], "label": 1, "es_results": []}, {"bug_id": 14591, "bug_title": "Region with reference hfile may split after a forced split in IncreasingToUpperBoundRegionSplitPolicy", "bug_description": "In the IncreasingToUpperBoundRegionSplitPolicy, a region with a store having hfile reference may split after a forced split. This will break many assumptions of design.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.15", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestRegionSplitPolicy.java", "org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy.java"], "label": 1, "es_results": []}, {"bug_id": 14604, "bug_title": "Improve MoveCostFunction in StochasticLoadBalancer", "bug_description": "The code in MoveCoseFunction:\n\n\n\nreturn scale(0, cluster.numRegions + META_MOVE_COST_MULT, moveCost);\n\n\n\nIt uses cluster.numRegions + META_MOVE_COST_MULT as the max value when scale moveCost to [0,1]. But this should use maxMoves as the max value when cluster have a lot of regions.\nAssume a cluster have 10000 regions, maxMoves is 2500, it only scale moveCost to [0, 0.25].\nImprove moveCost by use Math.min(cluster.numRegions, maxMoves) as the max cost, so it can scale moveCost to [0,1].\n\n\n\nreturn scale(0, Math.min(cluster.numRegions, maxMoves) + META_MOVE_COST_MULT, moveCost);\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.98.15", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java", "org.apache.hadoop.hbase.master.balancer.TestStochasticLoadBalancer.java", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java"], "label": 1, "es_results": []}, {"bug_id": 14761, "bug_title": "Deletes with and without visibility expression do not delete the matching mutation", "bug_description": "This is from the user list as reported by Anoop Sharma\n\n\n\n running into an issue related to visibility expressions and delete.\n\nExample run from hbase she will is listed below.\n\nWill appreciate any help on this issue.\n\nthanks.\n\nIn the example below, user running queries has MANAGER authorization.\n\n*First example:*\n\n\n\n  add a column with visib expr MANAGER\n\n\n\n  delete it by passing in visibility of MANAGER\n\n  This works and scan does not return anything.\n\n*Second example:*\n\n\n\n  add a column with visib expr MANAGER\n\n\n\n  delete it by not passing in any visibility.\n\n  This does not delete the column.\n\n  Scan does not return the row but RAW scan shows the column\n\n  marked as deleteColumn.\n\n  Now if delete is done again with visibility of MANAGER,\n\n  it still does not delete it and scan returns the original column.\n\n\n\n*Example 1:*\n\nhbase(main):096:0> create &apos;HBT1&apos;, &apos;cf&apos;\n\nhbase(main):098:0* *put &apos;HBT1&apos;, &apos;John&apos;, &apos;cf:a&apos;, &apos;CA&apos;,\n\n{VISIBILITY=>&apos;MANAGER&apos;}*\n\nhbase(main):099:0> *scan &apos;HBT1&apos;*\n\nROW\n\nCOLUMN+CELL\n\n\n\n John                 column=cf:a, timestamp=1446154722055,\n\nvalue=CA\n\n1 row(s) in 0.0030 seconds\n\nhbase(main):100:0> *delete &apos;HBT1&apos;, &apos;John&apos;, &apos;cf:a&apos;, {VISIBILITY=>&apos;MANAGER&apos;}*\n\n0 row(s) in 0.0030 seconds\n\n\n\nhbase(main):101:0> *scan &apos;HBT1&apos;*\n\nROW\n\nCOLUMN+CELL\n\n0 row(s) in 0.0030 seconds\n\n\n\n*Example 2:*\n\n\n\nhbase(main):010:0* *put &apos;HBT1&apos;, &apos;John&apos;, &apos;cf:a&apos;, &apos;CA&apos;,\n\n{VISIBILITY=>&apos;MANAGER&apos;}*\n\n0 row(s) in 0.0040 seconds\n\n\n\nhbase(main):011:0> *scan &apos;HBT1&apos;*\n\nROW\n\nCOLUMN+CELL\n\n John                 column=cf:a, timestamp=1446155346473,\n\nvalue=CA\n\n1 row(s) in 0.0060 seconds\n\n\n\nhbase(main):012:0> *delete &apos;HBT1&apos;, &apos;John&apos;, &apos;cf:a&apos;*\n\n0 row(s) in 0.0090 seconds\n\n\n\nhbase(main):013:0> *scan &apos;HBT1&apos;*\n\nROW\n\nCOLUMN+CELL\n\n John                 column=cf:a, timestamp=1446155346473,\n\nvalue=CA\n\n1 row(s) in 0.0050 seconds\n\nhbase(main):014:0> *scan &apos;HBT1&apos;, {RAW => true}*\n\n\n\nROW\n\nCOLUMN+CELL\n\n John                 column=cf:a, timestamp=1446155346519,\n\ntype=DeleteColumn\n\n1 row(s) in 0.0060 seconds\n\n\n\nhbase(main):015:0> *delete &apos;HBT1&apos;, &apos;John&apos;, &apos;cf:a&apos;, {VISIBILITY=>&apos;MANAGER&apos;}*\n\n0 row(s) in 0.0030 seconds\n\nhbase(main):016:0> *scan &apos;HBT1&apos;*\n\nROW\n\nCOLUMN+CELL\n\n John                 column=cf:a, timestamp=1446155346473,\n\nvalue=CA\n\n1 row(s) in 0.0040 seconds\n\nhbase(main):017:0> *scan &apos;HBT1&apos;, {RAW => true}*\n\nROW\n\nCOLUMN+CELL\n\n John                 column=cf:a, timestamp=1446155346601,\n\ntype=DeleteColumn\n\n\n\n1 row(s) in 0.0060 seconds\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.98.15", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker.java", "org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithDeletes.java"], "label": 1, "es_results": []}, {"bug_id": 15976, "bug_title": "RegionServerMetricsWrapperRunnable will be failure  when disable blockcache.", "bug_description": "When i disable blockcache, the code \"cacheStats = blockCache.getStats();\" will occur NPE in org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.RegionServerMetricsWrapperRunnable.\nIt lead to many regionserver&apos;s metrics&apos; value always equal 0.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.15", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java"], "label": 1, "es_results": []}, {"bug_id": 14905, "bug_title": "VerifyReplication does not honour versions option", "bug_description": "source:\nhbase(main):001:0> scan &apos;t1&apos;, \n{RAW => true, VERSIONS => 100}\nROW                                      COLUMN+CELL                                                                                                           \n r1                                      column=f1:, timestamp=1449030102091, value=value1112                                                                  \n r1                                      column=f1:, timestamp=1449029774173, value=value1001                                                                  \n r1                                      column=f1:, timestamp=1449029709974, value=value1002                                                                  \n\ntarget:\nhbase(main):023:0> scan &apos;t1&apos;, {RAW => true, VERSIONS => 100}\nROW                                      COLUMN+CELL                                                                                                           \n r1                                      column=f1:, timestamp=1449030102091, value=value1112                                                                  \n r1                                      column=f1:, timestamp=1449030090758, value=value1112                                                                  \n r1                                      column=f1:, timestamp=1449029984282, value=value1111                                                                  \n r1                                      column=f1:, timestamp=1449029774173, value=value1001                                                                  \n r1                                      column=f1:, timestamp=1449029709974, value=value1002   \n/bin/hbase org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication --versions=100 1 t1\norg.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$Counters\n\t\tGOODROWS=1\nDoes not show any mismatch. Ideally it should show. This is because in \nVerifyReplication Class maxVersion is not correctly set.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.16", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.java", "org.apache.hadoop.hbase.replication.TestReplicationSmallTests.java", "org.apache.hadoop.hbase.replication.TestReplicationBase.java"], "label": 1, "es_results": []}, {"bug_id": 14923, "bug_title": "VerifyReplication should not mask the exception during result comparison ", "bug_description": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java\nLine:154\n } catch (Exception e) \n{\n\n            logFailRowAndIncreaseCounter(context, Counters.CONTENT_DIFFERENT_ROWS, value);\n\n          }\n\nJust LOG.error needs to be added for more information for the failure.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.16", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.java"], "label": 1, "es_results": []}, {"bug_id": 15052, "bug_title": "Use EnvironmentEdgeManager in ReplicationSource ", "bug_description": "ReplicationSource is passing System.currentTimeMillis() to MetricsSource.setAgeOfLastShippedOp() which is subtracting that from EnvironmentEdgeManager.currentTime().\n\n\n\n// if there was nothing to ship and it&apos;s not an error\n\n// set \"ageOfLastShippedOp\" to <now> to indicate that we&apos;re current\n\nmetrics.setAgeOfLastShippedOp(System.currentTimeMillis(), walGroupId);\n\n\n\npublic void setAgeOfLastShippedOp(long timestamp, String walGroup) {\n\n    long age = EnvironmentEdgeManager.currentTime() - timestamp;\n\n\n\n we should just use EnvironmentEdgeManager.currentTime() in ReplicationSource", "project": "Apache", "sub_project": "HBASE", "version": "0.98.16.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java"], "label": 1, "es_results": []}, {"bug_id": 15582, "bug_title": "SnapshotManifestV1 too verbose when there are no regions", "bug_description": "swap the INFO for DEBUG in SnapshotManifestV1.loadRegionManifests() otherwise the cleaner will spam everyone for no reason", "project": "Apache", "sub_project": "HBASE", "version": "0.98.16.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.snapshot.SnapshotManifestV1.java"], "label": 1, "es_results": []}, {"bug_id": 15622, "bug_title": "Superusers does not consider the keytab credentials", "bug_description": "After HBASE-13755 the superuser we add by default (the process running hbase) does not take in consideration the keytab credential.\nWe have an env with the process user being hbase and the keytab being hbasefoo.\nfrom Superusers TRACE I see, the hbase being picked up\n\nTRACE Superusers: Current user name is hbase\n\n\n\nfrom the RS audit I see the hbasefoo making requests\n\n\"allowed\":true,\"serviceName\":\"HBASE-1\",\"username\":\"hbasefoo...\n\n\n\nlooking at the code in HRegionServer we do \n\n\n\npublic HRegionServer(Configuration conf, CoordinatedStateManager csm)\n\n      throws IOException {\n\n   ...\n\n    this.userProvider = UserProvider.instantiate(conf);\n\n    Superusers.initialize(conf);\n\n   ..\n\n   // login the server principal (if using secure Hadoop)\n\n    login(userProvider, hostName);\n\n  ..\n\n\n\nBefore HBASE-13755 we were initializing the super user in the ACL coprocessor, so after the login. but now we do that before the login.\nI&apos;m not sure if we can just move the Superuser.initialize() after the login Mikhail Antonov?", "project": "Apache", "sub_project": "HBASE", "version": "0.98.16.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegionServer.java"], "label": 1, "es_results": []}, {"bug_id": 15252, "bug_title": "Data loss when replaying wal if HDFS timeout", "bug_description": "This is a problem introduced by HBASE-13825 where we change the exception type in catch block in readNext method of ProtobufLogReader.\nProtobufLogReader.java\n\n\n      try {\n\n          ......\n\n          ProtobufUtil.mergeFrom(builder, new LimitInputStream(this.inputStream, size),\n\n            (int)size);\n\n        } catch (IOException ipbe) { // <------ used to be InvalidProtocolBufferException\n\n          throw (EOFException) new EOFException(\"Invalid PB, EOF? Ignoring; originalPosition=\" +\n\n            originalPosition + \", currentPosition=\" + this.inputStream.getPos() +\n\n            \", messageSize=\" + size + \", currentAvailable=\" + available).initCause(ipbe);\n\n        }\n\n\n\nHere if the inputStream throws an IOException due to timeout or something, we just convert it to an EOFException and at the bottom of this method, we ignore EOFException and return false. This because the upper layer think we reach the end of file. So when replaying we will treat the HDFS timeout error as a normal end of file and because data loss.", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.17", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.java", "org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java"], "label": 1, "es_results": []}, {"bug_id": 15393, "bug_title": "Enable table replication command will fail when parent znode is not default in peer cluster", "bug_description": "Enable table replication command will fail when parent znode is not /hbase(default) in peer cluster and there is only one peer cluster added in the source cluster.", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.17", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java", "org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.java"], "label": 1, "es_results": []}, {"bug_id": 15635, "bug_title": "Mean age of Blocks in cache (seconds) on webUI should be greater than zero", "bug_description": "", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.17", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.java", "org.apache.hadoop.hbase.io.hfile.CacheStats.java"], "label": 1, "es_results": []}, {"bug_id": 15664, "bug_title": "Use Long.MAX_VALUE instead of HConstants.FOREVER in CompactionPolicy", "bug_description": "The TTL per CF is in seconds, we will convert it to milliseconds when construct HStore. And if it is HConstants.FOREVER, we will set it to Long.MAX_VALUE.\nHStore.java\n\n\n  public static long determineTTLFromFamily(final HColumnDescriptor family) {\n\n    // HCD.getTimeToLive returns ttl in seconds.  Convert to milliseconds.\n\n    long ttl = family.getTimeToLive();\n\n    if (ttl == HConstants.FOREVER) {\n\n      // Default is unlimited ttl.\n\n      ttl = Long.MAX_VALUE;\n\n    } else if (ttl == -1) {\n\n      ttl = Long.MAX_VALUE;\n\n    } else {\n\n      // Second -> ms adjust for user data\n\n      ttl *= 1000;\n\n    }\n\n    return ttl;\n\n  }\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.19", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java", "org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactionPolicy.java"], "label": 1, "es_results": []}, {"bug_id": 15811, "bug_title": "Batch Get after batch Put does not fetch all Cells", "bug_description": "A big batch put followed by a batch get does not always return all Cells put. See attached test program by Robert Farr that reproduces the issue. It seems to be an issue to do with a cluster of more than one machine. Running against a single machine does not have the problem (though the single machine may have many regions). Robert was unable to make his program fail with a single machine only.\nI reproduced what Robert was seeing running his program. I was also unable to make a single machine fail. In a batch of 1000 puts, I see one to three Gets fail. I noticed too that if I wait a second after a fail and then re-get, the Get succeeds.", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.19", "fixed_version": "rel/1.3.0", "fixed_files": ["org.apache.hadoop.hbase.client.AsyncProcess.java", "org.apache.hadoop.hbase.client.TestAsyncProcess.java"], "label": 1, "es_results": []}, {"bug_id": 13945, "bug_title": "Prefix_Tree seekBefore() does not work correctly", "bug_description": "This is related to the TestSeekTo test case where the seekBefore() does not work with Prefix_Tree because of an issue in getFirstKeyInBlock(). In the trunk and branch-1 changing the return type of getFirstKeyInBlock() from BB to Cell resolved the problem, but the same cannot be done in 0.98. Hence we need a change in the KvUtil.copyToNewBuffer API to handle this.  Since the limit is made as the position - in seekBefore when we do \n\n\n\nbyte[] firstKeyInCurrentBlock = Bytes.getBytes(firstKey);\n\n\n\nin HFileReaderV2.seekBefore() we end up in an empty byte array and it would not be the expected one based on which we try to seek to load a new block.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.io.hfile.TestSeekTo.java", "org.apache.hadoop.hbase.io.encoding.DataBlockEncoder.java", "org.apache.hadoop.hbase.KeyValueUtil.java", "org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java"], "label": 1, "es_results": []}, {"bug_id": 16032, "bug_title": "Possible memory leak in StoreScanner", "bug_description": "We observed frequent fullGC of RS in our production environment, and after analyzing the heapdump, we found large memory occupancy by HStore#changedReaderObservers, the map is surprisingly containing 7500w objects...\nAfter some debugging, I located some possible memory leak in StoreScanner constructor:\n\n\n\n  public StoreScanner(Store store, ScanInfo scanInfo, Scan scan, final NavigableSet<byte[]> columns,\n\n      long readPt)\n\n  throws IOException {\n\n    this(store, scan, scanInfo, columns, readPt, scan.getCacheBlocks());\n\n    if (columns != null && scan.isRaw()) {\n\n      throw new DoNotRetryIOException(\"Cannot specify any column for a raw scan\");\n\n    }\n\n    matcher = new ScanQueryMatcher(scan, scanInfo, columns,\n\n        ScanType.USER_SCAN, Long.MAX_VALUE, HConstants.LATEST_TIMESTAMP,\n\n        oldestUnexpiredTS, now, store.getCoprocessorHost());\n\n\n\n    this.store.addChangedReaderObserver(this);\n\n\n\n    // Pass columns to try to filter out unnecessary StoreFiles.\n\n    List<KeyValueScanner> scanners = getScannersNoCompaction();\n\n    ...\n\n    seekScanners(scanners, matcher.getStartKey(), explicitColumnQuery\n\n        && lazySeekEnabledGlobally, parallelSeekEnabled);\n\n    ...\n\n    resetKVHeap(scanners, store.getComparator());\n\n  }\n\n\n\nIf there&apos;s any Exception thrown after this.store.addChangedReaderObserver(this), the returned scanner might be null and there&apos;s no chance to remove the scanner from changedReaderObservers, like in HRegion#get\n\n\n\n    RegionScanner scanner = null;\n\n    try {\n\n      scanner = getScanner(scan);\n\n      scanner.next(results);\n\n    } finally {\n\n      if (scanner != null)\n\n        scanner.close();\n\n    }\n\n\n\nWhat&apos;s more, all exception thrown in the HRegion#getScanner path will cause scanner==null then memory leak, so we also need to handle this part.", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.20", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.regionserver.StoreScanner.java"], "label": 1, "es_results": []}, {"bug_id": 16360, "bug_title": "TableMapReduceUtil addHBaseDependencyJars has the wrong class name for PrefixTreeCodec", "bug_description": "HBASE-15152 included the prefix tree module as dependency to TableMapReduceUtil. but the hardcoded string of the class name is wrong. \n\n\n\nClass.forName(\"org.apache.hadoop.hbase.code.prefixtree.PrefixTreeCodec\");\n\n\n\nshould be \".codec.\" instead of \".code.\"\n\n\n\nClass.forName(\"org.apache.hadoop.hbase.codec.prefixtree.PrefixTreeCodec\");\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.21", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java"], "label": 1, "es_results": []}, {"bug_id": 16471, "bug_title": "Region Server metrics context will be wrong when machine hostname contain \"master\" word", "bug_description": "While initializing RSRpcServices server name is formed as,\n\n\n\n    String name = rs.getProcessName() + \"/\" + initialIsa.toString();\n\n\n\nSo name will be like \"regionserver/host_Name/host_IP:port\".\nDuring MetricsHBaseServer intializing, we create server context name using String contains() which will be wrong when machine hostname contain \"master\" words.\nIn MetricsHBaseServerSourceFactory, \n\n\n\n  protected static String createContextName(String serverName) {\n\n    if (serverName.contains(\"HMaster\") || serverName.contains(\"master\")) {\n\n      return \"Master\";\n\n    } else if (serverName.contains(\"HRegion\") || serverName.contains(\"regionserver\")) {\n\n      return \"RegionServer\";\n\n    }\n\n    return \"IPC\";\n\n  }\n\n\n\nFor example, \"regionserver/node-master1-xyz/host-IP:16020\"", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.21", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceFactory.java", "org.apache.hadoop.hbase.ipc.TestRpcMetrics.java"], "label": 1, "es_results": []}, {"bug_id": 16552, "bug_title": "MiniHBaseCluster#getServerWith() does not ignore stopped RSs", "bug_description": "MiniHBaseCluster#getServerWith() does not ignore stopped RSs ", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.21", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.MiniHBaseCluster.java"], "label": 1, "es_results": []}, {"bug_id": 16460, "bug_title": "Cannot rebuild the BucketAllocator's data structures when BucketCache uses FileIOEngine", "bug_description": "When bucket cache use FileIOEngine, it will rebuild the bucket allocator&apos;s data structures from a persisted map. So it should first read the map from persistence file then use the map to new a BucketAllocator. But now the code has wrong sequence in retrieveFromFile() method of BucketCache.java.\n\n\n\n      BucketAllocator allocator = new BucketAllocator(cacheCapacity, bucketSizes, backingMap, realCacheSize);\n\n      backingMap = (ConcurrentHashMap<BlockCacheKey, BucketEntry>) ois.readObject();\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.22", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java", "org.apache.hadoop.hbase.io.hfile.CacheTestUtils.java", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.java", "org.apache.hadoop.hbase.io.hfile.bucket.TestBucketCache.java"], "label": 1, "es_results": []}, {"bug_id": 16612, "bug_title": "Use array to cache Types for KeyValue.Type.codeToType", "bug_description": "We don&apos;t rely on enum ordinals in KeyValye.Type. We have own code in it. In codeToType, we use a loop to find the Type which is not a good idea. We can just use an arryay[256] to cache all types.", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.22", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.KeyValue.java"], "label": 1, "es_results": []}, {"bug_id": 16165, "bug_title": "Decrease RpcServer.callQueueSize before writeResponse causes OOM", "bug_description": "In RpcServer, we use callQueueSizeInBytes to avoid queuing too many calls which causes OOM. But in CallRunner.run, we decrease it before send the response back. And even after calling sendResponseIfReady, the call object could stay in our heap for a long time if we can not write out the response(That&apos;s why we need a Responder thread...). This makes it possible that the actual size of all call object in heap is larger than maxQueueSizeInBytes and causes OOM.", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.22", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.RpcServer.java"], "label": 1, "es_results": []}, {"bug_id": 16723, "bug_title": "RMI registry is not destroyed after stopping JMX Connector Server", "bug_description": "We are creating RMI registry in JMXListener.startConnectorServer() ,\n\n\n\n    // Create the RMI registry\n\n    LocateRegistry.createRegistry(rmiRegistryPort);\n\n\n\nThis registry is never deregistered, should be destoyed after stopping JMX Connector server.", "project": "Apache", "sub_project": "HBASE", "version": "rel/0.98.22", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.JMXListener.java"], "label": 1, "es_results": []}, {"bug_id": 11374, "bug_title": "RpcRetryingCaller#callWithoutRetries has a timeout of zero", "bug_description": "This code is called by the client on the \"multi\" path.\nAs zero is detected as infinite value, we fallback to 2 seconds, which may not may correct.\nTypically, you can see this kind of message in the client (see the SocketTimeoutException: 2000)\n\n2014-08-08 17:22:43 o.a.h.h.c.AsyncProcess [INFO] #105158,\n\ntable=rt_global_monthly_campaign_deliveries, attempt=10/35 failed 500 ops,\n\nlast exception: java.net.SocketTimeoutException: Call to\n\nip-10-201-128-23.us-west-1.compute.internal/10.201.128.23:60020 failed\n\nbecause java.net.SocketTimeoutException: 2000 millis timeout while waiting\n\nfor channel to be ready for read. ch :\n\njava.nio.channels.SocketChannel[connected local=/10.248.130.152:46014\n\nremote=ip-10-201-128-23.us-west-1.compute.internal/10.201.128.23:60020] on\n\nip-10-201-128-23.us-west-1.compute.internal,60020,1405642103651, tracking\n\nstarted Fri Aug 08 17:21:55 UTC 2014, retrying after 10043 ms, replay 500\n\nops.\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.98.3", "fixed_version": "0.98.4", "fixed_files": ["org.apache.hadoop.hbase.client.AsyncProcess.java"], "label": 1, "es_results": []}, {"bug_id": 13734, "bug_title": "Improper timestamp checking with VisibilityScanDeleteTracker", "bug_description": "3 issues\n1. When VC is used and all the put cells and delete cells are not having any visibility associated with them, we are not correctly checking put cells ts against that of delete cell resulting in deletion of cells coming in after the delete ts\n2. Have a row r1 with 2 cells of same TS but different visibility. In order to delete both cells we have to apply 2 deletes with these 2 visibility being set to Delete. We are trying to do this using delete full row option or delete cf way. But only one cell is getting deleted.\n3. Same case as in #2 when I try to delete using family version delete, only one cell is getting deleted.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.4", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker.java", "org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithDeletes.java"], "label": 1, "es_results": []}, {"bug_id": 14445, "bug_title": "ExportSnapshot does not honor -chmod option", "bug_description": "Create a snapshot of an existing HBase table, export the snapshot using the -chuser, -chgroup, -chmod options.\nLook in hdfs filesystem for export. The files do not have the correct ownership, group, permissions\nThanks to Ian Roberts who first reported the issue.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.4", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.snapshot.ExportSnapshot.java"], "label": 1, "es_results": []}, {"bug_id": 14054, "bug_title": "Acknowledged writes may get lost if regionserver clock is set backwards", "bug_description": "We experience a small amount of lost acknowledged writes in production on July 1st (~700 identified so far).\nWhat happened was that we had NTP turned off since June 29th to prevent issues due to the leap second on June 30th. NTP was turned back on July 1st.\nThe next day, we noticed we were missing writes to a few of our higher throughput aggregation tables.\nWe found that this is caused by HBase taking the current time using System.currentTimeMillis, which may be set backwards by NTP, and using this without any checks to populate the timestamp of rows for which the client didn&apos;t supply a timestamp.\nOur application uses a read-modify-write pattern using get+checkAndPut to perform aggregation as follows:\n1. read version 1\n2. mutate\n3. write version 2\n4. read version 2\n5. mutate\n6. write version 3\nThe application retries the full read-modify-write if the checkAndPut fails.\nWhat must have happened on July 1st, after we started NTP back up, was this (timestamps added):\n1. read version 1 (timestamp 10)\n2. mutate\n3. write version 2 (HBase-assigned timestamp 11)\n4. read version 2 (timestamp 11)\n5. mutate\n6. write version 3 (HBase-assigned timestamp 10)\nHence, the last write was eclipsed by the first write, and hence, an acknowledged write was lost.\nWhile this seems to match documented behavior (paraphrasing: \"if timestamp is not specified HBase will assign a timestamp using System.currentTimeMillis\" \"the row with the highest timestamp will be returned by get\"), I think it is very unintuitive and needs at least a big warning in the documentation, along the lines of \"Acknowledged writes may not be visible unless the timestamp is explicitly specified and equal to or larger than the highest timestamp for that row\".\nI would also like to use this ticket to start a discussion on if we can make the behavior better:\nCould HBase assign a timestamp of max(max timestamp for the row, System.currentTimeMillis()) in the checkAndPut write path, instead of blindly taking System.currentTimeMillis(), similar to what has been done in HBASE-12449 for increment and append?\nThoughts?", "project": "Apache", "sub_project": "HBASE", "version": "0.98.6", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.regionserver.TestHRegion.java"], "label": 1, "es_results": []}, {"bug_id": 12198, "bug_title": "Fix the bug of not updating location cache", "bug_description": "Fix the bug of not updating location cache.\nAdd a testcase for it.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.7", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.AsyncProcess.java", "org.apache.hadoop.hbase.client.HTableMultiplexer.java"], "label": 1, "es_results": []}, {"bug_id": 12373, "bug_title": "Provide a command to list visibility labels", "bug_description": "A command to list visibility labels that are in place would be handy.\nThis is also in line with many of the other hbase list commands.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.7", "fixed_version": "1.0.0", "fixed_files": ["org.apache.hadoop.hbase.security.visibility.VisibilityController.java", "org.apache.hadoop.hbase.security.visibility.VisibilityClient.java", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.java", "org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.java", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelService.java", "org.apache.hadoop.hbase.security.visibility.ExpAsStringVisibilityLabelServiceImpl.java", "org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithDefaultVisLabelService.java"], "label": 1, "es_results": []}, {"bug_id": 12393, "bug_title": "The regionserver web will throw exception if we disable block cache", "bug_description": "The CacheConfig.getBlockCache() will return the null point when we set hfile.block.cache.size to zero.\nThe BlockCacheTmpl.jamon doesn&apos;t make a check on null blockcache.\n\n\n\n<%if cacheConfig == null %>\n\n<p>CacheConfig is null</p>\n\n<%else>\n\n<table class=\"table table-striped\">\n\n    <tr>\n\n        <th>Attribute</th>\n\n        <th>Value</th>\n\n        <th>Description</th>\n\n    </tr>\n\n    <tr>\n\n        <td>Size</td>\n\n        <td><% StringUtils.humanReadableInt(cacheConfig.getBlockCache().size()) %></td>\n\n        <td>Total size of Block Cache (bytes)</td>\n\n    </tr>\n\n    <tr>\n\n        <td>Free</td>\n\n        <td><% StringUtils.humanReadableInt(cacheConfig.getBlockCache().getFreeSize()) %></td>\n\n        <td>Free space in Block Cache (bytes)</td>\n\n    </tr>\n\n    <tr>\n\n        <td>Count</td>\n\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getBlockCount()) %></td>\n\n        <td>Number of blocks in Block Cache</td>\n\n    </tr>\n\n    <tr>\n\n        <td>Evicted</td>\n\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getEvictedCount()) %></td>\n\n        <td>Number of blocks evicted</td>\n\n    </tr>\n\n    <tr>\n\n        <td>Evictions</td>\n\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getEvictionCount()) %></td>\n\n        <td>Number of times an eviction occurred</td>\n\n    </tr>\n\n    <tr>\n\n        <td>Hits</td>\n\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getHitCount()) %></td>\n\n        <td>Number requests that were cache hits</td>\n\n    </tr>\n\n    <tr>\n\n        <td>Hits Caching</td>\n\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getHitCachingCount()) %></td>\n\n        <td>Cache hit block requests but only requests set to use Block Cache</td>\n\n    </tr>\n\n    <tr>\n\n        <td>Misses</td>\n\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getMissCount()) %></td>\n\n        <td>Number of requests that were cache misses</td>\n\n    </tr>\n\n    <tr>\n\n        <td>Misses Caching</td>\n\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getMissCount()) %></td>\n\n        <td>Block requests that were cache misses but only requests set to use Block Cache</td>\n\n    </tr>\n\n    <tr>\n\n        <td>Hit Ratio</td>\n\n        <td><% String.format(\"%,.2f\", cacheConfig.getBlockCache().getStats().getHitRatio() * 100) %><% \"%\" %></td>\n\n        <td>Hit Count divided by total requests count</td>\n\n    </tr>\n\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "0.98.7", "fixed_version": "1.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.java"], "label": 1, "es_results": []}, {"bug_id": 15221, "bug_title": "HTableMultiplexer improvements (stale region locations and resource leaks)", "bug_description": "It looks like HTableMultiplexer has a couple of issues.\nUpon failing to send a Put to the appropriate RS, the Put is re-queued back into the system. Normally this is fine as such an exception is transient and the Put would eventually succeed. However, in the case where the Put was rejected because of a NotServingRegionException (e.g. split, balance, merge), the re-queuing of the Put will end up using the same cached HRegionLocation. This means that the Put will just be repeatedly sent back to the same RS over and over again, eventually being dropped on the floor. Need to invalidate the location cache (or make sure we refresh it) when we re-queue the Put.\nThe internal ClusterConnection is also leaked. If a user creates many HTableMultiplexers, they&apos;ll eventually run into issues (memory, zk connections, etc) because they&apos;ll never get cleaned up. HTableMultiplexer needs a close method.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.7", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.HTableMultiplexer.java"], "label": 1, "es_results": []}, {"bug_id": 12832, "bug_title": "Describe table from she will no longer shows Table's attributes, only CF attributes", "bug_description": "When you describe a table with some attributes at the table level, it is not shown from she will any more:\n\n\n\nhbase(main):010:0> create &apos;usertable2&apos;, &apos;family&apos;, {REGION_REPLICATION => 2, CONFIGURATION => {&apos;hbase.hregion.scan.loadColumnFamiliesOnDemand&apos; => &apos;true&apos;}}\n\nhbase(main):011:0> describe &apos;usertable2&apos; \n\nTable usertable2 is ENABLED                                                                                                                                                                                 \n\nCOLUMN FAMILIES DESCRIPTION                                                                                                                                                                                 \n\n{NAME => &apos;family&apos;, DATA_BLOCK_ENCODING => &apos;NONE&apos;, BLOOMFILTER => &apos;ROW&apos;, REPLICATION_SCOPE => &apos;0&apos;, VERSIONS => &apos;1&apos;, COMPRESSION => &apos;NONE&apos;, MIN_VERSIONS => &apos;0&apos;, TTL => &apos;FOREVER&apos;, KEEP_DELETED_CELLS => &apos;FALS\n\nE&apos;, BLOCKSIZE => &apos;65536&apos;, IN_MEMORY => &apos;false&apos;, BLOCKCACHE => &apos;true&apos;}                                                                                                                                       \n\n1 row(s) in 0.0200 seconds\n\n\n\nMaster UI shows: \n\n\n\n&apos;usertable2&apos;, {TABLE_ATTRIBUTES => {REGION_REPLICATION => &apos;2&apos;, CONFIGURATION => {&apos;hbase.hregion.scan.loadColumnFamiliesOnDemand&apos; => &apos;true&apos;}}, {NAME => &apos;family&apos;}\n\n\n\nHBASE-10082 changed the formatting from she will for one line per CF. We should add the table level attributes back to the formatting.", "project": "Apache", "sub_project": "HBASE", "version": "0.98.8", "fixed_version": "1.0.0", "fixed_files": ["org.apache.hadoop.hbase.HTableDescriptor.java"], "label": 1, "es_results": []}, {"bug_id": 11542, "bug_title": "Unit Test  KeyStoreTestUtil.java compilation failure in IBM JDK ", "bug_description": "In trunk,  jira HBase-10336 added a utility test KeyStoreTestUtil.java, which leverages the following sun classes:\n import sun.security.x509.AlgorithmId;\n import sun.security.x509.CertificateAlgorithmId;\n ....\nthis because hbase compiler failure if using IBM JDK,  \nThere are similar classes like below in IBM jdk: \nimport com.ibm.security.x509.AlgorithmId;\nimport com.ibm.security.x509.CertificateAlgorithmId; \nThis jira is to add handling of the x509 references. \n", "project": "Apache", "sub_project": "HBASE", "version": "0.99.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.http.ssl.KeyStoreTestUtil.java"], "label": 1, "es_results": []}, {"bug_id": 12491, "bug_title": "TableMapReduceUtil.findContainingJar() NPE", "bug_description": "Adding a bootclasspath library causes an NPE when running hbase map reduce jobs in TableMapReduceUtil.findContainingJar().  Classes in the library added to the bootclasspath get a null classpathLoader.   Check for a null loader in  TableMapReduceUtil.findContainingJar().", "project": "Apache", "sub_project": "HBASE", "version": "0.99.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java"], "label": 1, "es_results": []}, {"bug_id": 13834, "bug_title": "Evict count not properly passed to HeapMemoryTuner.", "bug_description": "Evict count calculated inside the HeapMemoryManager class in tune function that is passed to HeapMemoryTuner via TunerContext is miscalculated. It is supposed to be Evict count between two intervals but its not. ", "project": "Apache", "sub_project": "HBASE", "version": "1.0.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HeapMemoryManager.java"], "label": 1, "es_results": []}, {"bug_id": 13847, "bug_title": "getWriteRequestCount function in HRegionServer uses int variable to return the count.", "bug_description": "Variable used to return the value of getWriteRequestCount is int, must be long. I think it causes cluster UI to show negative Write Request Count.", "project": "Apache", "sub_project": "HBASE", "version": "1.0.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegionServer.java"], "label": 1, "es_results": []}, {"bug_id": 13821, "bug_title": "WARN if hbase.bucketcache.percentage.in.combinedcache is set", "bug_description": "HBASE-11520 improved configuration of bucket cache to no longer require hbase.bucketcache.percentage.in.combinedcache. This was done rather aggressively, with this previously mandatory configuration being ignored. This can result in RS crashes for unsuspecting users. We should add a WARN when hbase.bucketcache.percentage.in.combinedcache is set to make debugging the crash more straight forward.", "project": "Apache", "sub_project": "HBASE", "version": "1.0.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.io.hfile.CacheConfig.java"], "label": 1, "es_results": []}, {"bug_id": 14663, "bug_title": "HStore::close does not honor config hbase.rs.evictblocksonclose", "bug_description": "I noticed moving regions was slow and due to the wait for the bucket cache to clear.  I tried setting hbase.rs.evictblocksonclose and it did not help.\nI see the HStore::close method has evictonclose hard coded to true instead of letting the config dictate:\n// close each store file in parallel\nCompletionService<Void> completionService =\n   new ExecutorCompletionService<Void>(storeFileCloserThreadPool);\nfor (final StoreFile f : result) {\n   completionService.submit(new Callable<Void>() {\n     @Override\n     public Void call() throws IOException \n{\n\n       f.closeReader(true);\n\n       return null;\n\n     }\n   });\n}", "project": "Apache", "sub_project": "HBASE", "version": "1.0.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HStore.java"], "label": 1, "es_results": []}, {"bug_id": 14624, "bug_title": "BucketCache.freeBlock is too expensive", "bug_description": "Moving regions is unacceptably slow when using bucket cache, as it takes too long to free all the blocks.", "project": "Apache", "sub_project": "HBASE", "version": "1.0.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.java"], "label": 1, "es_results": []}, {"bug_id": 14788, "bug_title": "Splitting a region does not support the hbase.rs.evictblocksonclose config when closing source region", "bug_description": "i have a table with bucket cache turned on and hbase.rs.evictblocksonclose set to false.  I split a region and watched that the closing of the source region did not complete until the bucketcache was flushed for that region.", "project": "Apache", "sub_project": "HBASE", "version": "1.0.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HStore.java", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.java", "org.apache.hadoop.hbase.regionserver.StoreFile.java"], "label": 1, "es_results": []}, {"bug_id": 15955, "bug_title": "Disable action in CatalogJanitor#setEnabled should wait for active cleanup scan to finish", "bug_description": "When user calls Admin.enableCatalogJanitor(false) to disable the catalog janitor, it expects that the janitor would stop working once the API returns.  It is not true, the janitor could have an active scan going on and clean up unused region.  The &apos;false&apos; state would be enforced during the next background runs.  \nTo avoid confusing, if &apos;CatalogJanitor.enabled&apos; is true and we want to set to false in CatalogJanitor#setEnabled, the function should wait for the on-going active scan to complete.", "project": "Apache", "sub_project": "HBASE", "version": "1.0.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.CatalogJanitor.java"], "label": 1, "es_results": []}, {"bug_id": 13958, "bug_title": "RESTApiClusterManager calls kill() instead of suspend() and resume()", "bug_description": "suspend() and resume() of the REST ClusterManager are calling the wrong method.\n\n\n\n  @Override\n\n  public void suspend(ServiceType service, String hostname, int port) throws IOException {\n\n    hBaseClusterManager.kill(service, hostname, port);\n\n  }\n\n\n\n  @Override\n\n  public void resume(ServiceType service, String hostname, int port) throws IOException {\n\n    hBaseClusterManager.kill(service, hostname, port);\n\n  }\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "1.0.1.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.RESTApiClusterManager.java"], "label": 1, "es_results": []}, {"bug_id": 13997, "bug_title": "ScannerCallableWithReplicas because Infinitely blocking", "bug_description": "Bug in ScannerCallableWithReplicas.addCallsForOtherReplicas method  \ncode in ScannerCallableWithReplicas.addCallsForOtherReplicas\n\n\nprivate int addCallsForOtherReplicas(\n\n      BoundedCompletionService<Pair<Result[], ScannerCallable>> cs, RegionLocations rl, int min,\n\n      int max) {\n\n    if (scan.getConsistency() == Consistency.STRONG) {\n\n      return 0; // not scheduling on other replicas for strong consistency\n\n    }\n\n    for (int id = min; id <= max; id++) {\n\n      if (currentScannerCallable.getHRegionInfo().getReplicaId() == id) {\n\n        continue; //this was already scheduled earlier\n\n      }\n\n      ScannerCallable s = currentScannerCallable.getScannerCallableForReplica(id);\n\n      if (this.lastResult != null) {\n\n        s.getScan().setStartRow(this.lastResult.getRow());\n\n      }\n\n      outstandingCallables.add(s);\n\n      RetryingRPC retryingOnReplica = new RetryingRPC(s);\n\n      cs.submit(retryingOnReplica);\n\n    }\n\n    return max - min + 1;\t//bug? should be \"max - min\",because \"continue\"\n\n                                        //always happen once\n\n  }\n\n\n\nIt can cause completed < submitted always so that the following code will be infinitely blocked.\ncode in ScannerCallableWithReplicas.call\n\n\n// submitted larger than the actual one\n\n submitted += addCallsForOtherReplicas(cs, rl, 0, rl.size() - 1);\n\n    try {\n\n      //here will be affected\n\n      while (completed < submitted) {\n\n        try {\n\n          Future<Pair<Result[], ScannerCallable>> f = cs.take();\n\n          Pair<Result[], ScannerCallable> are = f.get();\n\n          if (are != null && r.getSecond() != null) {\n\n            updateCurrentlyServingReplica(r.getSecond(), r.getFirst(), done, pool);\n\n          }\n\n          return are == null ? null : r.getFirst(); // great we got an answer\n\n        } catch (ExecutionException e) {\n\n          // if not cancel or interrupt, wait until all RPC&apos;s are done\n\n          // one of the tasks failed. Save the exception for later.\n\n          if (exceptions == null) exceptions = new ArrayList<ExecutionException>(rl.size());\n\n          exceptions.add(e);\n\n          completed++;\n\n        }\n\n      }\n\n    } catch (CancellationException e) {\n\n      throw new InterruptedIOException(e.getMessage());\n\n    } catch (InterruptedException e) {\n\n      throw new InterruptedIOException(e.getMessage());\n\n    } finally {\n\n      // We get there because we were interrupted or because one or more of the\n\n      // calls succeeded or failed. In all case, we stop all our tasks.\n\n      cs.cancelAll(true);\n\n    }\n\n\n\nIf all replica-RS occur ExecutionException ,it will be infinitely blocked in  cs.take()", "project": "Apache", "sub_project": "HBASE", "version": "1.0.1.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.TestClientScanner.java", "org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.java"], "label": 1, "es_results": []}, {"bug_id": 14153, "bug_title": "Typo in ProcedureManagerHost.MASTER_PROCEUDRE_CONF_KEY", "bug_description": "The constant should read PROCE DU RE.", "project": "Apache", "sub_project": "HBASE", "version": "1.0.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.procedure.ProcedureManagerHost.java", "org.apache.hadoop.hbase.procedure.TestProcedureManager.java", "org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost.java"], "label": 1, "es_results": []}, {"bug_id": 14224, "bug_title": "Fix coprocessor handling of duplicate classes", "bug_description": "While discussing with Misty Stanley-Jones over on HBASE-13907 we noticed some inconsistency when copros are loaded. Sometimes you can load them more than once, sometimes you can not. Need to consolidate.", "project": "Apache", "sub_project": "HBASE", "version": "1.0.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.TestHTableDescriptor.java", "org.apache.hadoop.hbase.HTableDescriptor.java", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java", "org.apache.hadoop.hbase.HConstants.java"], "label": 1, "es_results": []}, {"bug_id": 14759, "bug_title": "Avoid using Math.abs when selecting SyncRunner in FSHLog", "bug_description": "FSHLog.java\n\n\nint index = Math.abs(this.syncRunnerIndex++) % this.syncRunners.length;\n\n          try {\n\n            this.syncRunners[index].offer(sequence, this.syncFutures, this.syncFuturesCount);\n\n          } catch (Exception e) {\n\n            // Should NEVER get here.\n\n            requestLogRoll();\n\n            this.exception = new DamagedWALException(\"Failed offering sync\", e);\n\n          }\n\n\n\nMath.abs will return Integer.MIN_VALUE if you pass Integer.MIN_VALUE in since the actual absolute value of Integer.MIN_VALUE is out of range.\nI think this.syncRunnerIndex++ will overflow eventually if we keep the regionserver running for enough time.", "project": "Apache", "sub_project": "HBASE", "version": "1.0.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.wal.TestFSHLog.java", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.java"], "label": 1, "es_results": []}, {"bug_id": 14809, "bug_title": "Grant / revoke Namespace admin permission to group ", "bug_description": "Hi, \nWe are looking to roll out HBase and are in the process to design the security model. \nWe are looking to implement global DBAs and Namespace specific administrators. \nSo for example the global dba would create a namespace and grant a user/group admin privileges within that ns. \nSo that a given ns admin can in turn create objects and grant permission within the given ns only. \nWe have run into some issues at the ns admin level. It appears that a ns admin can NOT grant to a grop unless it also has global admin privilege. But once it has global admin privilege it can grant in any NS not just the one where it has admin privileges. \nBased on the HBase documentation at http://hbase.apache.org/book.html#appendix_acl_matrix \nTable 13. ACL Matrix \nInterface\tOperation\tPermissions \nAccessController grant(global level) global(A) \ngrant(namespace level) global(A)|NS(A) \ngrant at a namespace level should be possible for someone with global A OR (|) NS A permission. \nAs you will see in our test it does not work if NS A permission is granted but global A permission is not. \nHere you can see that group hbaseappltest_ns1admin has XCA permission on ns1. \n\n\n\nhbase(main):011:0> scan &apos;hbase:acl&apos; \n\nROW COLUMN+CELL \n\n@ns1 column=l:@hbaseappltest_ns1admin, timestamp=1446676679787, value=XCA \n\n\n\nHowever: \nHere you can see that a user who is member of the group hbaseappltest_ns1admin can not grant a WRX privilege to a group as it is missing global A privilege. \n\n\n\n$hbase she will \n\n15/11/13 10:02:23 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available \n\nHBase She Will; enter &apos;help<RETURN>&apos; for list of supported commands. \n\nType \"exit<RETURN>\" to leave the HBase She Will \n\nVersion 1.0.0-cdh5.4.7, rUnknown, Thu Sep 17 02:25:03 PDT 2015 \n\n\n\nhbase(main):001:0> whoami \n\nns1admin@WLAB.NET (auth:KERBEROS) \n\ngroups: hbaseappltest_ns1admin \n\n\n\nhbase(main):002:0> grant &apos;@hbaseappltest_ns1funct&apos; ,&apos;RWX&apos;,&apos;@ns1&apos; \n\n\n\nERROR: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions for user &apos;ns1admin&apos; (global, action=ADMIN) \n\n\n\nThe way I read the documentation a NS admin should be able to grant as it has ns level A privilege not only object level permission.\nCDH is a version 5.4.7 and Hbase is version 1.0. \nRegards, \nSteven", "project": "Apache", "sub_project": "HBASE", "version": "1.0.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.security.access.AccessController.java", "org.apache.hadoop.hbase.security.access.TestNamespaceCommands.java"], "label": 1, "es_results": []}, {"bug_id": 14717, "bug_title": "enable_table_replication command should only create specified table for a peer cluster", "bug_description": "For a peer only user specified tables should be created but enable_table_replication command is not honouring that.\neg:\nlike peer1 : t1:cf1, t2\ncreate &apos;t3&apos;, &apos;d&apos;\nenable_table_replication &apos;t3&apos; > should not create t3 in peer1\n", "project": "Apache", "sub_project": "HBASE", "version": "1.0.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.java", "org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java", "org.apache.hadoop.hbase.replication.ReplicationPeerZKImpl.java"], "label": 1, "es_results": []}, {"bug_id": 14770, "bug_title": "RowCounter argument input parse error", "bug_description": "I&apos;m tried to use https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java code and package a new jar then excuted following she will script:\n\n\n\nhadoop jar test.jar <tablename> --range=row001,row002 cf:c2\n\n\n\nThen I got \"NoSuchColumnFamilyException\".\nIt seems input argument parsing problem.\nAnd I tried to add \n\n\n\ncontinue; \n\n\n\nafter #L123 to avoid \"--range=*\" string be appended to qualifer.\nThen the problem seems solve.\n\ndata in table:\n\n\nrow\ncf:c1\ncf:c2\ncf:c3\ncf:c4\n\n\nrow001\nv1\nv2\n\n\n\n\nrow002\n\nv2\nv3\n\n\n\nrow003\n\n\nv3\nv4\n\n\nrow004\nv1\n\n\nv4\n\n\nException Message:\n\n\n\norg.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family --range=row001,row002 does not exist in region frank_rowcounttest1,,1446191360354.6c52c71a82f0fa041c467002a2bf433c. in table &apos;frank_rowcounttest1&apos;, {NAME => &apos;cf&apos;, DATA_BLOCK_ENCODING => &apos;NONE&apos;, BLOOMFILTER => &apos;ROW&apos;, REPLICATION_SCOPE => &apos;0&apos;, COMPRESSION => &apos;NONE&apos;, VERSIONS => &apos;1&apos;, TTL => &apos;FOREVER&apos;, MIN_VERSIONS => &apos;0&apos;, KEEP_DELETED_CELLS => &apos;false&apos;, BLOCKSIZE => &apos;65536&apos;, IN_MEMORY => &apos;false&apos;, BLOCKCACHE => &apos;true&apos;}\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.0.3", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.RowCounter.java", "org.apache.hadoop.hbase.mapreduce.TestRowCounter.java"], "label": 1, "es_results": []}, {"bug_id": 16294, "bug_title": "hbck reporting \"No HDFS region dir found\" for replicas", "bug_description": "simple test, create a table with replicas and then run hbck. \nwe don&apos;t filter out the replicas for the loadHdfsRegioninfo()\n\n$ hbase she will\n\nhbase(main):001:0> create &apos;myTable&apos;, &apos;myCF&apos;, {REGION_REPLICATION => &apos;3&apos;}\n\n\n\n$ hbase hbck\n\n2016-07-27 13:47:38,090 WARN  [hbasefsck-pool1-t2] util.HBaseFsck: No HDFS region dir found: { meta => myTable,,1469652448440_0002.9dea3506e09e00910158dc91fa21e550., hdfs => null, deployed => u1604srv,42895,1469652420413;myTable,,1469652448440_0002.9dea3506e09e00910158dc91fa21e550., replicaId => 2 } meta={ENCODED => 9dea3506e09e00910158dc91fa21e550, NAME => &apos;myTable,,1469652448440_0002.9dea3506e09e00910158dc91fa21e550.&apos;, STARTKEY => &apos;&apos;, ENDKEY => &apos;&apos;, REPLICA_ID => 2}\n\n2016-07-27 13:47:38,092 WARN  [hbasefsck-pool1-t1] util.HBaseFsck: No HDFS region dir found: { meta => myTable,,1469652448440_0001.a03250bca30781ff7002a91c281b4e92., hdfs => null, deployed => u1604srv,42895,1469652420413;myTable,,1469652448440_0001.a03250bca30781ff7002a91c281b4e92., replicaId => 1 } meta={ENCODED => a03250bca30781ff7002a91c281b4e92, NAME => &apos;myTable,,1469652448440_0001.a03250bca30781ff7002a91c281b4e92.&apos;, STARTKEY => &apos;&apos;, ENDKEY => &apos;&apos;, REPLICA_ID => 1}\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.0.3", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.util.HBaseFsck.java"], "label": 1, "es_results": []}, {"bug_id": 13617, "bug_title": "TestReplicaWithCluster.testChangeTable timeout", "bug_description": "In our internal test TestReplicaWithCluster.testChangeTable got a timeout.  \nHBASE-12170 bumped TestReplicaWithCluster.testReplicaAndReplication timeout from 2 minutes to 5 minutes, but leaves other tests in the same package unchanged.\nWhen I run the test in my fast Mac, the run time of TestReplicaWithCluster.testChangeTable (~16 seconds) is about half of TestReplicaWithCluster.testReplicaAndReplication (~31 seconds).  \nWe should increase the timeout in TestReplicaWithCluster.testChangeTable to avoid test failure in slow environment.\n", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.TestReplicaWithCluster.java"], "label": 1, "es_results": []}, {"bug_id": 13704, "bug_title": "Hbase throws OutOfOrderScannerNextException when MultiRowRangeFilter is used", "bug_description": "When using filter MultiRowRangeFilter with ranges closed to each other that there are no rows between ranges, then OutOfOrderScannerNextException is throwed.\nIn filterRowKey method when range is switched to the next range, currentReturnCode is set to SEEK_NEXT_USING_HINT (MultiRowRangeFilter: 118 in v1.1.0). But if new range is already contain this row, then we should include this row, not to seek for another one.\nReplacing line 118 to this code seems to be working fine:\n\n\n\nif (range.contains(buffer, offset, length)) {\n\n    currentReturnCode = ReturnCode.INCLUDE;\n\n} else {\n\n    currentReturnCode = ReturnCode.SEEK_NEXT_USING_HINT;\n\n}\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.filter.MultiRowRangeFilter.java", "org.apache.hadoop.hbase.filter.TestMultiRowRangeFilter.java"], "label": 1, "es_results": []}, {"bug_id": 13731, "bug_title": "TestReplicationAdmin should clean up MiniZKCluster resource", "bug_description": "org.apache.hadoop.hbase.client.replication.TestReplicationAdmin Unit test has a @BeforeClass component to start MiniZKCluster, but it does not have a @AfterClass component to shut down MiniZKCluster and clean up the resources from MiniZKCluster.  In Jenkins machine that continuously run tests, the resource leak could affect other tests. \nThe solution is trivial.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.java"], "label": 1, "es_results": []}, {"bug_id": 13733, "bug_title": "Failed MiniZooKeeperCluster startup did not shutdown ZK servers", "bug_description": "MiniZooKeeperCluster#startup() starts servers one-by-one, if everything is good, it would declare success of start:\n\n\n\n  public int startup(File baseDir, int numZooKeeperServers) \n\n    ...\n\n    // running all the ZK servers\n\n    for (int i = 0; i < numZooKeeperServers; i++) {\n\n    ...===> could throw exception in the loop and end the startup\n\n      // Start up this ZK server\n\n      standaloneServerFactory.startup(server);\n\n      ...\n\n      standaloneServerFactoryList.add(standaloneServerFactory);\n\n      zooKeeperServers.add(server);\n\n    }\n\n   ...\n\n    started = true;\n\n    ...\n\n  }\n\n\n\n\n\nHowever, if exception throws in the middle of start up (eg. some servers already started), the MiniZooKeeperCluster#shutdown() would not shut down them and clean up resources.  \n\n\n\n  public void shutdown() throws IOException {\n\n    if (!started) {\n\n      return;\n\n    }\n\n    ...\n\n  }\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.java"], "label": 1, "es_results": []}, {"bug_id": 13843, "bug_title": "Fix internal constant text in ReplicationManager.java", "bug_description": "ReplicationAdmin.java:  \npublic static final String CFNAME = \"columnFamlyName; (sic)\nFix.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java"], "label": 1, "es_results": []}, {"bug_id": 13923, "bug_title": "Loaded region coprocessors are not reported in she will status command", "bug_description": "I added a CP to a table using the she will&apos;s alter command. Now I tried to check if it was loaded (short of resorting to parsing the logs). I recalled the refguide mentioned the status &apos;detailed&apos; command, and tried that to no avail.\nThe UI shows the loaded class in the Software Attributes section, so the info is there. But a she will status command (even after waiting 12+ hours shows nothing. Here an example of a server that has it loaded according to describe and the UI, but the she will lists this:\n\n    slave-1.internal.larsgeorge.com:16020 1434486031598\n\n        requestsPerSecond=0.0, numberOfOnlineRegions=5, usedHeapMB=278, maxHeapMB=941, numberOfStores=5, numberOfStorefiles=3, storefileUncompressedSizeMB=2454, storefileSizeMB=2454, compressionRatio=1.0000, memstoreSizeMB=0, storefileIndexSizeMB=0, readRequestsCount=32070, writeRequestsCount=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=2086, totalStaticBloomSizeKB=480, totalCompactingKVs=0, currentCompactedKVs=0, compactionProgressPct=NaN, coprocessors=[]\n\n        \"testqauat:usertable,,1433747062257.4db0d7d73cbaac45cb8568d5b185e1f2.\"\n\n            numberOfStores=1, numberOfStorefiles=0, storefileUncompressedSizeMB=0, lastMajorCompactionTimestamp=0, storefileSizeMB=0, memstoreSizeMB=0, storefileIndexSizeMB=0, readRequestsCount=0, writeRequestsCount=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, totalCompactingKVs=0, currentCompactedKVs=0, compactionProgressPct=NaN, completeSequenceId=-1, dataLocality=0.0\n\n        \"testqauat:usertable,user0,1433747062257.f7c7fe3c7d26910010f40101b20f8d06.\"\n\n            numberOfStores=1, numberOfStorefiles=0, storefileUncompressedSizeMB=0, lastMajorCompactionTimestamp=0, storefileSizeMB=0, memstoreSizeMB=0, storefileIndexSizeMB=0, readRequestsCount=0, writeRequestsCount=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, totalCompactingKVs=0, currentCompactedKVs=0, compactionProgressPct=NaN, completeSequenceId=-1, dataLocality=0.0\n\n        \"testqauat:usertable,user1,1433747062257.dcd5395044732242dfed39b09aa05c36.\"\n\n            numberOfStores=1, numberOfStorefiles=1, storefileUncompressedSizeMB=820, lastMajorCompactionTimestamp=1434173025593, storefileSizeMB=820, compressionRatio=1.0000, memstoreSizeMB=0, storefileIndexSizeMB=0, readRequestsCount=32070, writeRequestsCount=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=699, totalStaticBloomSizeKB=160, totalCompactingKVs=0, currentCompactedKVs=0, compactionProgressPct=NaN, completeSequenceId=-1, dataLocality=1.0\n\n        \"testqauat:usertable,user7,1433747062257.9277fd1d34909b0cb150707cbd7a3907.\"\n\n            numberOfStores=1, numberOfStorefiles=1, storefileUncompressedSizeMB=816, lastMajorCompactionTimestamp=1434283025585, storefileSizeMB=816, compressionRatio=1.0000, memstoreSizeMB=0, storefileIndexSizeMB=0, readRequestsCount=0, writeRequestsCount=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=690, totalStaticBloomSizeKB=160, totalCompactingKVs=0, currentCompactedKVs=0, compactionProgressPct=NaN, completeSequenceId=-1, dataLocality=1.0\n\n        \"testqauat:usertable,user8,1433747062257.d930b52db8c7f07f3c3ab3e12e61a085.\"\n\n            numberOfStores=1, numberOfStorefiles=1, storefileUncompressedSizeMB=818, lastMajorCompactionTimestamp=1433771950960, storefileSizeMB=818, compressionRatio=1.0000, memstoreSizeMB=0, storefileIndexSizeMB=0, readRequestsCount=0, writeRequestsCount=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=697, totalStaticBloomSizeKB=160, totalCompactingKVs=0, currentCompactedKVs=0, compactionProgressPct=NaN, completeSequenceId=-1, dataLocality=1.0\n\n\n\nThe refguide shows an example of an older HBase version that has the CP class listed properly. Something is broken.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegionServer.java", "org.apache.hadoop.hbase.coprocessor.TestClassLoading.java"], "label": 1, "es_results": []}, {"bug_id": 13978, "bug_title": "Variable never assigned in SimpleTotalOrderPartitioner.getPartition() ", "bug_description": "See https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java#L104, which has an if statement that tries to limit the code to run only once, but since it does not assign this.lastReduces it will always trigger and recompute the splits (and log them).", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.java"], "label": 1, "es_results": []}, {"bug_id": 14206, "bug_title": "MultiRowRangeFilter returns records whose rowKeys are out of allowed ranges", "bug_description": "I haven&apos;t found a way to attach test program to JIRA issue, so put it below :\n\n\n\npublic class MultiRowRangeFilterTest {\n\n \n\n    byte[] key1Start = new byte[] {-3};\n\n    byte[] key1End  = new byte[] {-2};\n\n\n\n    byte[] key2Start = new byte[] {5};\n\n    byte[] key2End  = new byte[] {6};\n\n\n\n    byte[] badKey = new byte[] {-10};\n\n\n\n    @Test\n\n    public void testRanges() throws IOException {\n\n        MultiRowRangeFilter filter = new MultiRowRangeFilter(Arrays.asList(\n\n                new MultiRowRangeFilter.RowRange(key1Start, true, key1End, false),\n\n                new MultiRowRangeFilter.RowRange(key2Start, true, key2End, false)\n\n        ));\n\n        filter.filterRowKey(badKey, 0, 1);\n\n        /*\n\n        * FAILS -- includes BAD key!\n\n        * Expected :SEEK_NEXT_USING_HINT\n\n        * Actual   :INCLUDE\n\n        * */\n\n        assertEquals(Filter.ReturnCode.SEEK_NEXT_USING_HINT, filter.filterKeyValue(null));\n\n    }\n\n}\n\n\n\nIt seems to happen on 2.0.0-SNAPSHOT too, but I wasn&apos;t able to link one with included class.\nI have played some time with algorithm, and found that quick fix may be applied to \"getNextRangeIndex(byte[] rowKey)\" method (hbase-client:1.1.0) :\n\n\n\nif (insertionPosition == 0 && !rangeList.get(insertionPosition).contains(rowKey)) {\n\n        return ROW_BEFORE_FIRST_RANGE;\n\n}\n\n// FIX START\n\nif(!this.initialized) {\n\n    this.initialized = true;\n\n}\n\n// FIX END\n\nreturn insertionPosition;\n\n\n\nThanks, hope it will help.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.filter.MultiRowRangeFilter.java", "org.apache.hadoop.hbase.filter.TestMultiRowRangeFilter.java"], "label": 1, "es_results": []}, {"bug_id": 14166, "bug_title": "Per-Region metrics can be stale", "bug_description": "We&apos;re seeing some machines that are reporting only old region metrics. It seems like at some point the Hadoop metrics system decided which metrics to display and which not to. From then on it was not changing.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionAggregateSourceImpl.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionAggregateSource.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java", "org.apache.hadoop.metrics2.impl.JmxCacheBuster.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper.java", "org.apache.hadoop.metrics2.lib.MetricsExecutorImpl.java", "org.apache.hadoop.hbase.regionserver.MetricsRegion.java", "org.apache.hadoop.hbase.regionserver.TestMetricsRegionSourceImpl.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperStub.java"], "label": 1, "es_results": []}, {"bug_id": 14145, "bug_title": "Allow the Canary in regionserver mode to try all regions on the server, not just one", "bug_description": "We want a pretty in-depth canary that will try every region on a cluster. When doing that for the whole cluster one machine is too slow, so we wanted to split it up and have each regionserver run a canary. That works however the canary does less work as it just tries one random region.\nLet Us add a flag that will allow the canary to try all regions on a regionserver.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.tool.Canary.java"], "label": 1, "es_results": []}, {"bug_id": 13858, "bug_title": "RS/MasterDumpServlet dumps threads before its Stacks header", "bug_description": "The stacktraces are captured using a Hadoop helper method, then its output is merged with the current. I presume there is a simple flush after outputing the \"Stack\" header missing, before then the caught output is dumped.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.MasterDumpServlet.java"], "label": 1, "es_results": []}, {"bug_id": 13982, "bug_title": "Add info for visibility labels/cell TTLs to ImportTsv", "bug_description": "HBASE-9832 added support for two more optional, special TSV columns, but no usage info was added. Add.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.ImportTsv.java"], "label": 1, "es_results": []}, {"bug_id": 15319, "bug_title": "clearJmxCache does not take effect actually", "bug_description": "When trying to backport HBASE-14166 to 0.98.6, I find JmxCacheBuster::clearJmxCache() does no take effect actually. The related code are listed below:\norg.apache.hadoop.metrics2.impl.JmxCacheBuster.java\n\n\n// fut is initialized to null\n\nprivate static AtomicReference<ScheduledFuture> fut = new AtomicReference<>(null);\n\n\n\npublic static void clearJmxCache() {\n\n    // clearJmxCache return directly when fut is null, which is always true.\n\n    // the actual intent is &apos;if (future != null && !future.isDone ...)&apos; ?\n\n    ScheduledFuture future = fut.get();\n\n    if ((future == null || (!future.isDone() && future.getDelay(TimeUnit.MILLISECONDS) > 100))) {\n\n      return;\n\n    }\n\n    ......\n\n}\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "1.1.0.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.metrics2.impl.JmxCacheBuster.java"], "label": 1, "es_results": []}, {"bug_id": 13888, "bug_title": "Fix refill bug from HBASE-13686", "bug_description": "As I report the RateLimiter fail to limit in HBASE-13686, then Ashish Singhi fix that problem by support two kinds of RateLimiter:  AverageIntervalRateLimiter and FixedIntervalRateLimiter. But in my use of the code, I found a new bug about refill() in AverageIntervalRateLimiter.\n\n\n\n    long delta = (limit * (now - nextRefillTime)) / super.getTimeUnitInMillis();\n\n    if (delta > 0) {\n\n      this.nextRefillTime = now;\n\n      return Math.min(limit, available + delta);\n\n    }   \n\n\n\nWhen delta > 0, refill maybe return available + delta. Then in the canExecute(), avail will add refillAmount again. So the new avail maybe 2 * avail + delta.\n\n\n\n    long refillAmount = refill(limit, avail);\n\n    if (refillAmount == 0 && avail < amount) {\n\n      return false;\n\n    }   \n\n    // check for positive overflow\n\n    if (avail <= Long.MAX_VALUE - refillAmount) {\n\n      avail = Math.max(0, Math.min(avail + refillAmount, limit));\n\n    } else {\n\n      avail = Math.max(0, limit);\n\n    } \n\n\n\nI will add more unit tests for RateLimiter in the next days.\nReview Board: https://reviews.apache.org/r/35384/", "project": "Apache", "sub_project": "HBASE", "version": "1.1.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.quotas.RateLimiter.java", "org.apache.hadoop.hbase.quotas.TestRateLimiter.java", "org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter.java", "org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter.java"], "label": 1, "es_results": []}, {"bug_id": 14228, "bug_title": "Close BufferedMutator and connection in MultiTableOutputFormat", "bug_description": "Close BufferedMutator and connection in MultiTableRecordWriter of MultiTableOutputFormat.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 15247, "bug_title": "InclusiveStopFilter does not respect reverse Filter property", "bug_description": "InclusiveStopFilter only works with non-reversed Scans, it will not filter for reversed Scans, because it doesn&apos;t flip the cmp-operand in the reversed case. In fact, it doesn&apos;t even use the Filter.reverse flag.\nit should be something like this:\nif (reversed) {\n            if (cmp > 0) \n{\n\n                done = true;\n\n            }\n        }\n        else {\n            if (cmp < 0) {\n                done = true;\n            }\n        }", "project": "Apache", "sub_project": "HBASE", "version": "1.1.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.filter.TestFilter.java", "org.apache.hadoop.hbase.filter.InclusiveStopFilter.java"], "label": 1, "es_results": []}, {"bug_id": 15290, "bug_title": "Hbase Rest CheckAndAPI should save other cells along with compared cell", "bug_description": "Java CheckAndPut API allows users to save Cells (C1..C5) while comparing a Cell C1.\nBut in Rest API, even though caller sent multiple cells, hbase rest code is ignoring all the cells except for compare cell.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.rest.RowResource.java", "org.apache.hadoop.hbase.rest.RowResourceBase.java", "org.apache.hadoop.hbase.rest.TestGetAndPutResource.java"], "label": 1, "es_results": []}, {"bug_id": 15323, "bug_title": "Hbase Rest CheckAndDeleteAPi should be able to delete more cells", "bug_description": "Java CheckAndDelete API accepts Delete object which can be used to delete (a cell / cell version / multiple cells / column family or a row), but the rest api only allows to delete the cell (without any version)\nNeed to add this capability to rest api.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.rest.RowResource.java", "org.apache.hadoop.hbase.rest.RowResourceBase.java", "org.apache.hadoop.hbase.rest.TestGetAndPutResource.java", "org.apache.hadoop.hbase.rest.client.RemoteHTable.java"], "label": 1, "es_results": []}, {"bug_id": 15287, "bug_title": "mapreduce.RowCounter returns incorrect result with binary row key inputs", "bug_description": "org.apache.hadoop.hbase.mapreduce.RowCounter takes optional start/end key as inputs (-range option). It would work only when the string representation of value is identical to the string. When row key is binary,  the string representation of the value would look like this: \"\\x00\\x01\", which would be incorrect interpreted as 8 char string in the current implementation:\nhttps://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java\nTo fix that, we need change how the value is converted from command line inputs:\nChange \n      scan.setStartRow(Bytes.toBytes(startKey));\nto\n      scan.setStartRow(Bytes.toBytesBinary(startKey));\nDo the same conversion to end key as well.\nThe issue was discovered when the utility was used to calcualte row distribution on regions from table with binary row keys. The hbase:meta contains the start key of each region in format of above example. ", "project": "Apache", "sub_project": "HBASE", "version": "1.1.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.TestCellCounter.java", "org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.java", "org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.java", "org.apache.hadoop.hbase.mapreduce.CopyTable.java", "org.apache.hadoop.hbase.mapred.GroupingTableMap.java", "org.apache.hadoop.hbase.mapreduce.TestRowCounter.java", "org.apache.hadoop.hbase.mapreduce.RowCounter.java", "org.apache.hadoop.hbase.mapreduce.TestCopyTable.java", "org.apache.hadoop.hbase.mapreduce.CellCounter.java", "org.apache.hadoop.hbase.mapreduce.TableInputFormat.java", "org.apache.hadoop.hbase.mapreduce.Export.java", "org.apache.hadoop.hbase.mapreduce.TestImportExport.java"], "label": 1, "es_results": []}, {"bug_id": 14705, "bug_title": "Javadoc for KeyValue constructor is not correct.", "bug_description": "\n\n\n  /**\n\n   * Constructs KeyValue structure filled with null value.\n\n   * @param row - row key (arbitrary byte array)\n\n   * @param family family name\n\n   * @param qualifier column qualifier\n\n   */\n\n  public KeyValue(final byte [] row, final byte [] family,\n\n      final byte [] qualifier, final byte [] value) {\n\n    this(row, family, qualifier, HConstants.LATEST_TIMESTAMP, Type.Put, value);\n\n  }\n\n\n\nValue is not filled with null.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.KeyValue.java"], "label": 1, "es_results": []}, {"bug_id": 14784, "bug_title": "Port conflict is not resolved in HBaseTestingUtility.randomFreePort()", "bug_description": "If takenRandomPorts.contains(port) == true, it means port conflict, so randomFreePort() should rerun the loop. But continue statement leads to exit the loop, because port != 0.\nhbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java\n\n\npublic static int randomFreePort() {\n\n  int port = 0; \n\n  do { \n\n    port = randomPort();\n\n    if (takenRandomPorts.contains(port)) {\n\n      continue;\n\n    }    \n\n    takenRandomPorts.add(port);\n\n\n\n    ...\n\n\n\n  } while (port == 0);\n\n  return port;\n\n}\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "1.1.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.HBaseTestingUtility.java", "org.apache.hadoop.hbase.TestHBaseTestingUtility.java"], "label": 1, "es_results": []}, {"bug_id": 14778, "bug_title": "Make block cache hit percentages not integer in the metrics system", "bug_description": "Once you&apos;re close to the 90%+ it&apos;s hard to see a difference because getting a full percent change is rare.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java"], "label": 1, "es_results": []}, {"bug_id": 14793, "bug_title": "Allow limiting size of block into L1 block cache.", "bug_description": "G1GC does really badly with long lived large objects. Let Us allow limiting the size of a block that can be kept in the block cache.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java", "org.apache.hadoop.hbase.regionserver.TestMetricsRegionServer.java", "org.apache.hadoop.hbase.io.hfile.CacheStats.java", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.java", "org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java"], "label": 1, "es_results": []}, {"bug_id": 14936, "bug_title": "CombinedBlockCache should overwrite CacheStats#rollMetricsPeriod()", "bug_description": "It seems CombinedBlockCache should overwrite CacheStats#rollMetricsPeriod() as\n\n\n\npublic void rollMetricsPeriod() {\n\n  lruCacheStats.rollMetricsPeriod();\n\n  bucketCacheStats.rollMetricsPeriod();\n\n}\n\n\n\notherwise, CombinedBlockCache.getHitRatioPastNPeriods() and CombinedBlockCache.getHitCachingRatioPastNPeriods() will always return 0.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.io.hfile.CacheStats.java", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.java"], "label": 1, "es_results": []}, {"bug_id": 15120, "bug_title": "Backport HBASE-14883 to branch-1.1", "bug_description": "When checking branch-1.1 UT in HBASE-13590, found TestSplitTransactionOnCluster#testFailedSplit will fail at a 12/50 chance. The issue is fixed by HBASE-14883 but the change didn&apos;t go into branch-1.1", "project": "Apache", "sub_project": "HBASE", "version": "1.1.2", "fixed_version": "rel/1.1.4", "fixed_files": ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java"], "label": 1, "es_results": []}, {"bug_id": 16367, "bug_title": "Race between master and region server initialization may lead to premature server abort", "bug_description": "I was troubleshooting a case where hbase (1.1.2) master always dies shortly after start - see attached master log snippet.\nIt turned out that master initialization thread was racing with HRegionServer#preRegistrationInitialization() (initializeZooKeeper, actually) since HMaster extends HRegionServer.\nThrough additional logging in master:\n\n\n\n    this.oldLogDir = createInitialFileSystemLayout();\n\n    HFileSystem.addLocationsOrderInterceptor(conf);\n\n    LOG.info(\"creating splitLogManager\");\n\n\n\nI found that execution didn&apos;t reach the last log line before region server declared cluster Id being null.", "project": "Apache", "sub_project": "HBASE", "version": "1.1.2", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegionServer.java", "org.apache.hadoop.hbase.master.HMaster.java"], "label": 1, "es_results": []}, {"bug_id": 15279, "bug_title": "OrderedBytes.isEncodedValue does not check for int8 and int16 types", "bug_description": "OrderedBytes.isEncodedValue does not check for int8 and int16 types.  This also means that OrderedBytes.length may return an incorrect result, since it calls OrderedBytes.isEncodedValue.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.3", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.util.OrderedBytes.java", "org.apache.hadoop.hbase.util.TestOrderedBytes.java"], "label": 1, "es_results": []}, {"bug_id": 15358, "bug_title": "canEnforceTimeLimitFromScope should use timeScope instead of sizeScope", "bug_description": "A small but maybe critical bug", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.3", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.ScannerContext.java"], "label": 1, "es_results": []}, {"bug_id": 15378, "bug_title": "Scanner cannot handle heartbeat message with no results", "bug_description": "When a RS scanner get a TIME_LIMIT_REACHED_MID_ROW state, they will stop scanning and send back what it has read to client and mark the message as a heartbeat message. If there is no cell has been read, it will be an empty response. \nHowever, ClientScanner only handles the situation that the client gets an empty heartbeat and its cache is not empty. If the cache is empty too, it will be regarded as end-of-region and open a new scanner for next region.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.3", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestScannerHeartbeatMessages.java", "org.apache.hadoop.hbase.client.ClientScanner.java"], "label": 1, "es_results": []}, {"bug_id": 15325, "bug_title": "ResultScanner allowing partial result will miss the rest of the row if the region is moved between two rpc requests", "bug_description": "HBASE-11544 allow scan rpc return partial of a row to reduce memory usage for one rpc request. And client can setAllowPartial or setBatch to get several cells in a row instead of the whole row.\nHowever, the status of the scanner is saved on server and we need this to get the next part if there is a partial result before. If we move the region to another RS, client will get a NotServingRegionException and open a new scanner to the new RS which will be regarded as a new scan from the end of this row. So the rest cells of the row of last result will be missing.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.3", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.TestPartialResultsFromClientSide.java", "org.apache.hadoop.hbase.CellComparator.java", "org.apache.hadoop.hbase.client.ClientScanner.java"], "label": 1, "es_results": []}, {"bug_id": 15324, "bug_title": "Jitter may cause desiredMaxFileSize overflow in ConstantSizeRegionSplitPolicy and trigger unexpected split", "bug_description": "We introduce jitter for region split decision in HBASE-13412, but the following line in ConstantSizeRegionSplitPolicy may cause long value overflow if MAX_FILESIZE is specified to Long.MAX_VALUE:\n\n\n\nthis.desiredMaxFileSize += (long)(desiredMaxFileSize * (RANDOM.nextFloat() - 0.5D) * jitter);\n\n\n\nIn our case we specify MAX_FILESIZE to Long.MAX_VALUE to prevent target region to split.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.3", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy.java", "org.apache.hadoop.hbase.regionserver.TestRegionSplitPolicy.java"], "label": 1, "es_results": []}, {"bug_id": 15485, "bug_title": "Filter.reset() should not be called between batches", "bug_description": "As discussed in HBASE-15325, now we will resetFilters if partial result not formed, but we should not reset filters when batch limit reached", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.3", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.regionserver.ScannerContext.java"], "label": 1, "es_results": []}, {"bug_id": 16649, "bug_title": "Truncate table with splits preserved can cause both data loss and truncated data appeared again", "bug_description": "Since truncate table with splits preserved will delete hfiles and use the previous regioninfo. It can cause odd behaviors\n\nCase 1: Data appeared after truncate\nreproduce procedure\n1. create a table, let&apos;s say &apos;test&apos;\n2. write data to &apos;test&apos;, make sure memstore of &apos;test&apos; is not empty\n3. truncate &apos;test&apos; with splits preserved\n4. kill the regionserver hosting the region(s) of &apos;test&apos;\n5. start the regionserver, now it is the time to witness the miracle! the truncated data appeared in table &apos;test&apos;\n\n\nCase 2: Data loss\nreproduce procedure:\n1. create a table, let&apos;s say &apos;test&apos;\n2. write some data to &apos;test&apos;, no matter how many\n3. truncate &apos;test&apos; with splits preserved\n4. restart the regionserver to reset the seqid\n5. write some data, but less than 2 since we don&apos;t want the seqid to run over the one in 2\n6. kill the regionserver hosting the region(s) of &apos;test&apos;\n7. restart the regionserver. Congratulations! the data writen in 4 is now all lost\n\nWhy?\nfor case 1\nSince preserve splits in truncate table procedure will not change the regioninfo, when log replay happens, the &apos;unflushed&apos; data will restore back to the region\nfor case 2\nsince the flushedSequenceIdByRegion are stored in Master in a map with the region&apos;s encodedName. Although the table is truncated, the region&apos;s name is not changed since we chose to preserve the splits. So after truncate the table, the region&apos;s sequenceid is reset in the regionserver, but not reset in master. When flush comes and report to master, master will reject the update of sequenceid since the new one is smaller than the old one. The same happens in log replay, all the edits writen in 4 will be skipped since they have a smaller seqid", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.3", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.java", "org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.java", "org.apache.hadoop.hbase.master.CatalogJanitor.java", "org.apache.hadoop.hbase.master.TestCatalogJanitor.java", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java", "org.apache.hadoop.hbase.master.ServerManager.java"], "label": 1, "es_results": []}, {"bug_id": 14963, "bug_title": "Remove use of Guava Stopwatch from HBase client code", "bug_description": "We ran into an issue where an application bundled its own Guava (and that happened to be in the classpath first) and HBase&apos;s MetaTableLocator threw an exception due to the fact that Stopwatch&apos;s constructor wasn&apos;t compatible... Might be better to not depend on Stopwatch at all in MetaTableLocator since the functionality is easily doable without.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.4", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.zookeeper.MetaTableLocator.java"], "label": 1, "es_results": []}, {"bug_id": 15636, "bug_title": "hard coded wait time out value in HBaseTestingUtility#waitUntilAllRegionsAssigned might cause test failure", "bug_description": "HBaseTestingUtility#waitUntilAllRegionsAssigned(final TableName tableName) hard coded 60 seconds wait for region assignment in a table.  This could cause flaky tests, as the caller could create table with longs of region which takes longs time; or in CM, RS unavailable could slow down assignment; or in a slow machine, IT might take long to assign regions.  \nIn HBaseAdmin.java, we use a configurable value to wait for the table DDLs: this.syncWaitTimeout = this.conf.getInt(\"hbase.client.sync.wait.timeout.msec\", 10 * 60000); // 10min - we should use the config for the HBaseTestingUtility#waitUntilAllRegionsAssigned() too.\nOf course, the caller could use the \"public void waitUntilAllRegionsAssigned(final TableName tableName, final long timeout)\"; but it requires test change (currently, no test directly calls this signature). \n\n  \n\n/**\n\n   * Wait until all regions for a table in hbase:meta have a non-empty\n\n   * info:server, up to 60 seconds. This means all regions have been deployed,\n\n   * master has been informed and updated hbase:meta with the regions deployed\n\n   * server.\n\n   * @param tableName the table name\n\n   * @throws IOException\n\n   */\n\n  public void waitUntilAllRegionsAssigned(final TableName tableName) throws IOException {\n\n    waitUntilAllRegionsAssigned(tableName, 60000);\n\n  }\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.4", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.HBaseTestingUtility.java"], "label": 1, "es_results": []}, {"bug_id": 16572, "bug_title": "Sync method in RecoverableZooKeeper failed to pass callback function in", "bug_description": "\n\n\npublic void sync(String path, AsyncCallback.VoidCallback cb, Object ctx) throws KeeperException {\n\n    checkZk().sync(path, null, null); //callback function cb is not passed in\n\n  }\n\n\n\nIt is obvious that the callback method is not passed in.  Since sync operation in Zookeeper is a &apos;async&apos; operation, we need a callback method to notify the caller that the &apos;sync&apos; operation is finished.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.4", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java", "org.apache.hadoop.hbase.master.HMaster.java"], "label": 1, "es_results": []}, {"bug_id": 15880, "bug_title": "RpcClientImpl#tracedWriteRequest incorrectly closes HTrace span", "bug_description": "In this method we continue the span and then close it, which causes the current span (the one created by client app around HTabl#get() or similar API call) to be closed incorrectly.\n\n\n\n TraceScope ts = Trace.continueSpan(span);\n\n      try {\n\n        writeRequest(call, priority, span);\n\n      } finally {\n\n        ts.close();\n\n      }\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.5", "fixed_version": "rel/1.3.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.RpcClientImpl.java"], "label": 1, "es_results": []}, {"bug_id": 16207, "bug_title": "cannot restore snapshot without \"Admin\" permission", "bug_description": "MasterRpcServices.restoreSnapshot() tries to verify if the NS exists before starting the restore, but instead of calling ensureNamespaceExists() it calls master.getNamespace() which requires ADMIN permission to get the NS descriptor. \n\n\n\npublic RestoreSnapshotResponse restoreSnapshot(RpcController controller,\n\n...\n\n  // Ensure namespace exists. Will throw exception if non-known NS.\n\n  master.getNamespace(dstTable.getNamespaceAsString());\n\n\n\nunfortunately i&apos;m not aware of any unit-test that cover this kind of situations. we cover single ACLs from the TestAccessController but we don&apos;t exercise rpc calls and verify if there is more than one check on the ACLs like in this case", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.5", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.MasterRpcServices.java"], "label": 1, "es_results": []}, {"bug_id": 16283, "bug_title": "Batch Append/Increment will always fail if set ReturnResults to false", "bug_description": "If set Append/Increment&apos;s ReturnResult attribute to false, and batch the appends/increments to server. The batch operation will always return false.\nThe reason is that, since return result is set to false, append/increment will return null instead of Result object. But in ResponseConverter#getResults, there is some check code \n\n\n\nif (requestRegionActionCount != responseRegionActionResultCount) {\n\n      throw new IllegalStateException(\"Request mutation count=\" + requestRegionActionCount +\n\n          \" does not match response mutation result count=\" + responseRegionActionResultCount);\n\n    }\n\n\n\nThat means if the result count is not meet with request mutation count, it will fail the request.\nThe solution is simple, instead of returning a null result, returning a empty result if ReturnResult set to false.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.5", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.regionserver.wal.TestDurability.java", "org.apache.hadoop.hbase.client.TestIncrementsFromClientSide.java", "org.apache.hadoop.hbase.client.TestFromClientSide.java", "org.apache.hadoop.hbase.regionserver.TestAtomicOperation.java"], "label": 1, "es_results": []}, {"bug_id": 16017, "bug_title": "HBase TableOutputFormat has connection leak in getRecordWriter", "bug_description": "Currently getRecordWriter will not release the connection until jvm terminate, which is not a right assumption given that the function may be invoked many times in the same jvm lifecycle. Inside of mapreduce, the issue has already fixed. \n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.6", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapred.TableOutputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 16856, "bug_title": "Exception message in SyncRunner.run() should print currentSequence", "bug_description": "A very small bug, a typo in exception message:\n\n\n\nif (syncFutureSequence > currentSequence) {\n\n              throw new IllegalStateException(\"currentSequence=\" + syncFutureSequence\n\n                  + \", syncFutureSequence=\" + syncFutureSequence);\n\n            }\n\n\n\nIt should print currentSequence and syncFutureSequence, but print two syncFutureSequence", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.7", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.wal.FSHLog.java"], "label": 1, "es_results": []}, {"bug_id": 16889, "bug_title": "Proc-V2: verifyTables in the IntegrationTestDDLMasterFailover test after each table DDL is incorrect ", "bug_description": "In the IntegrationTestDDLMasterFailover test, verifyTables is called after each table DDL.  It iterates 3 lists of tables in ConcurrentHashMap (enabledTables, disabledTables, deletedTables) and tries to do some verification.  This is incorrect, eg. a table in enabledTables map could be picked up by DeleteTableAction and is disabled, while the verification tries to check whether table is enabled.  This leads to false assertion.  \nThe same for verifyNamespaces().  \nThe proposed fix is to verify maps only at the end of tests (while no active DDL operation is going on).  During test run, we only verify the target table before putting into map.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.7", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.IntegrationTestDDLMasterFailover.java"], "label": 1, "es_results": []}, {"bug_id": 16931, "bug_title": "Setting cell's seqId to zero in compaction flow might cause RS down.", "bug_description": "Compactor#performCompaction\n      do {\n        hasMore = scanner.next(cells, scannerContext);\n        // output to writer:\n        for (Cell c : cells) {\n          if (cleanSeqId && c.getSequenceId() <= smallestReadPoint) \n{\n\n            CellUtil.setSequenceId(c, 0);\n\n          }\n          writer.append(c);\n        }\n        cells.clear();\n      } while (hasMore);\nscanner.next will choose at most \"hbase.hstore.compaction.kv.max\" kvs, the last cell still reference by StoreScanner.prevCell, so if cleanSeqId is called when the scanner.next call StoreScanner.checkScanOrder may throw exception and because regionserver down.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.7", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestCompaction.java", "org.apache.hadoop.hbase.regionserver.compactions.Compactor.java"], "label": 1, "es_results": []}, {"bug_id": 17039, "bug_title": "SimpleLoadBalancer schedules large amount of invalid region moves", "bug_description": "After increasing one of our clusters to 1600 nodes, we observed a large amount of invalid region moves(more than 30k moves) fired by the balance chore. Thus we simulated the problem and printed out the balance plan, only to find out many servers that had two regions for a certain table(we use by table strategy), sent out both regions to other two servers that have zero region. \nIn the SimpleLoadBalancer&apos;s balanceCluster function,\nthe code block that determines the underLoadedServers might have a problem:\n\n\n\n      if (load >= min && load > 0) {\n\n        continue; // look for other servers which haven&apos;t reached min\n\n      }\n\n      int regionsToPut = min - load;\n\n      if (regionsToPut == 0)\n\n      {\n\n        regionsToPut = 1;\n\n      }\n\n\n\nif min is zero, some server that has load of zero, which equals to min would be marked as underloaded, which would cause the phenomenon mentioned above.\nSince we increased the cluster&apos;s size to 1600+, many tables that only have 1000 regions, now would encounter such issue.\nBy fixing it up, the balance plan went back to normal.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.7", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer.java"], "label": 1, "es_results": []}, {"bug_id": 17058, "bug_title": "Lower epsilon used for jitter verification from HBASE-15324", "bug_description": "The current epsilon used is 1E-6 and its too big it might overflow the desiredMaxFileSize. A trivial fix is to lower the epsilon to 2^-52 or even 2^-53. An option to consider too is just to shift the jitter to always decrement hbase.hregion.max.filesize (MAX_FILESIZE) instead of increase the size of the region and having to deal with the round off.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.1.7", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy.java"], "label": 1, "es_results": []}, {"bug_id": 13813, "bug_title": "Fix Javadoc warnings in Procedure.java", "bug_description": "[WARNING] Javadoc Warnings\n[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java:90: warning - @throw is an unknown tag.\n[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java:90: warning - @throw is an unknown tag.\n[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java:104: warning - @throw is an unknown tag.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.procedure2.Procedure.java"], "label": 1, "es_results": []}, {"bug_id": 14076, "bug_title": "ResultSerialization and MutationSerialization can throw InvalidProtocolBufferException when serializing a cell larger than 64MB", "bug_description": "This was reported in CRUNCH-534 but is a problem how we handle deserialization of large Cells (> 64MB) in ResultSerialization and MutationSerialization.\nThe fix is just re-using what it was done in HBASE-13230.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.MutationSerialization.java", "org.apache.hadoop.hbase.mapreduce.ResultSerialization.java"], "label": 1, "es_results": []}, {"bug_id": 14115, "bug_title": "Fix resource leak in HMasterCommandLine", "bug_description": "In HMasterCommandLine#stopMaster(), admin is not closed.\nHMasterCommandLine.java\n\n\ntry (Connection connection = ConnectionFactory.createConnection(conf)) {\n\n      try (Admin admin = connection.getAdmin()) {\n\n        connection.getAdmin().shutdown();\n\n      } catch (Throwable t) {\n\n        LOG.error(\"Failed to stop master\", t);\n\n        return 1;\n\n      }\n\n    }\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.HMasterCommandLine.java"], "label": 1, "es_results": []}, {"bug_id": 14146, "bug_title": "Once replication sees an error it slows down forever", "bug_description": "sleepMultiplier inside of HBaseInterClusterReplicationEndpoint and ReplicationSource never gets reset to zero.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java"], "label": 1, "es_results": []}, {"bug_id": 14473, "bug_title": "Compute region locality in parallel", "bug_description": "Right now on large clusters it&apos;s necessary to turn off the locality balance cost as it takes too long to compute the region locality. This is because it&apos;s computed when need in serial.\nWe should compute this in parallel before it&apos;s needed.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.java"], "label": 1, "es_results": []}, {"bug_id": 14555, "bug_title": "Deadlock in MVCC branch-1.2 toString()", "bug_description": "Just saw this in an IT test.\n\n\n\nThread 75 (PriorityRpcServer.handler=3,queue=1,port=16020):\n\n  State: BLOCKED\n\n  Blocked count: 691635\n\n  Waited count: 1557446\n\n  Blocked on java.util.LinkedList@32b53d9e\n\n  Blocked by 81 (PriorityRpcServer.handler=9,queue=1,port=16020)\n\n  Stack:\n\n    org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl.toString(MultiVersionConcurrencyControl.java:234)\n\n    java.lang.String.valueOf(String.java:2994)\n\n    java.lang.StringBuilder.append(StringBuilder.java:131)\n\n    org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl.waitForRead(MultiVersionConcurrencyControl.java:209)\n\n    org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl.completeAndWait(MultiVersionConcurrencyControl.java:144)\n\n    org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:3191)\n\n    org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2837)\n\n    org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2779)\n\n    org.apache.hadoop.hbase.regionserver.RSRpcServices.doBatchOp(RSRpcServices.java:697)\n\n    org.apache.hadoop.hbase.regionserver.RSRpcServices.doNonAtomicRegionMutation(RSRpcServices.java:659)\n\n    org.apache.hadoop.hbase.regionserver.RSRpcServices.multi(RSRpcServices.java:2047)\n\n    org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32594)\n\n    org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2120)\n\n    org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:106)\n\n    org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)\n\n    org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)\n\n    java.lang.Thread.run(Thread.java:745)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThread 81 (PriorityRpcServer.handler=9,queue=1,port=16020):\n\n  State: BLOCKED\n\n  Blocked count: 691858\n\n  Waited count: 1558138\n\n  Blocked on java.lang.Object@2a5e9ae8\n\n  Blocked by 75 (PriorityRpcServer.handler=3,queue=1,port=16020)\n\n  Stack:\n\n    org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl.complete(MultiVersionConcurrencyControl.java:191)\n\n    org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl.completeAndWait(MultiVersionConcurrencyControl.java:143)\n\n    org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:3191)\n\n    org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2837)\n\n    org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2779)\n\n    org.apache.hadoop.hbase.regionserver.RSRpcServices.doBatchOp(RSRpcServices.java:697)\n\n    org.apache.hadoop.hbase.regionserver.RSRpcServices.doNonAtomicRegionMutation(RSRpcServices.java:659)\n\n    org.apache.hadoop.hbase.regionserver.RSRpcServices.multi(RSRpcServices.java:2047)\n\n    org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32594)\n\n    org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2120)\n\n    org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:106)\n\n    org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)\n\n    org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)\n\n    java.lang.Thread.run(Thread.java:745)\n\nThread 80 (PriorityRpcServer.handler=8,queue=0,port=16020):\n\n\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl.java"], "label": 1, "es_results": []}, {"bug_id": 14211, "bug_title": "Add more rigorous integration tests of splits", "bug_description": "Add a chaos action that will turn down region size.\n\nEventually this will cause regions to split a lot.\nIt will need to have a min region size.\n\nAdd a chaos monkey action that will change split policy\n\nChange between Uniform and SplittingUpTo and back\n\nAdd chaos monkey action that will request splits of every region.\n\nWhen regions all reach the size a the exact same time the compactions add a lot of work.\nThis simulates a very well distributed write pattern reaching the region size.\n\nAdd the ability to start with fewer regions than normal to ITBLL", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.chaos.factories.StressAssignmentManagerMonkeyFactory.java", "org.apache.hadoop.hbase.chaos.factories.SlowDeterministicMonkeyFactory.java", "org.apache.hadoop.hbase.chaos.factories.MonkeyConstants.java", "org.apache.hadoop.hbase.HBaseTestingUtility.java", "org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java"], "label": 1, "es_results": []}, {"bug_id": 14597, "bug_title": "Fix Groups cache in multi-threaded env", "bug_description": "UGI doesn&apos;t hash based on the user as expected so since we have lots of ugi potentially created the cache doesn&apos;t do it&apos;s job.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.security.UserProvider.java", "org.apache.hadoop.hbase.security.TestUser.java", "org.apache.hadoop.hbase.security.User.java"], "label": 1, "es_results": []}, {"bug_id": 14658, "bug_title": "Allow loading a MonkeyFactory by class name", "bug_description": "Users should be able to define their own chaos monkey that is loaded by class name.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.chaos.factories.MonkeyFactory.java"], "label": 1, "es_results": []}, {"bug_id": 14257, "bug_title": "Periodic flusher only handles hbase:meta, not other system tables", "bug_description": "In HRegion.shouldFlush we have\n\n\n\n    long modifiedFlushCheckInterval = flushCheckInterval;\n\n    if (getRegionInfo().isMetaRegion() &&\n\n        getRegionInfo().getReplicaId() == HRegionInfo.DEFAULT_REPLICA_ID) {\n\n      modifiedFlushCheckInterval = META_CACHE_FLUSH_INTERVAL;\n\n    }\n\n\n\nThat method is called by the PeriodicMemstoreFlusher thread, and prefers the hbase:meta only for faster flushing. It should be doing the same for other system tables. I suggest to use HRI.isSystemTable().", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.regionserver.TestDefaultMemStore.java"], "label": 1, "es_results": []}, {"bug_id": 14742, "bug_title": "TestHeapMemoryManager is flakey", "bug_description": "On our internal build system we&apos;ve seen TestHeapMemoryManager fail twice.\n\n\n\nFailed tests: \n\n  TestHeapMemoryManager.testWhenClusterIsReadHeavy:174->assertHeapSpaceDelta:317 null\n\n  TestHeapMemoryManager.testWhenClusterIsWriteHeavy:136->assertHeapSpaceDelta:317 null\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestHeapMemoryManager.java"], "label": 1, "es_results": []}, {"bug_id": 14723, "bug_title": "Fix IT tests split too many times", "bug_description": "Splitting the whole table is happening too often. Let Us make this happen less frequently as there are more regions.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.chaos.actions.SplitAllRegionOfTableAction.java"], "label": 1, "es_results": []}, {"bug_id": 14706, "bug_title": "RegionLocationFinder should return multiple servernames by top host", "bug_description": "Multiple RS can run on the same host. But in current RegionLocationFinder, mapHostNameToServerName map one host to only one server. This will make LocalityCostFunction get wrong locality about region.\n\n\n\n    // create a mapping from hostname to ServerName for fast lookup\n\n    HashMap<String, ServerName> hostToServerName = new HashMap<String, ServerName>();\n\n    for (ServerName sn : regionServers) {\n\n      hostToServerName.put(sn.getHostname(), sn);\n\n    }\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.java"], "label": 1, "es_results": []}, {"bug_id": 14781, "bug_title": "Turn per cf flushing on for ITBLL by default", "bug_description": "", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java"], "label": 1, "es_results": []}, {"bug_id": 14802, "bug_title": "Replaying server crash recovery procedure after a failover causes incorrect handling of deadservers", "bug_description": "The way dead servers are processed is that a ServerCrashProcedure is launched for a server after it is added to the dead servers list. \nEvery time a server is added to the dead list, a counter \"numProcessing\" is incremented and it is decremented when a crash recovery procedure finishes. Since, adding a dead server and recovering it are two separate events, it can cause inconsistencies.\nIf a master failover occurs in the middle of the crash recovery, the numProcessing counter resets but the ServerCrashProcedure is replayed by the new master. This causes the counter to go negative and makes the master think that dead servers are still in process of recovery. \nThis has ramifications on the balancer that the balancer ceases to run after such a failover.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.TestDeadServer.java", "org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java", "org.apache.hadoop.hbase.master.DeadServer.java"], "label": 1, "es_results": []}, {"bug_id": 14942, "bug_title": "Allow turning off BoundedByteBufferPool", "bug_description": "The G1 does a great job of compacting, there&apos;s no reason to use the BoundedByteBufferPool when the JVM can it for us. So we should allow turning this off for people who are running new jvm&apos;s where the G1 is working well.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.RpcServer.java"], "label": 1, "es_results": []}, {"bug_id": 14953, "bug_title": "HBaseInterClusterReplicationEndpoint: Do not retry the whole batch of edits in case of RejectedExecutionException", "bug_description": "When we have wal provider set to multiwal, the ReplicationSource has multiple worker threads submitting batches to HBaseInterClusterReplicationEndpoint. In such a scenario, it is quite common to encounter RejectedExecutionException because it takes quite long for shipping edits to peer cluster compared to reading edits from source and submitting more batches to the endpoint. \nThe logs are just filled with warnings due to this very exception.\nSince we subdivide batches before actually shipping them, we don&apos;t need to fail and resend the whole batch if one of the sub-batches fails with RejectedExecutionException. Rather, we should just retry the failed sub-batches. ", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 14838, "bug_title": "Clarify that SimpleRegionNormalizer does not merge empty (<1MB) regions", "bug_description": "SImpleRegionNormalizer does not merge empty region of a table\nSteps to repro:\n\nCreate an empty table with few, say 5-6 regions without any data in any of them\nVerify hbase:meta table to verify the regions for the table or check HMaster UI\nEnable normalizer switch and normalization for this table\nRun normalizer, by &apos;normalize&apos; command from hbase she will\nVerify the regions for table by scanning hbase:meta table or checking HMaster web UI\n\nThe empty regions are not merged on running the region normalizer. This seems to be an edge case with completely empty regions since the Normalizer checks for: smallestRegion (in this case 0 size) + smallestNeighborOfSmallestRegion (in this case 0 size) > average region size (in this case 0 size)\nthanks to Josh Elser for verifying this from the source code side", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer.java"], "label": 1, "es_results": []}, {"bug_id": 14843, "bug_title": "TestWALProcedureStore.testLoad is flakey", "bug_description": "I see it twice recently, \nsee.\nhttps://builds.apache.org/job/PreCommit-HBASE-Build/16589//testReport/org.apache.hadoop.hbase.procedure2.store.wal/TestWALProcedureStore/testLoad/\nhttps://builds.apache.org/job/PreCommit-HBASE-Build/16532/testReport/org.apache.hadoop.hbase.procedure2.store.wal/TestWALProcedureStore/testLoad/\nLet&apos;s see what&apos;s happening.\nUpdate.\nIt failed once again today, \nhttps://builds.apache.org/job/PreCommit-HBASE-Build/16602/testReport/junit/org.apache.hadoop.hbase.procedure2.store.wal/TestWALProcedureStore/testLoad/\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java", "org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java"], "label": 1, "es_results": []}, {"bug_id": 15001, "bug_title": "Thread Safety issues in ReplicationSinkManager and HBaseInterClusterReplicationEndpoint", "bug_description": "ReplicationSinkManager is not thread-safe. This can cause problems in HBaseInterClusterReplicationEndpoint,  when the walprovider is multiwal. \nFor example: \n1. When multiple threads report bad sinks, the sink list can be non-empty but report a negative size because the ArrayList itself is not thread-safe. \n2. HBaseInterClusterReplicationEndpoint depends on the number of sinks to batch edits for shipping. However, it&apos;s quite possible that the following code makes it assume that there are no batches to process (sink size is non-zero, but by the time we reach the \"batching\" part, sink size becomes zero.)\n\n\n\nif (replicationSinkMgr.getSinks().size() == 0) {\n\n    return false;\n\n}\n\n...\n\nint n = Math.min(Math.min(this.maxThreads, entries.size()/100+1),\n\n               replicationSinkMgr.getSinks().size());\n\n\n\n[Update] This leads to ArithmeticException: division by zero at:\n\n\n\nentryLists.get(Math.abs(Bytes.hashCode(e.getKey().getEncodedRegionName())%n)).add(e);\n\n\n\nwhich is benign and will just lead to retries by the ReplicationSource.\nThe idea is to make all operations in ReplicationSinkManager thread-safe and do a verification on the size of replicated edits before we report success.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.java", "org.apache.hadoop.hbase.replication.regionserver.TestReplicationSinkManager.java", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager.java"], "label": 1, "es_results": []}, {"bug_id": 14867, "bug_title": "SimpleRegionNormalizer needs to have better heuristics to trigger merge operation", "bug_description": "SimpleRegionNormalizer needs to have better heuristics to trigger merge operation. SimpleRegionNormalizer is not able to trigger a merge action if the table&apos;s smallest region has neighboring regions that are larger than table&apos;s average region size, whereas there are other smaller regions whose combined size is less than the average region size. \nFor example, \n\nConsider a table with six region, say r1 to r6.\nKeep r1 as empty and create some data say, 100K rows of data for each of the regions r2, r3 and r4. Create smaller amount of data for regions r5 and r6, say about 27K rows of data.\nRun the normalizer. Verify the number the regions for that table and also check the master log to see if any merge action was triggered as a result of normalization.\n\nIn such scenario, it would be better to have a merge action triggered for those two smaller regions r5 and r6 even though either of them is not the smallest one", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer.java", "org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizer.java"], "label": 1, "es_results": []}, {"bug_id": 14512, "bug_title": "Cache UGI groups", "bug_description": "Right now every call gets a new User object.\nWe should keep the same user for the life of a connection. We should also cache the group names. However we can&apos;t cache the groups for forever as that would mean groups don&apos;t get refreshed every 5 mins.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.RpcServer.java", "org.apache.hadoop.hbase.security.UserProvider.java", "org.apache.hadoop.hbase.ipc.CallRunner.java", "org.apache.hadoop.hbase.security.User.java"], "label": 1, "es_results": []}, {"bug_id": 14771, "bug_title": "RpcServer#getRemoteAddress always returns null", "bug_description": "RpcServer.getRemoteAddress always returns null, because Call object is getting initialized with null.This seems to be happening because of using RpcServer.getRemoteIp() in  Call object constructor before RpcServer thread local &apos;CurCall&apos; being set in CallRunner.run method:\n\n// --- RpcServer.java ---\n\nprotected void processRequest(byte[] buf) throws IOException, InterruptedException {\n\n .................................\n\n// Call object getting initialized here with address \n\n// obtained from RpcServer.getRemoteIp()\n\nCall call = new Call(id, this.service, md, header, param, cellScanner, this, responder,\n\n              totalRequestSize, traceInfo, RpcServer.getRemoteIp());\n\n  scheduler.dispatch(new CallRunner(RpcServer.this, call));\n\n }\n\n\n\n// getRemoteIp method gets address from threadlocal &apos;CurCall&apos; which \n\n// gets set in CallRunner.run and calling it before this as in above case, will return null\n\n// --- CallRunner.java ---\n\npublic void run() {\n\n  .........................   \n\n  Pair<Message, CellScanner> resultPair = null;\n\n  RpcServer.CurCall.set(call);\n\n  ..............................\n\n}\n\n\n\n// Using &apos;this.addr&apos; in place of getRemoteIp method in RpcServer.java seems to be fixing this issue\n\nCall call = new Call(id, this.service, md, header, param, cellScanner, this, responder,\n\n              totalRequestSize, traceInfo, this.addr);\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.RpcServer.java", "org.apache.hadoop.hbase.ipc.AbstractTestIPC.java"], "label": 1, "es_results": []}, {"bug_id": 15102, "bug_title": "HeapMemoryTuner can \"overtune\" memstore size and suddenly drop it into blocking zone", "bug_description": "DefaultHeapMemoryTuner often resets the maximum step size for tuning to 8% of total heap size. Often, when the size of memstore is to be decreased while tuning, the 8% tuning can suddenly drop the memstore size below the low water mark of the previous memstore size (which could potentially be  the used size of the memstore)\nThis is problematic because suddenly it blocks all the updates by suddenly causing a situation where memstore used size is above high water mark. This has a very bad performance impact on an otherwise fine HBase cluster. ", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestHeapMemoryManager.java", "org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner.java"], "label": 1, "es_results": []}, {"bug_id": 15098, "bug_title": "Normalizer switch in configuration is not used", "bug_description": "The newly added global switch to enable the new normalizer functionality is never used apparently, meaning it is always on. The hbase-default.xml has this:\n\n  <property>\n\n    <name>hbase.normalizer.enabled</name>\n\n    <value>false</value>\n\n    <description>If set to true, Master will try to keep region size\n\n      within each table approximately the same.</description>\n\n  </property>\n\n\n\nBut only a test class uses it to set the switch to \"true\". We should implement a proper if statement that checks this value and properly disables the feature cluster wide if not wanted.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.HConstants.java", "org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizerOnCluster.java"], "label": 1, "es_results": []}, {"bug_id": 15285, "bug_title": "Forward-port respect for isReturnResult from HBASE-15095", "bug_description": "This issue is about forward-porting the bug fix done in HBASE-15095 so we respect the isReturnResult properly in append and increment.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.client.Append.java", "org.apache.hadoop.hbase.client.Mutation.java", "org.apache.hadoop.hbase.client.Increment.java"], "label": 1, "es_results": []}, {"bug_id": 15315, "bug_title": "Remove always set super user call as high priority", "bug_description": "Current implementation set superuser call as ADMIN_QOS, but we have many customers use superuser to do normal table operation such as put/get data and so on. If client put much data during region assignment, RPC from HMaster may timeout because of no handle. so it is better to remove always set super user call as high priority. ", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.AnnotationReadingPriorityFunction.java", "org.apache.hadoop.hbase.regionserver.TestPriorityRpc.java"], "label": 1, "es_results": []}, {"bug_id": 15137, "bug_title": "CallTimeoutException and CallQueueTooBigException should trigger PFFE", "bug_description": "If a region server is backed up enough that lots of calls are timing out then we should think about treating a server as failing.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.exceptions.PreemptiveFastFailException.java", "org.apache.hadoop.hbase.client.TestFastFail.java", "org.apache.hadoop.hbase.client.PreemptiveFastFailInterceptor.java", "org.apache.hadoop.hbase.client.FastFailInterceptorContext.java", "org.apache.hadoop.hbase.exceptions.ClientExceptionsUtil.java"], "label": 1, "es_results": []}, {"bug_id": 14256, "bug_title": "Flush task message may be confusing when region is recovered", "bug_description": "In HRegion.setRecovering() we have this code:\n\n\n\n    // force a flush only if region replication is set up for this region. Otherwise no need.\n\n      boolean forceFlush = getTableDesc().getRegionReplication() > 1;\n\n\n\n      // force a flush first\n\n      MonitoredTask status = TaskMonitor.get().createStatus(\n\n        \"Flushing region \" + this + \" because recovery is finished\");\n\n      try {\n\n        if (forceFlush) {\n\n          internalFlushcache(status);\n\n        }\n\n\n\nSo we only optionally force flush after a recovery of a region, but the message always is set to \"Flushing...\", which might be confusing. We should change the message based on forceFlush.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java"], "label": 1, "es_results": []}, {"bug_id": 15548, "bug_title": "SyncTable: sourceHashDir is supposed to be optional but will not work without ", "bug_description": "1) SyncTable code is contradictory. Usage said sourcehashdir is optional (https://github.com/apache/hbase/blob/ad3feaa44800f10d102255a240c38ccf23a82d49/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java#L687). However, the command won&apos;t run if sourcehashdir is missing (https://github.com/apache/hbase/blob/ad3feaa44800f10d102255a240c38ccf23a82d49/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java#L83-L85) \n2) There is no documentation on how to create the desired sourcehash", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.SyncTable.java"], "label": 1, "es_results": []}, {"bug_id": 15591, "bug_title": "ServerCrashProcedure not yielding", "bug_description": "ServerCrashProcedure is not propagating ProcedureYieldException to the ProcedureExecutor \nOne symptom is that while ServerCrashProcedure is waiting for meta to be up the Procedure WALs get filled up rapidly with all the executions.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java"], "label": 1, "es_results": []}, {"bug_id": 15093, "bug_title": "Replication can report incorrect size of log queue for the global source when multiwal is enabled", "bug_description": "Replication can  report incorrect size for the size of log queue for the global source when multiwal is enabled. This happens because the method MetricsSource#setSizeofLogQueue performs non-trivial operations in a multithreaded world, even though it is not synchronized. \nWe can simply divide MetricsSource#setSizeofLogQueue into MetricsSource#incrSizeofLogQueue and MetricsSource#decrSizeofLogQueue. Not sure why we are currently directly setting the size instead of incrementing/decrementing it.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.replication.regionserver.MetricsSource.java", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java", "org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationGlobalSourceSource.java", "org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSourceSourceImpl.java", "org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSourceSource.java"], "label": 1, "es_results": []}, {"bug_id": 15773, "bug_title": "CellCounter improvements", "bug_description": "Looking at the CellCounter map reduce, it seems like it can be improved in a few areas:\n\nit does not currently support setting scan batching.  This is important when we&apos;re fetching all versions for columns.  Actually, it would be nice to support all of the scan configuration currently provided in TableInputFormat.\ngenerating job counters containing row keys and column qualifiers is guaranteed to blow up on anything but the smallest table.  This is not usable and doesn&apos;t make any sense when the same counts are in the job output.  The row and qualifier specific counters should be dropped.\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/1.3.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.TableInputFormat.java", "org.apache.hadoop.hbase.mapreduce.CellCounter.java"], "label": 1, "es_results": []}, {"bug_id": 15465, "bug_title": "userPermission returned by getUserPermission() for the selected namespace does not have namespace set", "bug_description": "The request sent is with type = Namespace, but the response returned contains Global permissions (that is, the field of namespace is not set)\nIt is in hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java, from line 2380, and I made some comments into it\n\n\n\n/**\n\n   * A utility used to get permissions for selected namespace.\n\n   * <p>\n\n   * It&apos;s also called by the she will, in case you want to find references.\n\n   *\n\n   * @param protocol the AccessControlService protocol proxy\n\n   * @param namespace name of the namespace\n\n   * @throws ServiceException\n\n   */\n\n  public static List<UserPermission> getUserPermissions(\n\n      AccessControlService.BlockingInterface protocol,\n\n      byte[] namespace) throws ServiceException {\n\n    AccessControlProtos.GetUserPermissionsRequest.Builder builder =\n\n      AccessControlProtos.GetUserPermissionsRequest.newBuilder();\n\n    if (namespace != null) {\n\n      builder.setNamespaceName(ByteStringer.wrap(namespace)); \n\n    }\n\n    builder.setType(AccessControlProtos.Permission.Type.Namespace);  //builder is set with type = Namespace\n\n    AccessControlProtos.GetUserPermissionsRequest request = builder.build();  //I printed the request, its type is Namespace, which is correct.\n\n    AccessControlProtos.GetUserPermissionsResponse response =  \n\n       protocol.getUserPermissions(null, request);\n\n/* I printed the response, it contains Global permissions, as below, not a Namespace permission.\n\n\n\nuser_permission {\n\n  user: \"a1\"\n\n  permission {\n\n    type: Global\n\n    global_permission {\n\n      action: READ\n\n      action: WRITE\n\n      action: ADMIN\n\n      action: EXEC\n\n      action: CREATE\n\n    }\n\n  }\n\n}\n\n\n\nAccessControlProtos.GetUserPermissionsRequest has a member called type_ to store the type, but AccessControlProtos.GetUserPermissionsResponse does not.\n\n*/\n\n     \n\n    List<UserPermission> perms = new ArrayList<UserPermission>(response.getUserPermissionCount());\n\n    for (AccessControlProtos.UserPermission perm: response.getUserPermissionList()) {\n\n      perms.add(ProtobufUtil.toUserPermission(perm));  // (1)\n\n    }\n\n    return perms;\n\n  }\n\n\n\nit could be more reasonable to return user permissions with namespace set in getUserPermission() for selected namespace ?", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.security.access.AccessControlLists.java", "org.apache.hadoop.hbase.security.access.TestAccessController.java"], "label": 1, "es_results": []}, {"bug_id": 15698, "bug_title": "Increment TimeRange not serialized to server", "bug_description": "Before HBase-1.2, the Increment TimeRange set on the client was serialized over to the server. As of HBase 1.2, this appears to no longer be true, as my preIncrement coprocessor always gets HConstants.LATEST_TIMESTAMP as the value of increment.getTimeRange().getMax() regardless of what the client has specified.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.protobuf.ProtobufUtil.java"], "label": 1, "es_results": []}, {"bug_id": 16221, "bug_title": "Thrift server drops connection on long scans", "bug_description": "Thrift servers use connection cache and we drop connections after hbase.thrift.connection.max-idletime milliseconds from the last time a connection object was accessed. However, we never update this last accessed time on scan path. \nBy default, this will cause scanners to fail after 10 minutes, if the underlying connection object is not being used along other operation paths (like put).", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java", "org.apache.hadoop.hbase.util.ConnectionCache.java", "org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java"], "label": 1, "es_results": []}, {"bug_id": 16096, "bug_title": "Replication keeps accumulating znodes", "bug_description": "If there is an error while creating the replication source on adding the peer, the source if not added to the in memory list of sources but the replication peer is. \nHowever, in such a scenario, when you remove the peer, it is deleted from zookeeper successfully but for removing the in memory list of peers, we wait for the corresponding sources to get deleted (which as we said don&apos;t exist because of error creating the source). \nThe problem here is the ordering of operations for adding/removing source and peer. \nModifying the code to always remove queues from the underlying storage, even if there exists no sources also requires a small refactoring of TableBasedReplicationQueuesImpl to not abort on removeQueues() of an empty queue", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java"], "label": 1, "es_results": []}, {"bug_id": 16528, "bug_title": "Procedure-V2: ServerCrashProcedure misses owner information", "bug_description": "ServerCrashProcedure constructor does not set up owner information.  If someone wants to access the owner of ServerCrashProcedure, it would get NPE (eg. in case someone accidentally tries to abort a ServerCrashProcedure, the coprocessor to check owner of the procedure would throw NPE)", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.TestDeadServer.java", "org.apache.hadoop.hbase.master.procedure.TestServerCrashProcedure.java", "org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java", "org.apache.hadoop.hbase.master.ServerManager.java", "org.apache.hadoop.hbase.master.procedure.TestMasterProcedureEvents.java"], "label": 1, "es_results": []}, {"bug_id": 16375, "bug_title": "Mapreduce mini cluster using HBaseTestingUtility not setting correct resourcemanager and jobhistory webapp address of MapReduceTestingShim  ", "bug_description": "Starting mapreduce mini cluster using HBaseTestingUtility is not setting \"yarn.resourcemanager.webapp.address\" and \"mapreduce.jobhistory.webapp.address\" which are required for getting the submitted yarn apps using mapreduce webapp. These properties are not being copied from jobConf of MapReduceTestingShim resulting in default values.\n\nHBaseTestingUtility.java\n    // Allow the user to override FS URI for this map-reduce cluster to use.\n    mrCluster = new MiniMRCluster(servers,\n      FS_URI != null ? FS_URI : FileSystem.get(conf).getUri().toString(), 1,\n      null, null, new JobConf(this.conf));\n    JobConf jobConf = MapreduceTestingShim.getJobConf(mrCluster);\n    if (jobConf == null) \nUnknown macro: {\n      jobConf = mrCluster.createJobConf();\n    } \n    jobConf.set(\"mapreduce.cluster.local.dir\",\n      conf.get(\"mapreduce.cluster.local.dir\")); //Hadoop MiniMR overwrites this while it should not\n    LOG.info(\"Mini mapreduce cluster started\");\n    // In hadoop2, YARN/MR2 starts a mini cluster with its own conf instance and updates settings.\n    // Our HBase MR jobs need several of these settings in order to properly run.  So we copy the\n    // necessary config properties here.  YARN-129 required adding a few properties.\n    conf.set(\"mapreduce.jobtracker.address\", jobConf.get(\"mapreduce.jobtracker.address\"));\n    // this for mrv2 support; mr1 ignores this\n    conf.set(\"mapreduce.framework.name\", \"yarn\");\n    conf.setBoolean(\"yarn.is.minicluster\", true);\n    String rmAddress = jobConf.get(\"yarn.resourcemanager.address\");\n    if (rmAddress != null) \nUnknown macro: {\n      conf.set(\"yarn.resourcemanager.address\", rmAddress);\n    } \n    String historyAddress = jobConf.get(\"mapreduce.jobhistory.address\");\n    if (historyAddress != null) \nUnknown macro: {\n      conf.set(\"mapreduce.jobhistory.address\", historyAddress);\n    } \n    String schedulerAddress =\n      jobConf.get(\"yarn.resourcemanager.scheduler.address\");\n    if (schedulerAddress != null) \nUnknown macro: {\n      conf.set(\"yarn.resourcemanager.scheduler.address\", schedulerAddress);\n    } \nAs a immediate fix for phoenix e2e test to succeed, need the below lines to be added as well\n\n    String rmWebappAddress = jobConf.get(\"yarn.resourcemanager.webapp.address\");\n    if (rmWebappAddress != null) \nUnknown macro: {\n      conf.set(\"yarn.resourcemanager.webapp.address\", rmWebappAddress);\n    } \n    String historyWebappAddress = jobConf.get(\"mapreduce.jobhistory.webapp.address\");\n    if (historyWebappAddress != null) \nUnknown macro: {\n      conf.set(\"mapreduce.jobhistory.webapp.address\", historyWebappAddress);\n    } \nEventually, we should also see if we can copy over all the jobConf properties to HBaseTestingUtility conf object.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.HBaseTestingUtility.java", "org.apache.hadoop.hbase.TestHBaseTestingUtility.java"], "label": 1, "es_results": []}, {"bug_id": 15297, "bug_title": "error message is wrong when a wrong namspace is specified in grant in hbase she will", "bug_description": "In HBase she will, specify a non-existing namespace in \"grant\" command, such as\n\n\n\nhbase(main):001:0> grant &apos;a1&apos;, &apos;R&apos;, &apos;@aaa&apos;    <--- there is no namespace called \"aaa\"\n\n\n\nThe error message issued is not correct\n\n\n\nERROR: Unknown namespace a1!\n\n\n\na1 is the user name, not the namespace.\nThe following error message would be better\n\n\n\nERROR: Unknown namespace aaa!\n\n\n\nor\n\n\n\nCan&apos;t find a namespace: aaa\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.HBaseAdmin.java", "org.apache.hadoop.hbase.client.Admin.java"], "label": 1, "es_results": []}, {"bug_id": 15884, "bug_title": "NPE in StoreFileScanner#skipKVsNewerThanReadpoint during reverse scan", "bug_description": "Here is a part of skipKVsNewerThanReadpoint method:\n\n      hfs.next();\n\n      setCurrentCell(hfs.getKeyValue());\n\n      if (this.stopSkippingKVsIfNextRow\n\n          && getComparator().compareRows(cur.getRowArray(), cur.getRowOffset(),\n\n              cur.getRowLength(), startKV.getRowArray(), startKV.getRowOffset(),\n\n              startKV.getRowLength()) > 0) {\n\n\n\n\n\nIf hfs has no more KVs, cur will be set to Null and on on the next step will throw NPE. ", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.StoreFileScanner.java"], "label": 1, "es_results": []}, {"bug_id": 15946, "bug_title": "Eliminate possible security concerns in RS web UI's store file metrics", "bug_description": "More from static code analysis: it warns about the invoking of a separate command (\"hbase hfile -s -f ...\") as a possible security issue in hbase-server/src/main/resources/hbase-webapps/regionserver/storeFile.jsp.\nIt looks to me like one cannot inject arbitrary she will script or even arbitrary arguments: ProcessBuilder makes that fairly safe and only allows the user to specify the argument that comes after -f. However that does potentially allow them to have the daemon&apos;s user access files they shouldn&apos;t be able to touch, albeit only for reading.\nTo more explicitly eliminate any threats here, we should add some validation that the file is at least within HBase&apos;s root directory and use the Java API directly instead of invoking a separate executable.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.1", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.java"], "label": 1, "es_results": []}, {"bug_id": 14092, "bug_title": "hbck should run without locks by default and only disable the balancer when necessary", "bug_description": "HBCK is sometimes used as a way to check the health of the cluster. When doing that it&apos;s not necessary to turn off the balancer. As such it&apos;s not needed to lock other runs of hbck out.\nWe should add the --no-lock and --no-balancer command line flags.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.util.HBaseFsck.java"], "label": 1, "es_results": []}, {"bug_id": 15000, "bug_title": "Fix javadoc warn in LoadIncrementalHFiles", "bug_description": "[WARNING] Javadoc Warnings\n[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java:430: warning - @param argument \"hfilesDir\" is not a parameter name", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java"], "label": 1, "es_results": []}, {"bug_id": 15028, "bug_title": "Minor fix on RegionGroupingProvider", "bug_description": "Currently in RegionGroupingProvider#getWAL(String) we&apos;re trying to get a WAL instance from the cache using walCacheLock as the key (a typo when fixing HBASE-14306, my fault...), while actually we should have used group name. This won&apos;t because any fatal error but will slightly affect the perf since it will always run into the succeeding synchronized code. Will get this fixed in this JIRA", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.wal.RegionGroupingProvider.java"], "label": 1, "es_results": []}, {"bug_id": 15034, "bug_title": "IntegrationTestDDLMasterFailover does not clean created namespaces ", "bug_description": "I was running this test recently and notice that after every run there are new namespaces created by test and not cleared when test is finished. ", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.IntegrationTestDDLMasterFailover.java"], "label": 1, "es_results": []}, {"bug_id": 15397, "bug_title": "Create bulk load replication znode(hfile-refs) in ZK replication queue by default", "bug_description": "Create bulk load replication znode(hfile-refs) in ZK replication queue by default same as hbase replication znode. \nOtherwise the problem what happens is currently replication admin directly operates on ZK without routing through HM/RS. So suppose if a user enables the replication for bulk loaded data in server but fails to do the same in the client configurations then add peer will not add hfile-refs znode, resulting in replication failure for bulk loaded data.\nSo after fixing this the behavior will be same as mutation replication.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.replication.ReplicationStateZKBase.java", "org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java", "org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.java"], "label": 1, "es_results": []}, {"bug_id": 15425, "bug_title": "Failing to write bulk load event marker in the WAL is ignored", "bug_description": "During LoadIncrementalHFiles process if we fail to write the bulk load event marker in the WAL, it is ignored. So this will lead to data mismatch issue in source and peer cluster in case of bulk loaded data replication scenario.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java"], "label": 1, "es_results": []}, {"bug_id": 15515, "bug_title": "Improve LocalityBasedCandidateGenerator in Balancer", "bug_description": "There are some problems which need to fix.\n1. LocalityBasedCandidateGenerator.getLowestLocalityRegionOnServer should skip empty region.\n2. When use LocalityBasedCandidateGenerator to generate Cluster.Action, it should add random operation instead of pickLowestLocalityServer(cluster). Because the search function may stuck here if it always generate the same Cluster.Action.\n3. getLeastLoadedTopServerForRegion should get least loaded server which have better locality than current server.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.java", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java"], "label": 1, "es_results": []}, {"bug_id": 15578, "bug_title": "Handle HBASE-15234 for ReplicationHFileCleaner", "bug_description": "HBASE-15234 is handled for ReplicationLogCleaner need to handle similarly for ReplicationHFileCleaner.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.cleaner.TestReplicationHFileCleaner.java", "org.apache.hadoop.hbase.replication.master.ReplicationHFileCleaner.java"], "label": 1, "es_results": []}, {"bug_id": 15637, "bug_title": "TSHA Thrift-2 server should allow limiting call queue size", "bug_description": "Right now seems like thrift-2 hsha server always create unbounded queue, which could lead to OOM)", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.thrift2.ThriftServer.java"], "label": 1, "es_results": []}, {"bug_id": 15504, "bug_title": "Fix Balancer in 1.3 not moving regions off overloaded regionserver", "bug_description": "We pushed 1.3 to a couple of clusters. In some cases the regions were assigned VERY un-evenly and the regions would not move after that.\nWe ended up with one rs getting thousands of regions and most servers getting 0. Running balancer would do nothing. The balancer would say that it couldn&apos;t find a solution with less than the current cost.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java"], "label": 1, "es_results": []}, {"bug_id": 15668, "bug_title": "HFileReplicator$Copier fails to replicate other hfiles in the request when a hfile in not found in FS anywhere", "bug_description": "When a hfile is not found either in its source or archive directory then HFileReplicator$Copier will ignore that file and return instead we should ignore and continue with other hfiles replication in that request.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.replication.regionserver.HFileReplicator.java"], "label": 1, "es_results": []}, {"bug_id": 15360, "bug_title": "Fix flaky TestSimpleRpcScheduler", "bug_description": "There were several flaky tests added there as part of HBASE-15306 and likely HBASE-15136.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.AdaptiveLifoCoDelCallQueue.java", "org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java"], "label": 1, "es_results": []}, {"bug_id": 15697, "bug_title": "Excessive TestHRegion running time on branch-1", "bug_description": "On my dev box TestHRegion takes about 90 seconds to complete in master and about 60 seconds in 0.98, but about 370 seconds in branch-1. Furthermore TestHRegion in branch-1 blew past my open files ulimit. I had to raise it from default in order for the unit to complete at all.\nI am going to bisect the recent history of branch-1 in search of a culprit and report back.\nmaster\nRunning org.apache.hadoop.hbase.regionserver.TestHRegion\nTests run: 102, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 87.299 sec - in org.apache.hadoop.hbase.regionserver.TestHRegion\nRunning org.apache.hadoop.hbase.regionserver.TestHRegion\nTests run: 102, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 91.529 sec - in org.apache.hadoop.hbase.regionserver.TestHRegion\nRunning org.apache.hadoop.hbase.regionserver.TestHRegion\nTests run: 102, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 89.23 sec - in org.apache.hadoop.hbase.regionserver.TestHRegion\n\nbranch-1\nRunning org.apache.hadoop.hbase.regionserver.TestHRegion\nTests run: 102, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 368.868 sec - in org.apache.hadoop.hbase.regionserver.TestHRegion\nRunning org.apache.hadoop.hbase.regionserver.TestHRegion\nTests run: 102, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 366.203 sec - in org.apache.hadoop.hbase.regionserver.TestHRegion\nRunning org.apache.hadoop.hbase.regionserver.TestHRegion\nTests run: 102, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 345.806 sec - in org.apache.hadoop.hbase.regionserver.TestHRegion\n\n0.98\nRunning org.apache.hadoop.hbase.regionserver.TestHRegion\nTests run: 90, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 61.038 sec - in org.apache.hadoop.hbase.regionserver.TestHRegion\nRunning org.apache.hadoop.hbase.regionserver.TestHRegion\nTests run: 90, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 56.382 sec - in org.apache.hadoop.hbase.regionserver.TestHRegion\nRunning org.apache.hadoop.hbase.regionserver.TestHRegion\nTests run: 90, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 63.509 sec - in org.apache.hadoop.hbase.regionserver.TestHRegion\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestHRegion.java"], "label": 1, "es_results": []}, {"bug_id": 16047, "bug_title": "TestFastFail is broken again", "bug_description": "As found by Appy here - http://hbase.x10host.com/flaky-tests/\nHas been failing since https://builds.apache.org/job/HBase-Flaky-Tests/1294/#showFailuresLink,", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.TestFastFail.java"], "label": 1, "es_results": []}, {"bug_id": 16058, "bug_title": "TestHRegion fails on 1.4 builds", "bug_description": "Results :\nFailed tests: \norg.apache.hadoop.hbase.regionserver.TestHRegion.testFlushSizeAccounting(org.apache.hadoop.hbase.regionserver.TestHRegion)\n  Run 1: TestHRegion.testFlushSizeAccounting:465 expected:<384> but was:<368>\n  Run 2: TestHRegion.testFlushSizeAccounting:465 expected:<384> but was:<368>\n  Run 3: TestHRegion.testFlushSizeAccounting:465 expected:<384> but was:<368>\norg.apache.hadoop.hbase.regionserver.TestHRegion.testMemstoreSizeAccountingWithFailedPostBatchMutate(org.apache.hadoop.hbase.regionserver.TestHRegion)\n  Run 1: TestHRegion.testMemstoreSizeAccountingWithFailedPostBatchMutate:434 memstoreSize should be incremented expected:<448> but was:<432>\n  Run 2: TestHRegion.testMemstoreSizeAccountingWithFailedPostBatchMutate:434 memstoreSize should be incremented expected:<448> but was:<432>\n  Run 3: TestHRegion.testMemstoreSizeAccountingWithFailedPostBatchMutate:434 memstoreSize should be incremented expected:<448> but was:<432>\nTests run: 2567, Failures: 2, Errors: 0, Skipped: 56", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.4.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestHRegion.java"], "label": 1, "es_results": []}, {"bug_id": 16081, "bug_title": "Replication remove_peer gets stuck and blocks WAL rolling", "bug_description": "We use a blocking take from CompletionService in HBaseInterClusterReplicationEndpoint. When we remove a peer, we try to shut down all threads gracefully. But, under certain race condition, the underlying executor gets shutdown and the CompletionService#take will block forever, which means the remove_peer call will never gracefully finish.\nSince ReplicationSourceManager#removePeer and ReplicationSourceManager#recordLog lock on the same object, we are not able to roll WALs in such a situation and will end up with gigantic WALs.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.java", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java", "org.apache.hadoop.hbase.replication.ReplicationEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 16309, "bug_title": "TestDefaultCompactSelection.testCompactionRatio is flaky", "bug_description": "The aged major compaction condition is not stable.\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.4.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.TestDefaultCompactSelection.java", "org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java"], "label": 1, "es_results": []}, {"bug_id": 16699, "bug_title": "Overflows in AverageIntervalRateLimiter's refill() and getWaitInterval()", "bug_description": "It seems that there are more overflow in other places, and a concurrent issue.\nI will post a patch within one or 2 days after I figure out adding new unittest cases.\nPlease see the following two lines. Once it overflows, it will cause wrong behavior. For unconfigured RateLimiters, we should have simpler logic to byPass the check. \nhttps://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/AverageIntervalRateLimiter.java#L37\nhttps://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/AverageIntervalRateLimiter.java#L51", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.4.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.quotas.OperationQuota.java", "org.apache.hadoop.hbase.quotas.RateLimiter.java", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.java", "org.apache.hadoop.hbase.quotas.QuotaLimiter.java", "org.apache.hadoop.hbase.quotas.TestRateLimiter.java", "org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter.java", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota.java", "org.apache.hadoop.hbase.quotas.NoopOperationQuota.java", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.java"], "label": 1, "es_results": []}, {"bug_id": 16788, "bug_title": "Race in compacted file deletion between HStore close() and closeAndArchiveCompactedFiles()", "bug_description": "HBASE-13082 changed the way that compacted files are archived from being done inline on compaction completion to an async cleanup by the CompactedHFilesDischarger chore.  It looks like the changes to HStore to support this introduced a race condition in the compacted HFile archiving.\nIn the following sequence, we can wind up with two separate threads trying to archive the same HFiles, causing a regionserver abort:\n\ncompaction completes normally and the compacted files are added to compactedfiles in HStore&apos;s DefaultStoreFileManager\nthreadA: CompactedHFilesDischargeHandler runs in a RS executor service, calling closeAndArchiveCompactedFiles()\n\t\nobtains HStore readlock\ngets a copy of compactedfiles\nreleases readlock\n\n\nthreadB: calls HStore.close() as part of region close\n\t\nobtains HStore writelock\ncalls DefaultStoreFileManager.clearCompactedfiles(), getting a copy of same compactedfiles\n\n\nthreadA: calls HStore.removeCompactedfiles(compactedfiles)\n\t\narchives files in \n{compactedfiles}\n in HRegionFileSystem.removeStoreFiles()\ncall HStore.clearCompactedFiles()\nwaits on write lock\n\n\nthreadB: continues with close()\n\t\ncalls removeCompactedfiles(compactedfiles)\ncalls HRegionFIleSystem.removeStoreFiles() -> HFileArchiver.archiveStoreFiles()\nreceives FileNotFoundException because the files have already been archived by threadA\nthrows IOException\n\n\nRS aborts\n\nI think the combination of fetching the compactedfiles list and removing the files needs to be covered by locking.  Options I see are:\n\nModify HStore.closeAndArchiveCompactedFiles(): use writelock instead of readlock and move the call to removeCompactedfiles() inside the lock.  This means the read operations will be blocked while the files are being archived, which is bad.\nSynchronize closeAndArchiveCompactedFiles() and modify close() to call it instead of calling removeCompactedfiles() directly\nAdd a separate lock for compacted files removal and use in closeAndArchiveCompactedFiles() and close()\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HStore.java"], "label": 1, "es_results": []}, {"bug_id": 16752, "bug_title": "Upgrading from 1.2 to 1.3 can lead to replication failures due to difference in RPC size limit", "bug_description": "In HBase 1.2, we don&apos;t limit size of a single RPC but in 1.3 we limit it by default to 256 MB.  This means that during upgrade scenarios (or when source is 1.2 peer is already on 1.3), it&apos;s possible to encounter a situation where we try to send an rpc with size greater than 256 MB because we never unroll a WALEdit while sending replication traffic.\nRpcServer throws the underlying exception locally, but closes the connection with returning the underlying error to the client, and client only sees a \"Broken pipe\" error.\nI am not sure what is the proper fix here (or if one is needed) to make sure this does not happen, but we should return the underlying exception to the RpcClient, because without it, it can be difficult to diagnose the problem, especially for someone new to HBase.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.ipc.RpcServer.java", "org.apache.hadoop.hbase.exceptions.RequestTooBigException.java", "org.apache.hadoop.hbase.exceptions.ClientExceptionsUtil.java", "org.apache.hadoop.hbase.ipc.AbstractTestIPC.java"], "label": 1, "es_results": []}, {"bug_id": 16980, "bug_title": "TestRowProcessorEndpoint failing consistently", "bug_description": "Found while evaluating 1.2.4 RC1\n\n  TestRowProcessorEndpoint.testMultipleRows:246 expected:<3> but was:<2>\n\n  TestRowProcessorEndpoint.testReadModifyWrite:184 expected:<101> but was:<91>\n\n\n", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.2.4", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.coprocessor.TestRowProcessorEndpoint.java"], "label": 1, "es_results": []}, {"bug_id": 17032, "bug_title": "CallQueueTooBigException and CallDroppedException should not be triggering PFFE", "bug_description": "Back in HBASE-15137 we made it so that CQTBE causes preemptive fast fail exception on the client. \nIt seems those 2 load control mechanists don&apos;t exactly align here. Server throws CallQueueTooBigException, CallDroppedException (from deadline scheduler) when it feels overloaded. Client should accept that behavior and retry. When servers sheds the load, and client also bails out, the load shedding  bubbles up too high and high level impact on the client applications seems worse with PFFE turned on then without.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.client.TestFastFail.java", "org.apache.hadoop.hbase.client.PreemptiveFastFailInterceptor.java"], "label": 1, "es_results": []}, {"bug_id": 16992, "bug_title": "The usage of mutation from CP is weird.", "bug_description": "HRegion#doMiniBatchMutate\n\n\nMutation cpMutation = cpMutations[j];\n\nMap<byte[], List<Cell>> cpFamilyMap = cpMutation.getFamilyCellMap();\n\ncheckAndPrepareMutation(cpMutation, replay, cpFamilyMap, now);\n\n // Acquire row locks. If not, the whole batch will fail.\n\nacquiredRowLocks.add(getRowLockInternal(cpMutation.getRow(), true));\n\nif (cpMutation.getDurability() == Durability.SKIP_WAL) {\n\n  recordMutationWithoutWal(cpFamilyMap);\n\n}\n\n// Returned mutations from coprocessor correspond to the Mutation at index i. We can\n\n // directly add the cells from those mutations to the familyMaps of this mutation.\n\nmergeFamilyMaps(familyMaps[i], cpFamilyMap); // will get added to the memstore later\n\n\n\n1. Does the returned mutation from coprocessor have the same row as the corresponded mutation? If so, the acquiredRowLocks() can be saved. If not, the corresponded mutation may maintain the cells with different row due to mergeFamilyMaps().\n2. Is returned mutation&apos;s durability useful? If so, we should deal with the different durabilities before mergeFamilyMaps(). If not, the recordMutationWithoutWal can be saved. \n3. If both the returned mutation and corresponded mutation have Durability.SKIP_WAL, the recordMutationWithoutWal() may record the duplicate cells due to mergeFamilyMaps().\nAny comment? Thanks.", "project": "Apache", "sub_project": "HBASE", "version": "rel/1.3.0", "fixed_version": "rel/2.0.0", "fixed_files": ["org.apache.hadoop.hbase.regionserver.HRegion.java", "org.apache.hadoop.hbase.regionserver.TestHRegion.java", "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress.java"], "label": 1, "es_results": []}, {"bug_id": 3197, "bug_title": "Hive compile errors under Java 7 (JDBC 4.1)", "bug_description": "Hi, I&apos;ve been trying to compile Hive trunk from source and getting failures:\n\n\n\n    [javac] hive-svn/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveCallableStatement.java:48: error: HiveCallableStatement is not abstract and does not override abstract method <T>getObject(String,Class<T>) in CallableStatement\n\n    [javac] public class HiveCallableStatement implements java.sql.CallableStatement {\n\n    [javac]        ^\n\n    [javac]   where T is a type-variable:\n\n    [javac]     T extends Object declared in method <T>getObject(String,Class<T>)\n\n\n\nI think this is because JDBC 4.1 is part of Java 7, and is not source-compatible with older JDBC versions. Any chance you guys could add JDBC 4.1 support?", "project": "Apache", "sub_project": "HIVE", "version": "release-0.10.0", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.jdbc.HiveCallableStatement.java", "org.apache.hadoop.hive.jdbc.HivePreparedStatement.java", "org.apache.hadoop.hive.jdbc.HiveConnection.java", "org.apache.hadoop.hive.jdbc.HiveDriver.java", "org.apache.hadoop.hive.jdbc.HiveStatement.java", "org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java", "org.apache.hadoop.hive.jdbc.HiveDataSource.java", "org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java", "org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java"], "label": 1, "es_results": []}, {"bug_id": 3709, "bug_title": "Stop storing default ConfVars in temp file", "bug_description": "To work around issues with Hadoop&apos;s Configuration object, specifically it&apos;s addResource(InputStream), default configurations are written to a temp file (I think HIVE-2362 introduced this).\nThis, however, introduces the problem that once that file is deleted from /tmp the client crashes.  This is particularly problematic for long running services like the metastore server.\nWriting a custom InputStream to deal with the problems in the Configuration object should provide a work around, which does not introduce a time bomb into Hive.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.10.0", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 3428, "bug_title": "Fix log4j configuration errors when running hive on hadoop23", "bug_description": "There are log4j configuration errors when running hive on hadoop23, some of them may fail testcases, since the following log4j error message could printed to console, or to output file, which diffs from the expected output:\n[junit] < log4j:ERROR Could not find value for key log4j.appender.NullAppender\n[junit] < log4j:ERROR Could not instantiate appender named \"NullAppender\".\n[junit] < 12/09/04 11:34:42 WARN conf.HiveConf: hive-site.xml not found on CLASSPATH", "project": "Apache", "sub_project": "HIVE", "version": "release-0.10.0", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.shims.ShimLoader.java"], "label": 1, "es_results": []}, {"bug_id": 3308, "bug_title": "Mixing avro and snappy gives null values", "bug_description": "On default hive uses LazySimpleSerDe for output.\nWhen I now enable compression and \"select count from avrotable\" the output is a file with the .avro extension but this then will display null values since the file is in reality not an avro file but a file created by LazySimpleSerDe using compression so should be a .snappy file.\nThis causes any job (exception select * from avrotable is that not truly a job) to show null values.\nIf you use any serde other then avro you can temporarily fix this by setting \"set hive.output.file.extension=.snappy\" and it will correctly work again but this won&apos;t work on avro since it overwrites the hive.output.file.extension during initializing.\nWhen you dump the query result into a table with \"create table bla as\" you can rename the .avro file into .snappy and the \"select from bla\" will also magiacally work again.\nInput and Ouput serdes don&apos;t always match so when I use avro as an input format it should not set the hive.output.file.extension.\nOnces it&apos;s set all queries will use it and fail making the connection useless to reuse.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.10.0", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.serde2.avro.AvroSerDe.java"], "label": 1, "es_results": []}, {"bug_id": 3384, "bug_title": "HIVE JDBC module will not compile under JDK1.7 as new methods added in JDBC specification", "bug_description": "jdbc module couldn&apos;t be compiled with jdk7 as it adds some abstract method in the JDBC specification \nsome error info:\n error: HiveCallableStatement is not abstract and does not override abstract\nmethod <T>getObject(String,Class<T>) in CallableStatement\n.\n.\n.\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.10.0", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.jdbc.HiveCallableStatement.java", "org.apache.hadoop.hive.jdbc.HivePreparedStatement.java", "org.apache.hadoop.hive.jdbc.HiveConnection.java", "org.apache.hadoop.hive.jdbc.HiveDriver.java", "org.apache.hadoop.hive.jdbc.HiveStatement.java", "org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java", "org.apache.hadoop.hive.jdbc.HiveDataSource.java", "org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java", "org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java"], "label": 1, "es_results": []}, {"bug_id": 4911, "bug_title": "Enable QOP configuration for Hive Server 2 thrift transport", "bug_description": "The QoP for hive server 2 should be configurable to enable encryption. A new configuration should be exposed \"hive.server2.thrift.sasl.qop\". This would give greater control configuring hive server 2 service.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.11.0", "fixed_version": "release-0.12.0", "fixed_files": ["org.apache.hive.jdbc.HiveConnection.java", "org.apache.hive.service.auth.HiveAuthFactory.java", "org.apache.hadoop.hive.metastore.MetaStoreUtils.java", "org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java", "org.apache.hive.service.auth.KerberosSaslHelper.java", "org.apache.hadoop.hive.metastore.HiveMetaStore.java", "org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java", "org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java", "org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java"], "label": 1, "es_results": []}, {"bug_id": 5056, "bug_title": "MapJoinProcessor ignores order of values in removing RS", "bug_description": "http://www.mail-archive.com/user@hive.apache.org/msg09073.html", "project": "Apache", "sub_project": "HIVE", "version": "release-0.11.0", "fixed_version": "release-0.12.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 5256, "bug_title": "A map join operator may have in-consistent output row schema with the common join operator which it will replace", "bug_description": "When generating a common join operator, Semantic Analyzer gets the input RowResolver of each parent operators. It then uses the following piece of code to interate the tables from the RowResolver (refer to genJoinOperatorChildren()):\n        RowResolver inputRS = opParseCtx.get(input).getRowResolver();\n        Iterator<String> keysIter = inputRS.getTableNames().iterator();\n\t\t...\n        while (keysIter.hasNext()) {\n          String key = keysIter.next();\nNote that the interation order is not deterministic because of the current RowResolver implementation:\n  private  HashMap<String, LinkedHashMap<String, ColumnInfo>> rslvMap;\n  ...\n  public Set<String> getTableNames() \n{\n\n    return rslvMap.keySet();\n\n  }\n\nGenerally, the interation order has no problem. However, it may be problematic when a common join operator is being converted to a map join operator.\nMapJoinProcessor.convertMapJoin():\n      RowResolver inputRS = opParseCtxMap.get(newParentOps.get(pos)).getRowResolver();\n      List<ExprNodeDesc> values = new ArrayList<ExprNodeDesc>();\n      Iterator<String> keysIter = inputRS.getTableNames().iterator();\n      while (keysIter.hasNext()) {\nThe problem is that the table iteration order for a input RowResolver may be different from that in the generation of the common join operator, which result in an in-consistent output row schema. Thus wrong row schema may be input to child operators and will cause problems.\nI found this issue when running a TPC-DS query. And this issue happens to be exposed due to HIVE-4078.\nThe proposed fix is to change RowResolver to define rslvMap as LinkedHashMap instead of HashMap. Thus the table iteration order of a RowResolver is fixed.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.11.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 5102, "bug_title": "ORC getSplits should create splits based the stripes ", "bug_description": "Currently ORC inherits getSplits from FileFormat, which basically makes a split per an HDFS block. This can create too little parallelism and would be better done by having getSplits look at the file footer and create splits based on the stripes.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.11.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.shims.HadoopShims.java", "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java", "org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java", "org.apache.hadoop.hive.shims.Hadoop23Shims.java", "org.apache.hadoop.hive.shims.Hadoop20Shims.java", "org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java", "org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java", "org.apache.hadoop.hive.ql.io.orc.StripeInformation.java", "org.apache.hadoop.hive.shims.Hadoop20SShims.java"], "label": 1, "es_results": []}, {"bug_id": 6652, "bug_title": "Beeline gives evasive error message for any unrecognized command line arguement", "bug_description": "For any unrecognized command line argument, Beeline emits a warning message that&apos;s evasive and meaningless. For instance:\n\n\n\nbeeline abc\n\nabc (No such file or directory)\n\nBeeline version 0.14.0-SNAPSHOT by Apache Hive\n\n...\n\nbeeline -hh\n\n-hh (No such file or directory)\n\n\n\nThe error seeming suggests that Beeline accepts an argument as a file name. However, neither Beeline doc nor command line help indicates there is such an option. \n\n\n\nbeeline --help\n\nUsage: java org.apache.hive.cli.beeline.BeeLine \n\n   -you <database url>               the JDBC URL to connect to\n\n   -n <username>                   the username to connect as\n\n   -p <password>                   the password to connect as\n\n   -d <driver class>               the driver class to use\n\n   -e <query>                      query that should be executed\n\n   -f <file>                       script file that should be executed\n\n   --hiveconf property=value       Use value for given property\n\n   --hivevar name=value            hive variable name and value\n\n                                   This is Hive specific settings in which variables\n\n                                   can be set at session level and referenced in Hive\n\n                                   commands or queries.\n\n   --color=[true/false]            control whether color is used for display\n\n   --showHeader=[true/false]       show column names in query results\n\n   --headerInterval=ROWS;          the interval between which heades are displayed\n\n   --fastConnect=[true/false]      skip building table/column list for tab-completion\n\n   --autoCommit=[true/false]       enable/disable automatic transaction commit\n\n   --verbose=[true/false]          show verbose error messages and debug info\n\n   --showWarnings=[true/false]     display connection warnings\n\n   --showNestedErrs=[true/false]   display nested errors\n\n   --numberFormat=[pattern]        format numbers using DecimalFormat pattern\n\n   --force=[true/false]            continue running script even after errors\n\n   --maxWidth=MAXWIDTH             the maximum width of the terminal\n\n   --maxColumnWidth=MAXCOLWIDTH    the maximum width to use when displaying columns\n\n   --silent=[true/false]           be more silent\n\n   --autosave=[true/false]         automatically save preferences\n\n   --outputformat=[table/vertical/csv/tsv]   format mode for result display\n\n   --isolation=LEVEL               set the transaction isolation level\n\n   --nullemptystring=[true/false]  set to true to get historic behavior of printing null as empty string\n\n   --help                          display this message\n\n\n\nFurther research shows that this is a residual from SQLLine from which Beeline is derived, which allows user to specify a property file based on which SQLLine can make a DB connection.\nWhile this might be useful, this isn&apos;t documented and has caused a lot of confusions. And it&apos;s the root because for quite a few problems such as those described in HIVE-5677. HIVE-6173 had the same symptom, which uncovered another problem.\nThus, I&apos;d suggest we disable this option. If it&apos;s desirable to have this option, then we need at least corresponding documentation plus better error message.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.11.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.beeline.BeeLine.java"], "label": 1, "es_results": []}, {"bug_id": 5447, "bug_title": "HiveServer2 should allow secure impersonation over LDAP or other non-kerberos connection", "bug_description": "Currently the impersonation on a secure hadoop cluster only works when HS2 connection itself is kerberos. This forces clients to configure kerberos which can be a deployment nightmare.\nWe should allow other authentications mechanism to perform secure impersonation.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.11.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.service.cli.CLIService.java", "org.apache.hive.service.cli.thrift.ThriftCLIService.java"], "label": 1, "es_results": []}, {"bug_id": 6198, "bug_title": "ORC file and struct column names are case sensitive", "bug_description": "HiveQL document states that the \"Table names and column names are case insensitive\". But the struct behavior for ORC file is different. \nConsider a sample text file:\n\n\n\n$ cat data.txt\n\nline1|key11:value11,key12:value12,key13:value13|a,b,c|one,two\n\nline2|key21:value21,key22:value22,key23:value23|d,e,f|three,four\n\nline3|key31:value31,key32:value32,key33:value33|g,h,i|five,six\n\n\n\nCreating a table stored as txt and then using this to create a table stored as orc \n\n\n\nCREATE TABLE orig (\n\n  str STRING,\n\n  mp  MAP<STRING,STRING>,\n\n  lst ARRAY<STRING>,\n\n  strct STRUCT<A:STRING,B:STRING>\n\n) ROW FORMAT DELIMITED\n\n    FIELDS TERMINATED BY &apos;|&apos;\n\n    COLLECTION ITEMS TERMINATED BY &apos;,&apos;\n\n    MAP KEYS TERMINATED BY &apos;:&apos;;\n\nLOAD DATA LOCAL INPATH &apos;data.txt&apos; INTO TABLE orig;\n\n\n\nCREATE TABLE tableorc (\n\n  str STRING,\n\n  mp  MAP<STRING,STRING>,\n\n  lst ARRAY<STRING>,\n\n  strct STRUCT<A:STRING,B:STRING>\n\n) STORED AS ORC;\n\nINSERT OVERWRITE TABLE tableorc SELECT * FROM orig;\n\n\n\nSuppose we project columns or read the strct columns for both table types, here are the results. I have also tested the same with RC. The behavior is similar to txt files.\n\n\n\nhive> SELECT * FROM orig;\n\nline1   {\"key11\":\"value11\",\"key12\":\"value12\",\"key13\":\"value13\"} [\"a\",\"b\",\"c\"]  \n\n{\"a\":\"one\",\"b\":\"two\"}\n\nline2   {\"key21\":\"value21\",\"key22\":\"value22\",\"key23\":\"value23\"} [\"d\",\"e\",\"f\"]  \n\n{\"a\":\"three\",\"b\":\"four\"}\n\nline3   {\"key31\":\"value31\",\"key32\":\"value32\",\"key33\":\"value33\"} [\"g\",\"h\",\"i\"]  \n\n{\"a\":\"five\",\"b\":\"six\"}\n\nTime taken: 0.126 seconds, Fetched: 3 row(s)\n\n\n\nhive> SELECT * FROM tableorc;\n\nline1   {\"key12\":\"value12\",\"key11\":\"value11\",\"key13\":\"value13\"} [\"a\",\"b\",\"c\"]  \n\n{\"A\":\"one\",\"B\":\"two\"}\n\nline2   {\"key21\":\"value21\",\"key23\":\"value23\",\"key22\":\"value22\"} [\"d\",\"e\",\"f\"]  \n\n{\"A\":\"three\",\"B\":\"four\"}\n\nline3   {\"key33\":\"value33\",\"key31\":\"value31\",\"key32\":\"value32\"} [\"g\",\"h\",\"i\"]  \n\n{\"A\":\"five\",\"B\":\"six\"}\n\nTime taken: 0.178 seconds, Fetched: 3 row(s)\n\n\n\nhive> SELECT strct FROM tableorc;\n\n{\"a\":\"one\",\"b\":\"two\"}\n\n{\"a\":\"three\",\"b\":\"four\"}\n\n{\"a\":\"five\",\"b\":\"six\"}\n\n\n\nhive>SELECT strct.A FROM orig;\n\none\n\nthree\n\nfive\n\n\n\nhive>SELECT strct.a FROM orig;\n\none\n\nthree\n\nfive\n\n\n\nhive>SELECT strct.A FROM tableorc;\n\none\n\nthree\n\nfive\n\n\n\nhive>SELECT strct.a FROM tableorc;\n\nFAILED: Execution Error, return code 2 from\n\norg.apache.hadoop.hive.ql.exec.mr.MapRedTask\n\nMapReduce Jobs Launched: \n\nJob 0: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL\n\n\n\nSo it seems that ORC behaves differently for struct columns. Also why are we storing the column names for struct for the other types as CASE SENSITIVE? What is the standard for Hive QL with respect to structs?\nRegards\nViraj\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.11.0", "fixed_version": "release-1.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.orc.OrcStruct.java"], "label": 1, "es_results": []}, {"bug_id": 5204, "bug_title": "Change type compatibility methods to use PrimitiveCategory rather than TypeInfo", "bug_description": "The type compatibility methods in the FunctionRegistry (getCommonClass, implicitConvertable) compare TypeInfo objects directly when its doing its type compatibility logic. This won&apos;t work as well with qualified types (varchar, char, decimal), because we will need different TypeInfo objects to represent varchar(5) and varchar(10), and the equality comparisons won&apos;t work anymore. We can change this logic to look at the PrimitiveCategory for the TypeInfo instead.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FunctionRegistry.java", "org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java"], "label": 1, "es_results": []}, {"bug_id": 5511, "bug_title": "percentComplete returned by job status from WebHCat is null", "bug_description": "In hadoop1 the logging from MR is sent to stderr.  In H2, by default, to syslog.  templeton.tool.LaunchMapper expects to see the output on stderr to produce &apos;percentComplete&apos; in job status.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.shims.HadoopShims.java", "org.apache.hadoop.mapred.WebHCatJTShim20S.java", "org.apache.hive.hcatalog.templeton.tool.PigJobIDParser.java", "org.apache.hadoop.mapred.WebHCatJTShim23.java", "org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java", "org.apache.hive.hcatalog.templeton.tool.JarJobIDParser.java", "org.apache.hive.hcatalog.templeton.tool.HiveJobIDParser.java", "org.apache.hive.hcatalog.templeton.tool.HDFSStorage.java", "org.apache.hive.hcatalog.templeton.tool.TestTrivialExecService.java", "org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java", "org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java", "org.apache.hive.hcatalog.templeton.CompleteDelegator.java", "org.apache.hive.hcatalog.templeton.AppConfig.java"], "label": 1, "es_results": []}, {"bug_id": 6159, "bug_title": "Hive uses deprecated hadoop configuration in Hadoop 2.0", "bug_description": "Build hive against hadoop 2.0. Then run hive CLI, you&apos;ll see deprecated configurations warnings like this:\n13/12/14 01:00:51 INFO Configuration.deprecation: mapred.input.dir.recursive is\n deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.max.split.size is depre\n cated. Instead, use mapreduce.input.fileinputformat.split.maxsize\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size is depre\n cated. Instead, use mapreduce.input.fileinputformat.split.minsize\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.rack\n is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.r\n ack\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.node\n is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.n\n ode\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks is depreca\n ted. Instead, use mapreduce.job.reduces\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculativ\n e.execution is deprecated. Instead, use mapreduce.reduce.speculative", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.shims.Hadoop20Shims.java", "org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hadoop.hive.shims.Hadoop20SShims.java", "org.apache.hadoop.hive.shims.HadoopShimsSecure.java", "org.apache.hadoop.hive.shims.Hadoop23Shims.java", "org.apache.hadoop.hive.shims.HadoopShims.java"], "label": 1, "es_results": []}, {"bug_id": 6209, "bug_title": "'LOAD DATA INPATH ... OVERWRITE ..' does not overwrite current data", "bug_description": "In case where user loads data into table using overwrite, using a different file, it is not being overwritten.\n\n\n\n$ hdfs dfs -cat /tmp/data\n\naaa\n\nbbb\n\nccc\n\n$ hdfs dfs -cat /tmp/data2\n\nddd\n\neee\n\nfff\n\n$ hive\n\nhive> create table test (id string); \n\nhive> load data inpath &apos;/tmp/data&apos; overwrite into table test;\n\nhive> select * from test;\n\naaa\n\nbbb\n\nccc\n\nhive> load data inpath &apos;/tmp/data2&apos; overwrite into table test;\n\nhive> select * from test;\n\naaa\n\nbbb\n\nccc\n\nddd\n\neee\n\nfff\n\n\n\nIt seems it is broken by HIVE-3756 which added another condition to whether \"rmr\" should be run on old directory, and skips in this case.\nThere is a workaround of set fs.hdfs.impl.disable.cache=true; \nwhich sabotages this condition, but this condition should be removed in long-term.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 5901, "bug_title": "Query cancel should stop running MR tasks", "bug_description": "Currently, query canceling does not stop running MR job immediately.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.ql.Driver.java", "org.apache.hadoop.hive.ql.DriverContext.java", "org.apache.hadoop.hive.ql.exec.TaskRunner.java", "org.apache.hadoop.hive.ql.exec.ConditionalTask.java"], "label": 1, "es_results": []}, {"bug_id": 6176, "bug_title": "Beeline gives bogus error message if an unaccepted command line option is given", "bug_description": "\n\n\n$ beeline -o\n\n-o (No such file or directory)\n\nBeeline version 0.13.0-SNAPSHOT by Apache Hive\n\nbeeline> \n\n\n\nThe message suggests that beeline accepts a file (without -f option) while it enters interactive mode any way.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.beeline.BeeLine.java"], "label": 1, "es_results": []}, {"bug_id": 6687, "bug_title": "JDBC ResultSet fails to get value by qualified projection name", "bug_description": "Getting value from result set using fully qualified name would throw exception. Only solution today is to use position of the column as opposed to column label.\n\n\n\nString sql = \"select r1.x, r2.x from r1 join r2 on r1.y=r2.y\";\n\nResultSet res = stmt.executeQuery(sql);\n\nres.getInt(\"r1.x\");\n\n\n\nres.getInt(\"r1.x\"); would throw exception unknown column even though sql specifies it.\nFix is to fix resultsetschema in semantic analyzer.\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hive.jdbc.TestJdbcDriver2.java", "org.apache.hadoop.hive.jdbc.TestJdbcDriver.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 6492, "bug_title": "limit partition number involved in a table scan", "bug_description": "To protect the cluster, a new configure variable \"hive.limit.query.max.table.partition\" is added to hive configuration to\nlimit the table partitions involved in a table scan. \nThe default value will be set to -1 which means there is no limit by default. \nThis variable will not affect \"metadata only\" query.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java", "org.apache.hadoop.hive.ql.ErrorMsg.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.plan.TableScanDesc.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 6469, "bug_title": "skipTrash option in hive command line", "bug_description": "Th current behavior of hive metastore during a \"drop table <table_name>\" command is to delete the data from HDFS warehouse and put it into Trash.\nCurrently there is no way to provide a flag to tell the warehouse to skip trash while deleting table data.\nThis ticket is to add skipTrash configuration \"hive.warehouse.data.skipTrash\" , which when set to true, will skipTrash while dropping table data from hdfs warehouse. This will be set to false by default to keep current behavior.\nThis would be good feature to add, so that an admin of the cluster can specify when not to put data into the trash directory (eg. in a dev environment) and thus not to fill hdfs space instead of relying on trash interval and policy configuration to take care of disk filling issue.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl.java", "org.apache.hadoop.hive.common.FileUtils.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 6049, "bug_title": "Hive uses deprecated hadoop configuration in Hadoop 2.0", "bug_description": "Running hive CLI on hadoop 2.0, you&apos;ll see deprecated configurations warnings like this:\n13/12/14 01:00:51 INFO Configuration.deprecation: mapred.input.dir.recursive is\n deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.max.split.size is depre\n cated. Instead, use mapreduce.input.fileinputformat.split.maxsize\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size is depre\n cated. Instead, use mapreduce.input.fileinputformat.split.minsize\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.rack\n is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.r\n ack\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.node\n is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.n\n ode\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks is depreca\n ted. Instead, use mapreduce.job.reduces\n 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculativ\n e.execution is deprecated. Instead, use mapreduce.reduce.speculative", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.shims.Hadoop20Shims.java", "org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hadoop.hive.shims.Hadoop20SShims.java", "org.apache.hadoop.hive.shims.HadoopShimsSecure.java", "org.apache.hadoop.hive.shims.Hadoop23Shims.java", "org.apache.hadoop.hive.shims.HadoopShims.java"], "label": 1, "es_results": []}, {"bug_id": 5871, "bug_title": "Use multiple-characters as field delimiter", "bug_description": "By default, hive only allows user to use single character as field delimiter. Although there&apos;s RegexSerDe to specify multiple-character delimiter, it can be daunting to use, especially for amateurs.\nThe patch adds a new SerDe named MultiDelimitSerDe. With MultiDelimitSerDe, users can specify a multiple-character field delimiter when creating tables, in a way most similar to typical table creations. For example:\n\n\n\ncreate table test (id string,hivearray array<binary>,hivemap map<string,int>) ROW FORMAT SERDE &apos;org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe&apos; WITH SERDEPROPERTIES (\"field.delim\"=\"[,]\",\"collection.delim\"=\":\",\"mapkey.delim\"=\"@\");\n\n\n\nwhere field.delim is the field delimiter, collection.delim and mapkey.delim is the delimiter for collection items and key value pairs, respectively. Among these delimiters, field.delim is mandatory and can be of multiple characters, while collection.delim and mapkey.delim is optional and only support single character.\nTo use MultiDelimitSerDe, you have to add the hive-contrib jar to the class path, e.g. with the add jar command.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.serde2.lazy.LazyStruct.java", "org.apache.hadoop.hive.serde2.lazy.LazyBinary.java", "org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java"], "label": 1, "es_results": []}, {"bug_id": 6410, "bug_title": "Allow output serializations separators to be set for HDFS path as well.", "bug_description": "HIVE-3682 adds functionality for users to set serialization constants for &apos;insert overwrite local directory&apos;. The same functionality should be available for hdfs path as well. The workaround suggested is to create a table with required format and insert into the table, which enforces the users to know the schema of the result and create the table ahead. Though that works, it is good to have the functionality for loading into directory as well.\nI&apos;m planning to add the same functionality in &apos;insert overwrite directory&apos; in this jira.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.ql.plan.PlanUtils.java", "org.apache.hadoop.hive.ql.parse.QB.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 5672, "bug_title": "Insert with custom separator not supported for non-local directory", "bug_description": "https://issues.apache.org/jira/browse/HIVE-3682 is great but non local directory don&apos;t seem to be supported:\n\n\n\ninsert overwrite directory &apos;/tmp/test-02&apos;\n\nrow format delimited\n\nFIELDS TERMINATED BY &apos;:&apos;\n\nselect description FROM sample_07\n\n\n\n\n\n\nError while compiling statement: FAILED: ParseException line 2:0 cannot recognize input near &apos;row&apos; &apos;format&apos; &apos;delimited&apos; in select clause\n\n\n\nThis works (with &apos;local&apos;):\n\n\n\ninsert overwrite local directory &apos;/tmp/test-02&apos;\n\nrow format delimited\n\nFIELDS TERMINATED BY &apos;:&apos;\n\nselect code, description FROM sample_07\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-1.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.plan.PlanUtils.java", "org.apache.hadoop.hive.ql.parse.QB.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 10895, "bug_title": "ObjectStore does not close Query objects in some calls, causing a potential leak in some metastore db resources", "bug_description": "During testing, we&apos;ve noticed Oracle db running out of cursors. Might be related to this.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java", "org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java", "org.apache.hadoop.hive.metastore.ObjectStore.java"], "label": 1, "es_results": []}, {"bug_id": 12489, "bug_title": "Analyze for partition fails if partition value has special characters", "bug_description": "When analyzing a partition that has a special characters in the value, the analyze command fails with an exception. \nExample:\nhive> create table testtable (a int) partitioned by (b string);\nhive> insert into table testtable  partition (b=\"p\\\"1\") values (1);\nhive> ANALYZE TABLE testtable  PARTITION(b=\"p\\\"1\") COMPUTE STATISTICS for columns a;", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 12865, "bug_title": "Exchange partition does not show inputs field for post/pre execute hooks", "bug_description": "The pre/post execute hook interface has fields that indicate which Hive objects were read / written to as a result of running the query. For the exchange partition operation, the read entity field is empty.\nThis is an important issue as the hook interface may be configured to perform critical warehouse operations.\nSee\nql/src/test/results/clientpositive/exchange_partition3.q.out\n\n\n\n--- a/ql/src/test/results/clientpositive/exchange_partition3.q.out\n\n+++ b/ql/src/test/results/clientpositive/exchange_partition3.q.out\n\n@@ -65,9 +65,17 @@ ds=2013-04-05/hr=2\n\n PREHOOK: query: -- This will exchange both partitions hr=1 and hr=2\n\n ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds=&apos;2013-04-05&apos;) WITH TABLE exchange_part_test2\n\n PREHOOK: type: ALTERTABLE_EXCHANGEPARTITION\n\n+PREHOOK: Output: default@exchange_part_test1\n\n+PREHOOK: Output: default@exchange_part_test2\n\n POSTHOOK: query: -- This will exchange both partitions hr=1 and hr=2\n\n ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds=&apos;2013-04-05&apos;) WITH TABLE exchange_part_test2\n\n POSTHOOK: type: ALTERTABLE_EXCHANGEPARTITION\n\n+POSTHOOK: Output: default@exchange_part_test1\n\n+POSTHOOK: Output: default@exchange_part_test1@ds=2013-04-05/hr=1\n\n+POSTHOOK: Output: default@exchange_part_test1@ds=2013-04-05/hr=2\n\n+POSTHOOK: Output: default@exchange_part_test2\n\n+POSTHOOK: Output: default@exchange_part_test2@ds=2013-04-05/hr=1\n\n+POSTHOOK: Output: default@exchange_part_test2@ds=2013-04-05/hr=2\n\n PREHOOK: query: SHOW PARTITIONS exchange_part_test1\n\n PREHOOK: type: SHOWPARTITIONS\n\n PREHOOK: Input: default@exchange_part_test1\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.12.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.exec.DDLTask.java"], "label": 1, "es_results": []}, {"bug_id": 12963, "bug_title": "LIMIT statement with SORT BY creates additional MR job with hardcoded only one reducer", "bug_description": "I execute query:\nhive> select age from test1 sort by age.age  limit 10;                      \nTotal jobs = 2\nLaunching Job 1 out of 2\nNumber of reduce tasks not specified. Estimated from input data size: 1\nLaunching Job 2 out of 2\nNumber of reduce tasks determined at compile time: 1\nWhen I have a large number of rows then the last stage of the job takes a long time. I think we could allow to user choose number of reducers of last job or refuse extra MR job.\nThe same behavior I observed with querie:\nhive> create table new_test as select age from test1 group by age.age  limit 10;\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 6877, "bug_title": "TestOrcRawRecordMerger is deleting test.tmp.dir", "bug_description": "TestOrcRawRecordMerger seems to be deleting the directory pointed to by test.tmp.dir.  This can cause some failures in any tests that run after this test if they need to use any files in the tmp dir such as conf files or creating Hive tables.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java"], "label": 1, "es_results": []}, {"bug_id": 7011, "bug_title": "HiveInputFormat's split generation is not thread safe", "bug_description": "Tez will do split generation in parallel. Need to protect the inputformat cache against concurrent access.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.HiveInputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 6908, "bug_title": "TestThriftBinaryCLIService.testExecuteStatementAsync has intermittent failures", "bug_description": "This has failed sometimes in the pre-commit tests.\nThriftCLIServiceTest.testExecuteStatementAsync runs two statements.  They are given 100 second timeout total, not sure if its by intention.  As the first is a select query, it will take a majority of the time.  The second statement (create table) should be quicker, but it fails sometimes because timeout is already mostly used up.\nThe timeout should probably be reset after the first statement.  If the operation finishes before the timeout, it will not have any effect as it&apos;ll break out.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java"], "label": 1, "es_results": []}, {"bug_id": 7060, "bug_title": "Column stats give incorrect min and distinct_count", "bug_description": "It seems that the result from column statistics isn&apos;t correct on two measures for numeric columns: min (which is always 0) and distinct count. Here is an example:\n\n\n\nselect count(distinct avgTimeOnSite), min(avgTimeOnSite) from UserVisits_web_text_none;\n\n...\n\nOK\n\n9\t1\n\nTime taken: 9.747 seconds, Fetched: 1 row(s)\n\n\n\nThe statisitics for the column:\n\n\n\ndesc formatted UserVisits_web_text_none avgTimeOnSite\n\n...\n\n# col_name              data_type               min                     max                     num_nulls               distinct_count          avg_col_len             max_col_len             num_trues               num_falses              comment\n\n\n\navgTimeOnSite           int                     0                       9                       0                       11                      null                    null                    null               \n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.metastore.StatObjectConverter.java", "org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.java", "org.apache.hadoop.hive.metastore.api.LongColumnStatsData.java", "org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.java", "org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java", "org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java", "org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java", "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java", "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java"], "label": 1, "es_results": []}, {"bug_id": 7200, "bug_title": "Beeline output displays column heading even if --showHeader=false is set", "bug_description": "A few minor/cosmetic issues with the beeline CLI.\n1) Tool prints the column headers despite setting the --showHeader to false. This property only seems to affect the subsequent header information that gets printed based on the value of property \"headerInterval\" (default value is 100).\n2) When \"showHeader\" is true & \"headerInterval > 0\", the header after the first interval gets printed after <headerInterval - 1> rows. The code seems to count the initial header as a row, if you will.\n3) The table footer(the line that closes the table) does not get printed if the \"showHeader\" is false. I think the table should get closed irrespective of whether it prints the header or not.\n\n\n\n0: jdbc:hive2://localhost:10000> select * from stringvals;\n\n+------+\n\n| val  |\n\n+------+\n\n| t    |\n\n| f    |\n\n| T    |\n\n| F    |\n\n| 0    |\n\n| 1    |\n\n+------+\n\n6 rows selected (3.998 seconds)\n\n0: jdbc:hive2://localhost:10000> !set headerInterval 2\n\n0: jdbc:hive2://localhost:10000> select * from stringvals;\n\n+------+\n\n| val  |\n\n+------+\n\n| t    |\n\n+------+\n\n| val  |\n\n+------+\n\n| f    |\n\n| T    |\n\n+------+\n\n| val  |\n\n+------+\n\n| F    |\n\n| 0    |\n\n+------+\n\n| val  |\n\n+------+\n\n| 1    |\n\n+------+\n\n6 rows selected (0.691 seconds)\n\n0: jdbc:hive2://localhost:10000> !set showHeader false\n\n0: jdbc:hive2://localhost:10000> select * from stringvals;\n\n+------+\n\n| val  |\n\n+------+\n\n| t    |\n\n| f    |\n\n| T    |\n\n| F    |\n\n| 0    |\n\n| 1    |\n\n6 rows selected (1.728 seconds)\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.beeline.TableOutputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 7303, "bug_title": "IllegalMonitorStateException when stmtHandle is null in HiveStatement", "bug_description": "From http://www.mail-archive.com/dev@hive.apache.org/msg75617.html\nUnlock can be called even it&apos;s not locked in some situation.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.jdbc.HiveStatement.java"], "label": 1, "es_results": []}, {"bug_id": 7583, "bug_title": "Use FileSystem.access() if available to check file access for user", "bug_description": "Hive currently implements its own file access checks to determine if a user is allowed to perform an specified action on a file path (in StorageBasedAuthorizationProvider, also FileUtils). This can be prone to errors or inconsistencies with how file access is actually checked in Hadoop.\nHDFS-6570 adds a new FileSystem.access() API, so that we can perform the check using the actual HDFS logic rather than having to imitate that behavior in Hive. For versions of Hadoop that have this API available, we should use this API.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java", "org.apache.hadoop.hive.shims.Hadoop23Shims.java", "org.apache.hadoop.hive.common.FileUtils.java", "org.apache.hadoop.hive.shims.Hadoop20Shims.java", "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java", "org.apache.hadoop.hive.shims.HadoopShimsSecure.java", "org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java", "org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.java", "org.apache.hadoop.hive.shims.HadoopShims.java"], "label": 1, "es_results": []}, {"bug_id": 7399, "bug_title": "Timestamp type is not copied by ObjectInspectorUtils.copyToStandardObject", "bug_description": "Most of primitive types are non-mutable, so copyToStandardObject retuns input object as-is. But for Timestamp objects, it&apos;s used something like wrapper and inside value changed by hive. copyToStandardObject should return real copy of them.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.java", "org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java", "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.java"], "label": 1, "es_results": []}, {"bug_id": 8246, "bug_title": "HiveServer2 in http-kerberos mode is restrictive on client usernames", "bug_description": "Unable to use client usernames of the format:\n\n\n\nusername/host@REALM\n\nusername@FOREIGN_REALM\n\n\n\nThe following works fine:\n\n\n\nusername@REALM \n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.service.auth.HttpAuthUtils.java", "org.apache.hive.service.cli.thrift.ThriftHttpServlet.java"], "label": 1, "es_results": []}, {"bug_id": 8298, "bug_title": "Incorrect results for n-way join when join expressions are not in same order across joins", "bug_description": "select *  from srcpart a join srcpart b on a.key = b.key and a.hr = b.hr join srcpart c on a.hr = c.hr and a.key = c.key;\nis minimal query which reproduces it", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 8856, "bug_title": "Multiple joins must have conditionals in same order ", "bug_description": "SELECT \n   COUNT \nFROM \n   TBL_A,TBL_B,TBL_C\nWHERE \n   A.key1 = B.key1 AND A.key2 = B.key2\nAND \n   A.key2 = C.key2 AND A.Key1 = B.key1 \nWhere key1 is a string and key2 is a double. \nNote: This effects explicit joins as well\nA look at the query plan reveals the following:\nMap Join Operator\n              condition map:\n                   Inner Join 0 to 1\n                   Inner Join 0 to 2\n              condition expressions:\n                0 \n{prdct_id} {bu_cd}\n                1 {prdct_id}\n \n{bu_cd}\n                2 {prdct_id} {bu_cd}\n              keys:\n                0 UDFToDouble(prdct_id) (type: double), bu_cd (type: double)\n                1 UDFToDouble(prdct_id) (type: double), bu_cd (type: double)\n                2 bu_cd (type: double), UDFToDouble(prdct_id) (type: double)\nThe ordering of keys within a join should not dictate it&apos;s type. This is something the query optimizer should handle prior to making the plan. This way users do not have to worry about ordrering their conditionals. At the very least it should fail, instead it silently converts them to nulls and returns 0. ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 8297, "bug_title": "Wrong results with JDBC direct read of TIMESTAMP column in RCFile and ORC format", "bug_description": "For the case:\nSELECT * FROM [table]\nJDBC direct reads the table backing data, versus cranking up a MR and creating a result set.  Where table format is RCFile or ORC, incorrect results are delivered by JDBC direct read for TIMESTAMP columns.  If you force a result set, correct data is returned.\nTo reproduce using beeline:\n1) Create this file as follows in HDFS.\n$ cat > /tmp/ts.txt\n2014-09-28 00:00:00\n2014-09-29 00:00:00\n2014-09-30 00:00:00\n<ctrl-D>\n$ hadoop fs -copyFromLocal /tmp/ts.txt /tmp/ts.txt\n2) In beeline load above HDFS data to a TEXTFILE table, and verify ok:\n$ beeline\n> !connect jdbc:hive2://<host>:<port>/<db> hive pass org.apache.hive.jdbc.HiveDriver\n> drop table `TIMESTAMP_TEXT`;\n> CREATE TABLE `TIMESTAMP_TEXT` (`ts` TIMESTAMP) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\\001&apos;\nLINES TERMINATED BY &apos;\\012&apos; STORED AS TEXTFILE;\n> LOAD DATA INPATH &apos;/tmp/ts.txt&apos; OVERWRITE INTO TABLE\n`TIMESTAMP_TEXT`;\n> select * from `TIMESTAMP_TEXT`;\n3) In beeline create and load an RCFile from the TEXTFILE:\n> drop table `TIMESTAMP_RCFILE`;\n> CREATE TABLE `TIMESTAMP_RCFILE` (`ts` TIMESTAMP) stored as rcfile;\n> INSERT INTO TABLE `TIMESTAMP_RCFILE` SELECT * FROM `TIMESTAMP_TEXT`;\n4) Demonstrate incorrect direct JDBC read versus good read by inducing result set creation:\n> SELECT * FROM `TIMESTAMP_RCFILE`;\n------------------------\n\n\n  timestamp_rcfile.ts   \n\n\n------------------------\n\n\n 2014-09-30 00:00:00.0  \n\n\n 2014-09-30 00:00:00.0  \n\n\n 2014-09-30 00:00:00.0  \n\n\n------------------------\n>  SELECT * FROM `TIMESTAMP_RCFILE` where ts is not NULL;\n------------------------\n\n\n  timestamp_rcfile.ts   \n\n\n------------------------\n\n\n 2014-09-28 00:00:00.0  \n\n\n 2014-09-29 00:00:00.0  \n\n\n 2014-09-30 00:00:00.0  \n\n\n------------------------\nNote 1: The incorrect conduct demonstrated above replicates with a standalone Java/JDBC program.\nNote 2: Don&apos;t know if this is an issue with any other data types, also don&apos;t know what releases affected, however this occurs in Hive 13.  Direct JDBC read of TEXTFILE and SEQUENCEFILE work fine.  As above for RCFile and ORC wrong results are delivered, did not test any other file types.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.java", "org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java", "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.java"], "label": 1, "es_results": []}, {"bug_id": 11798, "bug_title": "The Beeline report should not display the header when --showHeader is set to false.", "bug_description": "In Beeline tool User sets the --showheader option as false.\nIn command line interface user inputs the command bin/beeline -you jdbc:hive2://10.19.92.183:10000  --showHeader=false\nActual Result : The Beeline report displays the column name.\nExpected Result : The Beeline report should not display the header when --showHeader is set to false.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.beeline.TableOutputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 12566, "bug_title": "Incorrect result returns when using COALESCE in WHERE condition with LEFT JOIN", "bug_description": "The left join query with on/where clause returns incorrect result (more rows are returned). See the reproducible sample below.\nLeft table with data:\n\n\n\nCREATE TABLE ltable (i int, la int, lk1 string, lk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;;\n\n---\n\n1,\\N,CD5415192314304,00071\n\n2,\\N,CD5415192225530,00071\n\n\n\nRight  table with data:\n\n\n\nCREATE TABLE rtable (ra int, rk1 string, rk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;;\n\n---\n\n1,CD5415192314304,00071\n\n45,CD5415192314304,00072\n\n\n\nQuery:\n\n\n\nSELECT * FROM ltable l LEFT OUTER JOIN rtable r on (l.lk1 = r.rk1 AND l.lk2 = r.rk2) WHERE COALESCE(l.la,&apos;EMPTY&apos;)=COALESCE(r.ra,&apos;EMPTY&apos;);\n\n\n\nResult returns:\n\n\n\n1\tNULL\tCD5415192314304\t00071\tNULL\tNULL\tNULL\n\n2\tNULL\tCD5415192225530\t00071\tNULL\tNULL\tNULL\n\n\n\nThe correct result should be\n\n\n\n2\tNULL\tCD5415192225530\t00071\tNULL\tNULL\tNULL\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 6712, "bug_title": "HS2 JDBC driver is inconsistent w.r.t. auto commit", "bug_description": "I see an inconsistency in HS2 JDBC driver code:\n\n\n\n  @Override\n\n  public void setAutoCommit(boolean autoCommit) throws SQLException {\n\n    if (autoCommit) {\n\n      throw new SQLException(\"enabling autocommit is not supported\");\n\n    }\n\n  }\n\n\n\nFrom above, it seems that auto commit is not supported. However, \n\n\n\n  @Override\n\n  public boolean getAutoCommit() throws SQLException {\n\n    return true;\n\n  }\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hive.jdbc.HiveConnection.java", "org.apache.hive.jdbc.TestJdbcDriver2.java"], "label": 1, "es_results": []}, {"bug_id": 7475, "bug_title": "Beeline requires newline at the end of each query in a file", "bug_description": "When using the -f option on beeline its required to have a newline at the end of each query otherwise the connection is closed before the query is run.\n\n\n\n$ cat test.hql\n\nshow databases;%\n\n$ beeline -you jdbc:hive2://localhost:10000 --incremental=true -f test.hql\n\nscan complete in 3ms\n\nConnecting to jdbc:hive2://localhost:10000\n\nConnected to: Apache Hive (version 0.13.1)\n\nDriver: Hive JDBC (version 0.13.1)\n\nTransaction isolation: TRANSACTION_REPEATABLE_READ\n\nBeeline version 0.13.1 by Apache Hive\n\n0: jdbc:hive2://localhost:10000> show databases;Closing: 0: jdbc:hive2://localhost:10000\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.1", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.beeline.BeeLine.java", "org.apache.hive.beeline.TestBeeLineWithArgs.java"], "label": 1, "es_results": []}, {"bug_id": 7390, "bug_title": "Make single quote character optional and configurable in BeeLine CSV/TSV output", "bug_description": "Currently when either the CSV or TSV output formats are used in beeline each column is wrapped in single quotes. Quote wrapping of columns should be optional and the user should be able to choose the character used to wrap the columns.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.1", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.beeline.BeeLine.java", "org.apache.hive.beeline.TestBeeLineWithArgs.java", "org.apache.hive.beeline.SeparatedValuesOutputFormat.java", "org.apache.hive.beeline.BeeLineOpts.java"], "label": 1, "es_results": []}, {"bug_id": 7341, "bug_title": "Support for Table replication across HCatalog instances", "bug_description": "The HCatClient currently doesn&apos;t provide very much support for replicating HCatTable definitions between 2 HCatalog Server (i.e. Hive metastore) instances. \nSystems similar to Apache Falcon might find the need to replicate partition data between 2 clusters, and keep the HCatalog metadata in sync between the two. This poses a couple of problems:\n\nThe definition of the source table might change (in column schema, I/O formats, record-formats, serde-parameters, etc.) The system will need a way to different 2 tables and update the target-metastore with the changes. E.g.\n\n\n\ntargetTable.resolve( sourceTable, targetTable.different(sourceTable) );\n\nhcatClient.updateTableSchema(dbName, tableName, targetTable);\n\n\n\nThe current HCatClient.addPartitions() API requires that the partition&apos;s schema be derived from the table&apos;s schema, thereby requiring that the table-schema be resolved before partitions with the new schema are added to the table. This is problematic, because it introduces race conditions when 2 partitions with differing column-schemas (e.g. right after a schema change) are copied in parallel. This can be avoided if each HCatAddPartitionDesc kept track of the partition&apos;s schema, in flight.\nThe source and target metastores might be running different/incompatible versions of Hive.\n\nThe impending patch attempts to address these concerns (with some caveats).\n\nHCatTable now has\n\t\na different() method, to compare against another HCatTable instance\na resolve(different) method to copy over specified table-attributes from another HCatTable\na serialize/deserialize mechanism (via HCatClient.serializeTable() and HCatClient.deserializeTable()), so that HCatTable instances constructed in other class-loaders may be used for comparison\n\n\nHCatPartition now provides finer-grained control over a Partition&apos;s column-schema, StorageDescriptor settings, etc. This allows partitions to be copied completely from source, with the ability to override specific properties if required (e.g. location).\nHCatClient.updateTableSchema() can now update the entire table-definition, not just the column schema.\nI&apos;ve cleaned up and removed most of the redundancy between the HCatTable, HCatCreateTableDesc and HCatCreateTableDesc.Builder. The prior API failed to separate the table-attributes from the add-table-operation&apos;s attributes. By providing fluent-interfaces in HCatTable, and composing an HCatTable instance in HCatCreateTableDesc, the interfaces are cleaner(ish). The old setters are deprecated, in favour of those in HCatTable. Likewise, HCatPartition and HCatAddPartitionDesc.\n\nI&apos;ll post a patch for trunk shortly.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.1", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hive.hcatalog.api.HCatPartition.java", "org.apache.hive.hcatalog.api.TestHCatClient.java", "org.apache.hive.hcatalog.api.HCatClientHMSImpl.java", "org.apache.hive.hcatalog.api.HCatAddPartitionDesc.java", "org.apache.hive.hcatalog.api.HCatCreateTableDesc.java", "org.apache.hive.hcatalog.api.HCatTable.java", "org.apache.hive.hcatalog.api.HCatClient.java"], "label": 1, "es_results": []}, {"bug_id": 9199, "bug_title": "Excessive exclusive lock used in some DDLs with DummyTxnManager", "bug_description": "In DummyTxnManager, the lockMode for a WriteEntity (e.g. database, table) is determined by \"complete\" instead of its writeType. But since DDL output WriteEntity is usually complete, some DDL operations might be given exclusive locks which are actually not needed, which causes unnecessary locking contention. For example, in createTable, DummyTxnManager suggests an exclusive lock to table database writeentity since it is complete.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.1", "fixed_version": "release-1.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java"], "label": 1, "es_results": []}, {"bug_id": 10541, "bug_title": "Beeline requires newline at the end of each query in a file", "bug_description": "Beeline requires newline at the end of each query in a file.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hive.beeline.BeeLine.java", "org.apache.hive.beeline.TestBeeLineWithArgs.java"], "label": 1, "es_results": []}, {"bug_id": 10709, "bug_title": "Update Avro version to 1.7.7", "bug_description": "We should update the avro version to 1.7.7 to consumer some of the nicer compatibility features.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 13093, "bug_title": "hive metastore does not exit on start failure", "bug_description": "If metastore startup fails for some reason, such as not being able to access the database, it fails to exit. Instead the process continues to be up in a bad state.\nThis is happening because of a non daemon thread.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.13.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.metastore.HiveMetaStore.java"], "label": 1, "es_results": []}, {"bug_id": 9278, "bug_title": "Cached expression feature broken in one case", "bug_description": "Different query result depending on whether hive.cache.expr.evaluation is true or false.  When true, no query results are produced (this is wrong).\nThe q file:\n\nset hive.cache.expr.evaluation=true;\n\n\n\nCREATE TABLE cache_expr_repro (date_str STRING);\n\nLOAD DATA LOCAL INPATH &apos;../../data/files/cache_expr_repro.txt&apos; INTO TABLE cache_expr_repro;\n\n\n\nSELECT MONTH(date_str) AS `mon`, CAST((MONTH(date_str) - 1) / 3 + 1 AS int) AS `quarter`,   YEAR(date_str) AS `year` FROM cache_expr_repro WHERE ((CAST((MONTH(date_str) - 1) / 3 + 1 AS int) = 1) AND (YEAR(date_str) = 2015)) GROUP BY MONTH(date_str), CAST((MONTH(date_str) - 1) / 3 + 1 AS int),   YEAR(date_str) ;\n\n\n\ncache_expr_repro.txt\n\n2015-01-01 00:00:00\n\n2015-02-01 00:00:00\n\n2015-01-01 00:00:00\n\n2015-02-01 00:00:00\n\n2015-01-01 00:00:00\n\n2015-01-01 00:00:00\n\n2015-02-01 00:00:00\n\n2015-02-01 00:00:00\n\n2015-01-01 00:00:00\n\n2015-01-01 00:00:00\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-1.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java"], "label": 1, "es_results": []}, {"bug_id": 9632, "bug_title": "inconsistent results between year(), month(), day(), and the actual values in formulas", "bug_description": "In wanting to create a date dimension value which would match our existing database environment, I figured I would be able to do as I have done in the past and use the following formula:\n(year(date)*10000)+(month(date)*100)+day(date)\nGiven the date of 2015-01-09, the above formula should result in a value of 20150109.  Instead, the resulting value is 20353515.\nSELECT\n                          > adjusted_activity_date_utc,\n                          > year(adjusted_activity_date_utc),\n                          > month(adjusted_activity_date_utc),\n                          > day(adjusted_activity_date_utc),\n                          > (year(adjusted_activity_date_utc)*10000)+(month(adjusted_activity_date_utc)*100)+day(adjusted_activity_date_utc),\n                          > (year(adjusted_activity_date_utc)*10000),\n                          > (month(adjusted_activity_date_utc)*100),\n                          > day(adjusted_activity_date_utc)\n                          > from event_histories limit 5;\nOK\nadjusted_activity_date_utc\t_c1\t_c2\t_c3\t_c4\t_c5\t_c6\t_c7\n2015-01-09\t2015\t1\t9\t20353515\t20150000\t100\t9\n2015-01-09\t2015\t1\t9\t20353515\t20150000\t100\t9\n2015-01-09\t2015\t1\t9\t20353515\t20150000\t100\t9\n2015-01-09\t2015\t1\t9\t20353515\t20150000\t100\t9\n2015-01-09\t2015\t1\t9\t20353515\t20150000\t100\t9\nOddly enough, this works as expected when a specific date value is used for the column.\nI have tried this with partition and non-partition columns and found the result to be the same.\nSELECT\n                          > adjusted_activity_date_utc,\n                          > year(adjusted_activity_date_utc),\n                          > month(adjusted_activity_date_utc),\n                          > day(adjusted_activity_date_utc),\n                          > (year(adjusted_activity_date_utc)*10000)+(month(adjusted_activity_date_utc)*100)+day(adjusted_activity_date_utc),\n                          > (year(adjusted_activity_date_utc)*10000),\n                          > (month(adjusted_activity_date_utc)*100),\n                          > day(adjusted_activity_date_utc)\n                          > from event_histories\n                          > where adjusted_activity_date_utc = &apos;2015-01-09&apos;\n                          > limit 5;\nOK\nadjusted_activity_date_utc\t_c1\t_c2\t_c3\t_c4\t_c5\t_c6\t_c7\n2015-01-09\t2015\t1\t9\t20150109\t20150000\t100\t9\n2015-01-09\t2015\t1\t9\t20150109\t20150000\t100\t9\n2015-01-09\t2015\t1\t9\t20150109\t20150000\t100\t9\n2015-01-09\t2015\t1\t9\t20150109\t20150000\t100\t9\n2015-01-09\t2015\t1\t9\t20150109\t20150000\t100\t9\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-1.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java"], "label": 1, "es_results": []}, {"bug_id": 9481, "bug_title": "allow column list specification in INSERT statement", "bug_description": "Given a table FOO(a int, b int, c int), ANSI SQL supports insert into FOO(c,b) select x,y from T.  The expectation is that &apos;x&apos; is written to column &apos;c&apos; and &apos;y&apos; is written column &apos;b&apos; and &apos;a&apos; is set to NULL, assuming column &apos;a&apos; is NULLABLE.\nHive does not support this.  In Hive one has to ensure that the data producing statement has a schema that matches target table schema.\nSince Hive doesn&apos;t support DEFAULT value for columns in CREATE TABLE, when target schema is explicitly provided, missing columns will be set to NULL if they are NULLABLE, otherwise an error will be raised.\nIf/when DEFAULT clause is supported, this can be enhanced to set default value rather than NULL.\nThus, given \n\ncreate table source (a int, b int);\n\ncreate table target (x int, y int, z int);\n\ncreate table target2 (x int, y int, z int);\n\n\n\n\ninsert into target(y,z) select * from source;\n\nwill mean \n\ninsert into target select null as x, a, b from source;\n\nand \n\ninsert into target(z,y) select * from source;\n\nwill meant \n\ninsert into target select null as x, b, a from source;\n\nAlso,\n\nfrom source \n\n  insert into target(y,z) select null as x, * \n\n  insert into target2(y,z) select null as x, source.*;\n\n\n\nand for partitioned tables, given\n\nGiven:\n\nCREATE TABLE pageviews (userid VARCHAR(64), link STRING, \"from\" STRING)\n\n  PARTITIONED BY (datestamp STRING) CLUSTERED BY (userid) INTO 256 BUCKETS STORED AS ORC;\n\n\n\nINSERT INTO TABLE pageviews PARTITION (datestamp = &apos;2014-09-23&apos;)(userid,link)  \n\n   VALUES (&apos;jsmith&apos;, &apos;mail.com&apos;);\n\n\n\nAnd dynamic partitioning\n\nINSERT INTO TABLE pageviews PARTITION (datestamp)(userid,datestamp,link) \n\n    VALUES (&apos;jsmith&apos;, &apos;2014-09-23&apos;, &apos;mail.com&apos;);\n\n\n\nIn all cases, the schema specification contains columns of the target table which are matched by position to the values produced by VALUES clause/SELECT statement.  If the producer side provides values for a dynamic partition column, the column should be in the specified schema.  Static partition values are part of the partition spec and thus are not produced by the producer and should not be part of the schema specification.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-1.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.QBParseInfo.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.QBMetaData.java", "org.apache.hadoop.hive.ql.parse.TestIUD.java"], "label": 1, "es_results": []}, {"bug_id": 9717, "bug_title": "The max/min function used by AggrStats for decimal type is not what we expected", "bug_description": "In current version hive-schema-1.2.0, in TABLE PART_COL_STATS, we store the \"BIG_DECIMAL_LOW_VALUE\" and \"BIG_DECIMAL_HIGH_VALUE\" as varchar. For example,\nderby\n\"BIG_DECIMAL_LOW_VALUE\" VARCHAR(4000), \"BIG_DECIMAL_HIGH_VALUE\" VARCHAR(4000)\nmssql\nBIG_DECIMAL_HIGH_VALUE varchar(255) NULL,\n    BIG_DECIMAL_LOW_VALUE varchar(255) NULL,\nmysql\n`BIG_DECIMAL_LOW_VALUE` varchar(4000) CHARACTER SET latin1 COLLATE latin1_bin,\n `BIG_DECIMAL_HIGH_VALUE` varchar(4000) CHARACTER SET latin1 COLLATE latin1_bin,\noracle\nBIG_DECIMAL_LOW_VALUE VARCHAR2(4000),\n BIG_DECIMAL_HIGH_VALUE VARCHAR2(4000),\npostgres\n\"BIG_DECIMAL_LOW_VALUE\" character varying(4000) DEFAULT NULL::character varying,\n \"BIG_DECIMAL_HIGH_VALUE\" character varying(4000) DEFAULT NULL::character varying,\nAnd, when we do the aggrstats, we do a MAX/MIN of all the BIG_DECIMAL_HIGH_VALU/BIG_DECIMAL_LOW_VALUEE of partitions. We are expecting a max/min of a decimal (a number). However, it is actually a max/min of a varchar (a string). As a result, &apos;900&apos; is more than &apos;1000&apos;. This also affects the extrapolation of the status. The proposed solution is to use a CAST function to cast it to decimal. ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-1.2.0", "fixed_files": ["org.apache.hadoop.hive.metastore.StatObjectConverter.java", "org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java", "org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hadoop.hive.metastore.IExtrapolatePartStatus.java", "org.apache.hadoop.hive.metastore.ObjectStore.java", "org.apache.hadoop.hive.metastore.LinearExtrapolatePartStatus.java"], "label": 1, "es_results": []}, {"bug_id": 11095, "bug_title": "SerDeUtils  another bug ,when Text is reused", "bug_description": "\nThe method transformTextFromUTF8 have a  error bug, It invoke a bad method of Text,getBytes()!\n\nThe method getBytes of Text returns the raw bytes; however, only data up to Text.length is valid.A better way is  use copyBytes()  if you need the returned array to be precisely the length of the data.\n\nBut the copyBytes is added behind hadoop1. \n\n\n\nHow I found this bug\nWhen i query data from a lzo table  I found in results  the length of the current row is always largr than the previous row and sometimesthe current row contains the contents of the previous row For example i execute a sql ,\n\n\n\nselect * from web_searchhub where logdate=2015061003\n\n\n\nthe result of sql see blow.Notice that ,the second row content contains the first row content.\n\nINFO [03:00:05.589] HttpFrontServer::FrontSH msgRecv:Remote=/10.13.193.68:42098,session=3151,thread=254 2015061003\n\nINFO [03:00:05.594] <18941e66-9962-44ad-81bc-3519f47ba274> session=901,thread=223ession=3151,thread=254 2015061003\n\n\n\nThe content of origin lzo file content see below ,just 2 rows.\n\nINFO [03:00:05.635] <b88e0473-7530-494c-82d8-e2d2ebd2666c_forweb> session=3148,thread=285\n\nINFO [03:00:05.635] HttpFrontServer::FrontSH msgRecv:Remote=/10.13.193.68:42095,session=3148,thread=285\n\n\n\nI think this error is caused by the Text reuse,and I found the solutions .\nAddicational, table create sql is : \n\n\n\nCREATE EXTERNAL TABLE `web_searchhub`(\n\n`line` string)\n\nPARTITIONED BY (\n\n`logdate` string)\n\nROW FORMAT DELIMITED\n\nFIELDS TERMINATED BY &apos;\n\nU0000&apos;\n\nWITH SERDEPROPERTIES (\n\n&apos;serialization.encoding&apos;=&apos;GBK&apos;)\n\nSTORED AS INPUTFORMAT \"com.hadoop.mapred.DeprecatedLzoTextInputFormat\"\n\nOUTPUTFORMAT \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\";\n\nLOCATION\n\n&apos;viewfs://nsX/user/hive/warehouse/raw.db/web/web_searchhub&apos; \n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.serde2.SerDeUtils.java"], "label": 1, "es_results": []}, {"bug_id": 11657, "bug_title": "HIVE-2573 introduces some issues during metastore init (and CLI init)", "bug_description": "HIVE-2573 introduced static reload functions call.\nIt has a few problems:\n1) When metastore client is initialized using an externally supplied config (i.e. Hive.get(HiveConf)), it still gets called during static init using the main service config. In my case, even though I have uris in the supplied config to connect to remote MS (which eventually happens), the static call creates objectstore, which is undesirable.\n2) It breaks compat - old metastores do not support this call so new clients will fail, and there&apos;s no workaround like not using a new feature because the static call is always made", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java", "org.apache.hadoop.hive.ql.metadata.Hive.java", "org.apache.hadoop.hive.ql.exec.FunctionTask.java"], "label": 1, "es_results": []}, {"bug_id": 12315, "bug_title": "vectorization_short_regress.q has a wrong result issue for a double calculation", "bug_description": "I suspect it is related to the fancy optimizations in vectorized double divide that try to quickly process the batch without checking each row for null.\n\n\n\n public static void setNullAndDivBy0DataEntriesDouble(\n\n      DoubleColumnVector v, boolean selectedInUse, int[] sel, int n, DoubleColumnVector denoms) {\n\n    assert v.isRepeating || !denoms.isRepeating;\n\n    v.noNulls = false;\n\n    double[] vector = denoms.vector;\n\n    if (v.isRepeating && (v.isNull[0] = (v.isNull[0] || vector[0] == 0))) {\n\n      v.vector[0] = DoubleColumnVector.NULL_VALUE;\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java"], "label": 1, "es_results": []}, {"bug_id": 12384, "bug_title": "Union Operator may produce incorrect result on TEZ", "bug_description": "Union queries may produce incorrect result on TEZ.\nTEZ removes union op, thus might loose the implicit cast in union op.\nReproduction test case:\nset hive.cbo.enable=false;\nset hive.execution.engine=tez;\nselect (x/sum over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select &apos;100000000&apos; x from (select * from src limit 2) s3)you order by y;\nselect (x/sum over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)you order by y;", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 12307, "bug_title": "Streaming API TransactionBatch.close() must abort any remaining transactions in the batch", "bug_description": "When the client of TransactionBatch API encounters an error it must close() the batch and start a new one.  This prevents attempts to continue writing to a file that may damaged in some way.\nThe close() should ensure to abort the any txns that still remain in the batch and close (best effort) all the files it&apos;s writing to.  The batch should also put itself into a mode where any future ops on this batch fail.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java", "org.apache.hive.hcatalog.streaming.HiveEndPoint.java", "org.apache.hive.hcatalog.streaming.ConnectionError.java", "org.apache.hive.hcatalog.streaming.TransactionBatch.java", "org.apache.hive.hcatalog.streaming.StrictJsonWriter.java", "org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java", "org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java", "org.apache.hive.hcatalog.streaming.TransactionError.java", "org.apache.hive.hcatalog.streaming.TestStreaming.java"], "label": 1, "es_results": []}, {"bug_id": 12717, "bug_title": "Enabled to accept quoting of all character backslash qooting mechanism to json_tuple UDTF", "bug_description": "Similar to HIVE-11825, we need to enable ALLOW_BACKSLASH_ESCAPING_ANY_CHARACTER property in json_tuple UDTF\nFor example in HIVE-11825, there are null return in below statement\n(https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-json_tuple)\n\n\n\nSELECT a.timestamp, b.*\n\n  FROM log a LATERAL VIEW json_tuple(a.appevent, &apos;eventid&apos;, &apos;eventname&apos;) b as f1, f2;\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java"], "label": 1, "es_results": []}, {"bug_id": 12660, "bug_title": "HS2 memory leak with .hiverc file use", "bug_description": "The Operation objects created to process .hiverc file in HS2 are not closed.\nIn HiveSessionImpl, GlobalHivercFileProcessor calls executeStatementInternal but ignores the OperationHandle it returns.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hive.service.cli.session.HiveSessionImpl.java", "org.apache.hive.service.cli.session.TestSessionGlobalInitFile.java"], "label": 1, "es_results": []}, {"bug_id": 13285, "bug_title": "Orc concatenation may drop old files from moving to final path", "bug_description": "ORC concatenation uses combine hive input format for merging files. Under specific case where all files within a combine split are incompatible for merge (old files without stripe statistics) then these files are added to incompatible file set. But this file set is not processed as closeOp() will not be called (no output file writer will exist which will skip super.closeOp()). As a result, these incompatible files are not moved to final path.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java", "org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java"], "label": 1, "es_results": []}, {"bug_id": 10022, "bug_title": "Authorization checks for non existent file/directory should not be recursive", "bug_description": "I am testing a query like : \nset hive.test.authz.sstd.hs2.mode=true;\nset hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;\nset hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;\nset hive.security.authorization.enabled=true;\nset user.name=user1;\ncreate table auth_noupd(i int) clustered by  into 2 buckets stored as orc location &apos;$\n{OUTPUT}\n&apos; TBLPROPERTIES (&apos;transactional&apos;=&apos;true&apos;);\nNow, in the above query,  since authorization is true, \nwe would end up calling doAuthorizationV2() which ultimately ends up calling SQLAuthorizationUtils.getPrivilegesFromFS() which calls a recursive method : FileUtils.isActionPermittedForFileHierarchy() with the object or the ancestor of the object we are trying to authorize if the object does not exist. \nThe logic in FileUtils.isActionPermittedForFileHierarchy() is DFS.\nNow assume, we have a path as a/b/c/d that we are trying to authorize.\nIn case, a/b/c/d does not exist, we would call FileUtils.isActionPermittedForFileHierarchy() with say a/b/ assuming a/b/c also does not exist.\nIf under the subtree at a/b, we have millions of files, then FileUtils.isActionPermittedForFileHierarchy()  is going to check file permission on each of those objects. \nI do not completely understand why do we have to check for file permissions in all the objects in  branch of the tree that we are not  trying to read from /write to.  \nWe could have checked file permission on the ancestor that exists and if it matches what we expect, the return true.\nPlease confirm if this is a bug so that I can submit a patch else let me know what I am missing ?", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java", "org.apache.hadoop.hive.common.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 12954, "bug_title": "NPE with str_to_map on null strings", "bug_description": "Running str_to_map on a null string will return a NullPointerException.\nWorkaround is to use coalesce.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.java"], "label": 1, "es_results": []}, {"bug_id": 14778, "bug_title": "document threading model of Streaming API", "bug_description": "The model is not obvious and needs to be documented properly.\nA StreamingConnection internally maintains 2 MetaStoreClient objects (each has 1 Thrift client for actual RPC). Let&apos;s call them \"primary\" and \"heartbeat\". Each TransactionBatch created from a given StreamingConnection, gets a reference to both of these MetaStoreClients. \nSo the model is that there is at most 1 outstanding (not closed) TransactionBatch for any given StreamingConnection and for any given TransactionBatch there can be at most 2 threads accessing it concurrently. 1 thread calling TransactionBatch.heartbeat() (and nothing else) and the other calling all other methods.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.14.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.hcatalog.streaming.StreamingConnection.java", "org.apache.hive.hcatalog.streaming.TransactionBatch.java"], "label": 1, "es_results": []}, {"bug_id": 264, "bug_title": "TBinarySortable Protocol should support null characters", "bug_description": "Currently TBinarySortable Protocol does not support serializing null \"\\0\" characters which confused a lot of users.\nWe should support that.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.3.0", "fixed_files": ["org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java", "org.apache.hadoop.hive.ql.QTestUtil.java"], "label": 1, "es_results": []}, {"bug_id": 251, "bug_title": "Failures in Transform do not stop the job", "bug_description": "If the program executed via a SELECT TRANSFORM() USING &apos;foo&apos; exits with a non-zero exit status, Hive proceeds as if nothing bad happened.  The main way that the user knows something bad has happened is if the user checks the logs (probably because he got no output).  This is doubly bad if the program only fails part of the time (say, on certain inputs) since the job will still produce output and thus the problem will likely go undetected.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.3.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FileSinkOperator.java", "org.apache.hadoop.hive.ql.exec.Operator.java"], "label": 1, "es_results": []}, {"bug_id": 342, "bug_title": "TestMTQueries is broken", "bug_description": "It has been broken for quite sometime but the build is not failing.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.3.0", "fixed_files": ["org.apache.hadoop.hive.ql.QTestUtil.java"], "label": 1, "es_results": []}, {"bug_id": 391, "bug_title": "udafcount merge does not handle nulls", "bug_description": "udafcount merge does not handle nulls\nIf the mapper does not emit any row on null input, i.e both count and count distinct are present, and the aggregation function is count, \nit will get a null pointer\nselect count(1), count(distinct x.value) from src x where x.key = 9999;", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.UDAFCount.java"], "label": 1, "es_results": []}, {"bug_id": 381, "bug_title": "[JDBC component] HiveResultSet next() always returned true due to bad string comparison", "bug_description": "Method next() is comparing String using \"!=\" operator resulted in \"true\" being returned all the time.  Can be fix by using String equals() operation to check. ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.jdbc.HiveResultSet.java", "org.apache.hadoop.hive.jdbc.TestJdbcDriver.java"], "label": 1, "es_results": []}, {"bug_id": 442, "bug_title": "Partition is created before data is moved thus creating a window where data is incomplete", "bug_description": "During the said window, processes waiting for the partition to be created can run queries on partial data thus causing untold misery.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.metadata.Table.java", "org.apache.hadoop.hive.ql.metadata.Partition.java", "org.apache.hadoop.hive.metastore.HiveMetaStore.java", "org.apache.hadoop.hive.metastore.Warehouse.java", "org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 534, "bug_title": "cli adds a new line at the beginning of every query", "bug_description": "this results in error messages always specify a line which is one more than the actual error.\nhive> select count* from abc; \nFAILED: Parse Error: line 2:14 cannot recognize input &apos;from&apos; in expression specification\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.cli.CliDriver.java"], "label": 1, "es_results": []}, {"bug_id": 443, "bug_title": "Remove deprecated functions from Hive.java", "bug_description": "remove deprecated createTable and dropTable functions", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.TestExecDriver.java", "org.apache.hadoop.hive.ql.history.TestHiveHistory.java", "org.apache.hadoop.hive.ql.QTestUtil.java", "org.apache.hadoop.hive.ql.exec.MoveTask.java", "org.apache.hadoop.hive.ql.metadata.Hive.java", "org.apache.hadoop.hive.metastore.MetaStoreUtils.java"], "label": 1, "es_results": []}, {"bug_id": 557, "bug_title": "Exception in FileSinkOperator's close should NOT be ignored", "bug_description": "FileSinkOperator currently ignores all IOExceptions from close() and commit(). We should not ignore them, or the output file can be incomplete or missing.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ExecMapper.java", "org.apache.hadoop.hive.ql.exec.FileSinkOperator.java", "org.apache.hadoop.hive.ql.exec.ExecReducer.java"], "label": 1, "es_results": []}, {"bug_id": 587, "bug_title": "Duplicate result from multiple TIPs of the same task", "bug_description": "On our cluster we found a job committed with duplicate output from different TIPs of the same Task (from FileSinkOperator).\nThe reason is that FileSinkOperator.commit can be called at multiple TIPs of the same task.\nFileSinkOperator.jobClose() (which is called at the Hive Client side) should do either:\nA. Get all successful TIPs and only move the output files of those TIPs to the output directory\nB. Ignore TIPs from the JobInProgress, but only move one file out of potentially several output files \nB is preferred because A might be slow (if the job finished and immediately got moved out of the JobTracker memory). Since we control the file name by ourselves, we know exactly what the file names are.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.Utilities.java", "org.apache.hadoop.hive.ql.exec.ScriptOperator.java", "org.apache.hadoop.hive.ql.exec.FileSinkOperator.java"], "label": 1, "es_results": []}, {"bug_id": 612, "bug_title": "Problem in removing temp files in FileSinkOperator.jobClose", "bug_description": "We are doing double delete for files with _tmp prefix.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.Utilities.java"], "label": 1, "es_results": []}, {"bug_id": 635, "bug_title": "UnionOperator fails when different inputs have different ObjectInspector (but the same TypeInfo)", "bug_description": "The current UnionOperator code assumes the ObjectInspectors from all parents are the same.\nBut in reality, they can be different, and UnionOperator needs to do conversion if necessary.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.UnionOperator.java"], "label": 1, "es_results": []}, {"bug_id": 592, "bug_title": "renaming internal table should rename HDFS and also change path of the table and partitions accordingly.", "bug_description": "rename table changes just the name of the table in metastore but not hdfs. so if a table with old name is created, it uses the hdfs directory pointing to the renamed table.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.metastore.TestHiveMetaStore.java", "org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java", "org.apache.hadoop.hive.metastore.HiveMetaStore.java", "org.apache.hadoop.hive.metastore.Warehouse.java", "org.apache.hadoop.hive.metastore.ObjectStore.java"], "label": 1, "es_results": []}, {"bug_id": 454, "bug_title": "Support escaping of ; in strings in cli", "bug_description": "If ; appears in string literals in a query the hive cli is not able to escape them properly.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.cli.CliDriver.java"], "label": 1, "es_results": []}, {"bug_id": 816, "bug_title": "MetastoreClient not being cached", "bug_description": "In org.apache.hadoop.hive.ql.metadata.Hive.getMSC(), we create a new MetaStoreClient on every call because the result is not getting properly cached in the threadLocal:\n\n\n\n  private IMetaStoreClient getMSC() throws MetaException {\n\n    IMetaStoreClient msc = threadLocalMSC.get();\n\n    if(msc == null) {\n\n      msc = this.createMetaStoreClient();\n\n      // THERE SHOULD BE A threadLocalMSC.set here!\n\n    }\n\n    return msc;\n\n  }\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.4.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 823, "bug_title": "Make table alise in MAPJOIN hint case insensitive", "bug_description": "If we use table alias in upper case for MAPJOIN hint, it is ignored. It must be specified in lower case.\nExample query:\nSELECT /*+ MAPJOIN(N) */ parse_url(ADATA.url,&apos;HOST&apos;) AS domain, N.type AS type\nFROM nikeusers N join adserves ADATA on (ADATA.user_id = N.uid)\nWHERE ADATA.data_date = &apos;20090901&apos;\nThis query features reducers in its execution. Attached is output of explain extended.\nAfter changing query to:\nSELECT /*+ MAPJOIN */ parse_url(adata.url,&apos;HOST&apos;) AS domain, n.type AS type\nFROM nikeusers n join adserves adata on (adata.user_id = N.uid)\nWHERE adata.data_date = &apos;20090901&apos;\nIt executes as expected. Attached is output of explain extended.\nThanks to Zheng for helping and catching this.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.4.0", "fixed_version": "release-0.5.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 876, "bug_title": "UDFOPNegative should deal with NULL gracefully", "bug_description": "UDFOPNegative is throwing out NullPointerException. It should return NULL for that.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.4.1", "fixed_version": "release-0.5.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.UDFOPNegative.java"], "label": 1, "es_results": []}, {"bug_id": 878, "bug_title": "Update the hash table entry before flushing in Group By hash aggregation", "bug_description": "This is a newly introduced bug from r796133.\nWe should first update the aggregation, and then we can flush the hash table. Otherwise the entry that we update might be already out of the hash table.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.4.0", "fixed_version": "release-0.4.1", "fixed_files": ["org.apache.hadoop.hive.ql.exec.GroupByOperator.java"], "label": 1, "es_results": []}, {"bug_id": 893, "bug_title": "Thrift serde does not work with the new version of thrift", "bug_description": "The new version of thrift rename the __isset to __isset_bit_vector in the generated Thrift java code. This causes __isset_bit_vector passed as a field in ThriftSerDe. ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.4.0", "fixed_version": "release-0.5.0", "fixed_files": ["org.apache.hadoop.hive.serde2.objectinspector.ThriftStructObjectInspector.java"], "label": 1, "es_results": []}, {"bug_id": 940, "bug_title": "restrict creation of partitions with empty partition keys", "bug_description": "create table pc (a int) partitioned by (b string, c string);\nalter table pc add partition (b=\"f\", c=&apos;&apos;);\nabove alter cmd fails but actually creates a partition with name &apos;b=f/c=&apos; but describe partition on the same name fails. creation of such partitions should not be allowed.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.5.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 595, "bug_title": "copyFiles does not report errors in file rename operations", "bug_description": "ql/../Hive.java:copyFiles() does not catch failures reported by fs.rename. this may cause load commands to look successful when they actually failed", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.5.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 458, "bug_title": "Setting fs.default.name incorrectly leads to meaningless error message", "bug_description": "In my hadoop-site.xml I accidentally set fs.default.name to \nhttp://wilbur21.labs.corp.sp1.yahoo.com:8020 \ninstead of the proper:\nhdfs://wilbur21.labs.corp.sp1.yahoo.com:8020\nThe result was\n\n\n\nhive> show tables;\n\nFAILED: Unknown exception : null\n\nFAILED: Unknown exception : null\n\nTime taken: 0.035 seconds\n\nhive>\n\n\n\nIt should give a meaningful error message.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.5.0", "fixed_files": ["org.apache.hadoop.hive.ql.Context.java"], "label": 1, "es_results": []}, {"bug_id": 1046, "bug_title": "Pass build.dir.hive and other properties to subant", "bug_description": "Currently we are not passing properties like \"build.dir.hive\" etc to subant.\nWe should do that, otherwise setting \"build.dir.hive\" is not useful.\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.4.1", "fixed_version": "release-0.5.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.MapRedTask.java", "org.apache.hadoop.hive.ql.QTestUtil.java"], "label": 1, "es_results": []}, {"bug_id": 1072, "bug_title": "\"show table extended like table partition(xxx) \" will show the result of the whole table if the partition does not exist", "bug_description": "See the following example, we should output an error for the second command.\n\n\n\nhive> show table extended like member_count;\n\nOK\n\ntableName:member_count\n\nowner:null\n\nlocation:/user/hive/member_count\n\ninputformat:org.apache.hadoop.mapred.SequenceFileInputFormat\n\noutputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n\ncolumns:struct columns { string count}\n\npartitioned:true\n\npartitionColumns:struct partition_columns { string ds}\n\ntotalNumberFiles:233933\n\ntotalFileSize:32802665\n\nmaxFileSize:257\n\nminFileSize:140\n\nlastAccessTime:1264017438860\n\nlastUpdateTime:1263949909703\n\n\n\nTime taken: 125.104 seconds\n\n\n\nhive> show table extended like member_count partition(ds = &apos;2009-10-11&apos;);\n\nOK\n\ntableName:member_count\n\nowner:null\n\nlocation:/user/hive/member_count\n\ninputformat:org.apache.hadoop.mapred.SequenceFileInputFormat\n\noutputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n\ncolumns:struct columns { string count}\n\npartitioned:true\n\npartitionColumns:struct partition_columns { string ds}\n\ntotalNumberFiles:233933\n\ntotalFileSize:32802665\n\nmaxFileSize:257\n\nminFileSize:140\n\nlastAccessTime:1264017438860\n\nlastUpdateTime:1263949909703\n\n\n\nTime taken: 24.618 seconds\n\n\n\nhive> show table extended like member_count partition(ds = &apos;2009-12-11&apos;);\n\nOK\n\ntableName:member_count\n\nowner:null\n\nlocation:/user/hive/member_count\n\ninputformat:org.apache.hadoop.mapred.SequenceFileInputFormat\n\noutputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n\ncolumns:struct columns { string count}\n\npartitioned:true\n\npartitionColumns:struct partition_columns { string ds}\n\ntotalNumberFiles:3495\n\ntotalFileSize:489417\n\nmaxFileSize:257\n\nminFileSize:140\n\nlastAccessTime:1262676533852\n\nlastUpdateTime:1263949909703\n\n\n\nTime taken: 0.549 seconds\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.4.1", "fixed_version": "release-0.5.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.DDLTask.java", "org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 763, "bug_title": "getSchema returns invalid column names, getThriftSchema does not return old style string schemas", "bug_description": "SELECT AVERAGE(total) as average,STDDEV(total) as stddevr FROM (SELECT COUNT(phrase) as total FROM TABLE GROUP BY phrase) t2\ngetSchema and getThriftSchema both return\ncol0: double\ncol1 : double\nexpected results\naverage : double\nstddevr : double\ncol0 & col1 are useless column names.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.4.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.ql.Driver.java", "org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.jdbc.TestJdbcDriver.java"], "label": 1, "es_results": []}, {"bug_id": 1129, "bug_title": "Fix Assertion in ExecDriver.execute when assertions are enabled in HADOOP_OPTS", "bug_description": "I noticed that when running hive CLI, assertions are not enabled, which was causing me some confusion when debugging an issue.\nSo, I added the following to my environment:\nexport HADOOP_OPTS=\"-ea -esa\"\nThis worked, and allowed me to see assertion failures when executing via CLI.\nBut then I tried to run a test, and got an assertion failure from the following code in ExecDriver.execute:\n    // Turn on speculative execution for reducers\n    HiveConf.setVar(job, HiveConf.ConfVars.HADOOPSPECULATIVEEXECREDUCERS,\n        HiveConf.getVar(job, HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS));\nThe assertion says it should be using getBoolVar/setBoolVar instead.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ExecDriver.java"], "label": 1, "es_results": []}, {"bug_id": 1195, "bug_title": "Increase ObjectInspector[] length on demand", "bug_description": "\n\n\nOperator.java\n\n  protected transient ObjectInspector[] inputObjInspectors = new ObjectInspector[Short.MAX_VALUE];\n\n\n\nAn array of 32K elements takes 256KB memory under 64-bit Java.\nWe are seeing hive client going out of memory because of that.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.Operator.java"], "label": 1, "es_results": []}, {"bug_id": 1207, "bug_title": "ScriptOperator AutoProgressor does not set the interval", "bug_description": "As title. I will show more details in the patch.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.AutoProgressor.java"], "label": 1, "es_results": []}, {"bug_id": 1242, "bug_title": "CombineHiveInputFormat does not work for compressed text files", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 1281, "bug_title": "Bucketing column names in create table should be case-insensitive", "bug_description": "This create table fails because &apos;userId&apos; != &apos;userid&apos;\n\n\n\nCREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS;\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 1116, "bug_title": "bug with alter table rename when table has property EXTERNAL=FALSE", "bug_description": "if the location is not an external location - this would be safer.\nthe problem right now is that it&apos;s tricky to use the drop and rename way of writing new data into a table. consider:\nInitialization block:\ndrop table a_tmp\ncreate table a_tmp like a;\nLoading block:\nload data <newdata> into a_tmp;\ndrop table a;\nalter table a_tmp rename to a;\nthis looks safe. but it&apos;s not. if one runs this multiple times - then data is lost (since &apos;a&apos; is pointing to &apos;a_tmp&apos;&apos;s location after any iteration. and dropping table &apos;a&apos; blows away loaded data in the next iteration). \nif the location is being managed by Hive - then &apos;rename&apos; should switch location as well.\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.metastore.HiveAlterHandler.java"], "label": 1, "es_results": []}, {"bug_id": 1345, "bug_title": "TypedBytesSerDe fails to create table with multiple columns.", "bug_description": "Creating a table with more than one columns fails when the row format SerDe is TypedBytesSerDe. \n\n\n\nhive> CREATE TABLE test (a STRING, b STRING) ROW FORMAT SERDE &apos;org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe&apos;;      \n\nFound class for org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe                                                       \n\nFAILED: Error in metadata: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1                                           \n\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask                                          \n\nhive> \n\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.java"], "label": 1, "es_results": []}, {"bug_id": 1350, "bug_title": "hive.query.id is not unique ", "bug_description": "if commands are executed by the same user within a second", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.ql.QueryPlan.java"], "label": 1, "es_results": []}, {"bug_id": 543, "bug_title": "provide option to run hive in local mode", "bug_description": "this is a little bit more than just mapred.job.tracker=local\nwhen run in this mode - multiple jobs are an issue since writing to same tmp directories is an issue. the following options:\nhadoop.tmp.dir\nmapred.local.dir\nneed to be randomized (perhaps based on queryid). ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.common.FileUtils.java", "org.apache.hadoop.hive.ql.Driver.java", "org.apache.hadoop.hive.ql.exec.Utilities.java", "org.apache.hadoop.hive.ql.exec.MapRedTask.java", "org.apache.hadoop.hive.ql.session.SessionState.java", "org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hadoop.hive.ql.Context.java", "org.apache.hadoop.hive.cli.CliDriver.java", "org.apache.hadoop.hive.ql.exec.ExecDriver.java"], "label": 1, "es_results": []}, {"bug_id": 1418, "bug_title": "column pruning not working with lateral view", "bug_description": "select myCol from tmp_pyang_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 1271, "bug_title": "Case sensitiveness of type information specified when using custom reducer causes type mismatch", "bug_description": "Type information specified  while using a custom reduce script is converted to lower case, and causes type mismatch during query semantic analysis .  The following REDUCE query where field name =  \"userId\" failed.\nhive> CREATE TABLE SS (\n   >                     a INT,\n   >                     b INT,\n   >                     vals ARRAY<STRUCT<userId:INT, y:STRING>>\n   >                 );\nOK\nhive> FROM (select * from srcTable DISTRIBUTE BY id SORT BY id) s\n   >     INSERT OVERWRITE TABLE SS\n   >     REDUCE *\n   >         USING &apos;myreduce.py&apos;\n   >         AS\n   >                     (a INT,\n   >                     b INT,\n   >                     vals ARRAY<STRUCT<userId:INT, y:STRING>>\n   >                     )\n   >         ;\nFAILED: Error in semantic analysis: line 2:27 Cannot insert into\ntarget table because column number/types are different SS: Cannot\nconvert column 2 from array<struct<userId:int,y:string>> to\narray<struct<userid:int,y:string>>.\nThe same query worked fine after changing \"userId\" to \"userid\".", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.java", "org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.java", "org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.java", "org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.java", "org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.java"], "label": 1, "es_results": []}, {"bug_id": 1385, "bug_title": "UDF field() does not work", "bug_description": "I tried it against one of my table:\nhive> desc r;\nOK\nkey int\nvalue string\na string\nhive> select * from r;\nOK\n4 val_356 NULL\n4 val_356 NULL\n484 val_169 NULL\n484 val_169 NULL\n2000 val_169 NULL\n2000 val_169 NULL\n3000 val_169 NULL\n3000 val_169 NULL\n4000 val_125 NULL\n4000 val_125 NULL\nhive> select *, field(value, &apos;val_169&apos;) from r; \nOK\n4 val_356 NULL 0\n4 val_356 NULL 0\n484 val_169 NULL 0\n484 val_169 NULL 0\n2000 val_169 NULL 0\n2000 val_169 NULL 0\n3000 val_169 NULL 0\n3000 val_169 NULL 0\n4000 val_125 NULL 0\n4000 val_125 NULL 0", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.generic.GenericUDFField.java"], "label": 1, "es_results": []}, {"bug_id": 1056, "bug_title": "Predicate push down does not work with UDTF's", "bug_description": "Predicate push down does not work with UDTF&apos;s in lateral views\n\n\n\n\n\nhive> SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS k WHERE k=1;\n\nFAILED: Unknown exception: null\n\nhive>\n\n\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java", "org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.ppd.OpProcFactory.java", "org.apache.hadoop.hive.ql.plan.api.OperatorType.java", "org.apache.hadoop.hive.ql.exec.OperatorFactory.java", "org.apache.hadoop.hive.ql.exec.UDTFOperator.java"], "label": 1, "es_results": []}, {"bug_id": 1471, "bug_title": "CTAS should unescape the column name in the select-clause. ", "bug_description": "The following query \n{{\n{\n\ncreate table T as select `to` from S;\n\n}\n}}\nfailed since `to` should be unescaped before creating the table. ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 1509, "bug_title": "Monitor the working set of the number of files ", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.MapOperator.java", "org.apache.hadoop.hive.ql.exec.ExecDriver.java", "org.apache.hadoop.hive.ql.exec.Operator.java", "org.apache.hadoop.hive.ql.exec.FileSinkOperator.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 1428, "bug_title": "ALTER TABLE ADD PARTITION fails with a remote Thrift metastore", "bug_description": "If the hive cli is configured to use a remote metastore, ALTER TABLE ... ADD PARTITION commands will fail with an error similar to the following:\n[pradeepk@chargesize:~/dev/howl]hive --auxpath ult-serde.jar -e \"ALTER TABLE mytable add partition(datestamp = &apos;20091101&apos;, srcid = &apos;10&apos;,action) location &apos;/user/pradeepk/mytable/20091101/10&apos;;\"\n10/06/16 17:08:59 WARN conf.Configuration: DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively\nHive history file=/tmp/pradeepk/hive_job_log_pradeepk_201006161709_1934304805.txt\nFAILED: Error in metadata: org.apache.thrift.TApplicationException: get_partition failed: unknown result\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask\n[pradeepk@chargesize:~/dev/howl]\nThis is due to a check that tries to retrieve the partition to see if it exists. If it does not, an attempt is made to pass a null value from the metastore. Since thrift does not support null return values, an exception is thrown.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.6.0", "fixed_files": ["org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hadoop.hive.metastore.HiveMetaStore.java", "org.apache.hadoop.hive.metastore.IMetaStoreClient.java", "org.apache.hadoop.hive.metastore.TestHiveMetaStore.java", "org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java", "org.apache.hadoop.hive.metastore.ObjectStore.java", "org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java", "org.apache.hadoop.hive.ql.metadata.Hive.java", "org.apache.hadoop.hive.metastore.RawStore.java"], "label": 1, "es_results": []}, {"bug_id": 741, "bug_title": "NULL is not handled correctly in join", "bug_description": "With the following data in table input4_cb:\nKey        Value\n------       --------\nNULL     325\n18          NULL\nThe following query:\n\n\n\nselect * from input4_cb a join input4_cb b on a.key = b.value;\n\n\n\nreturns the following result:\nNULL    325    18   NULL\nThe correct result should be empty set.\nWhen &apos;null&apos; is replaced by &apos;&apos; it works.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.MapJoinOperator.java", "org.apache.hadoop.hive.serde2.SerDeUtils.java", "org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java", "org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java", "org.apache.hadoop.hive.ql.exec.JoinOperator.java"], "label": 1, "es_results": []}, {"bug_id": 1600, "bug_title": "need to sort hook input/output lists for test result determinism", "bug_description": "Begin forwarded message:\nFrom: Ning Zhang <nzhang@facebook.com>\nDate: August 26, 2010 2:47:26 PM PDT\nTo: John Sichi <jsichi@facebook.com>\nCc: \"hive-dev@hadoop.apache.org\" <hive-dev@hadoop.apache.org>\nSubject: Re: failure in load_dyn_part1.q\nYes I saw this error before but if it does not repro. So it&apos;s probably an ordering issue in POSTHOOK. \nOn Aug 26, 2010, at 2:39 PM, John Sichi wrote:\nI&apos;m seeing this failure due to a result different when running tests on latest trunk:\nPOSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12\nPOSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=11\nPOSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=12\n-POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=11\n-POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=12\nPOSTHOOK: Output: default@nzhang_part1@ds=2008-04-08/hr=11\nPOSTHOOK: Output: default@nzhang_part1@ds=2008-04-08/hr=12\n+POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=11\n+POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=12\nDid something change recently?  Or are we missing a Java-level sort on the input/output list for determinism?\nJVS\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.ql.hooks.PreExecutePrinter.java", "org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.java"], "label": 1, "es_results": []}, {"bug_id": 1628, "bug_title": "Fix Base64TextInputFormat to be compatible with commons codec 1.4", "bug_description": "Commons-codec 1.4 made an incompatible change to the Base64 class that made line-wrapping default (boo!). This breaks the Base64TextInputFormat in contrib. This patch adds some simple reflection to use the new constructor that uses the old behavior.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.java", "org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 307, "bug_title": "\"LOAD DATA LOCAL INPATH\" fails when the table already contains a file of the same name", "bug_description": "Failed with exception checkPaths: /user/zshao/warehouse/tmp_user_msg_history/test_user_msg_history already exists\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 1771, "bug_title": "ROUND(infinity) chokes", "bug_description": "Since 1-arg ROUND returns an integer, it&apos;s hard to fix this without either losing data (return NULL) or making a backwards-incompatible change (return DOUBLE instead of BIGINT).\nIn any case, we should definitely fix 2-arg ROUND to preserve infinity/NaN/etc, since it is already returning double.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.UDFRound.java"], "label": 1, "es_results": []}, {"bug_id": 1857, "bug_title": "mixed case tablename on lefthand side of LATERAL VIEW results in query failing with confusing error message", "bug_description": "For the modified query below in lateral_view.q, the exception \"org.apache.hadoop.hive.ql.parse.SemanticException: line 3:7 Invalid Table Alias or Column Reference myCol\" is thrown.  The query should succeed.\nSELECT myCol from tmp_PYANG_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.QBParseInfo.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 1897, "bug_title": "Alter command execution \"when HDFS is down\" results in holding stale data in MetaStore ", "bug_description": "Let Us  consider, the  \"DFS\" is down , \nAnd on executing an alter query say  \"alter table firsttable rename to secondtable\".  \nthe query execution fails with the following exception:\n \nInvalidOperationException(message:Unable to access old location hdfs://localhost:9000/user/hive/warehouse/firsttable for table default.firsttable)\nNow after starting the DFS and then executing the same query , the client gets the following exception:\n\nNoSuchObjectException(message:default.firsttable table not found)\nRoot Because\nIn Alter Query execution flow, first \"MetaStore\" operation is executed successfully and then \"DFS\" operation is started. In this scenario, \"DFS\" is down. As a result, execution of the query failed and partial information of the operation is saved.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.metastore.HiveAlterHandler.java"], "label": 1, "es_results": []}, {"bug_id": 1908, "bug_title": "FileHandler leak on partial iteration of the resultset. ", "bug_description": "If the \"resultset\" is not iterated completely ,  one filehandler is leaking\nEx: We need only first row. This case one resource is leaking\n\n\n\n\n\nResultSet resultSet = createStatement.executeQuery(\"select * from sampletable\");\n\n\n\nif (resultSet.next())\n\n{\n\n\tSystem.out.println(resultSet.getString(1)+\"   \"+resultSet.getString(2));\n\n} \n\n\n\n\n\nCommand used for checking the filehandlers\n\n\n\nlsof -p {hive_process_id} > runjarlsof.txt\n\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.ql.Driver.java", "org.apache.hadoop.hive.ql.exec.FetchTask.java"], "label": 1, "es_results": []}, {"bug_id": 1465, "bug_title": "hive-site.xml ${user.name} not replaced for local-file derby metastore connection URL", "bug_description": "Seems that for this parameter\n\n\n\n<property>\n\n<name>javax.jdo.option.ConnectionURL</name>\n\n<value>jdbc:derby:;databaseName=/var/lib/hive/metastore/${user.name}_db;create=true</value>\n\n<description>JDBC connect string for a JDBC metastore</description>\n\n</property>\n\n\n\n${user.name} is never replaced by the actual user name:\n\n\n\n$ ls -la /var/lib/hive/metastore/\n\ntotal 24\n\ndrwxrwxrwt 3 root root 4096 Apr 30 12:37 .\n\ndrwxr-xr-x 3 root root 4096 Apr 30 12:25 ..\n\ndrwxrwxr-x 5 hadoop hadoop 4096 Apr 30 12:37 ${user.name}_db\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.7.0", "fixed_files": ["org.apache.hadoop.hive.metastore.ObjectStore.java"], "label": 1, "es_results": []}, {"bug_id": 1974, "bug_title": "In error scenario some opened streams may not closed in ScriptOperator.java, Utilities.java ", "bug_description": "1)In error scenario StreamProcessor may not be closed in ScriptOperator.java\n2)In error scenario XMLEncoder may not be closed in Utilities.java", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.Utilities.java", "org.apache.hadoop.hive.ql.exec.ScriptOperator.java"], "label": 1, "es_results": []}, {"bug_id": 1973, "bug_title": "Getting error when join on tables where name of table has uppercase letters", "bug_description": "When execute a join query on tables containing Uppercase letters in the table names hit an exception\n Ex:\n\n  create table a(b int);\n\n  create table tabForJoin(b int,c int);\n\n\n\n  select * from a join tabForJoin on(a.b=tabForJoin.b);\n\n\n\n  Got an exception like this\n\n  FAILED: Error in semantic analysis:  Invalid Table Alias tabForJoin\n\n\n\nBut if i give without capital letters ,It is working", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 1976, "bug_title": "Exception should be thrown when invalid jar,file,archive is given to add command", "bug_description": "When executed add command with non existing jar it should throw exception through   HiveStatement\nEx:\n\n  add jar /root/invalidpath/testjar.jar\n\n\n\nHere testjar.jar is not exist so it should throw exception.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.session.SessionState.java", "org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java", "org.apache.hadoop.hive.service.TestHiveServer.java"], "label": 1, "es_results": []}, {"bug_id": 2198, "bug_title": "While using Hive in server mode, HiveConnection.close() is not cleaning up server side resources", "bug_description": "org.apache.hadoop.hive.service.ThriftHive.Client.clean() method is called for every session end in CLI mode for the cleanup but in HiveServer mode this is not called.\nSo this can be integrate with the HiveConnection.close()", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.jdbc.HiveConnection.java", "org.apache.hadoop.hive.service.TestHiveServer.java"], "label": 1, "es_results": []}, {"bug_id": 2184, "bug_title": "Few improvements in org.apache.hadoop.hive.ql.metadata.Hive.close()", "bug_description": "1)Hive.close() will call HiveMetaStoreClient.close() in this method the variable \"standAloneClient\" is never become true then client.shutdown() never call.\n2)Hive.close() After calling metaStoreClient.close() need to make metaStoreClient=null", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java", "org.apache.hadoop.hive.metastore.HiveMetaStore.java", "org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 1975, "bug_title": "\"insert overwrite directory\" Not able to insert data with multi level directory path", "bug_description": "Below query execution is failed\nEx:\n\n   insert overwrite directory &apos;/HIVEFT25686/chinna/&apos; select * from dept_j;\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.MoveTask.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 11, "bug_title": "better error code from Hive describe command", "bug_description": "cryptic, non-informative error message\nhive> describe hive1_scribeloadertest\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask\nin this case the table was missing. better say that.\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.DDLTask.java"], "label": 1, "es_results": []}, {"bug_id": 1302, "bug_title": "describe parse_url throws an error", "bug_description": "descHive history file=/tmp/njain/hive_job_log_njain_201004121528_1840617354.txt\nhive> describe parse_url;\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask\nhive> describe extended parse_url;\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask\nhive> [njain@dev029 clientpositive]$ ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.DDLTask.java"], "label": 1, "es_results": []}, {"bug_id": 2290, "bug_title": "Improve error messages for DESCRIBE command", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.DDLTask.java"], "label": 1, "es_results": []}, {"bug_id": 2178, "bug_title": "Log related Check style Comments fixes", "bug_description": "Fix Log related Check style Comments", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java", "org.apache.hadoop.hive.ql.exec.DDLTask.java", "org.apache.hadoop.hive.jdbc.HiveConnection.java", "org.apache.hadoop.hive.ql.metadata.Partition.java", "org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.java", "org.apache.hadoop.hive.jdbc.HiveDataSource.java"], "label": 1, "es_results": []}, {"bug_id": 1444, "bug_title": "\"hdfs\" is hardcoded in few places in the code which inhibits use of other file systems", "bug_description": "In quite a few places \"hdfs\" is hardcoded, which is OK for majority of the cases, except when it is not really hdfs, but s3 or any other file system.\nThe place where it really breaks is:\nin ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java :\nmethod: private void applyConstraints(URI fromURI, URI toURI, Tree ast, boolean isLocal)\nFirst few lines are check for file system:\n    if (!fromURI.getScheme().equals(\"file\")\n        && !fromURI.getScheme().equals(\"hdfs\")) \n{\n\n      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(ast,\n\n          \"only \\\"file\\\" or \\\"hdfs\\\" file systems accepted\"));\n\n    }\n\n\"hdfs\" is hardcoded. \nI don&apos;t think you need to have this check at all as you are checking whether filesystem is local or not later on anyway and in regards to non locla file system - if one would be bad one you would get problems or have it look like local before you even come to \"applyConstraints\" method.\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.3.0", "fixed_version": "release-0.9.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 3853, "bug_title": "UDF unix_timestamp is deterministic if an argument is given, but it treated as non-deterministic preventing PPD", "bug_description": "unix_timestamp is declared as a non-deterministic function. But if user provides an argument, it makes deterministic result and eligible to PPD.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.4.1", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FunctionRegistry.java"], "label": 1, "es_results": []}, {"bug_id": 305, "bug_title": "Port Hadoop streaming's counters/status reporters to Hive Transforms", "bug_description": "https://issues.apache.org/jira/browse/HADOOP-1328\n\" Introduced a way for a streaming process to update global counters and status using stderr stream to emit information. Use \"reporter:counter:<group>,<counter>,<amount> \" to update  a counter. Use \"reporter:status:<message>\" to update status. \"\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.6.0", "fixed_version": "release-0.12.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ScriptOperator.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 1986, "bug_title": "partition pruner do not take effect for non-deterministic UDF", "bug_description": "hive udf can be deterministic or non-deterministic,but for non-deterministic udf such as rand and unix_timestamp,ppr do not take effect.\nand for unix_timestamp with para, for example unix_timestamp(&apos;2010-01-01&apos;),I think it is deterministic.\ncase :\nhive -hiveconf hive.root.logger=DEBUG,console\ncreate kv_part(key int,value string) partitioned by(ds string);\nalter table kv_part add partition (ds=2010) partition (ds=2011) partition (ds=2012);\ncreate kv2(key int,value string) partitioned by(ds string);\nalter table kv2 add partition (ds=2013) partition (ds=2014) partition (ds=2015);\nexplain select * from kv_part join kv2 on(kv_part.key=kv2.key) where kv_part.ds=2011 and rand() > 0.5\nrand() is non-deterministic ,so kv_part.ds=2011 no not filter the partition ds=2010,ds=2012\n.....\n11/02/14 12:22:32 DEBUG lazy.LazySimpleSerDe: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[key, value] columnTypes=[int, string] separator=[[B@1ac9683] nullstring=\\N lastColumnTakesRest=false\n11/02/14 12:22:32 INFO hive.log: DDL: struct kv_part \n{ i32 key, string value}\n11/02/14 12:22:32 DEBUG optimizer.GenMapRedUtils: Information added for path hdfs://172.25.38.253:54310/user/hive/warehouse/kv_part/ds=2010\n11/02/14 12:22:32 DEBUG optimizer.GenMapRedUtils: Information added for path hdfs://172.25.38.253:54310/user/hive/warehouse/kv_part/ds=2011\n11/02/14 12:22:32 DEBUG optimizer.GenMapRedUtils: Information added for path hdfs://172.25.38.253:54310/user/hive/warehouse/kv_part/ds=2012\n11/02/14 12:22:32 INFO parse.SemanticAnalyzer: Completed plan generation\n.....\nexplain select * from kv_part join kv2 on(kv_part.key=kv2.key) where kv_part.ds=2011 and sin(kv2.key) < 0.5;\nsin() is deterministic,so ppr work ok\n.....\n11/02/14 12:25:22 DEBUG optimizer.GenMapRedUtils: Information added for path hdfs://172.25.38.253:54310/user/hive/warehouse/kv_part/ds=2011\n....\nAnd user should get the deterministic info for UDF from wiki,or we shoud add this info to describe function", "project": "Apache", "sub_project": "HIVE", "version": "release-0.4.1", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FunctionRegistry.java"], "label": 1, "es_results": []}, {"bug_id": 1363, "bug_title": "'SHOW TABLE EXTENDED LIKE' command does not strip single/double quotes", "bug_description": "\n\n\nhive> SHOW TABLE EXTENDED LIKE pokes;\n\nOK\n\ntableName:pokes\n\nowner:carl\n\nlocation:hdfs://localhost/user/hive/warehouse/pokes\n\ninputformat:org.apache.hadoop.mapred.TextInputFormat\n\noutputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n\ncolumns:struct columns { i32 num}\n\npartitioned:false\n\npartitionColumns:\n\ntotalNumberFiles:0\n\ntotalFileSize:0\n\nmaxFileSize:0\n\nminFileSize:0\n\nlastAccessTime:0\n\nlastUpdateTime:1274517075221\n\n\n\nhive> SHOW TABLE EXTENDED LIKE \"p*\";\n\nFAILED: Error in metadata: MetaException(message:Got exception: javax.jdo.JDOUserException &apos;)&apos; expected at character 54 in \"database.name == dbName && ( tableName.matches(\"(?i)\"p.*\"\"))\")\n\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask\n\n\n\nhive> SHOW TABLE EXTENDED LIKE &apos;p*&apos;;\n\nOK\n\n\n\nhive> SHOW TABLE EXTENDED LIKE `p*`;\n\nOK\n\ntableName:pokes\n\nowner:carl\n\nlocation:hdfs://localhost/user/hive/warehouse/pokes\n\ninputformat:org.apache.hadoop.mapred.TextInputFormat\n\noutputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n\ncolumns:struct columns { i32 num}\n\npartitioned:false\n\npartitionColumns:\n\ntotalNumberFiles:0\n\ntotalFileSize:0\n\nmaxFileSize:0\n\nminFileSize:0\n\nlastAccessTime:0\n\nlastUpdateTime:1274517075221\n\n\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.5.0", "fixed_version": "release-0.14.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 12969, "bug_title": "Fix Javadoc for PredicatePushDown class", "bug_description": "Fix Javadocs for hive.optimize.ppd - Default Value: true\nAdded In: Hive 0.4.0 with HIVE-279, default changed to true in Hive 0.4.0 with HIVE-626\nNO PRECOMMIT TESTS", "project": "Apache", "sub_project": "HIVE", "version": "release-0.4.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java"], "label": 1, "es_results": []}, {"bug_id": 2042, "bug_title": "In error scenario some opened streams may not closed", "bug_description": "1) In error scenario PrintStream may not be closed in execute() of  ExplainTask.java\n2) In error scenario InputStream may not be closed in checkJobTracker() of Throttle.java ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ExplainTask.java", "org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker.java", "org.apache.hadoop.hive.ql.exec.Utilities.java", "org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.java", "org.apache.hadoop.hive.ql.exec.Throttle.java"], "label": 1, "es_results": []}, {"bug_id": 1988, "bug_title": "Make the delegation token issued by the MetaStore owned by the right user", "bug_description": "The &apos;owner&apos; of any delegation token issued by the MetaStore is set to the requesting user. When a delegation token is asked by the user himself during a job submission, this is fine. However, in the case where the token is requested for by services (e.g., Oozie), on behalf of the user, the token&apos;s owner is set to the user the service is running as. Later on, when the token is used by a MapReduce task, the MetaStore treats the incoming request as coming from Oozie and does operations as Oozie. This means any new directory creations (e.g., create_table) on the hdfs by the MetaStore will end up with Oozie as the owner.\nAlso, the MetaStore doesn&apos;t check whether a user asking for a token on behalf of some other user, is actually authorized to act on behalf of that other user. We should start using the ProxyUser authorization in the MetaStore (HADOOP-6510&apos;s APIs).", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java", "org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java", "org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java", "org.apache.hadoop.hive.metastore.HiveMetaStore.java", "org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java", "org.apache.hadoop.hive.metastore.IMetaStoreClient.java", "org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java", "org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.java"], "label": 1, "es_results": []}, {"bug_id": 2095, "bug_title": "auto convert map join bug", "bug_description": "1) \nwhen considering to choose one table as the big table candidate for a map join, if at compile time, hive can find out that the total known size of all other tables excluding the big table in consideration is bigger than a configured value, this big table candidate is a bad one, and should not put into plan. Otherwise, at runtime to filter this out may cause more time.\n2)\nadded a null check for back up tasks. Otherwise will see NullPointerException\n3)\nCommonJoinResolver needs to know a full mapping of pathToAliases. Otherwise it will make wrong decision.\n4)\nchanges made to the ConditionalResolverCommonJoin: added pathToAliases, aliasToSize (alias&apos;s input size that is known at compile time, by inputSummary), and intermediate dir path.\nSo the logic is, go over all the pathToAliases, and for each path, if it is from intermediate dir path, add this path&apos;s size to all aliases. And finally based on the size information and others like aliasToTask to choose the big table. \n5)\nConditional task&apos;s children contains wrong options, which may cause join fail or incorrect results. Basically when getting all possible children for the conditional task, should use a whitelist of big tables. Only tables in this while list can be considered as a big table.\nHere is the logic:\n\nGet a list of big table candidates. Only the tables in the returned set can be used as big table in the join operation.\nThe logic here is to scan the join condition array from left to right.\n\t\nIf see a inner join and the bigTableCandidates is empty, add both side of this inner join to big table candidates.\nIf see a left outer join, and the bigTableCandidates is empty, add the left side to it, and\nif the bigTableCandidates is not empty, do nothing (which means the bigTableCandidates is from left side).\nIf see a right outer join, clear the bigTableCandidates, and add right side to the bigTableCandidates, it means the right side of a right outer join always win.\nIf see a full outer join, return null immediately (no one can be the big table, can not do a mapjoin).\n\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java", "org.apache.hadoop.hive.ql.exec.Task.java", "org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java", "org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java", "org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 2045, "bug_title": "TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken() throws Null Pointer Exception in some cases", "bug_description": "1) In TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken() is doing null check for the tokenizer.\nIf tokenizer is null, fillTokenizer() method is called to get the tokenizer object. But fillTokenizer() method also can update the tokenizer with NULL , so NULL check should be done before using the tokenizer.\n2) Also improved some logging in TCTLSeparatedProtocol.java", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.java", "org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java"], "label": 1, "es_results": []}, {"bug_id": 2260, "bug_title": "ExecDriver::addInputPaths should pass the table properties to the record writer", "bug_description": "Currently when ExecDriver encounters a non-existent partition, it creates an empty file so that the query will be valid (and return 0 results).  However, when it does this and calls getHiveRecordWriter(), it creates a new instance of Properties, rather than providing the Properties associated with the table.\nThis causes RecordWriters that pull information from the table through the Properties to fail (such as Haivvreo).  The RecordWriter should be provided the table&apos;s Properties, as it is in all other cases where it&apos;s called.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ExecDriver.java"], "label": 1, "es_results": []}, {"bug_id": 1218, "bug_title": "CREATE TABLE t LIKE some_view should create a new empty base table, but instead creates a copy of view", "bug_description": "I think it should copy only the column definitions from the view metadata.  Currently it is copying the entire descriptor, resulting in a new view instead of a new base table.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.java", "org.apache.hadoop.hive.ql.exec.DDLTask.java"], "label": 1, "es_results": []}, {"bug_id": 2086, "bug_title": "Add test coverage for external table data loss issue", "bug_description": "Data loss when using \"create external table like\" statement. \n1) Set up an external table S, point to location L. Populate data in S.\n2) Create another external table T, using statement like this:\n    create external table T like S location L\n   Make sure table T point to the same location as the original table S.\n3) Query table T, see the same set of data in S.\n4) drop table T.\n5) Query table S will return nothing, and location L is deleted. ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.QTestUtil.java"], "label": 1, "es_results": []}, {"bug_id": 2080, "bug_title": "Few code improvements in the ql and serde packages.", "bug_description": "Few code improvements in the ql and serde packages.\n1) Little performance Improvements \n2) Null checks to avoid NPEs\n3) Effective varaible management.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.exec.UnionOperator.java", "org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFunction.java", "org.apache.hadoop.hive.ql.exec.TableScanOperator.java", "org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldType.java", "org.apache.hadoop.hive.ql.parse.ASTNode.java", "org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java", "org.apache.hadoop.hive.ql.exec.FilterOperator.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.exec.FileSinkOperator.java", "org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java", "org.apache.hadoop.hive.ql.exec.SelectOperator.java", "org.apache.hadoop.hive.ql.exec.TaskFactory.java", "org.apache.hadoop.hive.ql.exec.Operator.java", "org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeField.java", "org.apache.hadoop.hive.ql.parse.ParseContext.java", "org.apache.hadoop.hive.ql.exec.GroupByOperator.java"], "label": 1, "es_results": []}, {"bug_id": 2298, "bug_title": "Fix UDAFPercentile to tolerate null percentiles", "bug_description": "UDAFPercentile when passed null percentile list will throw a null pointer exception.\nSubmitting a small fix for that.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.UDAFPercentile.java"], "label": 1, "es_results": []}, {"bug_id": 1631, "bug_title": "JDBC driver returns wrong precision, scale, or column size for some data types", "bug_description": "For some data types, these methods return values that do not conform to the JDBC spec:\norg.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getPrecision(int)\norg.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getScale(int)\norg.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnDisplaySize(int)\norg.apache.hadoop.hive.jdbc.JdbcColumn.getColumnSize()", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.jdbc.TestJdbcDriver.java", "org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java", "org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java", "org.apache.hadoop.hive.jdbc.JdbcColumn.java"], "label": 1, "es_results": []}, {"bug_id": 1592, "bug_title": "ProxyFileSystem.close calls super.close twice.", "bug_description": "  public void close() throws IOException \n{\n\n    super.close();\n\n    super.close();\n\n  }", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.fs.ProxyFileSystem.java"], "label": 1, "es_results": []}, {"bug_id": 2520, "bug_title": "left semi join will duplicate data", "bug_description": "CREATE TABLE sales (name STRING, id INT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\\t&apos;;\nCREATE TABLE things (id INT, name STRING)\nROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\\t&apos;;\nThe &apos;sales&apos; table has data in a file: sales.txt, and the data is\nJoe 2\nHank 2\nThe &apos;things&apos; table has data int two files: things.txt and things2.txt\nThe content of things.txt is :\n2 Tie\nThe content of things2.txt is :\n2 Tie\nSELECT * FROM sales LEFT SEMI JOIN things ON (sales.id = things.id);\nwill output\nJoe 2\nJoe 2\nHank 2\nHank 2\nso the result is wrong.\nIn CommonJoinOperator left semi join should use \" genObject(null, 0, new IntermediateObject(new ArrayList[numAliases], 0), true); \" to generate data.\nbut now it uses \" genUniqueJoinObject(0, 0); \" to generate data.\nThis patch will solve this problem.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.9.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java"], "label": 1, "es_results": []}, {"bug_id": 2735, "bug_title": "PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table", "bug_description": "As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.9.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 2721, "bug_title": "ability to select a view qualified by the database / schema name", "bug_description": "HIVE-1517 added support for selecting tables from different databases (aka schemas) by qualifying the tables with the database name. The feature work did not however extend this support to views. Note that this point came up in the earlier JIRA, but was not addressed. See the following two comments:\nhttps://issues.apache.org/jira/browse/HIVE-1517?focusedCommentId=12996641&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12996641\nhttps://issues.apache.org/jira/browse/HIVE-1517?focusedCommentId=12996679&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12996679", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.UnparseTranslator.java"], "label": 1, "es_results": []}, {"bug_id": 1608, "bug_title": "use sequencefile as the default for storing intermediate results", "bug_description": "The only argument for having a text file for storing intermediate results seems to be better debuggability.\nBut, tailing a sequence file is possible, and it should be more space efficient", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 2309, "bug_title": "Incorrect regular expression for extracting task id from filename", "bug_description": "For producing the correct filenames for bucketed tables, there is a method in Utilities.java that extracts out the task id from the filename and replaces it with the bucket number. There is a bug in the regex that is used to extract this value for attempt numbers >= 10:\n\n\n\n>>> re.match(\"^.*?([0-9]+)(_[0-9])?(\\\\..*)?$\", &apos;attempt_201107090429_64965_m_001210_10&apos;).group(1)\n\n&apos;10&apos;\n\n>>> re.match(\"^.*?([0-9]+)(_[0-9])?(\\\\..*)?$\", &apos;attempt_201107090429_64965_m_001210_9&apos;).group(1)\n\n&apos;001210&apos;\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.Utilities.java"], "label": 1, "es_results": []}, {"bug_id": 2315, "bug_title": "DatabaseMetadata.getColumns() does not return partition column names for a table", "bug_description": "getColumns() method of DatabaseMetadata for HIVE JDBC Driver does not return the partition column names. Where as from HIVE CLI, if you do a &apos;describe tablename&apos; you get all columns including the partition columns. It would be nice if getColumns() method returns all columns.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.jdbc.TestJdbcDriver.java", "org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java"], "label": 1, "es_results": []}, {"bug_id": 2334, "bug_title": "DESCRIBE TABLE causes NPE when hive.cli.print.header=true", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.cli.CliDriver.java"], "label": 1, "es_results": []}, {"bug_id": 2369, "bug_title": "Minor typo in error message in HiveConnection.java (JDBC)", "bug_description": "There is a minor typo issue in HiveConnection.java (jdbc) :\n\nthrow new SQLException(\"Could not establish connecton to \"\n\n            + uri + \": \" + e.getMessage(), \"08S01\");\n\nIt seems like there&apos;s a \"i\" missing.\nI know it&apos;s a very minor typo but I report it anyway. I won&apos;t attach a patch because it would be too long for me to SVN checkout just for 1 letter.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.jdbc.HiveConnection.java"], "label": 1, "es_results": []}, {"bug_id": 2398, "bug_title": "Hive server does not return schema for 'set' command", "bug_description": "The Hive server does process the CLI commands like &apos;set&apos;, &apos;set -v&apos; sent by ODBC or JDBC clients. But currently only the data is returned to client but not schema for that resultset. This makes it unusable for a ODBC or JDBC client to use this option.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.service.HiveServer.java", "org.apache.hadoop.hive.ql.processors.SetProcessor.java", "org.apache.hadoop.hive.jdbc.TestJdbcDriver.java", "org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java"], "label": 1, "es_results": []}, {"bug_id": 2455, "bug_title": "Pass correct remoteAddress in proxy user authentication", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java", "org.apache.hadoop.hive.metastore.HiveMetaStore.java", "org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java"], "label": 1, "es_results": []}, {"bug_id": 2499, "bug_title": "small table filesize for automapjoin is not consistent in HiveConf.java and hive-default.xml", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 2466, "bug_title": "mapjoin_subquery  dump small table (mapjoin table) to the same file", "bug_description": "in mapjoin_subquery.q  there is a query\nSELECT /*+ MAPJOIN(z) */ subq.key1, z.value\nFROM\n(SELECT /*+ MAPJOIN */ x.key as key1, x.value as value1, y.key as key2, y.value as value2 \n FROM src1 x JOIN src y ON (x.key = y.key)) subq\n JOIN srcpart z ON (subq.key1 = z.key and z.ds=&apos;2008-04-08&apos; and z.hr=11);\nwhen dump x and z to a local file,there all dump to the same file, so we lost the data of x", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.plan.PlanUtils.java", "org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java", "org.apache.hadoop.hive.ql.exec.Utilities.java", "org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java", "org.apache.hadoop.hive.ql.plan.MapJoinDesc.java", "org.apache.hadoop.hive.ql.exec.MapJoinOperator.java", "org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java", "org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java", "org.apache.hadoop.hive.ql.exec.MapredLocalTask.java"], "label": 1, "es_results": []}, {"bug_id": 2705, "bug_title": "SemanticAnalyzer twice swallows an exception it should not", "bug_description": "Twice SemanticAnalyzer catches an exception and drops it, just passing on the original message&apos;s in a new SemanticException. This means that those that see the message in the output cannot tell what generated the original exception.  These original exceptions should be wrapped, as they are in other parts of the code.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.9.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 2918, "bug_title": "Hive Dynamic Partition Insert - move task not considering 'hive.exec.max.dynamic.partitions' from CLI", "bug_description": "Dynamic Partition insert showing an error with the number of partitions created even after the default value of &apos;hive.exec.max.dynamic.partitions&apos; is bumped high to 2000.\nError Message:\n\"Failed with exception Number of dynamic partitions created is 1413, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1413.\"\nThese are the following properties set on hive CLI\nhive> set hive.exec.dynamic.partition=true;\nhive> set hive.exec.dynamic.partition.mode=nonstrict;\nhive> set hive.exec.max.dynamic.partitions=2000;\nhive> set hive.exec.max.dynamic.partitions.pernode=2000;\nThis is the query with console error log\nhive> \n    > INSERT OVERWRITE TABLE partn_dyn Partition (pobox)\n    > SELECT country,state,pobox FROM non_partn_dyn;\nTotal MapReduce jobs = 2\nLaunching Job 1 out of 2\nNumber of reduce tasks is set to 0 since there&apos;s no reduce operator\nStarting Job = job_201204021529_0002, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201204021529_0002\nKill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_201204021529_0002\n2012-04-02 16:05:28,619 Stage-1 map = 0%,  reduce = 0%\n2012-04-02 16:05:39,701 Stage-1 map = 100%,  reduce = 0%\n2012-04-02 16:05:50,800 Stage-1 map = 100%,  reduce = 100%\nEnded Job = job_201204021529_0002\nEnded Job = 248865587, job is filtered out (removed at runtime).\nMoving data to: hdfs://0.0.0.0/tmp/hive-cloudera/hive_2012-04-02_16-05-24_919_5976014408587784412/-ext-10000\nLoading data to table default.partn_dyn partition (pobox=null)\nFailed with exception Number of dynamic partitions created is 1413, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1413.\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask\nI checked the job.xml of the first map only job, there the value hive.exec.max.dynamic.partitions=2000 is reflected but the move task is taking the default value from hive-site.xml . If I change the value in hive-site.xml then the job completes successfully. Bottom line,the property &apos;hive.exec.max.dynamic.partitions&apos;set on CLI is not being considered by move task\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 2540, "bug_title": "LATERAL VIEW with EXPLODE produces ConcurrentModificationException", "bug_description": "The following produces ConcurrentModificationException on the for loop inside EXPLODE:\n\n\n\ncreate table foo as select array(1, 2) a from src limit 1;\n\nselect a, x.b from foo lateral view explode(a) x as b;\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.7.1", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.serde2.lazy.LazyMap.java", "org.apache.hadoop.hive.serde2.lazy.LazyArray.java"], "label": 1, "es_results": []}, {"bug_id": 2631, "bug_title": "Make Hive work with Hadoop 1.0.0", "bug_description": "With Hadoop 1.0.0 around the corner ( http://mail-archives.apache.org/mod_mbox/hadoop-general/201111.mbox/%3C9D6B6144-F4E0-4A31-883F-2AC504727A1F%40hortonworks.com%3E ), it will be useful to make Hive work with it.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.8.1", "fixed_files": ["org.apache.hadoop.hive.shims.ShimLoader.java"], "label": 1, "es_results": []}, {"bug_id": 2746, "bug_title": "Metastore client does not log properly in case of connection failure to server", "bug_description": "LOG.error(e.getStackTrace()) in current code prints memory location of StackTraceElement[] instead of message.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.9.0", "fixed_files": ["org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java"], "label": 1, "es_results": []}, {"bug_id": 2792, "bug_title": "SUBSTR(CAST(<string> AS BINARY)) produces unexpected results", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.9.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.UDFSubstr.java"], "label": 1, "es_results": []}, {"bug_id": 2803, "bug_title": "utc_from_timestamp and utc_to_timestamp returns incorrect results.", "bug_description": "How to reproduce:\n\n$ echo \"2011-12-25 09:00:00.123456\" > /tmp/data5.txt\n\nhive> create table ts1(t1 timestamp);\n\nhive> load data local inpath &apos;/tmp/data5.txt&apos; overwrite into table ts1;\n\nhive> select t1, from_utc_timestamp(t1, &apos;JST&apos;), from_utc_timestamp(t1, &apos;JST&apos;) from ts1 limit 1;\n\n\n\nThe following result is expected:\n\n 2011-12-25 09:00:00.123456      2011-12-25 18:00:00.123456      2011-12-25 18:00:00.123456\n\n\n\nHowever, the above query return incorrect result like this:\n\n 2011-12-26 03:00:00.492456      2011-12-26 03:00:00.492456      2011-12-26 03:00:00.492456\n\n\n\nThis is because GenericUDFFromUtcTimestamp.applyOffset() does setTime() improperly.\nOn evaluating query, timestamp argument always returns the same instance.\nGenericUDFFromUtcTimestamp.applyOffset() does setTime() on the instance.\nThat means it adds all offsets in the query.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.java"], "label": 1, "es_results": []}, {"bug_id": 2757, "bug_title": "hive cannot find hadoop executor scripts without HADOOP_HOME set", "bug_description": "The trouble is that in Hadoop 0.23 HADOOP_HOME has been deprecated. I think it would be really nice if bin/hive can be modified to capture the which hadoop\nand pass that as a property into the JVM.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 3062, "bug_title": "Insert into table overwrites existing table if table name contains uppercase character", "bug_description": "\"Insert into table <table-name> ~~\" is expected to append query result into the table. But when the table name contains uppercase character, it overwrite existing table.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.QBParseInfo.java"], "label": 1, "es_results": []}, {"bug_id": 2498, "bug_title": "Group by operator does not estimate size of Timestamp & Binary data correctly", "bug_description": "It currently defaults to default case and returns constant value, whereas we can do better by getting actual size at runtime.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.GroupByOperator.java"], "label": 1, "es_results": []}, {"bug_id": 3232, "bug_title": "Resource Leak: Fix the File handle leak in EximUtil.java", "bug_description": "1) Not closing the file handle EximUtil after reading the metadata from the file.\n2) Nit: Get the path from URI to handle the Windows paths.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.EximUtil.java"], "label": 1, "es_results": []}, {"bug_id": 3596, "bug_title": "Regression - HiveConf static variable causes issues in long running JVM instances with /tmp/ data", "bug_description": "With Hive 0.8.x, HiveConf was changed to utilize the private, static member \"confVarURL\" which points to /tmp/hive-<user>-<tmp_number>.xml for job configuration settings. \nDuring long running JVMs, such as a Beeswax server, which creates multiple HiveConf objects over time this variable does not properly get updated between jobs and can cause job failure if the OS cleans /tmp/ during a cron job. ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.8.1", "fixed_files": ["org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 2517, "bug_title": "Support group by on struct type", "bug_description": "Currently group by on struct and union types are not supported. This issue will enable support for those.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.12.0", "fixed_files": ["org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java"], "label": 1, "es_results": []}, {"bug_id": 3191, "bug_title": "timestamp - timestamp causes null pointer exception", "bug_description": "select tts.rnum, tts.cts - tts.cts from cert.tts tts\nError: Query returned non-zero code: 12, because: FAILED: Hive Internal Error: java.lang.NullPointerException(null)\nSQLState:  42000\nErrorCode: 12\ncreate table if not exists CERT.TTS ( RNUM int , CTS timestamp) \nstored as sequencefile;", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.12.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.java"], "label": 1, "es_results": []}, {"bug_id": 3189, "bug_title": "cast ( <string type> as bigint) returning null values", "bug_description": "select rnum, c1, cast(c1 as bigint) from cert.tsdchar tsdchar where rnum in (0,1,2)\ncreate table if not exists CERT.TSDCHAR ( RNUM int , C1 string)\nrow format sequencefile\nrnum\tc1\t_c2\n0\t-1                         \t<null>\n1\t0                          \t<null>\n2\t10                         \t<null>", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.0", "fixed_version": "release-0.12.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.TestUDFDateAdd.java"], "label": 1, "es_results": []}, {"bug_id": 2913, "bug_title": "BlockMergeTask Does not Honor Job Configuration Properties when used directly", "bug_description": "BlockMergeTask has a main() and when used directly (instead of say partition concatenate feature), the -jobconf arguments are not honored. This is not something most people directly use.\nUsage:\nBlockMergeTask -input <colon seperated input paths>  -outputDir outputDir [-jobconf k1=v1 [-jobconf k2=v2] ...] \nTo reproduce:\nRun BlockMergeTask with say -jobconf mapred.job.name=test and launched job will have a different name.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.1", "fixed_version": "release-0.9.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java"], "label": 1, "es_results": []}, {"bug_id": 2963, "bug_title": "metastore delegation token is not getting used by hive commandline", "bug_description": "When metastore delegation tokens are used to run hive (or hcat) commands, the delegation token does not end up getting used.\nThis is because new Hive object is not created with value of hive.metastore.token.signature in its conf. This config parameter is missing in the list of HiveConf variables whose change results in metastore recreation.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.1", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 3057, "bug_title": "metastore.HiveMetaStore$HMSHandler should set the thread local raw store to null in shutdown()", "bug_description": "The shutdown() function of metastore.HiveMetaStore$HMSHandler does not set the thread local RawStore variable (in threadLocalMS) to null. Subsequent getMS() calls may get the wrong RawStore object.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.1", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.metastore.HiveMetaStore.java"], "label": 1, "es_results": []}, {"bug_id": 2955, "bug_title": "Queries consists of metadata-only-query returns always empty value", "bug_description": "For partitioned table, simple query on partition column returns always null or empty value, for example,\n\n\n\ncreate table emppart(empno int, ename string) partitioned by (deptno int);\n\n.. load partitions..\n\n\n\nselect distinct deptno from emppart; // empty\n\nselect min(deptno), max(deptno) from emppart;  // NULL and NULL\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.1", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java", "org.apache.hadoop.hive.ql.exec.ExecDriver.java"], "label": 1, "es_results": []}, {"bug_id": 3070, "bug_title": "Filter on outer join condition removed while merging join tree", "bug_description": "should the result of query A: \nselect s.aa, s.bb, c.key keyc from (select a.key aa, b.key bb from src a left outer join src b on a.key=b.key) s left outer join src c on s.bb=c.key and s.bb<10 where s.aa<20;\nbe the same as query B:\nselect a.key keya, b.key keyb, c.key keyc from src a left outer join src b on a.key=b.key left outer join src c on b.key=c.key and b.key<10 where a.key<20;\n?\nCurrently, the result is different, query B gets wrong result!\nIn SemanticAnalyzer.java, mergeJoins():\nArrayList<ArrayList<ASTNode>> filters = target.getFilters();\nfor (int i = 0; i < nodeRightAliases.length; i++) \n{\n\n  filters.add(node.getFilters().get(i + 1));\n\n}\n\nfilters in node.getFilters().get(0) are lost.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.1", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 3699, "bug_title": "Multiple insert overwrite into multiple tables query stores same results in all tables", "bug_description": "(Note: This might be related to HIVE-2750)\nI am doing a query with multiple INSERT OVERWRITE to multiple tables in order to scan the dataset only 1 time, and i end up having all these tables with the same content ! It seems the GROUP BY query that returns results is overwriting all the temp tables.\nWeird enough, if i had further GROUP BY queries into additional temp tables, grouped by a different field, then all temp tables, even the ones that would have been wrong content are all correctly populated.\nThis is the misbehaving query:\n    FROM nikon\n    INSERT OVERWRITE TABLE e1\n    SELECT qs_cs_s_aid AS Emplacements, COUNT AS Impressions\n    WHERE qs_cs_s_cat=&apos;PRINT&apos; GROUP BY qs_cs_s_aid\n    INSERT OVERWRITE TABLE e2\n    SELECT qs_cs_s_aid AS Emplacements, COUNT AS Vues\n    WHERE qs_cs_s_cat=&apos;VIEW&apos; GROUP BY qs_cs_s_aid\n    ;\nIt launches only one MR job and here are the results. Why does table &apos;e1&apos; contains results from table &apos;e2&apos; ?! Table &apos;e1&apos; should have been empty (see individual SELECTs further below)\n    hive> SELECT * from e1;\n    OK\n    NULL    2\n    1627575 25\n    1627576 70\n    1690950 22\n    1690952 42\n    1696705 199\n    1696706 66\n    1696730 229\n    1696759 85\n    1696893 218\n    Time taken: 0.229 seconds\n    hive> SELECT * from e2;\n    OK\n    NULL    2\n    1627575 25\n    1627576 70\n    1690950 22\n    1690952 42\n    1696705 199\n    1696706 66\n    1696730 229\n    1696759 85\n    1696893 218\n    Time taken: 0.11 seconds\nHere is are the result to the indiviual queries (only the second query returns a result set):\n    hive> SELECT qs_cs_s_aid AS Emplacements, COUNT AS Impressions FROM nikon\n    WHERE qs_cs_s_cat=&apos;PRINT&apos; GROUP BY qs_cs_s_aid;\n    (...)\n    OK\n          <- There are no results, this is normal\n    Time taken: 41.471 seconds\n    hive> SELECT qs_cs_s_aid AS Emplacements, COUNT AS Vues FROM nikon\n    WHERE qs_cs_s_cat=&apos;VIEW&apos; GROUP BY qs_cs_s_aid;\n    (...)\n    OK\n    NULL  2\n    1627575 25\n    1627576 70\n    1690950 22\n    1690952 42\n    1696705 199\n    1696706 66\n    1696730 229\n    1696759 85\n    1696893 218\n    Time taken: 39.607 seconds\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.1", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.ql.ppd.OpProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 3682, "bug_title": "when output hive table to file,users should could have a separator of their own choice", "bug_description": "By default,when output hive table to file ,columns of the Hive table are separated by ^A character (that is \\001).\nBut indeed users should have the right to set a seperator of their own choice.\nUsage Example:\ncreate table for_test (key string, value string);\nload data local inpath &apos;./in1.txt&apos; into table for_test\nselect * from for_test;\nUT-01default separator is \\001 line separator is \\n\ninsert overwrite local directory &apos;./test-01&apos; \nselect * from src ;\ncreate table array_table (a array<string>, b array<string>)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY &apos;\\t&apos;\nCOLLECTION ITEMS TERMINATED BY &apos;,&apos;;\nload data local inpath \"../hive/examples/files/arraytest.txt\" overwrite into table table2;\nCREATE TABLE map_table (foo STRING , bar MAP<STRING, STRING>)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY &apos;\\t&apos;\nCOLLECTION ITEMS TERMINATED BY &apos;,&apos;\nMAP KEYS TERMINATED BY &apos;:&apos;\nSTORED AS TEXTFILE;\nUT-02defined field separator as &apos;:&apos;\ninsert overwrite local directory &apos;./test-02&apos; \nrow format delimited \nFIELDS TERMINATED BY &apos;:&apos; \nselect * from src ;\nUT-03: line separator DO NOT ALLOWED to define as other separator \ninsert overwrite local directory &apos;./test-03&apos; \nrow format delimited \nFIELDS TERMINATED BY &apos;:&apos; \nselect * from src ;\nUT-04: define map separators \ninsert overwrite local directory &apos;./test-04&apos; \nrow format delimited \nFIELDS TERMINATED BY &apos;\\t&apos;\nCOLLECTION ITEMS TERMINATED BY &apos;,&apos;\nMAP KEYS TERMINATED BY &apos;:&apos;\nselect * from src;", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.1", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.ql.plan.PlanUtils.java", "org.apache.hadoop.hive.ql.parse.QB.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 2702, "bug_title": "Enhance listPartitionsByFilter to add support for integral types both for equality and non-equality", "bug_description": "listPartitionsByFilter supports only non-string partitions. This is because its explicitly specified in generateJDOFilterOverPartitions in ExpressionTree.java. \n//Can only support partitions whose types are string\n      if( ! table.getPartitionKeys().get(partitionColumnIndex).\n          getType().equals(org.apache.hadoop.hive.serde.Constants.STRING_TYPE_NAME) ) \n{\n\n        throw new MetaException\n\n        (\"Filtering is supported only on partition keys of type string\");\n\n      }", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.1", "fixed_version": "release-0.12.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java", "org.apache.hadoop.hive.metastore.TestHiveMetaStore.java", "org.apache.hadoop.hive.ql.exec.Utilities.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.metastore.parser.ExpressionTree.java"], "label": 1, "es_results": []}, {"bug_id": 2939, "bug_title": "LazyArray.getList changes array it previously returned ", "bug_description": "Simple query like:\nSELECT a, e\nFROM ikabiljo_test_string_array\nLATERAL VIEW EXPLODE(a) x1 AS e\n(table contains one column - ARRAY<STRING> - and has one row - [b,c,a] )\nfails with ConcurrentModificationException, since LazyArray.getList changes the cached array it returns.\nLazyArray.getList can easily:\n\nreturn cached array if present, without clearing and refiling. Hive is already not going to work properly if you change input parameters in an UDF. If that doesn&apos;t sound good - it can return Collections.unmodifiableList\nor just not cache anything\n\nSame is true for LazyMap.getMap", "project": "Apache", "sub_project": "HIVE", "version": "release-0.8.1", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.serde2.lazy.LazyMap.java", "org.apache.hadoop.hive.serde2.lazy.LazyArray.java"], "label": 1, "es_results": []}, {"bug_id": 2465, "bug_title": "Primitive Data Types returning null if the data is out of range of the data type.", "bug_description": "Primitive Data Types returning null if the input data is out of range of the data type. In this case it is better to log the message with the proper message and actual data then user get to know some data is missing.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.serde2.lazy.LazyDouble.java", "org.apache.hadoop.hive.serde2.lazy.LazyInteger.java", "org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.java", "org.apache.hadoop.hive.serde2.lazy.LazyBoolean.java", "org.apache.hadoop.hive.serde2.lazy.LazyShort.java", "org.apache.hadoop.hive.serde2.lazy.LazyLong.java", "org.apache.hadoop.hive.serde2.lazy.LazyByte.java", "org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.java", "org.apache.hadoop.hive.serde2.lazy.LazyBinary.java", "org.apache.hadoop.hive.serde2.lazy.LazyFloat.java", "org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java"], "label": 1, "es_results": []}, {"bug_id": 2253, "bug_title": "Merge failing of join tree in exceptional case", "bug_description": "In some very exceptional cases, SemanticAnayzer fails to merge join tree. Example is below.\ncreate table a (val1 int, val2 int)\ncreate table b (val1 int, val2 int)\ncreate table c (val1 int, val2 int)\ncreate table d (val1 int, val2 int)\ncreate table e (val1 int, val2 int)\n1. all same(single) join key --> one MR, good\nselect * from a join b on a.val1=b.val1 join c on a.val1=c.val1 join d on a.val1=d.val1 join e on a.val1=e.val1\n2. two join keys --> expected to have two MR, but resulted to three MR\nselect * from a join b on a.val1=b.val1 join c on a.val1=c.val1 join d on a.val1=d.val1 join e on a.val2=e.val2\n3. by changing the join order, we could attain two MR as first-expectation.\nselect * from a join e on a.val2=e.val2 join c on a.val1=c.val1 join d on a.val1=d.val1 join b on a.val1=b.val1", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.8.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 2907, "bug_title": "Hive error when dropping a table with large number of partitions", "bug_description": "Running into an \"Out Of Memory\" error when trying to drop a table with 128K partitions.\nThe methods dropTable in metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java \nand dropTable in ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java encounter out of memory errors \nwhen dropping tables with lots of partitions because they try to load the metadata for every partition into memory.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.DDLTask.java", "org.apache.hadoop.hive.metastore.TestHiveMetaStore.java", "org.apache.hadoop.hive.metastore.ObjectStore.java", "org.apache.hadoop.hive.metastore.HiveAlterHandler.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 3000, "bug_title": "Potential infinite loop / log spew in ZookeeperHiveLockManager", "bug_description": "See ZookeeperHiveLockManger.lock()\nIf Zookeeper is in a bad state, it&apos;s possible to get an exception (e.g. org.apache.zookeeper.KeeperException$SessionExpiredException) when we call lockPrimitive(). There is a bug in the exception handler where the loop does not exit because the break in the switch statement gets out the switch, not the do..while loop. Because tryNum was not incremented due to the exception, lockPrimitive() will be called in an infinite loop, as fast as possible. Since the exception is printed for each call, Hive will produce significant log spew.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java"], "label": 1, "es_results": []}, {"bug_id": 3125, "bug_title": "sort_array does not work with LazyPrimitive", "bug_description": "The sort_array function doesn&apos;t work against data that&apos;s actually come out of a table. The test suite only covers constants given in the query.\nIf you try and use sort_array on an array from a table, then you get a ClassCastException that you can&apos;t convert LazyX to Comparable.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.generic.GenericUDFSortArray.java"], "label": 1, "es_results": []}, {"bug_id": 3127, "bug_title": "Pass hconf values as XML instead of command line arguments to child JVM", "bug_description": "The maximum length of the DOS command string is 8191 characters (in Windows latest versions http://support.microsoft.com/kb/830473). This limit will be exceeded easily when it appends individual hconf values to the command string. To work around this problem, Write all changed hconf values to a temp file and pass the temp file path to the child jvm to read and initialize the -hconf parameters from file.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ExecDriver.java", "org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java", "org.apache.hadoop.hive.ql.exec.MapRedTask.java", "org.apache.hadoop.hive.ql.exec.MapredLocalTask.java"], "label": 1, "es_results": []}, {"bug_id": 3168, "bug_title": "LazyBinaryObjectInspector.getPrimitiveJavaObject copies beyond length of underlying BytesWritable", "bug_description": "LazyBinaryObjectInspector.getPrimitiveJavaObject copies the full capacity of the LazyBinary&apos;s underlying BytesWritable object, which can be greater than the size of the actual contents. \nThis leads to additional characters at the end of the ByteArrayRef returned. When the LazyBinary object gets re-used, there can be remnants of the later portion of previous entry. \nThis was not seen while reading through hive queries, which I think is because a copy elsewhere seems to create LazyBinary with length == capacity. (probably LazyBinary copy constructor). This was seen when MR or pig used Hcatalog to read the data.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.serde2.lazy.LazyUtils.java", "org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.java", "org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector.java", "org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.java", "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.java"], "label": 1, "es_results": []}, {"bug_id": 3248, "bug_title": "lack of semi-colon in .q file leads to missing the next statement", "bug_description": "set hive.check.par=1\nselect count(1) from src;\nselect count(1) from src;\nIf the above .q file is executed, the first statement is lost.\nFound this while reviewing https://issues.apache.org/jira/browse/HIVE-2848", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.processors.SetProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 2544, "bug_title": "Nullpointer on registering udfs.", "bug_description": "Currently the Function registry can throw NullPointers when multiple threads are trying to register the same function. The normal put() will replace the existing registered function object even if it&apos;s exactly the same function.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FunctionRegistry.java"], "label": 1, "es_results": []}, {"bug_id": 3126, "bug_title": "Generate & build the velocity based Hive tests on windows by fixing the path issues", "bug_description": "1)Escape the backward slash in Canonical Path if unit test runs on windows.\n2)Different comparison  \n     a.\tIgnore the extra spacing on windows\n     b.\tIgnore the different line endings on windows & Unix\n     c.\tConvert the file paths to windows specific. (Handle spaces etc..)\n3)Set the right file scheme & class path separators while invoking the junit task from ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.contrib.mr.TestGenericMR.java", "org.apache.hadoop.hive.ql.QTestUtil.java", "org.apache.hadoop.hive.ant.QTestGenTask.java", "org.apache.hadoop.fs.ProxyFileSystem.java", "org.apache.hadoop.fs.ProxyLocalFileSystem.java", "org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java", "org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hadoop.hive.ql.Context.java"], "label": 1, "es_results": []}, {"bug_id": 3098, "bug_title": "Memory leak from large number of FileSystem instances in FileSystem.CACHE", "bug_description": "The problem manifested from stress-testing HCatalog 0.4.1 (as part of testing the Oracle backend).\nThe HCatalog server ran out of memory (-Xmx2048m) when pounded by 60-threads, in under 24 hours. The heap-dump indicates that hadoop::FileSystem.CACHE had 1000000 instances of FileSystem, whose combined retained-mem consumed the entire heap.\nIt boiled down to hadoop::UserGroupInformation::equals() being implemented such that the \"Subject\" member is compared for equality (\"==\"), and not equivalence (\".equals()\"). This causes equivalent UGI instances to compare as unequal, and causes a new FileSystem instance to be created and cached.\nThe UGI.equals() is so implemented, incidentally, as a fix for yet another problem (HADOOP-6670); so it is unlikely that that implementation can be modified.\nThe solution for this is to check for UGI equivalence in HCatalog (i.e. in the Hive metastore), using an cache for UGI instances in the shims.\nI have a patch to fix this. I&apos;ll upload it shortly. I just ran an overnight test to confirm that the memory-leak has been arrested.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.metastore.TUGIBasedProcessor.java", "org.apache.hadoop.hive.shims.HadoopShimsSecure.java", "org.apache.hadoop.hive.shims.HadoopShims.java", "org.apache.hadoop.hive.shims.Hadoop20Shims.java", "org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java"], "label": 1, "es_results": []}, {"bug_id": 3243, "bug_title": "ignore white space between entries of hive/hbase table mapping", "bug_description": "In hive/hbase integration, when creating a hive/hbase table, white space is not ignored in hbase.columns.mapping. \ne.g. \"cf:foo, cf:bar\" will create two column families \"cf\" and \" cf\" in the underlying hbase table, which is certainly not what the user want and make them confused.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.10.0", "fixed_files": ["org.apache.hadoop.hive.hbase.HBaseSerDe.java"], "label": 1, "es_results": []}, {"bug_id": 4000, "bug_title": "Hive client goes into infinite loop at 100% cpu", "bug_description": "The Hive client starts multiple threads to track the progress of the MapReduce jobs. Unfortunately those threads access several static HashMaps that are not protected by locks. When the HashMaps are modified, they sometimes because race conditions that lead to the client threads getting stuck in infinite loops.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.ql.history.HiveHistory.java", "org.apache.hadoop.hive.ql.QueryPlan.java", "org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java"], "label": 1, "es_results": []}, {"bug_id": 3628, "bug_title": "Provide a way to use counters in Hive through UDF", "bug_description": "Currently it is not possible to generate counters through UDF. We should support this. \nPig currently allows this.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java", "org.apache.hadoop.hive.ql.exec.GroupByOperator.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDF.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.java", "org.apache.hadoop.hive.ql.exec.ExecReducer.java", "org.apache.hadoop.hive.ql.exec.ExecMapper.java", "org.apache.hadoop.hive.ql.exec.UDTFOperator.java"], "label": 1, "es_results": []}, {"bug_id": 2264, "bug_title": "Hive server is SHUTTING DOWN when invalid queries beeing executed.", "bug_description": "When invalid query is beeing executed, Hive server is shutting down.\n\n\"CREATE TABLE SAMPLETABLE(IP STRING , showtime BIGINT ) partitioned by (ds string,ipz int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\\040&apos;\"\n\n\n\n\"ALTER TABLE SAMPLETABLE add Partition(ds=&apos;sf&apos;) location &apos;/user/hive/warehouse&apos; Partition(ipz=100) location &apos;/user/hive/warehouse&apos;\"\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.ql.Driver.java", "org.apache.hadoop.hive.ql.exec.ExecDriver.java", "org.apache.hadoop.hive.ql.exec.Task.java", "org.apache.hadoop.hive.ql.exec.MapRedTask.java", "org.apache.hadoop.hive.ql.exec.MapredLocalTask.java"], "label": 1, "es_results": []}, {"bug_id": 3179, "bug_title": "HBase Handler does not handle NULLs properly", "bug_description": "We found a quite severe issue in the HBase Handler which actually means that Hive potentially returns incorrect data if a column has NULL values in HBase (which means the cell doesn&apos;t even exist)\nIn HBase She Will:\n\ncreate &apos;hive_hbase_test&apos;, &apos;test&apos;\n\nput &apos;hive_hbase_test&apos;, &apos;1&apos;, &apos;test:c1&apos;, &apos;c1-1&apos;\n\nput &apos;hive_hbase_test&apos;, &apos;1&apos;, &apos;test:c2&apos;, &apos;c2-1&apos;\n\nput &apos;hive_hbase_test&apos;, &apos;1&apos;, &apos;test:c3&apos;, &apos;c3-1&apos;\n\nput &apos;hive_hbase_test&apos;, &apos;2&apos;, &apos;test:c1&apos;, &apos;c1-2&apos;\n\n\n\nIn Hive:\n\nDROP TABLE IF EXISTS hive_hbase_test;\n\nCREATE EXTERNAL TABLE hive_hbase_test (\n\n  id int,\n\n  c1 string,\n\n  c2 string,\n\n  c3 string\n\n)\n\nSTORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;\n\nWITH SERDEPROPERTIES (\"hbase.columns.mapping\" =\n\n\":key#s,test:c1#s,test:c2#s,test:c3#s\")\n\nTBLPROPERTIES(\"hbase.table.name\" = \"hive_hbase_test\");\n\n\n\nhive> select * from hive_hbase_test;\n\nOK\n\n1\tc1-1\tc2-1\tc3-1\n\n2\tc1-2\tNULL\tNULL\n\n\n\nhive> select c1 from hive_hbase_test;\n\nc1-1\n\nc1-2\n\n\n\nhive> select c1, c2 from hive_hbase_test;\n\nc1-1\tc2-1\n\nc1-2\tNULL\n\n\n\nSo far everything is correct but now:\n\nhive> select c1, c2, c2 from hive_hbase_test;\n\nc1-1\tc2-1\tc2-1\n\nc1-2\tNULL\tc2-1\n\n\n\nSelecting c2 twice works the first time but the second time we\nactually get the value from the previous row.\n\nhive> select c1, c3, c2, c2, c3, c3, c1 from hive_hbase_test;\n\nc1-1\tc3-1\tc2-1\tc2-1\tc3-1\tc3-1\tc1-1\n\nc1-2\tNULL\tNULL\tc2-1\tc3-1\tc3-1\tc1-2\n\n\n\nWe&apos;ve narrowed this down to an early initialization of fieldsInited[fieldID] = true in LazyHBaseRow#uncheckedGetField and we&apos;ll try to provide a patch which surely needs review.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.11.0", "fixed_files": ["org.apache.hadoop.hive.hbase.TestLazyHBaseObject.java", "org.apache.hadoop.hive.hbase.LazyHBaseRow.java"], "label": 1, "es_results": []}, {"bug_id": 4222, "bug_title": "Timestamp type constants cannot be deserialized in JDK 1.6 or less", "bug_description": "For example,\n\nExprNodeConstantDesc constant = new ExprNodeConstantDesc(TypeInfoFactory.timestampTypeInfo, new Timestamp(100));\n\nString serialized = Utilities.serializeExpression(constant);\n\nExprNodeConstantDesc deserilized = (ExprNodeConstantDesc) Utilities.deserializeExpression(serialized, new Configuration());\n\n\n\nlogs error message\n\njava.lang.InstantiationException: java.sql.Timestamp\n\nContinuing ...\n\njava.lang.RuntimeException: failed to evaluate: <unbound>=Class.new();\n\nContinuing ...\n\n\n\nand makes NPE in final.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.12.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.TestUtilities.java", "org.apache.hadoop.hive.ql.exec.Utilities.java"], "label": 1, "es_results": []}, {"bug_id": 3756, "bug_title": "\"LOAD DATA\" does not honor permission inheritence", "bug_description": "When a \"LOAD DATA\" operation is performed the resulting data in hdfs for the table does not maintain permission inheritance. This remains true even with the \"hive.warehouse.subdir.inherit.perms\" set to true.\nThe issue is easily reproducible by creating a table and loading some data into it. After the load is complete just do a \"dfs -ls -R\" on the warehouse directory and you will see that the inheritance of permissions worked for the table directory but not for the data. ", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.12.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 3420, "bug_title": "Inefficiency in hbase handler when process query including rowkey range scan", "bug_description": "When query hive with hbase rowkey range, hive map tasks do not leverage startrow, endrow information in tablesplit. For example, if the rowkeys fit into 5 hbase files, then where will be 5 map tasks. Ideally, each task will process 1 file. But in current implementation, each task processes 5 files repeatedly. The behavior not only waste network bandwidth, but also worse the lock contention in HBase block cache as each task have to access the same block. The problem code is in HiveHBaseTableInputFormat.convertFilte as below:\n\n    if (tableSplit != null) \n{\n\n      tableSplit = new TableSplit(\n\n        tableSplit.getTableName(),\n\n        startRow,\n\n        stopRow,\n\n        tableSplit.getRegionLocation());\n\n    }\n    scan.setStartRow(startRow);\n    scan.setStopRow(stopRow);\n\nAs tableSplit already include startRow, endRow information of file, the better implementation will be:\n        \n        byte[] splitStart = startRow;\n        byte[] splitStop = stopRow;\n    if (tableSplit != null) {\n           if(tableSplit.getStartRow() != null)\n{\n\n                        splitStart = startRow.length == 0 ||\n\n          Bytes.compareTo(tableSplit.getStartRow(), startRow) >= 0 ?\n\n            tableSplit.getStartRow() : startRow;\n\n                }\n                if(tableSplit.getEndRow() != null)\n{\n\n                        splitStop = (stopRow.length == 0 ||\n\n          Bytes.compareTo(tableSplit.getEndRow(), stopRow) <= 0) &&\n\n          tableSplit.getEndRow().length > 0 ?\n\n            tableSplit.getEndRow() : stopRow;\n\n                }\n \n      tableSplit = new TableSplit(\n        tableSplit.getTableName(),\n        splitStart,\n        splitStop,\n        tableSplit.getRegionLocation());\n    }\n    scan.setStartRow(splitStart);\n    scan.setStopRow(splitStop);\n        \nIn my test, the changed code will improve performance more than 30%.", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 4247, "bug_title": "Filtering on a hbase row key duplicates results across multiple mappers", "bug_description": "Steps to reproduce\n1. Create a Hive external table with HiveHbaseHandler with enough data in the hbase table to spawn multiple mappers for the hive query.\n2. Write a query which has a filter (in the where clause) based on the hbase row key. \n3. Running the map reduce job leads to each mapper querying the entire data set.  duplicating the data for each mapper. Each mapper processes the entire filtered range and the results get multiplied as the number of mappers run.\nExpected behavior:\nEach mapper should process a different part of the data and should not duplicate.\nBecause:\nThe because seems to be the convertFilter method in HiveHBaseTableInputFormat. convertFilter has this piece of code which rewrites the start and the stop row for each split which leads each mapper to process the entire range\n if (tableSplit != null) \n{\n\n      tableSplit = new TableSplit(\n\n        tableSplit.getTableName(),\n\n        startRow,\n\n        stopRow,\n\n        tableSplit.getRegionLocation());\n\n    }\n\nThe scan already has the start and stop row set when the splits are created. So this piece of code is probably redundant.\n", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-0.13.0", "fixed_files": ["org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 9357, "bug_title": "Create ADD_MONTHS UDF", "bug_description": "ADD_MONTHS adds a number of months to startdate: \nadd_months(&apos;2015-01-14&apos;, 1) = &apos;2015-02-14&apos;\nadd_months(&apos;2015-01-31&apos;, 1) = &apos;2015-02-28&apos;\nadd_months(&apos;2015-02-28&apos;, 2) = &apos;2015-04-30&apos;\nadd_months(&apos;2015-02-28&apos;, 12) = &apos;2016-02-29&apos;", "project": "Apache", "sub_project": "HIVE", "version": "release-0.9.0", "fixed_version": "release-1.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FunctionRegistry.java"], "label": 1, "es_results": []}, {"bug_id": 10965, "bug_title": "direct SQL for stats fails in 0-column case", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "release-1.2.1", "fixed_files": ["org.apache.hadoop.hive.metastore.TestHiveMetaStore.java", "org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java", "org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java", "org.apache.hadoop.hive.ql.metadata.Hive.java", "org.apache.hadoop.hive.ql.stats.StatsUtils.java"], "label": 1, "es_results": []}, {"bug_id": 10048, "bug_title": "JDBC - Support SSL encryption regardless of Authentication mechanism", "bug_description": "JDBC driver currently only supports SSL Transport if the Authentication mechanism is SASL Plain with username and password. SSL transport  should be decoupled from Authentication mechanism. If the customer chooses to do Kerberos Authentication and SSL encryption over the wire it should be supported. The Server side already supports this but the driver does not.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hive.jdbc.HiveConnection.java"], "label": 1, "es_results": []}, {"bug_id": 12266, "bug_title": "When client exists abnormally, it does not release ACID locks", "bug_description": "if you start Hive CLI (locking enabled) and run some command that acquires locks and ^C the she will before command completes the locks for the command remain until they timeout.\nI believe Beeline has the same issue.\nNeed to add proper hooks to release locks when command dies. (As much as possible)", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.Driver.java"], "label": 1, "es_results": []}, {"bug_id": 12832, "bug_title": "RDBMS schema changes for HIVE-11388", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java", "org.apache.hadoop.hive.metastore.api.LockRequest.java", "org.apache.hive.beeline.HiveSchemaTool.java", "org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java", "org.apache.hadoop.hive.metastore.api.CheckLockRequest.java", "org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java", "org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java", "org.apache.hadoop.hive.metastore.api.TxnInfo.java"], "label": 1, "es_results": []}, {"bug_id": 12352, "bug_title": "CompactionTxnHandler.markCleaned() may delete too much", "bug_description": "   Worker will start with DB in state X (wrt this partition).\n   while it&apos;s working more txns will happen, against partition it&apos;s compacting.\n   then this will delete state up to X and since then.  There may be new delta files created\n   between compaction starting and cleaning.  These will not be compacted until more\n   transactions happen.  So this ideally should only delete\n   up to TXN_ID that was compacted (i.e. HWM in Worker?)  Then this can also run\n   at READ_COMMITTED.  So this means we&apos;d want to store HWM in COMPACTION_QUEUE when\n   Worker picks up the job.\nActually the problem is even worse (but also solved using HWM as above):\nSuppose some transactions (against same partition) have started and aborted since the time Worker ran compaction job.\nThat means there are never-compacted delta files with data that belongs to these aborted txns.\nFollowing will pick up these aborted txns.\ns = \"select txn_id from TXNS, TXN_COMPONENTS where txn_id = tc_txnid and txn_state = &apos;\" +\n          TXN_ABORTED + \"&apos; and tc_database = &apos;\" + info.dbname + \"&apos; and tc_table = &apos;\" +\n          info.tableName + \"&apos;\";\n        if (info.partName != null) s += \" and tc_partition = &apos;\" + info.partName + \"&apos;\";\nThe logic after that will delete relevant data from TXN_COMPONENTS and if one of these txns becomes empty, it will be picked up by cleanEmptyAbortedTxns().  At that point any metadata about an Aborted txn is gone and the system will think it&apos;s committed.\nHWM in this case would be (in ValidCompactorTxnList)\nif(minOpenTxn > 0)\nmin(highWaterMark, minOpenTxn) \nelse \nhighWaterMark", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.metastore.txn.ValidCompactorTxnList.java", "org.apache.hadoop.hive.ql.txn.compactor.Initiator.java", "org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java", "org.apache.hadoop.hive.ql.Driver.java", "org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java", "org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java", "org.apache.hadoop.hive.common.ValidTxnList.java", "org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hadoop.hive.metastore.txn.CompactionInfo.java", "org.apache.hadoop.hive.ql.txn.compactor.Worker.java"], "label": 1, "es_results": []}, {"bug_id": 13013, "bug_title": "Further Improve concurrency in TxnHandler", "bug_description": "There are still a few operations in TxnHandler that run at Serializable isolation.\nMost or all of them can be dropped to READ_COMMITTED now that we have SELECT ... FOR UPDATE support.  This will reduce number of deadlocks in the DBs.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.metastore.txn.TestTxnHandlerNegative.java", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java", "org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java", "org.apache.hadoop.hive.metastore.txn.TxnHandler.java"], "label": 1, "es_results": []}, {"bug_id": 12749, "bug_title": "Constant propagate returns string values in incorrect format", "bug_description": "STEP 1. Create and upload test data\nExecute in command line:\n\nnano stest.data\n\n\n\nAdd to file:\n\n000126,000777\n\n000126,000778\n\n000126,000779\n\n000474,000888\n\n000468,000889\n\n000272,000880\n\n\n\n\nhadoop fs -put stest.data /\n\n\n\n\nhive> create table stest(x STRING, y STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;;\n\nhive> LOAD DATA  INPATH &apos;/stest.data&apos; OVERWRITE INTO TABLE stest;\n\n\n\nSTEP 2. Execute test query (with cast for x)\n\nselect x from stest where cast(x as int) = 126;\n\n\n\nEXPECTED RESULT:\n\n000126\n\n000126\n\n000126\n\n\n\nACTUAL RESULT:\n\n126\n\n126\n\n126\n\n\n\nSTEP 3. Execute test query (no cast for x)\n\nhive> select x from stest where  x = 126; \n\n\n\nEXPECTED RESULT:\n\n000126\n\n000126\n\n000126\n\n\n\nACTUAL RESULT:\n\n126\n\n126\n\n126\n\n\n\nIn steps #2, #3 I expected &apos;000126&apos; because the origin type of x is STRING in stest table.\nNote, setting hive.optimize.constant.propagation=false fixes the issue.\n\nhive> set hive.optimize.constant.propagation=false;\n\nhive> select x from stest where  x = 126;\n\nOK\n\n000126\n\n000126\n\n000126\n\n\n\nRelated to HIVE-11104, HIVE-8555", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 13200, "bug_title": "Aggregation functions returning empty rows on partitioned columns", "bug_description": "Running aggregation functions like MAX, MIN, DISTINCT against partitioned columns will return empty rows if table has property: &apos;skip.header.line.count&apos;=&apos;1&apos;\nReproduce:\n\nDROP TABLE IF EXISTS test;\n\n\n\nCREATE TABLE test (a int) \n\nPARTITIONED BY (b int) \n\nROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;|&apos; \n\nTBLPROPERTIES(&apos;skip.header.line.count&apos;=&apos;1&apos;);\n\n\n\nINSERT OVERWRITE TABLE test PARTITION (b = 1) VALUES (1), (2), (3), (4);\n\nINSERT OVERWRITE TABLE test PARTITION (b = 2) VALUES (1), (2), (3), (4);\n\n\n\nSELECT * FROM test;\n\n\n\nSELECT DISTINCT b FROM test;\n\nSELECT MAX(b) FROM test;\n\nSELECT DISTINCT a FROM test;\n\n\n\nThe output:\n\n0: jdbc:hive2://localhost:10000/default> SELECT * FROM test;\n\n+---------+---------+--+\n\n| test.a  | test.b  |\n\n+---------+---------+--+\n\n| 2       | 1       |\n\n| 3       | 1       |\n\n| 4       | 1       |\n\n| 2       | 2       |\n\n| 3       | 2       |\n\n| 4       | 2       |\n\n+---------+---------+--+\n\n6 rows selected (0.631 seconds)\n\n\n\n0: jdbc:hive2://localhost:10000/default> SELECT DISTINCT b FROM test;\n\n+----+--+\n\n| b  |\n\n+----+--+\n\n+----+--+\n\nNo rows selected (47.229 seconds)\n\n\n\n0: jdbc:hive2://localhost:10000/default> SELECT MAX(b) FROM test;\n\n+-------+--+\n\n|  _c0  |\n\n+-------+--+\n\n| NULL  |\n\n+-------+--+\n\n1 row selected (49.508 seconds)\n\n\n\n0: jdbc:hive2://localhost:10000/default> SELECT DISTINCT a FROM test;\n\n+----+--+\n\n| a  |\n\n+----+--+\n\n| 2  |\n\n| 3  |\n\n| 4  |\n\n+----+--+\n\n3 rows selected (46.859 seconds)\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java", "org.apache.hadoop.hive.ql.plan.TableScanDesc.java"], "label": 1, "es_results": []}, {"bug_id": 13381, "bug_title": "Timestamp & date should have precedence in type hierarchy than string group", "bug_description": "Both sql server & oracle treats date/timestamp higher in hierarchy than varchars", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FunctionRegistry.java", "org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java", "org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java", "org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java"], "label": 1, "es_results": []}, {"bug_id": 13373, "bug_title": "Use most specific type for numerical constants", "bug_description": "tinyint & shortint are currently inferred as ints, if they are without postfix.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 13646, "bug_title": "make hive.optimize.sort.dynamic.partition compatible with ACID tables", "bug_description": "HIVE-8875 disabled hive.optimize.sort.dynamic.partition for ACID queries.\ndynamic inserts are common in ACID and this leaves users with few options if they are seeing OutOfMemory errors due to too many writers.\nhive.optimize.sort.dynamic.partition sorts data by partition col/bucket col/sort col to ensure each reducer only needs 1 writer.\nAcid requires data in each bucket file to be sorted by ROW__ID and thus doesn&apos;t allow end user to determine sorting.\nSo we should be able to support hive.optimize.sort.dynamic.partition with\nsort on partition col/bucket col/ROW__ID ", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 13932, "bug_title": "Hive SMB Map Join with small set of LIMIT failed with NPE", "bug_description": "1) prepare sample data:\na=1\nwhile [[ $a -lt 100 ]]; do echo $a ; let a=$a+1; done > data\n2) prepare source hive table:\nCREATE TABLE `s`(`c` string);\nload data local inpath &apos;data&apos; into table s;\n3) prepare the bucketed table:\nset hive.enforce.bucketing=true;\nset hive.enforce.sorting=true;\nCREATE TABLE `t`(`c` string) CLUSTERED BY (c) SORTED BY (c) INTO 5 BUCKETS;\ninsert into t select * from s;\n4) reproduce this issue:\nSET hive.auto.convert.sortmerge.join = true;\nSET hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;\nSET hive.auto.convert.sortmerge.join.noconditionaltask = true;\nSET hive.optimize.bucketmapjoin = true;\nSET hive.optimize.bucketmapjoin.sortedmerge = true;\nselect * from t join t t1 on t.c=t1.c limit 1;", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java"], "label": 1, "es_results": []}, {"bug_id": 14114, "bug_title": "Ensure RecordWriter in streaming API is using the same UserGroupInformation as StreamingConnection", "bug_description": "currently both DelimitedInputWriter and StrictJsonWriter perform some Metastore access operations but without using UGI created by the caller for Metastore operations made by matching StreamingConnection & TransactionBatch", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java", "org.apache.hive.hcatalog.streaming.HiveEndPoint.java", "org.apache.hive.hcatalog.streaming.StreamingConnection.java", "org.apache.hive.hcatalog.streaming.StrictJsonWriter.java", "org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java", "org.apache.hive.hcatalog.streaming.TestStreaming.java"], "label": 1, "es_results": []}, {"bug_id": 12204, "bug_title": "Tez queries stopped running with ApplicationNotRunningException", "bug_description": "In some error cases, if hive can no longer submit DAGs to tez, there is no use retrying to submit. We need to exit by throwing exception in this case.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java", "org.apache.hadoop.hive.ql.exec.tez.TezTask.java", "org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java"], "label": 1, "es_results": []}, {"bug_id": 12437, "bug_title": "SMB join in tez fails when one of the tables is empty", "bug_description": "It looks like a better check for empty tables is to depend on the existence of the record reader for the input from tez. ", "project": "Apache", "sub_project": "HIVE", "version": "release-1.0.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java", "org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java"], "label": 1, "es_results": []}, {"bug_id": 8485, "bug_title": "HMS on Oracle incompatibility", "bug_description": "Oracle does not distinguish between empty strings and NULL,which proves problematic for DataNucleus.\nIn the event a user creates a table with some property stored as an empty string the table will no longer be accessible.\ni.e. TBLPROPERTIES (&apos;serialization.null.format&apos;=&apos;&apos;)\nIf they try to select, describe, drop, etc the client prints the following exception.\nERROR ql.Driver: FAILED: SemanticException [Error 10001]: Table not found <table name>\nThe work around for this was to go into the hive metastore on the Oracle database and replace NULL with some other string. Users could then drop the tables or alter their data to use the new null format they just set.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "release-1.0.0", "fixed_files": ["org.apache.hadoop.hive.metastore.ObjectStore.java", "org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java", "org.apache.hadoop.hive.metastore.MetaStoreUtils.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 10996, "bug_title": "Aggregation / Projection over Multi-Join Inner Query producing incorrect results", "bug_description": "We see the following problem on 1.1.0 and 1.2.0 but not 0.13 which seems like a regression.\nThe following query (Q1) produces no results:\n\n\n\nselect s\n\nfrom (\n\n  select last.*, action.st2, action.n\n\n  from (\n\n    select purchase.s, purchase.timestamp, max (mevt.timestamp) as last_stage_timestamp\n\n    from (select * from purchase_history) purchase\n\n    join (select * from cart_history) mevt\n\n    on purchase.s = mevt.s\n\n    where purchase.timestamp > mevt.timestamp\n\n    group by purchase.s, purchase.timestamp\n\n  ) last\n\n  join (select * from events) action\n\n  on last.s = action.s and last.last_stage_timestamp = action.timestamp\n\n) list;\n\n\n\n\n\nWhile this one (Q2) does produce results :\n\n\n\nselect *\n\nfrom (\n\n  select last.*, action.st2, action.n\n\n  from (\n\n    select purchase.s, purchase.timestamp, max (mevt.timestamp) as last_stage_timestamp\n\n    from (select * from purchase_history) purchase\n\n    join (select * from cart_history) mevt\n\n    on purchase.s = mevt.s\n\n    where purchase.timestamp > mevt.timestamp\n\n    group by purchase.s, purchase.timestamp\n\n  ) last\n\n  join (select * from events) action\n\n  on last.s = action.s and last.last_stage_timestamp = action.timestamp\n\n) list;\n\n1\t21\t20\tBob\t1234\n\n1\t31\t30\tBob\t1234\n\n3\t51\t50\tJeff\t1234\n\n\n\n\n\nThe setup to test this is:\n\n\n\ncreate table purchase_history (s string, product string, price double, timestamp int);\n\ninsert into purchase_history values (&apos;1&apos;, &apos;Belt&apos;, 20.00, 21);\n\ninsert into purchase_history values (&apos;1&apos;, &apos;Socks&apos;, 3.50, 31);\n\ninsert into purchase_history values (&apos;3&apos;, &apos;Belt&apos;, 20.00, 51);\n\ninsert into purchase_history values (&apos;4&apos;, &apos;Shirt&apos;, 15.50, 59);\n\n\n\ncreate table cart_history (s string, cart_id int, timestamp int);\n\ninsert into cart_history values (&apos;1&apos;, 1, 10);\n\ninsert into cart_history values (&apos;1&apos;, 2, 20);\n\ninsert into cart_history values (&apos;1&apos;, 3, 30);\n\ninsert into cart_history values (&apos;1&apos;, 4, 40);\n\ninsert into cart_history values (&apos;3&apos;, 5, 50);\n\ninsert into cart_history values (&apos;4&apos;, 6, 60);\n\n\n\ncreate table events (s string, st2 string, n int, timestamp int);\n\ninsert into events values (&apos;1&apos;, &apos;Bob&apos;, 1234, 20);\n\ninsert into events values (&apos;1&apos;, &apos;Bob&apos;, 1234, 30);\n\ninsert into events values (&apos;1&apos;, &apos;Bob&apos;, 1234, 25);\n\ninsert into events values (&apos;2&apos;, &apos;Sam&apos;, 1234, 30);\n\ninsert into events values (&apos;3&apos;, &apos;Jeff&apos;, 1234, 50);\n\ninsert into events values (&apos;4&apos;, &apos;Ted&apos;, 1234, 60);\n\n\n\n\n\nI realize select * and select s are not all that interesting in this context but what lead us to this issue was select count(distinct s) was not returning results. The above queries are the simplified queries that produce the issue. \nI will note that if I convert the inner join to a table and select from that the issue does not appear.\nUpdate: Found that turning off  hive.optimize.remove.identity.project fixes this issue. This optimization was introduced in https://issues.apache.org/jira/browse/HIVE-8435", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "release-1.1.1", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java", "org.apache.hadoop.hive.ql.exec.ColumnInfo.java", "org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 12250, "bug_title": "Zookeeper connection leaks in Hive's HBaseHandler.", "bug_description": "HiveServer2 performance regresses severely due to what appears to be a leak in the ZooKeeper connections. lsof output on the HS2 process shows about 8000 TCP connections to the ZK ensemble nodes.\ngrep TCP lsof-hive-node11 | grep node11 | grep -E \"node03|node04|node05\" | wc -l\n    7866 \ngrep TCP lsof-hive-node11 | grep node11 | grep -E \"node03\" | wc -l\n    2615\ngrep TCP lsof-hive-node11 | grep node11 | grep -E \"node04\" | wc -l\n    2622\ngrep TCP lsof-hive-node11 | grep node11 | grep -E \"node05\" | wc -l\n    2629\nnode11 - HMS node\nnode03, node04 and node05 are the hosts for zookeeper ensemble.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java", "org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 12418, "bug_title": "HiveHBaseTableInputFormat.getRecordReader() causes Zookeeper connection leak.", "bug_description": "  @Override\n  public RecordReader<ImmutableBytesWritable, ResultWritable> getRecordReader(\n...\n...\n setHTable(HiveHBaseInputFormatUtil.getTable(jobConf));\n...\nThe HiveHBaseInputFormatUtil.getTable() creates new ZooKeeper connections(when HTable instance is created) which are never closed.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 12568, "bug_title": "Provide an option to specify network interface used by Spark remote client [Spark Branch]", "bug_description": "Spark client sends a pair of host name and port number to the remote driver so that the driver can connects back to HS2 where the user session is. Spark client has its own way determining the host name, and pick one network interface if the host happens to have multiple network interfaces. This can be problematic. For that, there is parameter, hive.spark.client.server.address, which user can pick an interface. Unfortunately, this interface isn&apos;t exposed.\nInstead of exposing this parameter, we can use the same logic as Hive in determining the host name. Therefore, the remote driver connecting to HS2 using the same network interface as a HS2 client would do.\nThere might be a case where user may want the remote driver to use a different network. This is rare if at all. Thus, for now it should be sufficient to use the same network interface.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hive.service.cli.thrift.ThriftCLIService.java", "org.apache.hive.spark.client.rpc.RpcConfiguration.java", "org.apache.hadoop.hive.common.ServerUtils.java"], "label": 1, "es_results": []}, {"bug_id": 12708, "bug_title": "Hive on Spark does not work with Kerboresed HBase [Spark Branch]", "bug_description": "Spark application launcher (spark-submit) acquires HBase delegation token on Hive user&apos;s behalf when the application is launched. This mechanism, which doesn&apos;t work for long-running sessions, is not in line with what Hive is doing. Hive actually acquires the token automatically whenever a job needs it. The right approach for Spark should be allowing applications to dynamically add whatever tokens they need to the spark context. While this needs work on Spark side, we provide a workaround solution in Hive.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java"], "label": 1, "es_results": []}, {"bug_id": 12790, "bug_title": "Metastore connection leaks in HiveServer2", "bug_description": "HiveServer2 keeps opening new connections to HMS each time it launches a task. These connections do not appear to be closed when the task completes thus causing a HMS connection leak. \"lsof\" for the HS2 process shows connections to port 9083.\n\n\n\n2015-12-03 04:20:56,352 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 11 out of 41\n\n2015-12-03 04:20:56,354 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://<anonymizedURL>:9083\n\n2015-12-03 04:20:56,360 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14824\n\n2015-12-03 04:20:56,360 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.\n\n....\n\n2015-12-03 04:21:06,355 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 12 out of 41\n\n2015-12-03 04:21:06,357 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://<anonymizedURL>:9083\n\n2015-12-03 04:21:06,362 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14825\n\n2015-12-03 04:21:06,362 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.\n\n...\n\n2015-12-03 04:21:08,357 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 13 out of 41\n\n2015-12-03 04:21:08,360 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://<anonymizedURL>:9083\n\n2015-12-03 04:21:08,364 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14826\n\n2015-12-03 04:21:08,365 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.\n\n... \n\n\n\nThe TaskRunner thread starts a new SessionState each time, which creates a new connection to the HMS (via Hive.get(conf).getMSC()) that is never closed.\nEven SessionState.close(), currently not being called by the TaskRunner thread, does not close this connection.\nAttaching a anonymized log snippet where the number of HMS connections reaches north of 25000+ connections.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.session.SessionState.java", "org.apache.hadoop.hive.ql.exec.TaskRunner.java"], "label": 1, "es_results": []}, {"bug_id": 12885, "bug_title": "LDAP Authenticator improvements", "bug_description": "Currently Hive&apos;s LDAP Atn provider assumes certain defaults to keep its configuration simple. \n1) One of the assumptions is the presence of an attribute \"distinguishedName\". In certain non-standard LDAP implementations, this attribute may not be available. So instead of basing all ldap searches on this attribute, getNameInNamespace() returns the same value. So this API is to be used instead.\n2) It also assumes that the \"user\" value being passed in, will be able to bind to LDAP. However, certain LDAP implementations, by default, only allow the full DN to be used, just short user names are not permitted. We will need to be able to support short names too when hive configuration only has \"BaseDN\" specified (not userDNPatterns). So instead of hard-coding \"uid\" or \"CN\" as keys for the short usernames, it probably better to make this a configurable parameter.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java"], "label": 1, "es_results": []}, {"bug_id": 12941, "bug_title": "Unexpected result when using MIN() on struct with NULL in first field", "bug_description": "Using MIN() on struct with NULL in first field of a row yields NULL as result.\nExample:\nselect min(a) FROM (select 1 as a union all select 2 as a union all select cast(null as int) as a) tmp;\nOK\n_c0\n1\nAs expected. But if we wrap it in a struct:\nselect min(a) FROM (select named_struct(\"field\",1) as a union all select named_struct(\"field\",2) as a union all select named_struct(\"field\",cast(null as int)) as a) tmp;\nOK\n_c0\nNULL\nUsing MAX() works as expected for structs.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java"], "label": 1, "es_results": []}, {"bug_id": 12619, "bug_title": "(Parquet) Switching the field order within an array of structs causes the query to fail", "bug_description": "Switching the field order within an array of structs causes the query to fail or return the wrong data for the fields, but switching the field order within just a struct works.\nHow to reproduce:\nCase1 if the two fields have the same type, query will return wrong data for the fields\ndrop table if exists schema_test;\ncreate table schema_test (message array<struct<f1: string, f2: string>>) stored as parquet;\ninsert into table schema_test select stack(2, array(named_struct(&apos;f1&apos;, &apos;abc&apos;, &apos;f2&apos;, &apos;abc2&apos;)), array(named_struct(&apos;f1&apos;, &apos;efg&apos;, &apos;f2&apos;, &apos;efg2&apos;))) from one limit 2;\nselect * from schema_test;\n--returns\n--[\n{\"f1\":\"efg\",\"f2\":\"efg2\"}\n]\n--[\n{\"f1\":\"abc\",\"f2\":\"abc2\"}\n]\nalter table schema_test change message message array<struct<f2: string, f1: string>>;\nselect * from schema_test;\n--returns\n--[\n{\"f2\":\"efg\",\"f1\":\"efg2\"}\n]\n--[\n{\"f2\":\"abc\",\"f1\":\"abc2\"}\n]\nCase2: if the two fields have different type, the query will fail\ndrop table if exists schema_test;\ncreate table schema_test (message array<struct<f1: string, f2: int>>) stored as parquet;\ninsert into table schema_test select stack(2, array(named_struct(&apos;f1&apos;, &apos;abc&apos;, &apos;f2&apos;, 1)), array(named_struct(&apos;f1&apos;, &apos;efg&apos;, &apos;f2&apos;, 2))) from one limit 2;\nselect * from schema_test;\n--returns\n--[\n{\"f1\":\"efg\",\"f2\":2}\n]\n--[\n{\"f1\":\"abc\",\"f2\":1}\n]\nalter table schema_test change message message array<struct<f2: int, f1: string>>;\nselect * from schema_test;\nFailed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to org.apache.hadoop.io.IntWritable", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.java", "org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java", "org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.java"], "label": 1, "es_results": []}, {"bug_id": 13527, "bug_title": "Using deprecated APIs in HBase client causes zookeeper connection leaks.", "bug_description": "When running queries against hbase-backed hive tables, the following log messages are seen in the HS2 log.\n\n\n\n2016-04-11 07:25:23,657 WARN org.apache.hadoop.hbase.mapreduce.TableInputFormatBase: You are using an HTable instance that relies on an HBase-managed Connection. This is usually due to directly creating an HTable, which is deprecated. Instead, you should create a Connection object and then request a Table instance from it. If you don&apos;t need the Table instance for your own use, you should instead use the TableInputFormatBase.initalizeTable method directly.\n\n2016-04-11 07:25:23,658 INFO org.apache.hadoop.hbase.mapreduce.TableInputFormatBase: Creating an additional unmanaged connection because user provided one can&apos;t be used for administrative actions. We&apos;ll close it when we close out the table.\n\n\n\nIn a HS2 log file, there are 1366 zookeeper connections established but only a small fraction of them were closed. So lsof would show 1300+ open TCP connections to Zookeeper.\ngrep \"org.apache.zookeeper.ClientCnxn: Session establishment complete on server\" * |wc -l\n1366\ngrep \"INFO org.apache.zookeeper.ZooKeeper: Session:\" * |grep closed |wc -l\n54\nAccording to the comments in TableInputFormatBase, the recommended means for subclasses like HiveHBaseTableInputFormat is to call initializeTable() instead of setHTable() that it currently uses.\n\"\nSubclasses MUST ensure initializeTable(Connection, TableName) is called for an instance to function properly. Each of the entry points to this class used by the MapReduce framework, \n{@link #createRecordReader(InputSplit, TaskAttemptContext)}\n and \n{@link #getSplits(JobContext)}\n, will call \n{@link #initialize(JobContext)}\n as a convenient centralized location to handle retrieving the necessary configuration information. If your subclass overrides either of these methods, either call the parent version or call initialize yourself.\n\"\nCurrently setHTable() also creates an additional Admin connection, even though it is not needed.\nSo the use of deprecated APIs are to be replaced.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 13502, "bug_title": "Beeline does not support session parameters in JDBC URL as documentation states.", "bug_description": "https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-ConnectionURLs\ndocuments that sessions variables like credentials etc are accepted as part of the URL. However, Beeline does not support such URLs today.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.beeline.BeeLine.java", "org.apache.hive.beeline.TestBeeLineWithArgs.java", "org.apache.hive.beeline.Commands.java", "org.apache.hive.jdbc.Utils.java"], "label": 1, "es_results": []}, {"bug_id": 13749, "bug_title": "Memory leak in Hive Metastore", "bug_description": "Looking a heap dump of 10GB, a large number of Configuration objects(> 66k instances) are being retained. These objects along with its retained set is occupying about 95% of the heap space. This leads to HMS crashes every few days.\nI will attach an exported snapshot from the eclipse MAT.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.metastore.HiveMetaStore.java"], "label": 1, "es_results": []}, {"bug_id": 14116, "bug_title": "TBLPROPERTIES does not allow empty string values when Metastore is backed by Oracle database.", "bug_description": "DDL commands like:\nALTER TABLE test SET TBLPROPERTIES(&apos;serialization.null.format&apos;=&apos;&apos;);\nare silently ignored if the database backing Metastore is Oracle. This appears to be because Oracle treats an empty string as null.\nUnlike when using MySql, no entry is created in the TBL_PARAMS table.\nSteps to reproduce:\nCreate a table with a string field.\neg table mytable, field mystringfield.\nALTER TABLE mytable SET TBLPROPERTIES(&apos;serialization.null.format&apos;=&apos;&apos;);\nDESCRIBE FORMATTED mytable;\nwith mysql backed Metastore, the entry will be displayed:\nserialization.null.format \nand an entry is created in the TBL_PARAMS for the parameter.\nWith Oracle backed metastore, it is not, and no entry is created in TBL_PARAMS.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "release-1.0.0", "fixed_files": ["org.apache.hadoop.hive.metastore.ObjectStore.java", "org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java", "org.apache.hadoop.hive.metastore.MetaStoreUtils.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 13422, "bug_title": "Analyse command not working for column having datatype as decimal(38,0)", "bug_description": "For the repro\n\n\n\ndrop table sample_test;\n\nCREATE TABLE IF NOT EXISTS sample_test( key decimal(38,0),b int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE;\n\nload data local inpath &apos;/home/hive/analyse.txt&apos; into table sample_test;\n\nANALYZE TABLE sample_test COMPUTE STATISTICS FOR COLUMNS;\n\n\n\nSample data\n\n\n\n20234567894567498250824983000004 0\n\n50320807548878498250695083000004 0\n\n40120807548878498250687183000004 0\n\n20120807548878498250667783000004 0\n\n40120807548878496250656783000004 0\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java"], "label": 1, "es_results": []}, {"bug_id": 14345, "bug_title": "Beeline result table has erroneous characters ", "bug_description": "Beeline returns query results with erroneous characters. For example:\n\n\n\n0: jdbc:hive2://xxxx:10000/def> select 10;\n\n+------+--+\n\n| _c0  |\n\n+------+--+\n\n| 10   |\n\n+------+--+\n\n1 row selected (3.207 seconds)\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.beeline.TableOutputFormat.java"], "label": 1, "es_results": []}, {"bug_id": 13610, "bug_title": "Hive exec module will not compile with IBM JDK", "bug_description": "org.apache.hadoop.hive.ql.debug.Utils explicitly import com.sun.management.HotSpotDiagnosticMXBean which is not supported by IBM JDK.\nSo we can make HotSpotDiagnosticMXBean as runtime but not compile.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.debug.Utils.java"], "label": 1, "es_results": []}, {"bug_id": 15061, "bug_title": "Metastore types are sometimes case sensitive", "bug_description": "Impala recently encountered an issue with the metastore (IMPALA-4260 ) where column stats would get dropped when adding a column to a table.\nThe reason seems to be that Hive does a case sensitive check on the column stats types during an \"alter table\" and expects the types to be all lower case. This case sensitive check doesn&apos;t appear to happen when the stats are set in the first place.\nWe&apos;re solving this on the Impala end by storing types in the metastore as all lower case, but Hive&apos;s behavior here is very confusing. It should either always be case sensitive, so that you can&apos;t create column stats with types that Hive considers invalid, or it should never be case sensitive.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.metastore.HiveMetaStore.java", "org.apache.hadoop.hive.metastore.HiveAlterHandler.java"], "label": 1, "es_results": []}, {"bug_id": 11208, "bug_title": "Can not drop a default partition __HIVE_DEFAULT_PARTITION__ which is not a \"string\" type", "bug_description": "When partition is not a string type, for example, if it is a int type, when drop the default partition _HIVE_DEFAULT_PARTITION_, you will get:\nSemanticException Unexpected unknown partitions\nReproduce:\n\nSET hive.exec.dynamic.partition=true;\n\nSET hive.exec.dynamic.partition.mode=nonstrict;\n\nset hive.exec.max.dynamic.partitions.pernode=10000;\n\n\n\nDROP TABLE IF EXISTS test;\n\nCREATE TABLE test (col1 string) PARTITIONED BY (p1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\\001&apos; STORED AS TEXTFILE;\n\nINSERT OVERWRITE TABLE test PARTITION (p1) SELECT code, IF(salary > 600, 100, null) as p1 FROM jsmall;\n\n\n\nhive> SHOW PARTITIONS test;\n\nOK\n\np1=100\n\np1=__HIVE_DEFAULT_PARTITION__\n\nTime taken: 0.124 seconds, Fetched: 2 row(s)\n\n\n\nhive> ALTER TABLE test DROP partition (p1 = &apos;__HIVE_DEFAULT_PARTITION__&apos;);\n\nFAILED: SemanticException Unexpected unknown partitions for (p1 = null)\n\n\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java", "org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java", "org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java"], "label": 1, "es_results": []}, {"bug_id": 12506, "bug_title": "SHOW CREATE TABLE command creates a table that does not work for RCFile format", "bug_description": "See the following test case:\n1) Create a table with RCFile format:\n\n\n\nDROP TABLE IF EXISTS test;\n\nCREATE TABLE test (a int) PARTITIONED BY (p int)\n\nROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;|&apos; \n\nSTORED AS RCFILE;\n\n\n\n2) run \"DESC FORMATTED test\"\n\n\n\n# Storage Information\n\nSerDe Library:      \torg.apache.hadoop.hive.serde2.columnar.ColumnarSerDe\n\nInputFormat:        \torg.apache.hadoop.hive.ql.io.RCFileInputFormat\n\nOutputFormat:       \torg.apache.hadoop.hive.ql.io.RCFileOutputFormat\n\n\n\nshows that SerDe used is \"ColumnarSerDe\"\n3) run \"SHOW CREATE TABLE\" and get the output:\n\n\n\nCREATE TABLE `test`(\n\n  `a` int)\n\nPARTITIONED BY (\n\n  `p` int)\n\nROW FORMAT DELIMITED\n\n  FIELDS TERMINATED BY &apos;|&apos;\n\nSTORED AS INPUTFORMAT\n\n  &apos;org.apache.hadoop.hive.ql.io.RCFileInputFormat&apos;\n\nOUTPUTFORMAT\n\n  &apos;org.apache.hadoop.hive.ql.io.RCFileOutputFormat&apos;\n\nLOCATION\n\n  &apos;hdfs://node5.lab.cloudera.com:8020/user/hive/warehouse/case_78732.db/test&apos;\n\nTBLPROPERTIES (\n\n  &apos;transient_lastDdlTime&apos;=&apos;1448343875&apos;)\n\n\n\nNote that there is no mention of \"ColumnarSerDe\"\n4) Drop the table and then create the table again using the output from 3)\n5) Check the output of \"DESC FORMATTED test\"\n\n\n\n# Storage Information\n\nSerDe Library:      \torg.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n\nInputFormat:        \torg.apache.hadoop.hive.ql.io.RCFileInputFormat\n\nOutputFormat:       \torg.apache.hadoop.hive.ql.io.RCFileOutputFormat\n\n\n\nThe SerDe falls back to \"LazySimpleSerDe\", which is not correct.\nAny further query tries to INSERT or SELECT this table will fail with errors\nI suspect that we can&apos;t specify ROW FORMAT DELIMITED with ROW FORMAT SERDE at the same time at table creation, this causes confusion to end users as copy table structure using \"SHOW CREATE TABLE\" will not work.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.metastore.MetaStoreUtils.java", "org.apache.hadoop.hive.ql.exec.DDLTask.java"], "label": 1, "es_results": []}, {"bug_id": 12664, "bug_title": "Bug in reduce deduplication optimization causing ArrayOutOfBoundException", "bug_description": "The optimisation check for reduce deduplication only checks the first child node for join and the check itself also contains a major bug causing ArrayOutOfBoundException no matter what.\nSample data table form:\n\n\ntime\nuser\nhost\npath\nreferer\ncode\nagent\nsize\nmethod\n\n\nint\nstring\nstring\nstring\nstring\nbigint\nstring\nbigint\nstring\n\n\nSample query\n\n\n\nSELECT \n\n  t1.host,\n\n  COUNT(DISTINCT t1.`date`) AS login_count,\n\n  MAX(t2.code) AS code,\n\n  unix_timestamp() AS time\n\nFROM (\n\n    SELECT \n\n      HOST,\n\n      MIN(time) AS DATE\n\n    FROM\n\n      www_access\n\n    WHERE\n\n      HOST IS NOT NULL\n\n    GROUP BY\n\n      HOST\n\n  ) t1\n\nJOIN (\n\n    SELECT \n\n      HOST,\n\n      MIN(time) AS code\n\n    FROM\n\n      www_access\n\n    WHERE\n\n      HOST IS NOT NULL\n\n    GROUP BY\n\n      HOST\n\n  ) t2\n\n  ON t1.host = t2.host\n\nGROUP BY\n\n  t1.host\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java"], "label": 1, "es_results": []}, {"bug_id": 12788, "bug_title": "Setting hive.optimize.union.remove to TRUE will break UNION ALL with aggregate functions", "bug_description": "See the test case below:\n\n\n\n0: jdbc:hive2://localhost:10000/default> create table test (a int);\n\n\n\n0: jdbc:hive2://localhost:10000/default> insert overwrite table test values (1);\n\n\n\n0: jdbc:hive2://localhost:10000/default> set hive.optimize.union.remove=true;\n\nNo rows affected (0.01 seconds)\n\n\n\n0: jdbc:hive2://localhost:10000/default> set hive.mapred.supports.subdirectories=true;\n\nNo rows affected (0.007 seconds)\n\n\n\n0: jdbc:hive2://localhost:10000/default> SELECT COUNT(1) FROM test UNION ALL SELECT COUNT(1) FROM test;\n\n+----------+--+\n\n| _u1._c0  |\n\n+----------+--+\n\n+----------+--+\n\n\n\nUNION ALL without COUNT function will work as expected:\n\n\n\n0: jdbc:hive2://localhost:10000/default> select * from test UNION ALL SELECT * FROM test;\n\n+--------+--+\n\n| _u1.a  |\n\n+--------+--+\n\n| 1      |\n\n| 1      |\n\n+--------+--+\n\n\n\nRun the same query without setting hive.mapred.supports.subdirectories and hive.optimize.union.remove to true will give correct result:\n\n\n\n0: jdbc:hive2://localhost:10000/default> set hive.optimize.union.remove;\n\n+-----------------------------------+--+\n\n|                set                |\n\n+-----------------------------------+--+\n\n| hive.optimize.union.remove=false  |\n\n+-----------------------------------+--+\n\n\n\n0: jdbc:hive2://localhost:10000/default> SELECT COUNT(1) FROM test UNION ALL SELECT COUNT(1) FROM test;\n\n+----------+--+\n\n| _u1._c0  |\n\n+----------+--+\n\n| 1        |\n\n| 1        |\n\n+----------+--+\n\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java"], "label": 1, "es_results": []}, {"bug_id": 12650, "bug_title": "Improve error messages for Hive on Spark in case the cluster has no resources available", "bug_description": "I think hive.spark.client.server.connect.timeout should be set greater than spark.yarn.am.waitTime. The default value for \nspark.yarn.am.waitTime is 100s, and the default value for hive.spark.client.server.connect.timeout is 90s, which is not good. We can increase it to a larger value such as 120s.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.spark.client.SparkClientImpl.java", "org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java", "org.apache.hadoop.hive.ql.exec.spark.SparkTask.java", "org.apache.hadoop.hive.ql.exec.spark.status.LocalSparkJobMonitor.java", "org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java"], "label": 1, "es_results": []}, {"bug_id": 13883, "bug_title": "WebHCat leaves token crc file never gets deleted", "bug_description": "In one of our long run environment, there are thousands of /tmp/.templeton*.tmp.crc files, \n\nomm@szxciitslx17645:/> ll /tmp/.templeton*.tmp.crc \n...\nrw-rr- 1 omm  wheel 12 May 26 18:15 /tmp/.templeton6676048390600607654.tmp.crc\nrw-rr- 1 omm  wheel 12 May 26 18:14 /tmp/.templeton2733383617337556503.tmp.crc\nrw-rr- 1 omm  wheel 12 May 26 18:12 /tmp/.templeton2183121761801669064.tmp.crc\nrw-rr- 1 omm  wheel 12 May 26 18:11 /tmp/.templeton2689764046140543879.tmp.crc\n...\n\nomm@szxciitslx17645:/> ll /tmp/.templeton*.tmp.crc  | wc -l\n17986\nIt&apos;s created by webhcat, https://github.com/apache/hive/blob/master/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/SecureProxySupport.java#L193  and never gets deleted https://github.com/apache/hive/blob/master/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/SecureProxySupport.java#L110", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.hcatalog.templeton.SecureProxySupport.java"], "label": 1, "es_results": []}, {"bug_id": 14964, "bug_title": "Failing Test: Fix TestBeelineArgParsing tests", "bug_description": "Failing last several builds:\n\n org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0]\t0.12 sec\t12\n\n org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0]\t29 ms\t12\n\n org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1]\t42 ms\t12\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.1.1", "fixed_version": "rel/release-1.2.2", "fixed_files": ["org.apache.hive.beeline.TestBeelineArgParsing.java", "org.apache.hive.beeline.ClassNameCompleter.java"], "label": 1, "es_results": []}, {"bug_id": 10444, "bug_title": "HIVE-10223 breaks hadoop-1 build", "bug_description": "FileStatus.isFile() and FileStatus.isDirectory() methods added in HIVE-10223 are not present in hadoop 1.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java", "org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java"], "label": 1, "es_results": []}, {"bug_id": 10971, "bug_title": "count(*) with count(distinct) gives wrong results when hive.groupby.skewindata=true", "bug_description": "When hive.groupby.skewindata=true, the following query based on TPC-H gives wrong results:\n\n\n\nset hive.groupby.skewindata=true;\n\n\n\nselect l_returnflag, count(*), count(distinct l_linestatus)\n\nfrom lineitem\n\ngroup by l_returnflag\n\nlimit 10;\n\n\n\nThe query plan shows that it generates only one MapReduce job instead of two theoretically, which is dictated by hive.groupby.skewindata=true.\nThe problem arises only when \n\ncount(*)\n\n and \n\ncount(distinct)\n\n exist together.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-1.2.1", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java"], "label": 1, "es_results": []}, {"bug_id": 11112, "bug_title": "ISO-8859-1 text output has fragments of previous longer rows appended", "bug_description": "If a LazySimpleSerDe table is created using ISO 8859-1 encoding, query results for a string column are incorrect for any row that was preceded by a row containing a longer string.\nExample steps to reproduce:\n1. Create a table using ISO 8859-1 encoding:\n\n\n\nCREATE TABLE person_lat1 (name STRING)\n\nROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos; WITH SERDEPROPERTIES (&apos;serialization.encoding&apos;=&apos;ISO8859_1&apos;);\n\n\n\n2. Copy an ISO-8859-1 encoded text file into the appropriate warehouse folder in HDFS. I&apos;ll attach an example file containing the following text: \n\nMller,Thomas\n\nJrgensen,Jrgen\n\nPea,Andrs\n\nNm,Fk\n\n\n\n3. Execute SELECT * FROM person_lat1\nResult - The following output appears:\n\n+-------------------+--+\n\n| person_lat1.name |\n\n+-------------------+--+\n\n| Mller,Thomas |\n\n| Jrgensen,Jrgen |\n\n| Pea,Andrsrgen |\n\n| Nm,Fkdrsrgen |\n\n+-------------------+--+\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.serde2.SerDeUtils.java"], "label": 1, "es_results": []}, {"bug_id": 10880, "bug_title": "The bucket number is not respected in insert overwrite.", "bug_description": "When hive.enforce.bucketing is true, the bucket number defined in the table is no longer respected in current master and 1.2. \nReproduce:\n\n\n\nCREATE TABLE IF NOT EXISTS buckettestinput( \n\ndata string \n\n) \n\nROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;;\n\n\n\nCREATE TABLE IF NOT EXISTS buckettestoutput1( \n\ndata string \n\n)CLUSTERED BY(data) \n\nINTO 2 BUCKETS \n\nROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;;\n\n\n\nCREATE TABLE IF NOT EXISTS buckettestoutput2( \n\ndata string \n\n)CLUSTERED BY(data) \n\nINTO 2 BUCKETS \n\nROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;;\n\n\n\nThen I inserted the following data into the \"buckettestinput\" table:\n\nfirstinsert1 \n\nfirstinsert2 \n\nfirstinsert3 \n\nfirstinsert4 \n\nfirstinsert5 \n\nfirstinsert6 \n\nfirstinsert7 \n\nfirstinsert8 \n\nsecondinsert1 \n\nsecondinsert2 \n\nsecondinsert3 \n\nsecondinsert4 \n\nsecondinsert5 \n\nsecondinsert6 \n\nsecondinsert7 \n\nsecondinsert8\n\n\n\n\n\n\nset hive.enforce.bucketing = true; \n\nset hive.enforce.sorting=true;\n\ninsert overwrite table buckettestoutput1 \n\nselect * from buckettestinput where data like &apos;first%&apos;;\n\nset hive.auto.convert.sortmerge.join=true; \n\nset hive.optimize.bucketmapjoin = true; \n\nset hive.optimize.bucketmapjoin.sortedmerge = true; \n\nselect * from buckettestoutput1 a join buckettestoutput2 b on (a.data=b.data);\n\n\n\n\nError: Error while compiling statement: FAILED: SemanticException [Error 10141]: Bucketed table metadata is not correct. Fix the metadata or don&apos;t use bucketed mapjoin, by setting hive.enforce.bucketmapjoin to false. The number of buckets for table buckettestoutput1 is 2, whereas the number of files is 1 (state=42000,code=10141)\n\n\n\nThe related debug information related to insert overwrite:\n\n0: jdbc:hive2://localhost:10000> insert overwrite table buckettestoutput1 \n\nselect * from buckettestinput where data like &apos;first%&apos;insert overwrite table buckettestoutput1 \n\n0: jdbc:hive2://localhost:10000> ;\n\nselect * from buckettestinput where data like &apos; \n\nfirst%&apos;;\n\nINFO  : Number of reduce tasks determined at compile time: 2\n\nINFO  : In order to change the average load for a reducer (in bytes):\n\nINFO  :   set hive.exec.reducers.bytes.per.reducer=<number>\n\nINFO  : In order to limit the maximum number of reducers:\n\nINFO  :   set hive.exec.reducers.max=<number>\n\nINFO  : In order to set a constant number of reducers:\n\nINFO  :   set mapred.reduce.tasks=<number>\n\nINFO  : Job running in-process (local Hadoop)\n\nINFO  : 2015-06-01 11:09:29,650 Stage-1 map = 86%,  reduce = 100%\n\nINFO  : Ended Job = job_local107155352_0001\n\nINFO  : Loading data to table default.buckettestoutput1 from file:/user/hive/warehouse/buckettestoutput1/.hive-staging_hive_2015-06-01_11-09-28_166_3109203968904090801-1/-ext-10000\n\nINFO  : Table default.buckettestoutput1 stats: [numFiles=1, numRows=4, totalSize=52, rawDataSize=48]\n\nNo rows affected (1.692 seconds)\n\n\n\nInsert use dynamic partition does not have the issue. ", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.Utilities.java"], "label": 1, "es_results": []}, {"bug_id": 11498, "bug_title": "HIVE Authorization v2 should not check permission for dummy entity", "bug_description": "The queries like SELECT 1+1;, The target table and database will set to _dummy_database _dummy_table, authorization should skip these kinds of databases or tables.\nFor authz v1. it has skip them.\neg1. Source code at github\n\nfor (WriteEntity write : outputs) {\n\n        if (write.isDummy() || write.isPathType()) {\n\n          continue;\n\n        }\n\n\n\neg2. Source code at github\n\nfor (ReadEntity read : inputs) {\n\n        if (read.isDummy() || read.isPathType()) {\n\n          continue;\n\n        }\n\n       ...\n\n        }\n\n\n\n...\nThis patch will fix authz v2.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-1.2.1", "fixed_files": ["org.apache.hadoop.hive.ql.Driver.java"], "label": 1, "es_results": []}, {"bug_id": 11502, "bug_title": "Map side aggregation is extremely slow", "bug_description": "For the query as following:\n\ncreate table tbl2 as \n\nselect col1, max(col2) as col2 \n\nfrom tbl1 group by col1;\n\n\n\nIf the column for group by has many different values (for example 400000) and it is in type double, the map side aggregation is very slow. I ran the query which took more than 3 hours , after 3 hours, I have to kill the query.\nThe same query can finish in 7 seconds, if I turn off map side aggregation by:\n\nset hive.map.aggr = false;\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java"], "label": 1, "es_results": []}, {"bug_id": 11123, "bug_title": "Fix how to confirm the RDBMS product name at Metastore.", "bug_description": "I use PostgreSQL to Hive Metastore. And I saw the following message at PostgreSQL log.\n\n\n\n< 2015-06-26 10:58:15.488 JST >ERROR:  syntax error at or near \"@@\" at character 5\n\n< 2015-06-26 10:58:15.488 JST >STATEMENT:  SET @@session.sql_mode=ANSI_QUOTES\n\n< 2015-06-26 10:58:15.489 JST >ERROR:  relation \"v$instance\" does not exist at character 21\n\n< 2015-06-26 10:58:15.489 JST >STATEMENT:  SELECT version FROM v$instance\n\n< 2015-06-26 10:58:15.490 JST >ERROR:  column \"version\" does not exist at character 10\n\n< 2015-06-26 10:58:15.490 JST >STATEMENT:  SELECT @@version\n\n\n\nWhen Hive CLI and Beeline embedded mode are carried out, this message is output to PostgreSQL log.\nThese queries are called from MetaStoreDirectSql#determineDbType. And if we use MetaStoreDirectSql#getProductName, we need not to call these queries.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java"], "label": 1, "es_results": []}, {"bug_id": 11587, "bug_title": "Fix memory estimates for mapjoin hashtable", "bug_description": "Due to the legacy in in-memory mapjoin and conservative planning, the memory estimation code for mapjoin hashtable is currently not very good. It allocates the probe erring on the side of more memory, not taking data into account because unlike the probe, it&apos;s free to resize, so it&apos;s better for perf to allocate big probe and hope for the best with regard to future data size. It is not true for hybrid case.\nThere&apos;s code to cap the initial allocation based on memory available (memUsage argument), but due to some code rot, the memory estimates from planning are not even passed to hashtable anymore (there used to be two config settings, hashjoin size fraction by itself, or hashjoin size fraction for group by case), so it never caps the memory anymore below 1 Gb. \nInitial capacity is estimated from input key count, and in hybrid join cache can exceed Java memory due to number of segments.\nThere needs to be a review and fix of all this code.\nSuggested improvements:\n1) Make sure \"initialCapacity\" argument from Hybrid case is correct given the number of segments. See how it&apos;s calculated from keys for regular case; it needs to be adjusted accordingly for hybrid case if not done already.\n1.5) Note that, knowing the number of rows, the maximum capacity one will ever need for probe size (in longs) is row count (assuming key per row, i.e. maximum possible number of keys) divided by load factor, plus some very small number to round up. That is for flat case. For hybrid case it may be more complex due to skew, but that is still a good upper bound for the total probe capacity of all segments.\n2) Rename memUsage to maxProbeSize, or something, make sure it&apos;s passed correctly based on estimates that take into account both probe and data size, esp. in hybrid case.\n3) Make sure that memory estimation for hybrid case also doesn&apos;t come up with numbers that are too small, like 1-byte hashtable. I am not very familiar with that code but it has happened in the past.\nOther issues we have seen:\n4) Cap single write buffer size to 8-16Mb. The whole point of WBs is that you should not allocate large array in advance. Even if some estimate passes 500Mb or 40Mb or whatever, it doesn&apos;t make sense to allocate that.\n5) For hybrid, don&apos;t pre-allocate WBs - only allocate on write.\n6) Change everywhere rounding up to power of two is used to rounding down, at least for hybrid case \nI wanted to put all of these items in single JIRA so we could keep track of fixing all of them.\nI think there are JIRAs for some of these already, feel free to link them to this one.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java", "org.apache.hadoop.hive.serde2.WriteBuffers.java", "org.apache.hadoop.hive.ql.exec.MapJoinOperator.java", "org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java", "org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 10755, "bug_title": "Rework on HIVE-5193 to enhance the column oriented table access", "bug_description": "Add the support of column pruning for column oriented table access which was done in HIVE-5193 but was reverted due to the join issue in HIVE-10720.\nIn 1.3.0, the patch posted by Viray didn&apos;t work, probably due to some jar reference. That seems to get fixed and that patch works in 2.0.0 now.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hive.hcatalog.pig.HCatLoader.java", "org.apache.hive.hcatalog.pig.TestHCatLoader.java", "org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java"], "label": 1, "es_results": []}, {"bug_id": 12277, "bug_title": "Hive macro results on macro_duplicate.q different after adding ORDER BY", "bug_description": "Added an order-by to the query in macro_duplicate.q:\n\n-select math_square(a), math_square(b),factorial(a), factorial(b), math_add(a), math_add(b),int(c) from macro_testing;\n\n\\ No newline at end of file\n\n+select math_square(a), math_square(b),factorial(a), factorial(b), math_add(a), math_add(b),int(c) from macro_testing order by int(c);\n\n\n\nAnd the results from math_add() changed unexpectedly:\n\n-1      4       1       2       2       4       3\n\n-16     25      24      120     8       10      6\n\n+1      4       1       2       1       4       3\n\n+16     25      24      120     16      25      6\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java"], "label": 1, "es_results": []}, {"bug_id": 12406, "bug_title": "HIVE-9500 introduced incompatible change to LazySimpleSerDe public interface", "bug_description": "In the process of fixing HIVE-9500, an incompatibility was introduced that will break 3rd party code that relies on LazySimpleSerde. In HIVE-9500, the nested class SerDeParamaters was removed and the method LazySimpleSerDe.initSerdeParms was also removed. They were replaced by a standalone class LazySerDeParameters.\nSince this has already been released, I don&apos;t think we should revert the change since that would mean breaking compatibility again. Instead, the best approach would be to support both interfaces, if possible. ", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java", "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java", "org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java"], "label": 1, "es_results": []}, {"bug_id": 12951, "bug_title": "Reduce Spark executor prewarm timeout to 5s", "bug_description": "Currently it&apos;s set to 30s, which tends to be longer than needed. Reduce it to 5s, only considering jvm startup time. (Eventually, we may want to make this configurable.)", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java"], "label": 1, "es_results": []}, {"bug_id": 13020, "bug_title": "Hive Metastore and HiveServer2 to Zookeeper fails with IBM JDK", "bug_description": "HiveServer2 and Hive Metastore Zookeeper component is hardcoded to only support the Oracle/Open JDK. I was performing testing of Hadoop running on the IBM JDK and discovered this issue and have since drawn up the attached patch. This looks to resolve the issue in a similar manner as how the Hadoop core folks handle the IBM JDK.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.shims.Utils.java"], "label": 1, "es_results": []}, {"bug_id": 13063, "bug_title": "Create UDFs for CHR and REPLACE ", "bug_description": "Create UDFS for these functions.\nCHR: convert n where n : [0, 256) into the ascii equivalent as a varchar. If n is less than 0 or greater than 255, return the empty string. If n is 0, return null.\nREPLACE: replace all substrings of &apos;str&apos; that match &apos;search&apos; with &apos;rep&apos;.\nExample. SELECT REPLACE(&apos;Hack and Hue&apos;, &apos;H&apos;, &apos;BL&apos;);\nEquals &apos;BLack and BLue&apos;\"", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FunctionRegistry.java"], "label": 1, "es_results": []}, {"bug_id": 13216, "bug_title": "ORC Reader will leave file open until GC when opening a malformed ORC file", "bug_description": "In ORC extractMetaInfoFromFooter method of ReaderImpl.java:\nA new input stream is open without try-catch-finally to enforce closing.\nOnce the footer parse has some exception, the stream close will miss. \nUntil GC happen to close the stream.\nprivate static FileMetaInfo extractMetaInfoFromFooter(FileSystem fs,\n                                                        Path path,\n                                                        long maxFileLength\n                                                        ) throws IOException \n{\n\n    FSDataInputStream file = fs.open(path);\n\n\n\n    ...\n\n    file.close();\n\n\n\n    return new FileMetaInfo(\n\n        ps.getCompression().toString(),\n\n        (int) ps.getCompressionBlockSize(),\n\n        (int) ps.getMetadataLength(),\n\n        buffer,\n\n        ps.getVersionList(),\n\n        writerVersion\n\n        );\n\n  }", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java"], "label": 1, "es_results": []}, {"bug_id": 13141, "bug_title": "Hive on Spark over HBase should accept parameters starting with \"zookeeper.znode\"", "bug_description": "HBase related paramters has been added by HIVE-12708.\nFollowing the same way,parameters starting with \"zookeeper.znode\" should be add too,which are also HBase related paramters .\nRefering to http://blog.cloudera.com/blog/2013/10/what-are-hbase-znodes/\nI have seen a failure with Hive on Spark over HBase  due to customize zookeeper.znode.parent.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java"], "label": 1, "es_results": []}, {"bug_id": 13493, "bug_title": "Fix TransactionBatchImpl.getCurrentTxnId() and mis logging fixes", "bug_description": "sort list of transaction IDs deleted by performTimeouts\nsort list of \"empty aborted\"\nlog the list of lock id removed due to timeout\nfix TransactionBatchImpl.getCurrentTxnId() not to look past end of array (see HIVE-13489)\nbeginNextTransactionImpl()\nif ( currentTxnIndex >= txnIds.size() )//todo: this condition is bogus should check currentTxnIndex + 1", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.hcatalog.streaming.HiveEndPoint.java", "org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java", "org.apache.hive.hcatalog.streaming.TestStreaming.java", "org.apache.hadoop.hive.metastore.txn.TxnHandler.java"], "label": 1, "es_results": []}, {"bug_id": 13561, "bug_title": "HiveServer2 is leaking ClassLoaders when add jar / temporary functions are used", "bug_description": "I can repo this on branch-1.2 and branch-2.0.\nIt looks to be the same issues as: HIVE-11408\nThe patch from HIVE-11408 looks to fix the issue as well.\nI&apos;ve updated the patch from HIVE-11408 to be aligned with branch-1.2 and master\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.jdbc.TestJdbcWithMiniHS2.java", "org.apache.hadoop.hive.ql.exec.Registry.java"], "label": 1, "es_results": []}, {"bug_id": 14581, "bug_title": "Add chr udf", "bug_description": "http://docs.aws.amazon.com/redshift/latest/dg/r_CHR.html", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FunctionRegistry.java"], "label": 1, "es_results": []}, {"bug_id": 9486, "bug_title": "Use session classloader instead of application loader", "bug_description": "From http://www.mail-archive.com/dev@hive.apache.org/msg107615.html\nLooks reasonable", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-1.2.0", "fixed_files": ["org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java", "org.apache.hadoop.hive.accumulo.Utils.java", "org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java", "org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.java", "org.apache.hive.hcatalog.mapreduce.HCatSplit.java", "org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.java", "org.apache.hive.hcatalog.messaging.MessageFactory.java", "org.apache.hadoop.hive.common.JavaUtils.java", "org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.java", "org.apache.hadoop.hive.ql.plan.PTFDeserializer.java", "org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java", "org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java", "org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java", "org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java", "org.apache.hive.hcatalog.templeton.tool.JobState.java", "org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java", "org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java", "org.apache.hadoop.hive.ql.plan.PlanUtils.java", "org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java", "org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java", "org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java", "org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java", "org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java", "org.apache.hadoop.hive.ql.io.orc.WriterImpl.java", "org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java", "org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.java", "org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java", "org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java", "org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java"], "label": 1, "es_results": []}, {"bug_id": 12006, "bug_title": "Enable Columnar Pushdown for RC/ORC File for HCatLoader", "bug_description": "This initially enabled by HIVE-5193. However, HIVE-10752 reverted it since there is issue in original implementation.\nWe shall fix the issue an reenable it.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hive.hcatalog.pig.HCatLoader.java", "org.apache.hive.hcatalog.pig.TestHCatLoader.java", "org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java"], "label": 1, "es_results": []}, {"bug_id": 12218, "bug_title": "Unable to create a like table for an hbase backed table", "bug_description": "For an HBase backed table:\n\n\n\nCREATE TABLE hbasetbl (key string, state string, country string, country_id int)\n\nSTORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;\n\nWITH SERDEPROPERTIES (\n\n\"hbase.columns.mapping\" = \"info:state,info:country,info:country_id\"\n\n);\n\n\n\nCreate its like table using query such as \ncreate table hbasetbl_like like hbasetbl;\nIt fails with error:\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.apache.hadoop.hive.ql.metadata.HiveException: must specify an InputFormat class\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.DDLTask.java"], "label": 1, "es_results": []}, {"bug_id": 12280, "bug_title": "HiveConnection does not try other HS2 after failure for service discovery", "bug_description": "Found this while mocking some bad connection data in znode.. will try to add a test for this.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java"], "label": 1, "es_results": []}, {"bug_id": 12249, "bug_title": "Improve logging with tez", "bug_description": "We need to improve logging across the board. TEZ-2851 added a caller context so that one can correlate logs with the application. This jira adds a new configuration for users that can be used to correlate the logs.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.cli.CliDriver.java", "org.apache.hive.service.cli.session.HiveSessionImpl.java", "org.apache.hadoop.hive.ql.hooks.ATSHook.java", "org.apache.hadoop.hive.ql.exec.tez.TezTask.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 12346, "bug_title": "Internally used variables in HiveConf should not be settable via command", "bug_description": "Some HiveConf variables such as hive.added.jars.path are only for internal use and should not be settable via set command. \nWe saw a lot of cases that users mistakenly set these variables using set command despite some of them have been documented as \"internal parameter\" in Hive. The command usually succeeds but it sometimes does not effect, which causes some confusions. For example, the hive.added.jars.path can be set via set command but it is sometimes overridden by session resource jars during runtime.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 12311, "bug_title": "explain CTAS fails if the table already exists", "bug_description": "Explain of a CTAS will fail if the table already exists.\nThis is an annoyance when you&apos;re seeing if a large body of SQL queries will function by putting explain in front of every query. \n\n\n\nhive> create table temp (x int);\n\nOK\n\nTime taken: 0.252 seconds\n\nhive> create table temp2 (x int);\n\nOK\n\nTime taken: 0.407 seconds\n\nhive> explain create table temp as select * from temp2;\n\nFAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: Table already exists: mydb.temp\n\n\n\nIf we compare to Postgres \"The Zinc Standard of SQL Compliance\":\n\n\n\ncarter=# create table temp (x int);\n\nCREATE TABLE\n\ncarter=# create table temp2 (x int);\n\nCREATE TABLE\n\ncarter=# explain create table temp as select * from temp2;\n\n                       QUERY PLAN\n\n---------------------------------------------------------\n\n Seq Scan on temp2  (cost=0.00..34.00 rows=2400 width=4)\n\n(1 row)\n\n\n\nIf the CTAS is something complex it would be nice to see the query plan in advance.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 12312, "bug_title": "Excessive logging in PPD code", "bug_description": "One of my very complex queries takes about 14 minutes to compile with PPD on. Profiling it I saw a lot of time spent in this stack which is called many many thousands of times.\n\n\n\njava.lang.Throwable.getStackTraceElement(-2)\n\njava.lang.Throwable.getOurStackTrace(827)\n\njava.lang.Throwable.getStackTrace(816)\n\nsun.reflect.GeneratedMethodAccessor5.invoke(-1)\n\nsun.reflect.DelegatingMethodAccessorImpl.invoke(43)\n\njava.lang.reflect.Method.invoke(497)\n\norg.apache.log4j.spi.LocationInfo.<init>(139)\n\norg.apache.log4j.spi.LoggingEvent.getLocationInformation(253)\n\norg.apache.log4j.helpers.PatternParser$LocationPatternConverter.convert(500)\n\norg.apache.log4j.helpers.PatternConverter.format(65)\n\norg.apache.log4j.PatternLayout.format(506)\n\norg.apache.log4j.WriterAppender.subAppend(310)\n\norg.apache.log4j.DailyRollingFileAppender.subAppend(369)\n\norg.apache.log4j.WriterAppender.append(162)\n\norg.apache.log4j.AppenderSkeleton.doAppend(251)\n\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(66)\n\norg.apache.log4j.Category.callAppenders(206)\n\norg.apache.log4j.Category.forcedLog(391)\n\norg.apache.log4j.Category.log(856)\n\norg.apache.commons.logging.impl.Log4JLogger.info(176)\n\norg.apache.hadoop.hive.ql.ppd.OpProcFactory$DefaultPPD.logExpr(707)\n\norg.apache.hadoop.hive.ql.ppd.OpProcFactory$DefaultPPD.mergeWithChildrenPred(752)\n\norg.apache.hadoop.hive.ql.ppd.OpProcFactory$FilterPPD.process(437)\n\n\n\nlogExpr is set to log at INFO level, but I think DEBUG is more appropriate. When I set log level to debug I see > 20% speedup in compile time:\nBefore:\n\n\n\nreal    14m47.972s\n\nuser    15m25.609s\n\nsys    0m20.282s\n\n\n\nAfter:\n\n\n\nreal    11m30.946s\n\nuser    12m10.870s\n\nsys    0m7.320s\n\n\n\nIt looks like there&apos;s a lot of stuff in the PPD code that could be optimized, when I turn PPD off the query compiles in 2m 30s. But this seems like an easy and low risk win.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.ppd.OpProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 12466, "bug_title": "SparkCounter not initialized error", "bug_description": "During a query, lots of the following error found in executor&apos;s log:\n\n03:47:28.759 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_0] has not initialized before.\n\n03:47:28.762 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_0] has not initialized before.\n\n03:47:30.707 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.tmp_tmp] has not initialized before.\n\n03:47:33.385 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.\n\n03:47:33.388 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.\n\n03:47:33.495 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.\n\n03:47:35.141 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.\n\n\n\n...........\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java", "org.apache.hadoop.hive.ql.exec.spark.SparkTask.java", "org.apache.hadoop.hive.ql.exec.FileSinkOperator.java"], "label": 1, "es_results": []}, {"bug_id": 12184, "bug_title": "DESCRIBE of fully qualified table fails when db and table name match and non-default database is in use", "bug_description": "DESCRIBE of fully qualified table fails when db and table name match and non-default database is in use.\nRepro:\n\n\n\n: jdbc:hive2://localhost:10000/default> create database foo;\n\nNo rows affected (0.116 seconds)\n\n0: jdbc:hive2://localhost:10000/default> create table foo.foo(i int);\n\n\n\n0: jdbc:hive2://localhost:10000/default> describe foo.foo;\n\n+-----------+------------+----------+--+\n\n| col_name  | data_type  | comment  |\n\n+-----------+------------+----------+--+\n\n| i         | int        |          |\n\n+-----------+------------+----------+--+\n\n1 row selected (0.049 seconds)\n\n\n\n0: jdbc:hive2://localhost:10000/default> use foo;\n\n\n\n0: jdbc:hive2://localhost:10000/default> describe foo.foo;\n\nError: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Error in getting fields from serde.Invalid Field foo (state=08S01,code=1)\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 11241, "bug_title": "Database prefix does not work properly if table has same name", "bug_description": "If you do the following it will fail: \n\n\n\n0: jdbc:hive2://cdh54-1.test.com:10000/defaul> create database test4; \n\nNo rows affected (0.881 seconds) \n\n0: jdbc:hive2://cdh54-1.test.com:10000/defaul> use test4; \n\nNo rows affected (0.1 seconds) \n\n0: jdbc:hive2://cdh54-1.test.com:10000/defaul> create table test4 (c1 char(200)); \n\nNo rows affected (0.306 seconds) \n\n0: jdbc:hive2://cdh54-1.test.com:10000/defaul> desc test4.test4; \n\nError: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. cannot find field test4 from [0:c1] (state=08S01,code=1)\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 12505, "bug_title": "Insert overwrite in same encrypted zone silently fails to remove some existing files", "bug_description": "With HDFS Trash enabled but its encryption zone lower than Hive data directory, insert overwrite command silently fails to trash the existing files during overwrite, which could lead to unexpected incorrect results (more rows returned than expected)", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java", "org.apache.hadoop.hive.common.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 12473, "bug_title": "DPP: UDFs on the partition column side does not evaluate correctly", "bug_description": "Related to HIVE-12462\n\n\n\n\n\nselect count(1) from accounts a, transactions t where year(a.dt) = year(t.dt) and account_id = 22;\n\n\n\n$hdt$_0:$hdt$_1:a\n\n  TableScan (TS_2)\n\n    alias: a\n\n    filterExpr: (((account_id = 22) and year(dt) is not null) and (year(dt)) IN (RS[6])) (type: boolean)\n\n\n\nEnds up being evaluated as year(cast(dt as int)) because the pruner only checks for final type, not the column type.\n\n\n\n    ObjectInspector oi =\n\n        PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(TypeInfoFactory\n\n            .getPrimitiveTypeInfo(si.fieldInspector.getTypeName()));\n\n\n\n    Converter converter =\n\n        ObjectInspectorConverters.getConverter(\n\n            PrimitiveObjectInspectorFactory.javaStringObjectInspector, oi);\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.plan.DynamicPruningEventDesc.java", "org.apache.hadoop.hive.ql.plan.MapWork.java", "org.apache.hadoop.hive.ql.parse.GenTezUtils.java", "org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.java", "org.apache.hadoop.hive.ql.metadata.Table.java", "org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java", "org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java"], "label": 1, "es_results": []}, {"bug_id": 12610, "bug_title": "Hybrid Grace Hash Join should fail task faster if processing first batch fails, instead of continuing processing the rest", "bug_description": "During processing the spilled partitions, if there&apos;s any fatal error, such as Kryo exception, then we should exit early, instead of moving on to process the rest of spilled partitions.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.MapJoinOperator.java"], "label": 1, "es_results": []}, {"bug_id": 12742, "bug_title": "NULL table comparison within CASE does not work as previous hive versions", "bug_description": "drop table test_1; \ncreate table test_1 (id int, id2 int); \ninsert into table test_1 values (123, NULL);\nSELECT cast(CASE WHEN id = id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b \nFROM test_1; \n--NULL\nBut the output should be true (confirmed with postgres.)", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 12744, "bug_title": "GROUPING__ID failed to be recognized in multiple insert", "bug_description": "When using multiple insert with multiple group by, grouping__id will failed to be parse.\nhive> create temporary table testtable3 (id string, name string);\nOK\nTime taken: 1.019 seconds\nhive> create temporary table testtable2 (id string, name string);\nOK\nTime taken: 0.069 seconds\nhive> create temporary table testtable1 (id string, name string);\nOK\nTime taken: 0.066 seconds\nhive> insert into table testtable1 values (\"id\", \"2333\");\n...\nOK\nTime taken: 32.515 seconds\nhive> from testtable1\n    > insert into table testtable2 select\n    >     id, GROUPING__ID\n    > group by id, name with cube;\n...\nOK\nTime taken: 42.032 seconds\nhive> from testtable1\n    > insert into table testtable2 select\n    >     id, GROUPING__ID\n    > group by id, name with cube\n    > insert into table testtable3 select\n    >     id, name\n    > group by id, name grouping sets ((id), (id, name));\nFAILED: SemanticException [Error 10025]: Line 3:8 Expression not in GROUP BY key &apos;GROUPING__ID&apos;", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.ErrorMsg.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 12682, "bug_title": "Reducers in dynamic partitioning job spend a lot of time running hadoop.conf.Configuration.getOverlay", "bug_description": "I tested this on Hive 1.2.1 but looks like it&apos;s still applicable to 2.0.\nI ran this query:\n\n\n\ncreate table flights (\n\n\n\n)\n\nPARTITIONED BY (Year int)\n\nCLUSTERED BY (Month)\n\nSORTED BY (DayofMonth) into 12 buckets\n\nSTORED AS ORC\n\nTBLPROPERTIES(\"orc.bloom.filter.columns\"=\"*\")\n\n;\n\n\n\n(Taken from here: https://github.com/t3rmin4t0r/all-airlines-data/blob/master/ddl/orc.sql)\nI profiled just the reduce phase and noticed something odd, the attached graph shows where time was spent during the reducer phase.\n\nProblem seems to relate to https://github.com/apache/hive/blob/branch-2.0/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java#L903\n/cc Gopal V", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FileSinkOperator.java"], "label": 1, "es_results": []}, {"bug_id": 12867, "bug_title": "Semantic Exception Error Message should be with in the range of \"10000 to 19999\"", "bug_description": "At many places errors encountered during semantic exception is translated as generic error(GENERIC_ERROR, 40000) message as opposed to semantic error message.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.EximUtil.java", "org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.TaskCompiler.java", "org.apache.hadoop.hive.ql.ErrorMsg.java", "org.apache.hadoop.hive.ql.plan.CreateTableDesc.java"], "label": 1, "es_results": []}, {"bug_id": 13056, "bug_title": "delegation tokens do not work with HS2 when used with http transport and kerberos", "bug_description": "We&apos;re getting a HiveSQLException on secure windows clusters.\n\n\n\n2016-02-08 13:48:09,535|beaver.machine|INFO|6114|140264674350912|MainThread|Job ID : 0000000-160208134528402-oozie-oozi-W\n\n2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------\n\n2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|Workflow Name : hive2-wf\n\n2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|App Path      : wasb://oozie1-hbs24@humbtestings5jp.blob.core.windows.net/user/hrt_qa/test_hiveserver2\n\n2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|Status        : KILLED\n\n2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|Run           : 0\n\n2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|User          : hrt_qa\n\n2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|Group         : -\n\n2016-02-08 13:48:09,547|beaver.machine|INFO|6114|140264674350912|MainThread|Created       : 2016-02-08 13:47 GMT\n\n2016-02-08 13:48:09,548|beaver.machine|INFO|6114|140264674350912|MainThread|Started       : 2016-02-08 13:47 GMT\n\n2016-02-08 13:48:09,552|beaver.machine|INFO|6114|140264674350912|MainThread|Last Modified : 2016-02-08 13:48 GMT\n\n2016-02-08 13:48:09,553|beaver.machine|INFO|6114|140264674350912|MainThread|Ended         : 2016-02-08 13:48 GMT\n\n2016-02-08 13:48:09,553|beaver.machine|INFO|6114|140264674350912|MainThread|CoordAction ID: -\n\n2016-02-08 13:48:09,566|beaver.machine|INFO|6114|140264674350912|MainThread|\n\n2016-02-08 13:48:09,566|beaver.machine|INFO|6114|140264674350912|MainThread|Actions\n\n2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------\n\n2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|ID                                                                            Status    Ext ID                 Ext Status Err Code\n\n2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------\n\n2016-02-08 13:48:09,571|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@:start:                                  OK        -                      OK         -\n\n2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------\n\n2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@hive-node                                ERROR     -                      ERROR      HiveSQLException\n\n2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------\n\n2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@fail                                     OK        -                      OK         E0729\n\n2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.service.auth.HiveAuthFactory.java"], "label": 1, "es_results": []}, {"bug_id": 12981, "bug_title": "ThriftCLIService uses incompatible getShortName() implementation", "bug_description": "ThriftCLIService has a local implementation getShortName() that assumes a short name is always the part before \"@\" and \"/\". This is not always the case as Kerberos Rules (from Hadoop&apos;s KerberosName) might actually transform a name to something else.\nConsidering a pending change to getShortName() (#HADOOP-12751) and the normal use of KerberosName in other parts of Hive it only seems logical to use the standard implementation.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-1.2.2", "fixed_files": ["org.apache.hive.service.cli.thrift.ThriftCLIService.java"], "label": 1, "es_results": []}, {"bug_id": 13021, "bug_title": "GenericUDAFEvaluator.isEstimable(agg) always returns false", "bug_description": "GenericUDAFEvaluator.isEstimable(agg) always returns false, because annotation AggregationType has default RetentionPolicy.CLASS and cannot be retained by the VM at run time.\nAs result estimate method will never be executed.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java"], "label": 1, "es_results": []}, {"bug_id": 12808, "bug_title": "Logical PPD: Push filter clauses through PTF(Windowing) into TS", "bug_description": "Simplified repro case of HCC #8880, with the slow query showing the push-down miss. \nAnd the manually rewritten query to indicate the expected one.\nPart of the problem could be the window range not being split apart for PPD, but the FIL is not pushed down even if the rownum filter is removed.\n\n\n\ncreate temporary table positions (regionid string, id bigint, deviceid string, ts string);\n\n\n\ninsert into positions values(&apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&apos;, 1422792010, &apos;6c5d1a30-2331-448b-a726-a380d6b3a432&apos;, &apos;2016-01-01&apos;),\n\n(&apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&apos;, 1422792010, &apos;6c5d1a30-2331-448b-a726-a380d6b3a432&apos;, &apos;2016-01-01&apos;),\n\n(&apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&apos;, 1422792010, &apos;6c5d1a30-2331-448b-a726-a380d6b3a432&apos;, &apos;2016-01-02&apos;),\n\n(&apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&apos;, 1422792010, &apos;6c5d1a30-2331-448b-a726-a380d6b3a432&apos;, &apos;2016-01-02&apos;);\n\n\n\n\n\n-- slow query\n\nexplain\n\nWITH t1 AS \n\n( \n\n         SELECT   *, \n\n                  Row_number() over ( PARTITION BY regionid, id, deviceid ORDER BY ts DESC) AS rownos\n\n         FROM     positions ), \n\nlatestposition as ( \n\n       SELECT * \n\n       FROM   t1 \n\n       WHERE  rownos = 1) \n\nSELECT * \n\nFROM   latestposition \n\nWHERE  regionid=&apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&apos; \n\nAND    id=1422792010 \n\nAND    deviceid=&apos;6c5d1a30-2331-448b-a726-a380d6b3a432&apos;;\n\n\n\n-- fast query\n\nexplain\n\nWITH t1 AS \n\n( \n\n         SELECT   *, \n\n                  Row_number() over ( PARTITION BY regionid, id, deviceid ORDER BY ts DESC) AS rownos\n\n         FROM     positions \n\n         WHERE  regionid=&apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&apos; \n\n         AND    id=1422792010 \n\n         AND    deviceid=&apos;6c5d1a30-2331-448b-a726-a380d6b3a432&apos;\n\n),latestposition as ( \n\n       SELECT * \n\n       FROM   t1 \n\n       WHERE  rownos = 1) \n\nSELECT * \n\nFROM   latestposition \n\n;\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java", "org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTransposeRule.java", "org.apache.hadoop.hive.ql.parse.CalcitePlanner.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 13108, "bug_title": "Operators: SORT BY randomness is not safe with network partitions", "bug_description": "SORT BY relies on a transient Random object, which is initialized once per deserialize operation.\nThis results in complications during a network partition and when Tez/Spark reuses a cached plan.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java"], "label": 1, "es_results": []}, {"bug_id": 13144, "bug_title": "HS2 can leak ZK ACL objects when curator retries to create the persistent ephemeral node", "bug_description": "When the node gets deleted from ZK due to connection loss and curator tries to recreate the node, it might leak ZK ACL.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.service.server.HiveServer2.java"], "label": 1, "es_results": []}, {"bug_id": 13217, "bug_title": "Replication for HoS mapjoin small file needs to respect dfs.replication.max", "bug_description": "Currently Hive on Spark Mapjoin replicates small table file to a hard-coded value of 10.  See SparkHashTableSinkOperator.MIN_REPLICATION. \nWhen dfs.replication.max is less than 10, HoS query fails.  This constant should cap at dfs.replication.max.\nNormally dfs.replication.max seems set at 512.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java"], "label": 1, "es_results": []}, {"bug_id": 12367, "bug_title": "Lock/unlock database should add current database to inputs and outputs of authz hook", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.Driver.java", "org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 13372, "bug_title": "Hive Macro overwritten when multiple macros are used in one column", "bug_description": "When multiple macros are used in one column, results of the later ones are over written by that of the first.\nFor example:\nSuppose we have created a table called macro_test with single column x in STRING type, and with data as:\n\"a\"\n\"bb\"\n\"ccc\"\nWe also create three macros:\n\n\n\nCREATE TEMPORARY MACRO STRING_LEN(x string) length(x);\n\nCREATE TEMPORARY MACRO STRING_LEN_PLUS_ONE(x string) length(x)+1;\n\nCREATE TEMPORARY MACRO STRING_LEN_PLUS_TWO(x string) length(x)+2;\n\n\n\nWhen we ran the following query, \n\n\n\nSELECT\n\n    CONCAT(STRING_LEN(x), \":\", STRING_LEN_PLUS_ONE(x), \":\", STRING_LEN_PLUS_TWO(x)) a\n\nFROM macro_test\n\nSORT BY a DESC;\n\n\n\nWe get result:\n3:3:3\n2:2:2\n1:1:1\ninstead of expected:\n3:4:5\n2:3:4\n1:2:3\nCurrently we are using Hive 1.2.1, and have applied both HIVE-11432 and HIVE-12277 patches.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java"], "label": 1, "es_results": []}, {"bug_id": 13320, "bug_title": "Apply HIVE-11544 to explicit conversions as well as implicit ones", "bug_description": "Parsing 1 million blank values through cast(x as int) is 3x slower than parsing a valid single digit.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.UDFToByte.java", "org.apache.hadoop.hive.ql.udf.UDFToLong.java", "org.apache.hadoop.hive.ql.udf.UDFToInteger.java", "org.apache.hadoop.hive.ql.udf.UDFToFloat.java", "org.apache.hadoop.hive.ql.udf.UDFToDouble.java", "org.apache.hadoop.hive.ql.udf.UDFToShort.java"], "label": 1, "es_results": []}, {"bug_id": 13439, "bug_title": "JDBC: provide a way to retrieve GUID to query Yarn ATS", "bug_description": "HIVE-9673 added support for passing base64 encoded operation handles to ATS. We should a method on client side to retrieve that.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.jdbc.TestJdbcDriver2.java", "org.apache.hive.jdbc.HiveStatement.java"], "label": 1, "es_results": []}, {"bug_id": 13240, "bug_title": "GroupByOperator: Drop the hash aggregates when closing operator", "bug_description": "GroupByOperator holds onto the Hash aggregates accumulated when the plan is cached.\nDrop the hashAggregates in case of error during forwarding to the next operator.\nAdded for PTF, TopN and all GroupBy cases.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java", "org.apache.hadoop.hive.ql.exec.GroupByOperator.java"], "label": 1, "es_results": []}, {"bug_id": 13390, "bug_title": "HiveServer2: Add more test to ZK service discovery using MiniHS2", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/release-1.2.2", "fixed_files": ["org.apache.hive.jdbc.HiveConnection.java", "org.apache.hive.service.auth.HiveAuthFactory.java", "org.apache.hive.jdbc.miniHS2.MiniHS2.java", "org.apache.hive.jdbc.TestSSL.java"], "label": 1, "es_results": []}, {"bug_id": 13602, "bug_title": "TPCH q16 return wrong result when CBO is on", "bug_description": "Running tpch with factor 2, \nq16 returns 1,160 rows when CBO is on,\nwhile returns 24,581 rows when CBO is off.\nSee attachment for detail .", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-1.2.2", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.VirtualColumn.java", "org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java", "org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.java", "org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 13264, "bug_title": "JDBC driver makes 2 Open Session Calls for every open session", "bug_description": "When HTTP is used as the transport mode by the Hive JDBC driver, we noticed that there is an additional open/close session just to validate the connection. \nTCLIService.Iface client = new TCLIService.Client(new TBinaryProtocol(transport));\n      TOpenSessionResp openResp = client.OpenSession(new TOpenSessionReq());\n      if (openResp != null) \n{\n\n        client.CloseSession(new TCloseSessionReq(openResp.getSessionHandle()));\n\n      }\n\nThe open session call is a costly one and should not be used to test transport. ", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.minikdc.TestJdbcWithMiniKdc.java", "org.apache.hive.jdbc.HiveConnection.java"], "label": 1, "es_results": []}, {"bug_id": 14019, "bug_title": "HiveServer2: Enable Kerberos with SSL for TCP transport", "bug_description": "Currently, there is a limitation where an HS2 user needs to use the auth-conf SASL qop value to achieve encryption when Kerberos is used as the authentication mechanism and transport is TCP. ", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hive.jdbc.HiveConnection.java"], "label": 1, "es_results": []}, {"bug_id": 14178, "bug_title": "Hive::needsToCopy should reuse FileUtils::equalsFileSystem", "bug_description": "Clear bug triggered from missing FS checks in Hive.java\n\n\n\n//Check if different FileSystems\n\nif (!srcFs.getClass().equals(destFs.getClass()))\n\n{ \n\nreturn true;\n\n }\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 14027, "bug_title": "NULL values produced by left outer join do not behave as NULL", "bug_description": "Consider the following setup:\n\n\n\ncreate table tbl (n bigint, t string); \n\n\n\ninsert into tbl values (1, &apos;one&apos;); \n\ninsert into tbl values(2, &apos;two&apos;);\n\n\n\nselect a.n, a.t, isnull(b.n), isnull(b.t) from (select * from tbl where n = 1) a  left outer join  (select * from tbl where 1 = 2) b on a.n = b.n;\n\n\n\n1    one    false    true\n\n\n\nThe query should return true for isnull(b.n).\nI&apos;ve tested by inserting a row with null value for the bigint column into tbl, and isnull returns true in that case. ", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.MapJoinOperator.java", "org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java"], "label": 1, "es_results": []}, {"bug_id": 14175, "bug_title": "Fix creating buckets without scheme information", "bug_description": "If a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.TestUtilities.java", "org.apache.hadoop.hive.ql.exec.Utilities.java"], "label": 1, "es_results": []}, {"bug_id": 14222, "bug_title": "PTF: Operator initialization does not clean state", "bug_description": "PTFOperator::initializeOp() does not reset currentKeys to null.\n\n\n\n      if (currentKeys != null && !keysAreEqual) {\n\n        ptfInvocation.finishPartition();\n\n      }\n\n....\n\n      if (currentKeys == null) {\n\n          currentKeys = newKeys.copyKey();\n\n        } else {\n\n          currentKeys.copyKey(newKeys);\n\n        }\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.PTFOperator.java"], "label": 1, "es_results": []}, {"bug_id": 14349, "bug_title": "Vectorization: LIKE should anchor the regexes", "bug_description": "RLIKE works like contains() and LIKE works like matches().\nThe UDFLike LIKE -> Regex conversion returns unanchored regexes making the vectorized LIKE behave like RLIKE.\n\n\n\ncreate temporary table x (a string) stored as orc;\n\ninsert into x values(&apos;XYZa&apos;), (&apos;badXYZa&apos;);\n\n\n\nselect * from x where a LIKE &apos;XYZ%a%&apos; order by 1;\n\nOK\n\nXYZa\n\nbadXYZa\n\nTime taken: 4.029 seconds, Fetched: 2 row(s)\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.java"], "label": 1, "es_results": []}, {"bug_id": 14294, "bug_title": "HiveSchemaConverter for Parquet does not translate TINYINT and SMALLINT into proper Parquet types", "bug_description": "To reproduce this issue, run the following DDL:\n\n\n\nCREATE TABLE foo STORED AS PARQUET AS SELECT CAST(1 AS TINYINT);\n\n\n\nAnd then check the schema of the written Parquet file:\n\n$ parquet-schema $WAREHOUSE_PATH/foo/000000_0\n\nmessage hive_schema {\n\n  optional int32 _c0;\n\n}\n\n\n\nWhen translating Hive types into Parquet types, TINYINT and SMALLINT should be translated into the int32 (INT_8) and int32 (INT_16) respectively. However, HiveSchemaConverter converts all of TINYINT, SMALLINT, and INT into Parquet int32. This causes problem when accessing Parquet files generated by Hive in other systems since type information gets wrong.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.java", "org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.java"], "label": 1, "es_results": []}, {"bug_id": 14390, "bug_title": "Wrong Table alias when CBO is on", "bug_description": "There are 5 web_sales references in query95 of tpcds ,with alias ws1-ws5.\nBut the query plan only has ws1 when CBO is on.\nquery95 :\n\nSELECT count(distinct ws1.ws_order_number) as order_count,\n\n               sum(ws1.ws_ext_ship_cost) as total_shipping_cost,\n\n               sum(ws1.ws_net_profit) as total_net_profit\n\nFROM web_sales ws1\n\nJOIN customer_address ca ON (ws1.ws_ship_addr_sk = ca.ca_address_sk)\n\nJOIN web_site s ON (ws1.ws_web_site_sk = s.web_site_sk)\n\nJOIN date_dim d ON (ws1.ws_ship_date_sk = d.d_date_sk)\n\nLEFT SEMI JOIN (SELECT ws2.ws_order_number as ws_order_number\n\n                               FROM web_sales ws2 JOIN web_sales ws3\n\n                               ON (ws2.ws_order_number = ws3.ws_order_number)\n\n                               WHERE ws2.ws_warehouse_sk <> ws3.ws_warehouse_sk\n\n                        ) ws_wh1\n\nON (ws1.ws_order_number = ws_wh1.ws_order_number)\n\nLEFT SEMI JOIN (SELECT wr_order_number\n\n                               FROM web_returns wr\n\n                               JOIN (SELECT ws4.ws_order_number as ws_order_number\n\n                                          FROM web_sales ws4 JOIN web_sales ws5\n\n                                          ON (ws4.ws_order_number = ws5.ws_order_number)\n\n                                         WHERE ws4.ws_warehouse_sk <> ws5.ws_warehouse_sk\n\n                                ) ws_wh2\n\n                               ON (wr.wr_order_number = ws_wh2.ws_order_number)) tmp1\n\nON (ws1.ws_order_number = tmp1.wr_order_number)\n\nWHERE d.d_date between &apos;2002-05-01&apos; and &apos;2002-06-30&apos; and\n\n               ca.ca_state = &apos;GA&apos; and\n\n               s.web_company_name = &apos;pri&apos;;\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan.java"], "label": 1, "es_results": []}, {"bug_id": 13756, "bug_title": "Map failure attempts to delete reducer _temporary directory on multi-query pig query", "bug_description": "A pig script, executed with multi-query enabled, that reads the source data and writes it as-is into TABLE_A as well as performing a group-by operation on the data which is written into TABLE_B can produce erroneous results if any map fails. This results in a single MR job that writes the map output to a scratch directory relative to TABLE_A and the reducer output to a scratch directory relative to TABLE_B.\nIf one or more maps fail it will delete the attempt data relative to TABLE_A, but it also deletes the _temporary directory relative to TABLE_B. This has the unintended side-effect of preventing subsequent maps from committing their data. This means that any maps which successfully completed before the first map failure will have its data committed as expected, other maps not, resulting in an incomplete result set.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java"], "label": 1, "es_results": []}, {"bug_id": 13754, "bug_title": "Fix resource leak in HiveClientCache", "bug_description": "Found that the users reference count can go into negative values, which prevents tearDownIfUnused from closing the client connection when called.\nThis leads to a build up of clients which have been evicted from the cache, are no longer in use, but have not been shutdown.\nGC will eventually call finalize, which forcibly closes the connection and cleans up the client, but I have seen as many as several hundred open client connections as a result.\nThe main resource for this is caused by RetryingMetaStoreClient, which will call reconnect on acquire, which calls close. This will decrement users to -1 on the reconnect, then acquire will increase this to 0 while using it, and back to -1 when it releases it.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.hcatalog.common.HCatUtil.java", "org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java", "org.apache.hive.hcatalog.common.TestHiveClientCache.java", "org.apache.hive.hcatalog.common.HiveClientCache.java", "org.apache.hadoop.hive.metastore.IMetaStoreClient.java", "org.apache.hive.hcatalog.common.HCatConstants.java"], "label": 1, "es_results": []}, {"bug_id": 14591, "bug_title": "HS2 is shut down unexpectedly during the startup time", "bug_description": "If there is issue with Zookeeper (e.g. connection issues), then it takes HS2 some time to connect. During this time, Ambari could issue health checks against HS2 and the CloseSession call will trigger the shutdown of HS2, which is not expected. That triggering should happen only when the HS2 has been deregistered with Zookeeper, not during the startup time when HS2 is not registered with ZK yet.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.jdbc.miniHS2.AbstractHiveService.java", "org.apache.hive.jdbc.miniHS2.MiniHS2.java", "org.apache.hive.service.server.HiveServer2.java", "org.apache.hive.service.cli.session.SessionManager.java"], "label": 1, "es_results": []}, {"bug_id": 14686, "bug_title": "Get unexpected command type when execute query \"CREATE TABLE IF NOT EXISTS ... AS\"", "bug_description": "See the query: \n\n\n\ncreate table if not exists DST as select * from SRC;\n\n\n\nif the table DST doesn&apos;t exist, SessionState.get().getHiveOperation() will return HiveOperation.CREATETABLE_AS_SELECT;\nBut if the table DST already exists, it will return HiveOperation.CREATETABLE;\nIt really makes some trouble for those who judge operation type by SessionState.get().getHiveOperation().\nThe reason I find out is that the function analyzeCreateTable in SemanticAnalyzer.java will return null and won&apos;t set the correct command type if the table already exists.\nHere is the related code:\n\n\n\n// check for existence of table\n\n    if (ifNotExists) {\n\n      try {\n\n        Table table = getTable(qualifiedTabName, false);\n\n        if (table != null) { // table exists\n\n          return null;\n\n        }\n\n      } catch (HiveException e) {\n\n        // should not occur since second parameter to getTableWithQN is false\n\n        throw new IllegalStateException(\"Unxpected Exception thrown: \" + e.getMessage(), e);\n\n      }\n\n    }\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 12222, "bug_title": "Define port range in property for RPCServer", "bug_description": "Creating this JIRA after discussin with Xuefu on the dev mailing list. Would need some help to review and update the fields in this JIRA ticket, thanks.\nI notice that in \n./spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java\nThe port number is assigned with 0 which means it will be a random port every time when the RPC Server is created to talk to Spark in the same session.\nBecause of this, this is causing problems to configure firewall between the \nHiveCLI RPC Server and Spark due to unpredictable port numbers here. In other word, users need to open all hive ports range \nfrom Data Node => HiveCLI (edge node).\n\n\n\n this.channel = new ServerBootstrap()\n\n      .group(group)\n\n      .channel(NioServerSocketChannel.class)\n\n      .childHandler(new ChannelInitializer<SocketChannel>() {\n\n          @Override\n\n          public void initChannel(SocketChannel ch) throws Exception {\n\n            SaslServerHandler saslHandler = new SaslServerHandler(config);\n\n            final Rpc newRpc = Rpc.createServer(saslHandler, config, ch, group);\n\n            saslHandler.rpc = newRpc;\n\n\n\n            Runnable cancelTask = new Runnable() {\n\n                @Override\n\n                public void run() {\n\n                  LOG.warn(\"Timed out waiting for hello from client.\");\n\n                  newRpc.close();\n\n                }\n\n            };\n\n            saslHandler.cancelTask = group.schedule(cancelTask,\n\n                RpcServer.this.config.getServerConnectTimeoutMs(),\n\n                TimeUnit.MILLISECONDS);\n\n\n\n          }\n\n      })\n\n\n\n2 Main reasons.\n\nMost users (what I see and encounter) use HiveCLI as a command line tool, and in order to use that, they need to login to the edge node (via SSH). Now, here comes the interesting part.\nCould be true or not, but this is what I observe and encounter from time to time. Most users will abuse the resource on that edge node (increasing HADOOP_HEAPSIZE, dumping output to local disk, running huge python workflow, etc), this may cause the HS2 process to run into OOME, choke and die, etc. various resource issues including others like login, etc.\n\n\nAnalyst connects to Hive via HS2 + ODBC. So HS2 needs to be highly available. This makes sense to run it on the gateway node or a service node and separated from the HiveCLI.\nThe logs are located in different location, monitoring and auditing is easier to run HS2 with a daemon user account, etc. so we don&apos;t want users to run HiveCLI where HS2 is running.\nIt&apos;s better to isolate the resource this way to avoid any memory, file handlers, disk space, issues.\n\nFrom a security standpoint, \n\nSince users can login to edge node (via SSH), the security on the edge node needs to be fortified and enhanced. Therefore, all the FW comes in and auditing.\n\n\nRegulation/compliance for auditing is another requirement to monitor all traffic, specifying ports and locking down the ports makes it easier since we can focus\non a range to monitor and audit.\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.spark.client.rpc.TestRpc.java", "org.apache.hive.spark.client.rpc.RpcServer.java", "org.apache.hive.spark.client.rpc.RpcConfiguration.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 14966, "bug_title": "JDBC: Make cookie-auth work in HTTP mode", "bug_description": "HiveServer2 cookie-auth is non-functional and forces authentication to be repeated for the status check loop, row fetch loop and the get logs loop.\nThe repeated auth in the fetch-loop is a performance issue, but is also causing occasional DoS responses from the remote auth-backend if this is not using local /etc/passwd.\nThe HTTP-Cookie auth once made functional will behave similarly to the binary protocol, authenticating exactly once per JDBC session and not causing further load on the authentication backend irrespective how many rows are returned from the JDBC request.\nThis due to the fact that the cookies are not sent out with matching flags for SSL usage.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.service.cli.thrift.ThriftHttpServlet.java", "org.apache.hive.service.cli.thrift.ThriftCliServiceTestWithCookie.java", "org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 15099, "bug_title": "PTFOperator.PTFInvocation did not properly reset the input partition", "bug_description": "There is an issue with PTFOperator.PTFInvocation where the inputPart is not reset properly. The inputPart has been closed and its content (member variables) has been cleaned up, but since itself is not nullified, it&apos;s reused in the next round and caused NPE issue.", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.PTFOperator.java"], "label": 1, "es_results": []}, {"bug_id": 13947, "bug_title": "HoS print wrong number for hash table size in map join scenario", "bug_description": "In sparkHashTableSinkOperator, when flushToFile, before close output stream, it try to get the file length, and will get 0 for it,  take hashTableSinkOperator for ref, it should get length after output stream closed", "project": "Apache", "sub_project": "HIVE", "version": "release-1.2.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java"], "label": 1, "es_results": []}, {"bug_id": 12933, "bug_title": "Beeline will hang when authenticating with PAM when libjpam.so is missing", "bug_description": "When we setup PAM authentication, we need to have libjpam.so under java.library.path. If it happens to misplace the .so file, rather than giving an exception, the client will hang forever.\nSeems we should catch the exception when the lib is missing.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.service.auth.PamAuthenticationProviderImpl.java"], "label": 1, "es_results": []}, {"bug_id": 12993, "bug_title": "user and password supplied from URL is overwritten by the empty user and password of the JDBC connection string when it is calling from beeline", "bug_description": "When we make the call beeline -you \"jdbc:hive2://localhost:10000/;user=aaa;password=bbb\", the user and password are overwritten by the blank ones since internally it constructs a \"connect <url> &apos;&apos; &apos;&apos; <driver>\" call with empty user and password. ", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.beeline.DatabaseConnection.java", "org.apache.hive.jdbc.Utils.java"], "label": 1, "es_results": []}, {"bug_id": 13036, "bug_title": "Split hive.root.logger separately to make it compatible with log4j1.x (for remaining services)", "bug_description": "Similar to HIVE-12402 but for HS2 and metastore this time.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.common.cli.CommonCliOptions.java", "org.apache.hive.service.server.HiveServer2.java", "org.apache.hadoop.hive.cli.OptionsProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 12927, "bug_title": "HBase metastore: sequences should be one per row, not all in one row", "bug_description": "\n  long getNextSequence(byte[] sequence) throws IOException {\n\n\n\nIs not safe in presence of any concurrency. It should use HBase increment API.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.java", "org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java"], "label": 1, "es_results": []}, {"bug_id": 13105, "bug_title": "LLAP token hashCode and equals methods are incorrect", "bug_description": "I had wrong assumptions about object vs functional equality. This would need to go to 2.0.1 (target version field is AWOL)\n\"Luckily\" the implications are spurious access denied errors, and not the other way around.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java"], "label": 1, "es_results": []}, {"bug_id": 13126, "bug_title": "Clean up MapJoinOperator properly to avoid object cache reuse with unintentional states", "bug_description": "For a given job, one task may reuse other task&apos;s object cache (plan cache) such as MapJoinOperator. This is fine. But if we have some dirty states left over, it may cause issue like wrong results.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.MapJoinOperator.java"], "label": 1, "es_results": []}, {"bug_id": 13163, "bug_title": "ORC MemoryManager thread checks are fatal, should WARN ", "bug_description": "The MemoryManager is tied to a WriterOptions on create, which can occur in a different thread from the writer calls.\nThis is unexpected, but safe and needs a warning not a fatal.\n\n\n\n  /**\n\n   * Light weight thread-safety check for multi-threaded access patterns\n\n   */\n\n  private void checkOwner() {\n\n    Preconditions.checkArgument(ownerLock.isHeldByCurrentThread(),\n\n        \"Owner thread expected %s, got %s\",\n\n        ownerLock.getOwner(),\n\n        Thread.currentThread());\n\n  }\n\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.orc.impl.MemoryManager.java"], "label": 1, "es_results": []}, {"bug_id": 13186, "bug_title": "ALTER TABLE RENAME should lowercase table name and hdfs location", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java", "org.apache.hadoop.hive.metastore.HiveAlterHandler.java"], "label": 1, "es_results": []}, {"bug_id": 13096, "bug_title": "Cost to choose side table in MapJoin conversion based on cumulative cardinality", "bug_description": "HIVE-11954 changed the logic to choose the side table in the MapJoin conversion algorithm. Initial heuristic for the cost was based on number of heavyweight operators.\nThis extends that work so the heuristic is based on accumulate cardinality. In the future, we should choose the side based on total latency for the input.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java"], "label": 1, "es_results": []}, {"bug_id": 13236, "bug_title": "LLAP: token renewal interval needs to be set", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.llap.security.SecretManager.java"], "label": 1, "es_results": []}, {"bug_id": 13201, "bug_title": "Compaction should not be allowed on non-ACID table", "bug_description": "Looks like compaction is allowed on non-ACID table, although that&apos;s of no sense and does nothing. Moreover the compaction request will be enqueued into COMPACTION_QUEUE metastore table, which brings unnecessary overhead.\nWe should prevent compaction commands being allowed on non-ACID tables.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.ErrorMsg.java", "org.apache.hadoop.hive.ql.exec.DDLTask.java"], "label": 1, "es_results": []}, {"bug_id": 13260, "bug_title": "ReduceSinkDeDuplication throws exception when pRS key is empty", "bug_description": "Steps to reproduce:\n\n\n\nset hive.mapred.mode=nonstrict;\n\nset hive.cbo.enable=false;\n\n\n\nset hive.map.aggr=false;\n\n\n\nset hive.groupby.skewindata=false;\n\nset mapred.reduce.tasks=31;\n\n\n\nselect compute_stats(a,16),compute_stats(b,16),compute_stats(c,16),compute_stats(d,16)\n\nfrom\n\n(\n\nselect\n\n  average(DISTINCT substr(src.value,5)) as a,\n\n  max(substr(src.value,5)) as b,\n\n  variance(substr(src.value,5)) as c,\n\n  var_samp(substr(src.value,5)) as d\n\n from src)subq;\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java"], "label": 1, "es_results": []}, {"bug_id": 13299, "bug_title": "Column Names trimmed of leading and trailing spaces", "bug_description": "PROBLEM:\nAs per the Hive Language DDL: \nhttps://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL\nIn Hive 0.12 and earlier, only alphanumeric and underscore characters are allowed in table and column names.\nIn Hive 0.13 and later, column names can contain any Unicode character (see HIVE-6013). Any column name that is specified within backticks (`) is treated literally.\nHowever column names\n\n\n\n` left` resulted in `left`\n\n` middle ` resulted in `middle`\n\n`right ` resulted in `right`\n\n`middle space` resulted in `middle space`\n\n` middle space ` resulted in `middle space`\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.metastore.hbase.HBaseStore.java", "org.apache.hadoop.hive.metastore.ObjectStore.java"], "label": 1, "es_results": []}, {"bug_id": 13125, "bug_title": "Support masking and filtering of rows/columns", "bug_description": "Traditionally, access control at the row and column level is implemented through views. Using views as an access control method works well only when access rules, restrictions, and conditions are monolithic and simple. It however becomes ineffective when view definitions become too complex because of the complexity and granularity of privacy and security policies. It also becomes costly when a large number of views must be manually updated and maintained. In addition, the ability to update views proves to be challenging. As privacy and security policies evolve, required updates to views may negatively affect the security logic particularly when database applications reference the views directly by name. HIVE row and column access control helps resolve all these problems.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java", "org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationValidator.java", "org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java", "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java", "org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java", "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.DummyHiveAuthorizationValidator.java"], "label": 1, "es_results": []}, {"bug_id": 13286, "bug_title": "Query ID is being reused across queries", "bug_description": "Aihua Xu I see this commit made via HIVE-11488. I see that query id is being reused across queries. This defeats the purpose of a query id. I am not sure what the purpose of the change in that jira is but it breaks the assumption about a query id being unique for each query. Please take a look into this at the earliest.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.Driver.java", "org.apache.hadoop.hive.cli.CliDriver.java", "org.apache.hive.service.cli.session.HiveSessionImpl.java", "org.apache.hive.service.cli.session.TestHiveSessionImpl.java"], "label": 1, "es_results": []}, {"bug_id": 13151, "bug_title": "Clean up UGI objects in FileSystem cache for transactions", "bug_description": "One issue with FileSystem.CACHE is that it does not clean itself. The key in that cache includes UGI object. When new UGI objects are created and used with the FileSystem api, new entries get added to the cache.\nWe need to manually clean up those UGI objects once they are no longer in use.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.txn.compactor.Initiator.java", "org.apache.hadoop.hive.ql.txn.compactor.Worker.java", "org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java", "org.apache.hive.hcatalog.streaming.HiveEndPoint.java", "org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java", "org.apache.hadoop.hive.ql.TestTxnCommands2.java"], "label": 1, "es_results": []}, {"bug_id": 13326, "bug_title": "HiveServer2: Make ZK config publishing configurable", "bug_description": "We should revert to older behaviour when config publishing is disabled.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java", "org.apache.hive.jdbc.authorization.TestHS2AuthzContext.java", "org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.java", "org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.java", "org.apache.hive.service.server.HiveServer2.java", "org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth.java", "org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java", "org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hive.jdbc.TestJdbcWithMiniMr.java", "org.apache.hive.jdbc.miniHS2.MiniHS2.java"], "label": 1, "es_results": []}, {"bug_id": 13376, "bug_title": "HoS emits too many logs with application state", "bug_description": "The logs get flooded with something like:\n> Mar 28, 3:12:21.851 PM        INFO    org.apache.hive.spark.client.SparkClientImpl\n> [stderr-redir-1]: 16/03/28 15:12:21 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)\n> Mar 28, 3:12:21.912 PM        INFO    org.apache.hive.spark.client.SparkClientImpl\n> [stderr-redir-1]: 16/03/28 15:12:21 INFO yarn.Client: Application report for application_1458679386200_0149 (state: RUNNING)\n> Mar 28, 3:12:22.853 PM        INFO    org.apache.hive.spark.client.SparkClientImpl\n> [stderr-redir-1]: 16/03/28 15:12:22 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)\n> Mar 28, 3:12:22.913 PM        INFO    org.apache.hive.spark.client.SparkClientImpl\n> [stderr-redir-1]: 16/03/28 15:12:22 INFO yarn.Client: Application report for application_1458679386200_0149 (state: RUNNING)\n> Mar 28, 3:12:23.855 PM        INFO    org.apache.hive.spark.client.SparkClientImpl\n> [stderr-redir-1]: 16/03/28 15:12:23 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)\nWhile this is good information, it is a bit much.\nSeems like SparkJobMonitor hard-codes its interval to 1 second.  It should be higher and perhaps made configurable.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java"], "label": 1, "es_results": []}, {"bug_id": 13333, "bug_title": "StatsOptimizer throws ClassCastException", "bug_description": "mvn test -Dtest=TestCliDriver -Dtest.output.overwrite=true -Dqfile=cbo_rp_udf_udaf.q -Dhive.compute.query.using.stats=true repros the issue.\nIn StatsOptimizer with return path on, we may have aggr($f0), aggr($f1) in GBY\nand then select aggr($f1), aggr($f0) in SEL.\nThus we need to use colExp to find out which position is\ncorresponding to which position.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java"], "label": 1, "es_results": []}, {"bug_id": 13428, "bug_title": "ZK SM in LLAP should have unique paths per cluster", "bug_description": "Noticed this while working on some other patch", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.llap.security.SecretManager.java"], "label": 1, "es_results": []}, {"bug_id": 13405, "bug_title": "Fix Connection Leak in OrcRawRecordMerger", "bug_description": "In OrcRawRecordMerger.getLastFlushLength, if the opened stream throws an IOException on .available() or on .readLong(), the function will exit without closing the stream.\nThis patch adds a try-with-resources to fix this.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java"], "label": 1, "es_results": []}, {"bug_id": 13410, "bug_title": "PerfLog metrics scopes not closed if there are exceptions on HS2", "bug_description": "If there are errors, the HS2 PerfLog api scopes are not closed.  Then there are sometimes messages like &apos;java.io.IOException: Scope named api_parse is not closed, cannot be opened.&apos;\nI had simply forgetting to close the dangling scopes if there is an exception.  Doing so now.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java", "org.apache.hadoop.hive.ql.Driver.java", "org.apache.hadoop.hive.ql.log.PerfLogger.java", "org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java"], "label": 1, "es_results": []}, {"bug_id": 13553, "bug_title": "CTE with upperCase alias throws exception", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 13463, "bug_title": "Fix ImportSemanticAnalyzer to allow for different src/dst filesystems", "bug_description": "In ImportSemanticAnalyzer, there is an assumption that the src filesystem for import and the final location are on the same filesystem. Therefore the check for emptiness and getExternalTmpLocation will be looking on the wrong filesystem and will cause an error. The output path should be fed into getExternalTmpLocation to get a temporary file on the correct filesystem. The check for emptiness should use the output filesystem.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 13512, "bug_title": "Make initializing dag ids in TezWork thread safe for parallel compilation", "bug_description": "When parallel query compilation is enabled, it is possible for concurrent running threads to create TezWork objects that have the same dag id. This is because the counter used to obtain the next dag id is not thread safe. The counter should be an AtomicInteger rather than an int.\n\n\n\n  private static int counter;\n\n  ...\n\n  public TezWork(String queryId, Configuration conf) {\n\n    this.dagId = queryId + \":\" + (++counter);\n\n    ...\n\n  }\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.plan.TezWork.java"], "label": 1, "es_results": []}, {"bug_id": 13619, "bug_title": "Bucket map join plan is incorrect", "bug_description": "Same as HIVE-12992. Missed a single line check. TPCDS query 4 with bucketing can produce this issue.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.OperatorUtils.java"], "label": 1, "es_results": []}, {"bug_id": 13542, "bug_title": "Missing stats for tables in TPCDS performance regression suite", "bug_description": "These are the tables whose stats are missing in data/files/tpcds-perf/metastore_export/csv/TAB_COL_STATS.txt:\n\ncatalog_returns\ncatalog_sales\ninventory\nstore_returns\nstore_sales\nweb_returns\nweb_sales\n\nThanks to Jesus Camacho Rodriguez for discovering this issue.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.QTestUtil.java"], "label": 1, "es_results": []}, {"bug_id": 13618, "bug_title": "Trailing spaces in partition column will be treated differently", "bug_description": "We store the partition spec value in the metastore. In mysql (and derby i think), the trailing space is ignored. That is, if you have a partition column \"col\" (type varchar or string) with value \"a \" and then select from the table where col = \"a\", it will return. However, in postgres and Oracle, the trailing space is not ignored. ", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.metastore.ObjectStore.java"], "label": 1, "es_results": []}, {"bug_id": 12996, "bug_title": "Temp tables should not be locked", "bug_description": "Internally, INSERT INTO ... VALUES statements use temp table to accomplish its functionality. But temp tables shouldn&apos;t be stored in the metastore tables for ACID, because they are by definition only visible inside the session that created them, and we don&apos;t allow multiple threads inside a session. If a temp table is used in a query, it should be ignored by lock manager.\n\n\n\nmysql> select * from COMPLETED_TXN_COMPONENTS;\n\n+-----------+--------------+-----------------------+------------------+\n\n| CTC_TXNID | CTC_DATABASE | CTC_TABLE             | CTC_PARTITION    |\n\n+-----------+--------------+-----------------------+------------------+\n\n|         1 | acid         | t1                    | NULL             |\n\n|         1 | acid         | values__tmp__table__1 | NULL             |\n\n|         2 | acid         | t1                    | NULL             |\n\n|         2 | acid         | values__tmp__table__2 | NULL             |\n\n|         3 | acid         | values__tmp__table__3 | NULL             |\n\n|         3 | acid         | t1                    | NULL             |\n\n|         4 | acid         | values__tmp__table__1 | NULL             |\n\n|         4 | acid         | t2p                   | ds=today         |\n\n|         5 | acid         | values__tmp__table__1 | NULL             |\n\n|         5 | acid         | t3p                   | ds=today/hour=12 |\n\n+-----------+--------------+-----------------------+------------------+\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java"], "label": 1, "es_results": []}, {"bug_id": 13293, "bug_title": "Query occurs performance degradation after enabling parallel order by for Hive on Spark", "bug_description": "I use TPCx-BB to do some performance test on Hive on Spark engine. And found query 10 has performance degradation when enabling parallel order by.\nIt seems that sampling cost much time before running the real query.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.spark.SortByShuffler.java", "org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java"], "label": 1, "es_results": []}, {"bug_id": 13608, "bug_title": "We should provide better error message while constraints with duplicate names are created", "bug_description": "\n\n\nPREHOOK: query: create table t1(x int, constraint pk1 primary key (x) disable novalidate)\n\nPREHOOK: type: CREATETABLE\n\nPREHOOK: Output: database:default\n\nPREHOOK: Output: default@t1\n\nPOSTHOOK: query: create table t1(x int, constraint pk1 primary key (x) disable novalidate)\n\nPOSTHOOK: type: CREATETABLE\n\nPOSTHOOK: Output: database:default\n\nPOSTHOOK: Output: default@t1\n\nPREHOOK: query: create table t2(x int, constraint pk1 primary key (x) disable novalidate)\n\nPREHOOK: type: CREATETABLE\n\nPREHOOK: Output: database:default\n\nPREHOOK: Output: default@t2\n\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don&apos;t support retries at the client level.)\n\n\n\nIn the above case, it seems like useful error message is lost. It looks like a  generic problem with metastore server/client exception handling and message propagation. Seems like exception parsing logic of RetryingMetaStoreClient::invoke() needs to be updated.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.QTestUtil.java", "org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java"], "label": 1, "es_results": []}, {"bug_id": 13699, "bug_title": "Make JavaDataModel#get thread safe for parallel compilation", "bug_description": "The class JavaDataModel has a static method, #get, that is not thread safe. This may be an issue when parallel query compilation is enabled because two threads may attempt to call JavaDataModel#get at the same time, etc.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.util.JavaDataModel.java"], "label": 1, "es_results": []}, {"bug_id": 12643, "bug_title": "For self describing InputFormat do not replicate schema information in partitions", "bug_description": "Since self describing Input Formats don&apos;t use individual partition schemas for schema resolution, there is no need to send that info to tasks.\nDoing this should cut down plan size.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java", "org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java", "org.apache.hadoop.hive.ql.exec.Utilities.java", "org.apache.hadoop.hive.metastore.MetaStoreUtils.java", "org.apache.hadoop.hive.ql.plan.PartitionDesc.java"], "label": 1, "es_results": []}, {"bug_id": 13844, "bug_title": "Invalid index handler in org.apache.hadoop.hive.ql.index.HiveIndex class", "bug_description": "Class org.apache.hadoop.hive.ql.index.HiveIndex has invalid handler name &apos;org.apache.hadoop.hive.ql.AggregateIndexHandler&apos;. The actual FQ class name is &apos;org.apache.hadoop.hive.ql.index.AggregateIndexHandler&apos;\n\n\n\n  public static enum IndexType {\n\n    AGGREGATE_TABLE(\"aggregate\", \"org.apache.hadoop.hive.ql.AggregateIndexHandler\"),\n\n    COMPACT_SUMMARY_TABLE(\"compact\", \"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler\"),\n\n    BITMAP_TABLE(\"bitmap\",\"org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler\");\n\n\n\n    private IndexType(String indexType, String className) {\n\n      indexTypeName = indexType;\n\n      this.handlerClsName = className;\n\n    }\n\n\n\n    private final String indexTypeName;\n\n    private final String handlerClsName;\n\n\n\n    public String getName() {\n\n      return indexTypeName;\n\n    }\n\n\n\n    public String getHandlerClsName() {\n\n      return handlerClsName;\n\n    }\n\n  }\n\n  \n\n\n\nBecause all of the above statement like &apos;SHOW INDEXES ON MY_TABLE&apos; doesn&apos;t work in case of configured &apos;org.apache.hadoop.hive.ql.index.AggregateIndexHandler&apos; as index handler. In hive server log is observed java.lang.NullPointerException.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.index.HiveIndex.java"], "label": 1, "es_results": []}, {"bug_id": 13858, "bug_title": "LLAP: A preempted task can end up waiting on completeInitialization if some part of the executing code suppressed the interrupt", "bug_description": "An interrupt along with a HiveProcessor.abort call is made when attempting to preempt a task.\nIn this specific case, the task was in the middle of HDFS IO - which &apos;handled&apos; the interrupt by retrying. As a result the interrupt status on the thread was reset - so instead of skipping the future.get in completeInitialization - the task ended up blocking there.\nEnd result - a single executor slot permanently blocked in LLAP. Depending on what else is running - this can cause a cluster level deadlock.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java", "org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java", "org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java", "org.apache.hadoop.hive.ql.exec.Operator.java", "org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java", "org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 13838, "bug_title": "Set basic stats as inaccurate for all ACID tables", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.StatsTask.java"], "label": 1, "es_results": []}, {"bug_id": 13833, "bug_title": "Add an initial delay when starting the heartbeat", "bug_description": "Since the scheduling of heartbeat happens immediately after lock acquisition, it&apos;s unnecessary to send heartbeat at the time when locks is acquired. Add an initial delay to skip this.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java"], "label": 1, "es_results": []}, {"bug_id": 13809, "bug_title": "Hybrid Grace Hash Join memory usage estimation did not take into account the bloom filter size", "bug_description": "Memory estimation is important during hash table loading, because we need to make the decision of whether to load the next hash partition in memory or spill it. If the assumption is there&apos;s enough memory but it turns out not the case, we will run into OOM problem.\nCurrently hybrid grace hash join memory usage estimation didn&apos;t take into account the bloom filter size. In large test cases (TB scale) the bloom filter grows as big as hundreds of MB, big enough to cause estimation error.\nThe solution is to count in the bloom filter size into memory estimation.\nAnother issue this patch will fix is possible NPE due to object cache reuse during hybrid grace hash join.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.MapJoinOperator.java", "org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java", "org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java"], "label": 1, "es_results": []}, {"bug_id": 13159, "bug_title": "TxnHandler should support datanucleus.connectionPoolingType = None", "bug_description": "Right now, one has to choose bonecp or dbcp.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.metastore.txn.TxnHandler.java"], "label": 1, "es_results": []}, {"bug_id": 14132, "bug_title": "Do not fail config validation for removed configs", "bug_description": "Users may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.processors.SetProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 13191, "bug_title": "DummyTable map joins mix up columns between tables", "bug_description": "\n\n\nSELECT\n\n  a.key,\n\n  a.a_one,\n\n  b.b_one,\n\n  a.a_zero,\n\n  b.b_zero\n\nFROM\n\n(\n\n    SELECT\n\n      11 key,\n\n      0 confuse_you,\n\n      1 a_one,\n\n      0 a_zero\n\n) a\n\nLEFT JOIN\n\n(\n\n    SELECT\n\n      11 key,\n\n      0 confuse_you,\n\n      1 b_one,\n\n      0 b_zero\n\n) b\n\nON a.key = b.key\n\n;\n\n\n\n11      1       0       0       1\n\n\n\nThis should be 11, 1, 1, 0, 0 instead. \nDisabling map-joins & using shuffle-joins returns the right result.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java"], "label": 1, "es_results": []}, {"bug_id": 14236, "bug_title": "CTAS with UNION ALL puts the wrong stats in Tez", "bug_description": "to repo. in Tez, create table t as select * from src union all select * from src;", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FileSinkOperator.java", "org.apache.hadoop.hive.ql.exec.MoveTask.java", "org.apache.hadoop.hive.ql.parse.GenTezUtils.java", "org.apache.hadoop.hive.ql.plan.FileSinkDesc.java", "org.apache.hadoop.hive.ql.stats.StatsCollectionContext.java", "org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java", "org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java"], "label": 1, "es_results": []}, {"bug_id": 11918, "bug_title": "Implement/Enable constant related optimization rules in Calcite", "bug_description": "Right now, Hive optimizer (Calcite) is short of the constant related optimization rules. For example, constant folding, constant propagation and constant transitive rules. Although Hive later provides those rules in the logical optimizer, we would like to implement those inside Calcite. This will benefit the current optimization as well as the optimization based on return path that we are planning to use in the future. This JIRA is the umbrella JIRA to implement/enable those rules.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.QTestUtil.java", "org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java"], "label": 1, "es_results": []}, {"bug_id": 14229, "bug_title": "the jars in hive.aux.jar.paths are not added to session classpath ", "bug_description": "The jars in hive.reloadable.aux.jar.paths are being added to HiveServer2 classpath while hive.aux.jar.paths is not. \nThen the local task like &apos;select udf from src&apos; will fail to find needed udf class.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.session.SessionState.java", "org.apache.hadoop.hive.ql.session.TestSessionState.java", "org.apache.hive.service.cli.session.HiveSessionImpl.java", "org.apache.hadoop.hive.ql.exec.Utilities.java", "org.apache.hadoop.hive.ql.processors.ReloadProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 14367, "bug_title": "Estimated size for constant nulls is 0", "bug_description": "since type is incorrectly assumed as void.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.stats.StatsUtils.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java", "org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java"], "label": 1, "es_results": []}, {"bug_id": 14322, "bug_title": "Postgres db issues after Datanucleus 4.x upgrade", "bug_description": "With the upgrade to  datanucleus 4.x versions in HIVE-6113, hive does not work properly with postgres.\nThe nullable fields in the database have string \"NULL::character varying\" instead of real NULL values. This causes various issues.\nOne example is -\n\n\n\nhive> create table t(i int);\n\nOK\n\nTime taken: 1.9 seconds\n\nhive> create view v as select * from t;\n\nOK\n\nTime taken: 0.542 seconds\n\nhive> select * from v;\n\nFAILED: SemanticException Unable to fetch table v. java.net.URISyntaxException: Relative path in absolute URI: NULL::character%20varying\n\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 13383, "bug_title": "RetryingMetaStoreClient retries non retriable embedded metastore client ", "bug_description": "Embedded metastore clients can&apos;t be retried, they throw an exception - \"For direct MetaStore DB connections, we don&apos;t support retries at the client level.\"\nThis tends to mask the real error that caused the attempts to retry. RetryingMetaStoreClient shouldn&apos;t even attempt to reconnect when direct/embedded metastore client is used.\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java"], "label": 1, "es_results": []}, {"bug_id": 14251, "bug_title": "Union All of different types resolves to incorrect data", "bug_description": "create table src(c1 date, c2 int, c3 double);\ninsert into src values (&apos;2016-01-01&apos;,5,1.25);\nselect * from \n(select c1 from src union all\nselect c2 from src union all\nselect c3 from src) t;\nIt will return NULL for the c1 values. Seems the common data type is resolved to the last c3 which is double.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FunctionRegistry.java", "org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java", "org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java"], "label": 1, "es_results": []}, {"bug_id": 13066, "bug_title": "Hive on Spark gives incorrect results when speculation is on", "bug_description": "The issue is reported by users. One possible reason is that we always append 0 as the attempt ID for each task so that hive won&apos;t be able to distinguish between speculative tasks and original ones.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.spark.HivePairFlatMapFunction.java"], "label": 1, "es_results": []}, {"bug_id": 15096, "bug_title": "hplsql registerUDF conflicts with pom.xml", "bug_description": "in hplsql code, registerUDF code is\n    sql.add(\"ADD JAR \" + dir + \"hplsql.jar\");\n    sql.add(\"ADD JAR \" + dir + \"antlr-runtime-4.5.jar\");\n    sql.add(\"ADD FILE \" + dir + Conf.SITE_XML);\nbut pom configufation is\n  <parent>\n    <groupId>org.apache.hive</groupId>\n    <artifactId>hive</artifactId>\n    <version>2.2.0-SNAPSHOT</version>\n    <relativePath>../pom.xml</relativePath>\n  </parent>\n  <artifactId>hive-hplsql</artifactId>\n  <packaging>jar</packaging>\n  <name>Hive HPL/SQL</name>\n    <dependency>\n       <groupId>org.antlr</groupId>\n       <artifactId>antlr4-runtime</artifactId>\n       <version>4.5</version>\n    </dependency>\nwhen run hplsql , errors occur as below\n Error while processing statement: /opt/apps/apache-hive-2.0.0-bin/lib/hplsql.jar does not exist", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.hplsql.Exec.java"], "label": 1, "es_results": []}, {"bug_id": 13904, "bug_title": "Ignore case when retrieving ColumnInfo from RowResolver", "bug_description": "To reproduce:\n\n-- upper case in subq\n\nexplain\n\nselect * from src b\n\nwhere exists\n\n  (select a.key from src a\n\n  where b.VALUE = a.VALUE\n\n  );\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.1", "fixed_version": "rel/release-2.1.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.QBSubQuery.java"], "label": 1, "es_results": []}, {"bug_id": 13987, "bug_title": "Clarify current error shown when HS2 is down", "bug_description": "When HS2 is down and a query is run, the following error is shown in beeline:\n\n\n\n0: jdbc:hive2://localhost:10000> show tables;\n\nError: org.apache.thrift.transport.TTransportException (state=08S01,code=0)\n\n\n\nIt may be more helpful to also indicate that the reason for this is that HS2 is down, such as:\n\n\n\n0: jdbc:hive2://localhost:10000> show tables;\n\nHS2 may be unavailable, check server status\n\nError: org.apache.thrift.transport.TTransportException (state=08S01,code=0)\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.beeline.BeeLine.java"], "label": 1, "es_results": []}, {"bug_id": 14074, "bug_title": "RELOAD FUNCTION should update dropped functions", "bug_description": "Due to HIVE-2573, functions are stored in a per-session registry and only loaded in from the metastore when hs2 or hive cli is started. Running RELOAD FUNCTION in the current session is a way to force a reload of the functions, so that changes that occurred in other running sessions will be reflected in the current session, without having to restart the current session. However, while functions that are created in other sessions will now appear in the current session, functions that have been dropped are not removed from the current session&apos;s registry. It seems inconsistent that created functions are updated while dropped functions are not.", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 14433, "bug_title": "refactor LLAP plan cache avoidance and fix issue in merge processor", "bug_description": "Map and reduce processors do this:\n\n    if (LlapProxy.isDaemon()) {\n\n      cache = new org.apache.hadoop.hive.ql.exec.mr.ObjectCache(); // do not cache plan\n\n...\n\n\n\nbut merge processor just gets the plan. If it runs in LLAP, it can get a cached plan. Need to move this logic into ObjectCache itself, via a isPlan arg or something. That will also fix this issue for merge processor", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.MapJoinOperator.java", "org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java", "org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java", "org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.java", "org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 14805, "bug_title": "Subquery inside a view will have the object in the subquery as the direct input ", "bug_description": "Here is the repro steps.\n\ncreate table t1(col string);\n\ncreate view v1 as select * from t1;\n\ncreate view dataview as select * from  (select * from v1) v2;\n\nselect * from dataview;\n\n\n\nIf hive is configured with authorization hook like Sentry, it will require the access not only for dataview but also for v1, which should not be required.\nThe subquery seems to not carry insideview property from the parent query.\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.plan.TestViewEntity.java"], "label": 1, "es_results": []}, {"bug_id": 14820, "bug_title": "RPC server for spark inside HS2 is not getting server address properly", "bug_description": "When hive.spark.client.rpc.server.address is configured, this property is not retrieved properly because we are getting the value by String hiveHost = config.get(HiveConf.ConfVars.SPARK_RPC_SERVER_ADDRESS);  which always returns null in getServerAddress() call of RpcConfiguration.java. Rather it should be String hiveHost = config.get(HiveConf.ConfVars.SPARK_RPC_SERVER_ADDRESS.varname);.\n", "project": "Apache", "sub_project": "HIVE", "version": "release-2.0.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.spark.client.rpc.RpcConfiguration.java", "org.apache.hive.spark.client.rpc.TestRpc.java"], "label": 1, "es_results": []}, {"bug_id": 12644, "bug_title": "Support for offset in HiveSortMergeRule", "bug_description": "After HIVE-11531 goes in, HiveSortMergeRule needs to be extended to support offset properly when it merges operators that contain Limit. Otherwise, limit pushdown through outer join optimization (introduced in HIVE-11684) will not work properly.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "release-2.0.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortJoinReduceRule.java", "org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortMergeRule.java"], "label": 1, "es_results": []}, {"bug_id": 13645, "bug_title": "Beeline needs null-guard around hiveVars and hiveConfVars read", "bug_description": "Beeline has a bug wherein if a user does a !save ever, then on next load, if beeline.hiveVariables or beeline.hiveconfvariables are empty, i.e. {} or unspecified, then it loads it as null, and then, on next connect, there is no null-check on these variables leading to an NPE.  ", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/release-1.2.2", "fixed_files": ["org.apache.hive.beeline.DatabaseConnection.java"], "label": 1, "es_results": []}, {"bug_id": 13882, "bug_title": "When hive.server2.async.exec.async.compile is turned on, from JDBC we will get \"The query did not generate a result set\" ", "bug_description": " The following would fail with  \"The query did not generate a result set\"\n    stmt.execute(\"SET hive.driver.parallel.compilation=true\");\n    stmt.execute(\"SET hive.server2.async.exec.async.compile=true\");\n    ResultSet res =  stmt.executeQuery(\"SELECT * FROM \" + tableName);\n    res.next();", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.service.cli.OperationStatus.java", "org.apache.hive.jdbc.TestJdbcWithMiniHS2.java", "org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java", "org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hive.service.cli.thrift.ThriftCLIService.java", "org.apache.hive.jdbc.HiveStatement.java", "org.apache.hive.service.cli.operation.Operation.java", "org.apache.hive.service.rpc.thrift.TGetOperationStatusResp.java"], "label": 1, "es_results": []}, {"bug_id": 14003, "bug_title": "queries running against llap hang at times - preemption issues", "bug_description": "The preemption logic in the Hive processor needs some more work. There are definitely windows where the abort flag is completely dropped within the Hive processor.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/release-2.1.1", "fixed_files": ["org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java", "org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java", "org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java", "org.apache.hadoop.hive.ql.exec.MapredContext.java", "org.apache.hadoop.hive.ql.exec.Operator.java", "org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java", "org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java", "org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 14059, "bug_title": "Missing license headers for two files", "bug_description": "As noted by Sushanth Sowmyan, two files are missing the Apache headers:\n\n/Users/sush/t/rel/apache-hive-2.1.0-src/common/src/java/org/apache/hive/common/util/DateParser.java\n\n/Users/sush/t/rel/apache-hive-2.1.0-src/common/src/test/org/apache/hive/common/util/TestDateParser.java\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.common.util.DateParser.java", "org.apache.hive.common.util.TestDateParser.java"], "label": 1, "es_results": []}, {"bug_id": 14060, "bug_title": "Hive: Remove bogus \"localhost\" from Hive splits", "bug_description": "On remote filesystems like Azure, GCP and S3, the splits contain a filler location of \"localhost\".\nThis is worse than having no location information at all - on large clusters yarn waits upto 200[1] seconds for heartbeat from \"localhost\" before allocating a container.\nTo speed up this process, the split affinity provider should scrub the bogus \"localhost\" from the locations and allow for the allocation of \"*\" containers instead on each heartbeat.\n[1] - yarn.scheduler.capacity.node-locality-delay=40 x heartbeat of 5s", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.tez.Utils.java"], "label": 1, "es_results": []}, {"bug_id": 14062, "bug_title": "Changes from HIVE-13502 overwritten by HIVE-13566", "bug_description": "Appears that changes from HIVE-13566 overwrote the changes from HIVE-13502. I will confirm with the author that it was inadvertent before I re-add it. Thanks", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.beeline.Commands.java", "org.apache.hive.beeline.BeeLine.java", "org.apache.hive.beeline.TestBeeLineWithArgs.java", "org.apache.hive.jdbc.Utils.java"], "label": 1, "es_results": []}, {"bug_id": 13997, "bug_title": "Insert overwrite directory does not overwrite existing files", "bug_description": "Can be easily reproduced by running INSERT OVERWRITE DIRECTORY to the same dir twice.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.MoveTask.java", "org.apache.hadoop.hive.ql.metadata.Hive.java"], "label": 1, "es_results": []}, {"bug_id": 14073, "bug_title": "update config whiltelist for sql std authorization ", "bug_description": "New configs that should go in security whitelist have been added. Whitelist needs updating.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.java", "org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 14126, "bug_title": "With ranger enabled, partitioned columns is returned first when you execute select star", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.metadata.Table.java"], "label": 1, "es_results": []}, {"bug_id": 14122, "bug_title": "VectorMapOperator: Missing update to AbstractMapOperator::numRows", "bug_description": "The INPUT_RECORDS counter is out of sync with the actual # of rows-read in vectorized and non-vectorized modes.\nThis means Tez record summaries are off by a large margin or is 0 for those vertices.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java"], "label": 1, "es_results": []}, {"bug_id": 13945, "bug_title": "Decimal value is displayed as rounded when selecting where clause with that decimal value.", "bug_description": "Create a table withe a column of decimal type(38,18) and insert &apos;4327269606205.029297&apos;. Then select with that value displays its rounded value, which is 4327269606205.029300000000000000\n\n0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4> drop table if exists test;\n\nNo rows affected (0.229 seconds)\n\n0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4>\n\n0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4> create table test (dc decimal(38,18));\n\nNo rows affected (0.125 seconds)\n\n0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4>\n\n0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4> insert into table test values (4327269606205.029297);\n\nNo rows affected (2.372 seconds)\n\n0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4>\n\n0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4> select * from test;\n\n+-----------------------------------+--+\n\n|              test.dc              |\n\n+-----------------------------------+--+\n\n| 4327269606205.029297000000000000  |\n\n+-----------------------------------+--+\n\n1 row selected (0.123 seconds)\n\n0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4>\n\n0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4> select * from test where dc = 4327269606205.029297000000000000;\n\n+-----------------------------------+--+\n\n|              test.dc              |\n\n+-----------------------------------+--+\n\n| 4327269606205.029300000000000000  |\n\n+-----------------------------------+--+\n\n1 row selected (0.109 seconds)\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.java", "org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.java", "org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java", "org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java", "org.apache.hadoop.hive.ql.udf.UDFToShort.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.java", "org.apache.orc.impl.ConvertTreeReaderFactory.java", "org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java", "org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java", "org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java", "org.apache.hadoop.hive.ql.udf.UDFToLong.java", "org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.java", "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.common.type.HiveDecimal.java", "org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java", "org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java", "org.apache.orc.impl.RecordReaderImpl.java", "org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java", "org.apache.hadoop.hive.ql.udf.UDFToInteger.java", "org.apache.hadoop.hive.ql.exec.FileSinkOperator.java", "org.apache.hadoop.hive.ql.parse.ParseUtils.java", "org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java"], "label": 1, "es_results": []}, {"bug_id": 14197, "bug_title": "LLAP service driver precondition failure should include the values", "bug_description": "LLAP service driver&apos;s precondition failure message are like below\n\n\n\nWorking memory + cache has to be smaller than the container sizing\n\n\n\nIt will be better to include the actual values for the sizes in the precondition failure message.\nNO PRECOMMIT TESTS", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java"], "label": 1, "es_results": []}, {"bug_id": 14176, "bug_title": "CBO nesting windowing function within each other when merging Project operators", "bug_description": "The translation into a physical plan does not support this way of expressing windowing functions. Instead, we will not merge the Project operators when we find this pattern.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java", "org.apache.hadoop.hive.ql.parse.CalcitePlanner.java"], "label": 1, "es_results": []}, {"bug_id": 14115, "bug_title": "Custom FetchFormatter is not supported", "bug_description": "The following code is supported only FetchFormatter of ThriftFormatter and DefaultFetchFormatter. It can not be used Custom FetchFormatter.\n\n\n\n        if (SessionState.get().isHiveServerQuery()) {\n\n          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER,ThriftFormatter.class.getName());\n\n        } else {\n\n          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, DefaultFetchFormatter.class.getName());\n\n        }\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.TaskCompiler.java"], "label": 1, "es_results": []}, {"bug_id": 13887, "bug_title": "LazySimpleSerDe should parse \"NULL\" dates faster", "bug_description": "Date string which contain \"NULL\" or \"(null)\" are being parsed through a very slow codepath involving exception handling as a normal codepath.\nThese are currently ~4x slower than parsing an actual date field.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.java", "org.apache.hive.benchmark.serde.LazySimpleSerDeBench.java", "org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java", "org.apache.hadoop.hive.serde2.lazy.LazyUtils.java", "org.apache.hadoop.hive.serde2.lazy.LazyDate.java"], "label": 1, "es_results": []}, {"bug_id": 14141, "bug_title": "Fix for HIVE-14062 breaks indirect urls in beeline", "bug_description": "Looks like the patch for HIVE-14062 breaks indirect urls which uses environment variables to get the url in beeline\nIn order to reproduce this issue:\n\n$ export BEELINE_URL_DEFAULT=\"jdbc:hive2://localhost:10000\"\n\n$ beeline -you default\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.beeline.Commands.java"], "label": 1, "es_results": []}, {"bug_id": 13258, "bug_title": "LLAP: Add hdfs bytes read and spilled bytes to tez print summary", "bug_description": "When printing counters to console it will be useful to print hdfs bytes read and spilled bytes which will help with debugging issues faster. ", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.llap.counters.QueryFragmentCounters.java", "org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java", "org.apache.hadoop.hive.llap.LlapUtil.java", "org.apache.hadoop.hive.llap.tezplugins.LlapTezUtils.java", "org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java", "org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java", "org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter.java", "org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java", "org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java", "org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java", "org.apache.hadoop.hive.llap.counters.LlapIOCounters.java"], "label": 1, "es_results": []}, {"bug_id": 14144, "bug_title": "Permanent functions are showing up in show functions, but describe says it does not exist", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.util.ResourceDownloader.java", "org.apache.hadoop.hive.ql.exec.Registry.java"], "label": 1, "es_results": []}, {"bug_id": 14254, "bug_title": "Correct the hive version by changing \"svn\" to \"git\"", "bug_description": "When running \"hive --version\", \"subversion\" is displayed below, which should be \"git\".\n$ hive --version\nHive 2.1.0-SNAPSHOT\nSubversion git://", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.common.util.HiveVersionInfo.java", "org.apache.hive.common.HiveVersionAnnotation.java"], "label": 1, "es_results": []}, {"bug_id": 13934, "bug_title": "Configure Tez to make nocondiional task size memory available for the Processor", "bug_description": "Currently, noconditionaltasksize is not validated against the container size, the reservations made in the container by Tez for Inputs / Outputs etc.\nCheck this at compile time to see if enough memory is available, or set up the vertex to reserve additional memory for the Processor.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.plan.BaseWork.java", "org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java", "org.apache.hadoop.hive.ql.exec.tez.DagUtils.java", "org.apache.hadoop.hive.conf.HiveConf.java", "org.apache.hadoop.hive.ql.parse.TezCompiler.java", "org.apache.hadoop.hive.ql.exec.tez.TezTask.java"], "label": 1, "es_results": []}, {"bug_id": 14308, "bug_title": "While using column stats estimated data size may become 0", "bug_description": "Found during a run of HIVE-12181", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.stats.StatsUtils.java"], "label": 1, "es_results": []}, {"bug_id": 14326, "bug_title": "Merging outer joins without conditions can lead to wrong results", "bug_description": "HIVE-13069 enabled cartesian product merging. However, merge should only be performed between INNER joins.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java"], "label": 1, "es_results": []}, {"bug_id": 14338, "bug_title": "Delete/Alter table calls failing with HiveAccessControlException", "bug_description": "Many Hcatalog/Webhcat tests are failing with below error, when tests try to alter/delete/describe tables. Error is thrown when the same user or a different user (same group) who created the table is trying to run the delete/alter table call.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.hcatalog.cli.HCatCli.java", "org.apache.hive.hcatalog.cli.TestPermsGrp.java"], "label": 1, "es_results": []}, {"bug_id": 14357, "bug_title": "TestDbTxnManager2#testLocksInSubquery failing in branch-2.1", "bug_description": "\ncheckCmdOnDriver(driver.compileAndRespond(\"insert into R select * from S where a in (select a from T where b = 1)\"));\n\n    txnMgr.openTxn(\"three\");\n\n    txnMgr.acquireLocks(driver.getPlan(), ctx, \"three\");\n\n    locks = getLocks();\n\n    Assert.assertEquals(\"Unexpected lock count\", 3, locks.size());\n\n    checkLock(LockType.SHARED_READ, LockState.ACQUIRED, \"default\", \"T\", null, locks.get(0));\n\n    checkLock(LockType.SHARED_READ, LockState.ACQUIRED, \"default\", \"S\", null, locks.get(1));\n\n    checkLock(LockType.SHARED_READ, LockState.ACQUIRED, \"default\", \"R\", null, locks.get(2));\n\n\n\nThis test case is failing. The expected order of locks is supposed to be T, S, R. But upon closer inspection, it seems to be R,S,T. \nI&apos;m not much familiar with what these locks are and why the order is important. Raising this jira so while I try to understand it all. Meanwhile, if somebody can explain here, would be helpful. \n", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java", "org.apache.hadoop.hive.ql.TestTxnCommands.java"], "label": 1, "es_results": []}, {"bug_id": 14397, "bug_title": "Queries ran after reopening of tez session launches additional sessions", "bug_description": "Say we have configured hive.server2.tez.default.queues with 2 queues q1 and q2 with default expiry interval of 5 mins.\nAfter 5 mins of non-usage the sessions corresponding to queues q1 and q2 will be expired. When new set of queries are issue after this expiry, the default sessions backed by q1 and q2 and reopened again. Now when we run more queries the reopened sessions are not used instead new session is opened. \nAt this point there will be 4 sessions running (2 abandoned sessions and 2 current sessions). ", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java", "org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java", "org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java"], "label": 1, "es_results": []}, {"bug_id": 12656, "bug_title": "Turn hive.compute.query.using.stats on by default", "bug_description": "We now have hive.compute.query.using.stats=false by default. We plan to turn it on by default so that we can have better performance. We can also set it to false in some test cases to maintain the original purpose of those tests..", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.conf.HiveConf.java"], "label": 1, "es_results": []}, {"bug_id": 14563, "bug_title": "StatsOptimizer treats NULL in a wrong way", "bug_description": "\n\n\nOSTHOOK: query: explain select count(key) from (select null as key from src)src\n\nPOSTHOOK: type: QUERY\n\nSTAGE DEPENDENCIES:\n\n  Stage-0 is a root stage\n\n\n\nSTAGE PLANS:\n\n  Stage: Stage-0\n\n    Fetch Operator\n\n      limit: 1\n\n      Processor Tree:\n\n        ListSink\n\n\n\nPREHOOK: query: select count(key) from (select null as key from src)src\n\nPREHOOK: type: QUERY\n\nPREHOOK: Input: default@src\n\n#### A masked pattern was here ####\n\nPOSTHOOK: query: select count(key) from (select null as key from src)src\n\nPOSTHOOK: type: QUERY\n\nPOSTHOOK: Input: default@src\n\n#### A masked pattern was here ####\n\n500\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.service.cli.session.TestSessionManagerMetrics.java", "org.apache.hive.jdbc.TestJdbcWithMiniHS2.java", "org.apache.hive.beeline.TestBeeLineWithArgs.java", "org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java"], "label": 1, "es_results": []}, {"bug_id": 14652, "bug_title": "incorrect results for not in on partition columns", "bug_description": "\ncreate table foo (i int) partitioned by (s string);\n\n\n\ninsert overwrite table foo partition(s=&apos;foo&apos;) select cint from alltypesorc limit 10;\n\ninsert overwrite table foo partition(s=&apos;bar&apos;) select cint from alltypesorc limit 10;\n\n\n\nselect * from foo where s not in (&apos;bar&apos;);\n\n\n\nNo results. IN ... works correctly", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.ParseContext.java", "org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 14530, "bug_title": "Union All query returns incorrect results", "bug_description": "\ncreate table dw_tmp.l_test1 (id bigint,val string,trans_date string) row format delimited fields terminated by &apos; &apos; ;\ncreate table dw_tmp.l_test2 (id bigint,val string,trans_date string) row format delimited fields terminated by &apos; &apos; ;  \nselect * from dw_tmp.l_test1;\n1       table_1      2016-08-11\nselect * from dw_tmp.l_test2;\n2       table_2      2016-08-11\n right like this\nselect \n    id,\n    &apos;table_1&apos; ,\n    trans_date\nfrom dw_tmp.l_test1\nunion all\nselect \n    id,\n    val,\n    trans_date\nfrom dw_tmp.l_test2 ;\n1       table_1     2016-08-11\n2       table_2     2016-08-11\n incorrect\nselect \n    id,\n    999,\n    &apos;table_1&apos; ,\n    trans_date\nfrom dw_tmp.l_test1\nunion all\nselect \n    id,\n    999,\n    val,\n    trans_date\nfrom dw_tmp.l_test2 ;\n1       999     table_1     2016-08-11\n2       999     table_1     2016-08-11     <-- here is wrong\n incorrect\nselect \n    id,\n    999,\n    666,\n    &apos;table_1&apos; ,\n    trans_date\nfrom dw_tmp.l_test1\nunion all\nselect \n    id,\n    999,\n    666,\n    val,\n    trans_date\nfrom dw_tmp.l_test2 ;\n1       999     666     table_1 2016-08-11\n2       999     666     table_1 2016-08-11     <-- here is wrong\n right\nselect \n    id,\n    999,\n    &apos;table_1&apos; ,\n    trans_date,\n    &apos;2016-11-11&apos;\nfrom dw_tmp.l_test1\nunion all\nselect \n    id,\n    999,\n    val,\n    trans_date,\n    trans_date\nfrom dw_tmp.l_test2 ;\n1       999     table_1 2016-08-11      2016-11-11\n2       999     table_2 2016-08-11      2016-08-11", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java"], "label": 1, "es_results": []}, {"bug_id": 14697, "bug_title": "Can not access kerberized HS2 Web UI", "bug_description": "Failed to access kerberized HS2 WebUI with following error message:\n\n\n\ncurl -v -you : --negotiate http://util185.phx2.cbsig.net:10002/ \n\n> GET / HTTP/1.1 \n\n> Host: util185.phx2.cbsig.net:10002 \n\n> Authorization: Negotiate YIIU7...[redacted]... \n\n> User-Agent: curl/7.42.1 \n\n> Accept: */* \n\n> \n\n< HTTP/1.1 413 FULL head \n\n< Content-Length: 0 \n\n< Connection: close \n\n< Server: Jetty(7.6.0.v20120127) \n\n\n\nIt is because the Jetty default request header (4K) is too small in some kerberos case.\nSo this patch is to increase the request header to 64K.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hive.http.HttpServer.java"], "label": 1, "es_results": []}, {"bug_id": 14726, "bug_title": "delete statement fails when spdo is on", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java"], "label": 1, "es_results": []}, {"bug_id": 14098, "bug_title": "Logging task properties, and environment variables might contain passwords", "bug_description": "Hive MapredLocalTask Can Print Environment Passwords, like -Djavax.net.ssl.trustStorePassword.\nThe same could happen, when logging spark properties", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java", "org.apache.hadoop.hive.ql.exec.TestUtilities.java", "org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java", "org.apache.hadoop.hive.ql.exec.Utilities.java"], "label": 1, "es_results": []}, {"bug_id": 14766, "bug_title": "ObjectStore.initialize() needs retry mechanisms in case of connection failures", "bug_description": "RetryingHMSHandler handles retries to most HMSHandler calls. However, one area where we do not have retries is in the very instantiation of ObjectStore. The lack of retries here sometimes means that a flaky db connect around the time the metastore is started yields an unresponsive metastore.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.metastore.ObjectStore.java"], "label": 1, "es_results": []}, {"bug_id": 14865, "bug_title": "Fix comments after HIVE-14350", "bug_description": "there are still some comments in the code that should&apos;ve been updated in HIVE-14350", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.io.AcidUtils.java"], "label": 1, "es_results": []}, {"bug_id": 14873, "bug_title": "Add UDF for extraction of 'day of week'", "bug_description": "", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.exec.FunctionRegistry.java", "org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java"], "label": 1, "es_results": []}, {"bug_id": 14959, "bug_title": "Fix DISTINCT with windowing when CBO is enabled/disabled", "bug_description": "For instance, the following query with CBO off:\n\n\n\nselect distinct last_value(i) over ( partition by si order by i ),\n\n  first_value(t)  over ( partition by si order by i )\n\nfrom over10k limit 50;\n\n\n\nwill fail, with the following message:\n\nSELECT DISTINCT not allowed in the presence of windowing functions when CBO is off\n\n\n", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java", "org.apache.hadoop.hive.ql.parse.CalcitePlanner.java"], "label": 1, "es_results": []}, {"bug_id": 15029, "bug_title": "Add logic to estimate stats for BETWEEN operator", "bug_description": "Currently, BETWEEN operator is considered in the default case: reduces the input rows to the half. This may lead to wrong estimates for the number of rows produced by Filter operators.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java"], "label": 1, "es_results": []}, {"bug_id": 15030, "bug_title": "Fixes in inference of collation for Tez cost model", "bug_description": "Tez cost model might get NPE if collation returned by join algorithm is null.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.0", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel.java", "org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveAlgorithmsUtil.java"], "label": 1, "es_results": []}, {"bug_id": 15137, "bug_title": "metastore add partitions background thread should use current username", "bug_description": "The background thread used in HIVE-13901 for adding partitions needs to be reinitialized with current UGI for each invocation. Otherwise the user in context while thread was created would be the current UGI during the actions in the thread.", "project": "Apache", "sub_project": "HIVE", "version": "rel/release-2.1.1", "fixed_version": "rel/storage-release-2.2.0", "fixed_files": ["org.apache.hadoop.hive.metastore.HiveMetaStore.java"], "label": 1, "es_results": []}, {"bug_id": 111, "bug_title": "org.apache.commons.codec.net.URLCodec.ESCAPE_CHAR is not final but should be", "bug_description": "org.apache.commons.codec.net.URLCodec.ESCAPE_CHAR isn&apos;t final but should be", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_2", "fixed_version": "commons-codec-1.5", "fixed_files": ["org.apache.commons.codec.net.URLCodec.java"], "label": 1, "es_results": []}, {"bug_id": 61, "bug_title": "Base64.encodeBase64() throws NegativeArraySizeException on large files", "bug_description": "The NegativeArraySizeException exception is thrown by Base64.EncodeBase64() for arrays larger than 268435455 bytes (2^31/8-1).\npublic static byte[] encodeBase64(byte[] binaryData, boolean isChunked)  starts with the following three lines:\n        int lengthDataBits = binaryData.length * EIGHTBIT;\n        int fewerThan24bits = lengthDataBits % TWENTYFOURBITGROUP;\n        int numberTriplets = lengthDataBits / TWENTYFOURBITGROUP;\nThe first of the lines will cause an integer overflow in lengthDataBits for lengths larger than 2^31/8-1, making it a negative number. The fix is trivial (but not tested on the running code, I just ran through a few numbers to validate that it computes the same results as the original code):\n        int lengthData = binaryData.length;\n        int fewerThan24bits = lengthData % (TWENTYFOURBITGROUP / EIGHTBIT) * EIGHTBIT;\n        int numberTriplets = lengthData / (TWENTYFOURBITGROUP / EIGHTBIT);\nThis way the encoder will be able to process files of up to 2^31-1 bytes in length, which is much better than ~250MB.\nThe issue was found in commons 1.3; the source code above was taken from SVN trunk so I assume it&apos;s still present in 1.4: http://svn.apache.org/repos/asf/commons/proper/codec/trunk/src/java/org/apache/commons/codec/binary/Base64.java", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_3", "fixed_version": "CODEC_1_4", "fixed_files": ["org.apache.commons.codec.binary.Base64.java"], "label": 1, "es_results": []}, {"bug_id": 68, "bug_title": "isBase64 throws ArrayIndexOutOfBoundsException on some non-BASE64 bytes", "bug_description": "the following code throws an ArrayIndexOutOfBoundsException although it is perfectly valid (the byte 0x9c should be ignored according to the standard):\n\nbyte[x] = new byte[] { &apos;n&apos;, &apos;A&apos;, &apos;=&apos;, &apos;=&apos;, 0x9c };\nBase64.decodeBase64(x);\n\n\nThe problem is the following method:\n\n    private static boolean isBase64(byte octect) {\n        if (octect == PAD) {\n            return true;\n        } else if (base64Alphabet[octect] == -1) {\n            return false;\n        } else {\n            return true;\n        }\n    }\n\n\nin Java octect is a signed value, so it is not correct to use it as an offset for an array [0..254] which base64Alphabet is. 0x9c is -100!\nFIX:\nuse base64Alphabet[ 0xff & octect ] in the \"else if\" block to convert the octet prior using it as an offset for the lookup table", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_3", "fixed_version": "CODEC_1_4", "fixed_files": ["org.apache.commons.codec.binary.Base64Test.java"], "label": 1, "es_results": []}, {"bug_id": 89, "bug_title": "new Base64().encode() appends a CRLF, and chunks results into 76 character lines", "bug_description": "The instance encode() method (e.g. new Base64().encode()) appends a CRLF.  Actually it&apos;s fully chunking the output into 76 character lines.  Commons-Codec-1.3 did not do this.  The static Base64.encodeBase64() method behaves the same in both 1.3 and 1.4, so this problem only affects the instance encode() method.\n\n\n\nimport org.apache.commons.codec.binary.*;\n\n\n\npublic class B64 {\n\n\n\n  public static void main(String[] args) throws Exception {\n\n    Base64 b64 = new Base64();\n\n\n\n    String s1 = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\";\n\n    String s2 = \"aaaaaaaaaa\";\n\n    String s3 = \"a\";\n\n    \n\n    byte[] b1 = s1.getBytes(\"UTF-8\");\n\n    byte[] b2 = s2.getBytes(\"UTF-8\");\n\n    byte[] b3 = s3.getBytes(\"UTF-8\");\n\n\n\n    byte[] result;\n\n    result = Base64.encodeBase64(b1);\n\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n\n    result = b64.encode(b1);\n\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n\n\n\n    result = Base64.encodeBase64(b2);\n\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n\n    result = b64.encode(b2);\n\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n\n\n\n    result = Base64.encodeBase64(b3);\n\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n\n    result = b64.encode(b3);\n\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n\n\n\n  }\n\n}\n\n\n\nHere&apos;s my output:\n\n$ java -cp commons-codec-1.3.jar:. B64\n\n[YWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYQ==]\n\n[YWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYQ==]\n\n[YWFhYWFhYWFhYQ==]\n\n[YWFhYWFhYWFhYQ==]\n\n[YQ==]\n\n[YQ==]\n\n\n\n\n\n$ java -cp commons-codec-1.4.jar:. B64\n\n[YWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYQ==]\n\n[YWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFh\n\nYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYQ==\n\n]\n\n[YWFhYWFhYWFhYQ==]\n\n[YWFhYWFhYWFhYQ==\n\n]\n\n[YQ==]\n\n[YQ==\n\n]\n\n\n", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_4", "fixed_version": "commons-codec-1.5", "fixed_files": ["org.apache.commons.codec.binary.Base64OutputStream.java", "org.apache.commons.codec.binary.Base64InputStream.java", "org.apache.commons.codec.binary.Base64.java"], "label": 1, "es_results": []}, {"bug_id": 101, "bug_title": "Base64InputStream#read(byte[]) incorrectly returns 0 at end of any stream which is multiple of 3 bytes long", "bug_description": "Using new InputStreamReader(new Base64InputStream(in, true)) sometimes fails with \"java.io.IOException: Underlying input stream returned zero bytes\".\nThis is been tracked down that Base64InputStream#read(byte[]) incorrectly returns 0 at end of any stream which is multiple of 3 bytes long.", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_4", "fixed_version": "commons-codec-1.5", "fixed_files": ["org.apache.commons.codec.binary.Base64InputStreamTest.java", "org.apache.commons.codec.binary.Base64InputStream.java", "org.apache.commons.codec.binary.Base64TestData.java"], "label": 1, "es_results": []}, {"bug_id": 99, "bug_title": "Base64.encodeBase64String() should not chunk", "bug_description": "Base64.encodeBase64String() shouldn&apos;t chunk.\nChange this:\n\n\n\npublic static String encodeBase64String(byte[] binaryData) {\n\n    return StringUtils.newStringUtf8(encodeBase64(binaryData, true));\n\n}\n\n\n\nTo this:\n\n\n\npublic static String encodeBase64String(byte[] binaryData) {\n\n    return StringUtils.newStringUtf8(encodeBase64(binaryData, false));\n\n}\n\n\n\nThis will fix the following tests ggregory added a few minutes ago:\n        //assertEquals(\"Zg==\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"f\")));\n        //assertEquals(\"Zm8=\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"fo\")));\n        //assertEquals(\"Zm9v\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"foo\")));\n        //assertEquals(\"Zm9vYg==\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"foob\")));\n        //assertEquals(\"Zm9vYmE=\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"fooba\")));\n        //assertEquals(\"Zm9vYmFy\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"foobar\")));\n", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_4", "fixed_version": "commons-codec-1.5", "fixed_files": ["org.apache.commons.codec.binary.Base64.java", "org.apache.commons.codec.binary.Base64Test.java"], "label": 1, "es_results": []}, {"bug_id": 112, "bug_title": "Base64.encodeBase64(byte[] binaryData, boolean isChunked, boolean urlSafe, int maxResultSize) throws IAE for valid maxResultSize if isChunked is false", "bug_description": "If isChunked is false, Base64.encodeBase64(byte[] binaryData, boolean isChunked, boolean urlSafe, int maxResultSize) throws IAE for valid maxResultSize.\nTest case and fix will be applied shortly.", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_4", "fixed_version": "commons-codec-1.5", "fixed_files": ["org.apache.commons.codec.binary.Base64Test.java", "org.apache.commons.codec.binary.Base64.java"], "label": 1, "es_results": []}, {"bug_id": 113, "bug_title": "org.apache.commons.codec.language.RefinedSoundex.US_ENGLISH_MAPPING should be package protected MALICIOUS_CODE", "bug_description": "Findbugs says:\norg.apache.commons.codec.language.RefinedSoundex.US_ENGLISH_MAPPING should be package protected\tMALICIOUS_CODE\thttp://findbugs.sourceforge.net/bugDescriptions.html#MS_PKGPROTECT", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_4", "fixed_version": "commons-codec-1.5", "fixed_files": ["org.apache.commons.codec.language.RefinedSoundex.java"], "label": 1, "es_results": []}, {"bug_id": 114, "bug_title": "org.apache.commons.codec.language.Soundex.US_ENGLISH_MAPPING should be package protected MALICIOUS_CODE", "bug_description": "Findbugs says:\norg.apache.commons.codec.language.Soundex.US_ENGLISH_MAPPING should be package protected MALICIOUS_CODE http://findbugs.sourceforge.net/bugDescriptions.html#MS_PKGPROTECT", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_4", "fixed_version": "commons-codec-1.5", "fixed_files": ["org.apache.commons.codec.language.Soundex.java"], "label": 1, "es_results": []}, {"bug_id": 115, "bug_title": "DoubleMetaphone.maxCodeLen should probably be private", "bug_description": "DoubleMetaphone.maxCodeLen should probably be private - it has public getter and setter anyway.", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_4", "fixed_version": "commons-codec-1.5", "fixed_files": ["org.apache.commons.codec.language.DoubleMetaphone.java"], "label": 1, "es_results": []}, {"bug_id": 117, "bug_title": "Caverphone encodes names starting and ending with \"mb\" incorrectly.", "bug_description": "Caverphone encode names starting and ending with \"mb\" incorrectly.\nAccording to the spec:\n\"If the name ends with mb make it m2\".\nThis has been coded as:\n\"If the name starts with mb make it m2\".", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_4", "fixed_version": "commons-codec-1.5", "fixed_files": ["org.apache.commons.codec.language.CaverphoneTest.java", "org.apache.commons.codec.language.Caverphone.java"], "label": 1, "es_results": []}, {"bug_id": 185, "bug_title": "Base64 user-facing docs should say that decode can handle URL Safe encoding", "bug_description": "Base64 has static methods specific to doing URLSafe encoding, but has no corresponding URLSafe decoding method.\nThis is be because the decoding table transparently handles both URLSafe and standard alphabets. Unfortunately this is only documented as a javadoc on a private member.\nThere should be a mention of it\n\nIn the class javadocs\non the static decode methods\non the static URLSafe encode methods\n\n", "project": "Commons", "sub_project": "CODEC", "version": "CODEC_1_4", "fixed_version": "1.10", "fixed_files": ["org.apache.commons.codec.binary.Base64.java"], "label": 1, "es_results": []}, {"bug_id": 123, "bug_title": "ColognePhonetic Javadoc should use HTML entities for special characters.", "bug_description": "The ColognePhonetic class contains Javadoc with umlauts and other characters that do not always play well in editors. Change these characters to HTML entities. This means we should also be able to remove the UTF-8 settings in the POM for Javadoc.", "project": "Commons", "sub_project": "CODEC", "version": "commons-codec-1.5", "fixed_version": "1_6", "fixed_files": ["org.apache.commons.codec.language.ColognePhonetic.java"], "label": 1, "es_results": []}, {"bug_id": 130, "bug_title": "Base64InputStream.skip skips underlying stream, not output", "bug_description": "Base64InputStream.skip() skips within underlying stream, leading to unexpected behaviour.\nThe following code will reproduce the issue:\n@Test\npublic void testSkip() throws Throwable {\n    InputStream ins =\n            new ByteArrayInputStream(\"AAAA////\".getBytes(\"ISO-8859-1\"));//should decode to \n{0, 0, 0, 255, 255, 255}\n    Base64InputStream instance = new Base64InputStream(ins);\n    assertEquals(3L, instance.skip(3L)); //should skip 3 decoded characters, or 4 encoded characters\n    assertEquals(255, instance.read()); //Currently returns 3, as it is decoding \"A/\", not \"//\" \n}\nThe following code, if added to Base64InputStream, or (BaseNCodecInputStream in the dev build) would resolve the issue:\n@Override\npublic long skip(long n) throws IOException {\n    //delegate to read()\n    long bytesRead = 0;\n    while ((bytesRead < n) && (read() != -1)) \n{\n\n        bytesRead++;\n\n    }\n    return bytesRead;\n}\nMore efficient code may be possible.", "project": "Commons", "sub_project": "CODEC", "version": "commons-codec-1.5", "fixed_version": "1.7", "fixed_files": ["org.apache.commons.codec.binary.BaseNCodecInputStream.java", "org.apache.commons.codec.binary.Base32InputStreamTest.java", "org.apache.commons.codec.binary.Base64InputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 121, "bug_title": "QuotedPrintableCodec does not support soft line break per the 'quoted-printable' example on Wikipedia", "bug_description": "Writing a unit test I discovered that the example Wikipedia uses for quoted-printable data does not decode but instead throws an exception.  \nTheir example is here:  http://en.wikipedia.org/wiki/Quoted-printable#Example\ntest:\n  String qpdata   = \"If you believe that truth=3Dbeauty, then surely=20=\\r\\n\" +\n\t\t    \"mathematics is the most beautiful branch of philosophy.\";\n  String expected = \"If you believe that truth=beauty, then surely \" +\n\t\t    \"mathematics is the most beautiful branch of philosophy.\";\n  assertEquals( expected,  new QuotedPrintableCodec().decode(qpdata) );\nI suppose I could fix if you like but currently I&apos;m not a registered developer.  \n", "project": "Commons", "sub_project": "CODEC", "version": "commons-codec-1.5", "fixed_version": "1.10", "fixed_files": ["org.apache.commons.codec.net.QuotedPrintableCodecTest.java", "org.apache.commons.codec.net.QuotedPrintableCodec.java"], "label": 1, "es_results": []}, {"bug_id": 132, "bug_title": "BeiderMorseEncoder OOM issues", "bug_description": "In Lucene/Solr, we integrated this encoder into the latest release.\nOur tests use a variety of random strings, and we have recent jenkins failures\nfrom some input streams (of length <= 10), using huge amounts of memory (e.g. > 64MB),\nresulting in OOM.\nI&apos;ve created a test case (length is 30 here) that will OOM with -Xmx256M. \nI haven&apos;t dug into this much as to what&apos;s causing it, but I suspect there might be a bug\nrevolving around certain punctuation characters: we didn&apos;t see this happening until\nwe beefed up our random string generation to start producing \"html-like\" strings.", "project": "Commons", "sub_project": "CODEC", "version": "1_6", "fixed_version": "1.7", "fixed_files": ["org.apache.commons.codec.language.bm.PhoneticEngine.java", "org.apache.commons.codec.language.bm.BeiderMorseEncoder.java", "org.apache.commons.codec.language.bm.PhoneticEngineTest.java", "org.apache.commons.codec.language.bm.BeiderMorseEncoderTest.java"], "label": 1, "es_results": []}, {"bug_id": 152, "bug_title": "DigestUtils.getDigest(String) looses the orginal exception", "bug_description": "DigestUtils.getDigest(String) looses the orginal exception. It should pass the checked exception it catches into the constructor of the new unchecked exception.", "project": "Commons", "sub_project": "CODEC", "version": "1_6", "fixed_version": "1.7", "fixed_files": ["org.apache.commons.codec.digest.DigestUtils.java"], "label": 1, "es_results": []}, {"bug_id": 156, "bug_title": "DigestUtils: add APIs named after standard alg name SHA-1", "bug_description": "DigestUtils already has SHA APIs but they use the \"SHA\" algorithm name which is not on the standard name list here: http://docs.oracle.com/javase/6/docs/technotes/guides/security/StandardNames.html. \nSHA-1 is on the list though, so use that for SHA-1 APIs.", "project": "Commons", "sub_project": "CODEC", "version": "1_6", "fixed_version": "1.7", "fixed_files": ["org.apache.commons.codec.digest.DigestUtilsTest.java", "org.apache.commons.codec.digest.DigestUtils.java"], "label": 1, "es_results": []}, {"bug_id": 184, "bug_title": "NullPointerException in DoubleMetaPhone.isDoubleMetaphoneEqual when using empty strings", "bug_description": "isDoubleMetaphoneEqual does not work with empty strings: The following test throws a NullPointerException:\n\n\n\n  public void test1() throws Throwable {\n\n    org.apache.commons.codec.language.DoubleMetaphone var0 = new org.apache.commons.codec.language.DoubleMetaphone();\n\n    boolean var3 = var0.isDoubleMetaphoneEqual(\"\", \"\", false);\n\n  }\n\n\n", "project": "Commons", "sub_project": "CODEC", "version": "1.9", "fixed_version": "1.10", "fixed_files": ["org.apache.commons.codec.language.DoubleMetaphoneTest.java", "org.apache.commons.codec.language.DoubleMetaphone.java", "org.apache.commons.codec.binary.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 187, "bug_title": "Beider Morse Phonetic Matching producing incorrect tokens", "bug_description": "I believe the Beider Morse Phonetic Matching algorithm was added in Commons Codec 1.6\nThe BMPM algorithm is an EVOLVING algorithm that is currently on version 3.02 though it had been static since version 3.01 dated 19 Dec 2011 (it was first available as opensource as version 1.00 on 6 May 2009).\nI can see nothing in the Commons Codec Docs to say which version of BMPM was implemented so I am not sure if the problem with the algorithm as coded in the Codec is simply an old version or whether there are more basic problems with the implementation.\nHow do I determine the version of the algorithm that was implemented in the Commons Codec?\nHow do we ensure that the algorithm is updated if/when the BMPM algorithm changes?\nHow do we ensure that the algorithm as coded in the Commons Codec is accurate and working as expected?", "project": "Commons", "sub_project": "CODEC", "version": "1.9", "fixed_version": "1.10", "fixed_files": ["org.apache.commons.codec.language.bm.BeiderMorseEncoder.java", "org.apache.commons.codec.language.bm.PhoneticEngineTest.java", "org.apache.commons.codec.language.bm.Languages.java", "org.apache.commons.codec.language.bm.PhoneticEngineRegressionTest.java", "org.apache.commons.codec.language.bm.Rule.java", "org.apache.commons.codec.language.bm.Lang.java", "org.apache.commons.codec.language.bm.PhoneticEngine.java"], "label": 1, "es_results": []}, {"bug_id": 191, "bug_title": "Encoding data using Base64OutputStream omits part of the input in some cases", "bug_description": "In case Base64OutputStream is used for encoding byte array produced from some strings, an incomplete result (that is inconsistent with both other Base64 implementations and Base64InputStream implementation) is produced. It appears that \"s\" character is getting omitted if it&apos;s in the end of the string to be encoded; there may be other cases as well.\nHere is the test that allows to reproduce the problem: http://kiberion.net/kibertoad/temp/codec-base64-error.zip", "project": "Commons", "sub_project": "CODEC", "version": "1.9", "fixed_version": "1.10", "fixed_files": ["org.apache.commons.codec.binary.Base32OutputStream.java", "org.apache.commons.codec.binary.Base64OutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 217, "bug_title": "Flat3Map.Entry.setValue() overwrites other Entry values", "bug_description": "Flat3Map&apos;s Entry objects will overwrite other Entry&apos;s values if Entry.setValue() is called on one.  It should only overwrite the Entry at hand.\nI&apos;ve looked at the source, and the case statement incorrectly falls through, rather than returning like it should:\nFlat3Map.java, lines 646-660:\n        public Object setValue(Object value) {\n            if (canRemove == false) \n{\n                throw new IllegalStateException(AbstractHashedMap.SETVALUE_INVALID);\n            }\n            Object old = getValue();\n            switch (nextIndex) \n{\n                case 3: \n                    parent.value3 = value;\n                case 2:\n                    parent.value2 = value;\n                case 1:\n                    parent.value1 = value;\n            }\n            return old;\n        }\nWith this code, if I set the value of the third item in the EntrySet, then all three values are set to the new value.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_3_2", "fixed_version": "COLLECTIONS_3_2_2", "fixed_files": ["org.apache.commons.collections.map.Flat3Map.java", "org.apache.commons.collections.map.TestFlat3Map.java"], "label": 1, "es_results": []}, {"bug_id": 261, "bug_title": "Flat3Map.remove() does not return the correct value when size <= 3", "bug_description": "        final Flat3Map m = new Flat3Map();\n        m.put( new Integer( 1 ), new Integer( 1 ) );\n        m.put( new Integer( 0 ), new Integer( 0 ) );\n        System.out.println( m.remove( new Integer( 1 ) ) );\nThe above code will print \"0\" when it should print \"1\"", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_3_2", "fixed_version": "COLLECTIONS_3_2_2", "fixed_files": ["org.apache.commons.collections.map.Flat3Map.java", "org.apache.commons.collections.map.TestFlat3Map.java"], "label": 1, "es_results": []}, {"bug_id": 304, "bug_title": "SetUniqueList set method use corrupts uniqness", "bug_description": "When set method is used to put element (Strawberry) on list, then it is possible to add the same element (Strawberry) with add method. Also you cannot add element (Lemon) that has been once removed with set method. Reproduction code below:\nList<String> list = new LinkedList<String>();\nSetUniqueList decoratedList = SetUniqueList.decorate(list);\ndecoratedList.add(\"Apple\");\ndecoratedList.add(\"Lemon\");\ndecoratedList.add(\"Orange\");\nSystem.out.println(decoratedList.toString());\ndecoratedList.set(1, \"Strawberry\");\nSystem.out.println(decoratedList.toString());\ndecoratedList.add(1, \"Strawberry\");\nSystem.out.println(decoratedList.toString());\ndecoratedList.add(1, \"Lemon\");\nSystem.out.println(decoratedList.toString());", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_3_2", "fixed_version": "COLLECTIONS_3_2_2", "fixed_files": ["org.apache.commons.collections.list.SetUniqueList.java", "org.apache.commons.collections.list.TestSetUniqueList.java"], "label": 1, "es_results": []}, {"bug_id": 294, "bug_title": "Fix case-insensitive string handling", "bug_description": "For example, the behavior of the CaseInsensitiveMap is currently platform-dependent, please see Common Bug #3 for details.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_3_2", "fixed_version": "COLLECTIONS_3_2_2", "fixed_files": ["org.apache.commons.collections.map.TestCaseInsensitiveMap.java", "org.apache.commons.collections.map.CaseInsensitiveMap.java"], "label": 1, "es_results": []}, {"bug_id": 266, "bug_title": "Issue with MultiKey when serialized/deserialized via RMI", "bug_description": "This is because the hash code of MultiKey is calculated only once. \nSo if the MultiKey is deserialized in an other jvm, and if one at least of the subkeys defines its hash code with System.identityHashCode() (for example all the enums does), then the hash code of the MultiKey is no longer valid, and you can&apos;t retreive the key in your Map.\nI fixed it by making the cached hash code field transient, and by recalculating the hash code during deserialization. ", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_3_2", "fixed_version": "COLLECTIONS_3_2_2", "fixed_files": ["org.apache.commons.collections.keyvalue.MultiKey.java", "org.apache.commons.collections.keyvalue.TestMultiKey.java"], "label": 1, "es_results": []}, {"bug_id": 307, "bug_title": "SetUniqueList.subList().contains() method checks against full parent list, not sublist range", "bug_description": "The view returned by the subList() method of a SetUniqueList checks contains() against the set of the original list.\nAs shown by the following test snippet.\n\t\tList list = new ArrayList();\n\t\tList uniqueList = SetUniqueList.decorate(list);\n\t\tuniqueList.add(\"Hello\");\n\t\tuniqueList.add(\"World\");\n\t\tList subList = list.subList(0, 0);\n\t\tList subUniqueList = uniqueList.subList(0, 0);\n\t\tassertFalse(subList.contains(\"World\")); // passes\n\t\tassertFalse(subUniqueList.contains(\"World\")); // fails", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_3_2", "fixed_version": "COLLECTIONS_3_2_2", "fixed_files": ["org.apache.commons.collections.list.SetUniqueList.java", "org.apache.commons.collections.list.TestSetUniqueList.java"], "label": 1, "es_results": []}, {"bug_id": 249, "bug_title": "SetUniqueList.addAll(int index, Collection coll) adds to end of list instead of at specified index", "bug_description": "When you call SetUniqueList.addAll(int index, Collection coll), it incorrectly adds the new elements to the end of the list instead of at the specified index.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_3_2", "fixed_version": "COLLECTIONS_3_2_2", "fixed_files": ["org.apache.commons.collections.list.SetUniqueList.java", "org.apache.commons.collections.list.TestSetUniqueList.java"], "label": 1, "es_results": []}, {"bug_id": 350, "bug_title": "MapUtils.getNumber sends output to System.out", "bug_description": "MapUtils.getNumber calls \"logInfo\" on a ParseException and it produces output on the System.out, instead of just returning \"null\" as the documentation states.\nSince the expected behavior is to return null, not print to System.out.  I recommend we remove the call to logInfo and/or remove the System.out.println inside it.\nAt minimum, if that&apos;s not agreed upon, the \"side-effect\" of writing to System.out should be documented.\nI&apos;m happy to offer code for either of these fixes.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_3_2", "fixed_version": "COLLECTIONS_3_2_2", "fixed_files": ["org.apache.commons.collections.MapUtils.java"], "label": 1, "es_results": []}, {"bug_id": 228, "bug_title": "MultiValueMap put and putAll do not return the correct values", "bug_description": "MultiValueMap put and putAll do not return the correct values.\nputAll(Object, Collection) should return true if the map is changed. But if the key is new then this behaviour fails.\nput(Object, Object) should return the added value if the map is changed. But if the key is new then this behaviour fails.\nThis was hidden because the test case referred to MultiHashMap and not MultValueMap.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_3_2", "fixed_version": "COLLECTIONS_3_2_2", "fixed_files": ["org.apache.commons.collections.map.MultiValueMap.java", "org.apache.commons.collections.map.TestMultiValueMap.java"], "label": 1, "es_results": []}, {"bug_id": 219, "bug_title": "The CollectionUtils.removeAll method calls the ListUtils.retainAll method instead of the ListUtils.removeAll method.", "bug_description": "The CollectionUtils.removeAll method calls the ListUtils.retainAll method instead of the ListUtils.removeAll method.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_3_2", "fixed_version": "COLLECTIONS_3_2_2", "fixed_files": ["org.apache.commons.collections.TestListUtils.java", "org.apache.commons.collections.TestCollectionUtils.java", "org.apache.commons.collections.CollectionUtils.java"], "label": 1, "es_results": []}, {"bug_id": 475, "bug_title": "Wrong timeout handling in expiration policies of PassiveExpiringMap", "bug_description": "The timeunit parameter in the ctor of ConstantTimeToLiveExpirationPolicy is not used.\nAdditionally, the validateAndConvertToMillis method is wrong, it should be changed like this:\n\n    private static long validateAndConvertToMillis(final long timeToLive,\n                                                   final TimeUnit timeUnit) {\n  if (timeUnit == null) {\n    throw new IllegalArgumentException(\"Time unit must not be null\");\n  }\n  return TimeUnit.MILLISECONDS.convert(timeToLive, timeUnit);\n}\n\n", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0_ALPHA1", "fixed_version": "COLLECTIONS_4_0", "fixed_files": ["org.apache.commons.collections4.map.PassiveExpiringMap.java", "org.apache.commons.collections4.map.PassiveExpiringMapTest.java"], "label": 1, "es_results": []}, {"bug_id": 496, "bug_title": "UnmodifiableBoundedCollection does not implement Unmodifiable marker interface", "bug_description": "The UnmodifiableBoundedCollection does not implement the Unmodifiable interface.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0_ALPHA1", "fixed_version": "COLLECTIONS_4_0", "fixed_files": ["org.apache.commons.collections4.collection.UnmodifiableBoundedCollection.java", "org.apache.commons.collections4.collection.UnmodifiableBoundedCollectionTest.java"], "label": 1, "es_results": []}, {"bug_id": 500, "bug_title": "Rename MultiMap.remove(K, V) to avoid clashes with newly introduced default methods in Java 8", "bug_description": "Java 8 will introduce new default methods for the Map interface which clash with the existing method \"V remove(K key, V value)\" in our MultiMap interface.\nTo avoid future problems rename the method to a more specific version and change the return type to be more logical. Brief discussion on the ml favored either:\n\nboolean removeMapping(K key, V value)\nboolean removeValue(K key, V value)\n\n", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0_ALPHA1", "fixed_version": "COLLECTIONS_4_0", "fixed_files": ["org.apache.commons.collections4.MultiMap.java", "org.apache.commons.collections4.map.MultiValueMapTest.java", "org.apache.commons.collections4.map.MultiValueMap.java"], "label": 1, "es_results": []}, {"bug_id": 501, "bug_title": "Rename MultiKeyMap.remove(K key1, K key2) to avoid clashes with newly introduced default methods in Java 8", "bug_description": "Java 8 will introduce new default methods for the Map interface which clash with the existing method \"V remove(K key1, K key2)\" in our MultiKeyMap interface.\nTo avoid future problems rename the method (and other similar methods with more multi key arguments) to a more specific version:\n\nV removeMultiKey(K key1, K key2, ...)\nV removeKey(K key1, K key2, ...)\n\n", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0_ALPHA1", "fixed_version": "COLLECTIONS_4_0", "fixed_files": ["org.apache.commons.collections4.map.MultiKeyMapTest.java", "org.apache.commons.collections4.map.MultiKeyMap.java"], "label": 1, "es_results": []}, {"bug_id": 506, "bug_title": "Result of CollectionUtils are different between version 3.2.1 and version 4.0", "bug_description": "CollectionUtils V3 uses equals to compute results but not CollectionUtils v4 (it seems to use ==)\nThe following exemple with subtract method :\n List<ObjectTest> listA = new ArrayList<ObjectTest>();\n        List<ObjectTest> listB = new ArrayList<ObjectTest>();\n        listA.add(new ObjectTest(\"Test1\"));\n        listA.add(new ObjectTest(\"Test2\"));\n        listA.add(new ObjectTest(\"Test3\"));\n        listB.add(new ObjectTest(\"Test1\"));\n        listB.add(new ObjectTest(\"Test2\"));\n        Collection<?> res1 = org.apache.commons.collections.CollectionUtils.subtract(listA, listB);\n        System.out.println(\"Res1 size = \" +res1.size());\n        Collection<?> res2 =  org.apache.commons.collections4.CollectionUtils.subtract(listA, listB);\n        System.out.println(\"Res2 size = \" +res2.size());\nProduces this : \nRes1 size = 1\nRes2 size = 3\nThe new behaviour is not useful. It would be better to have the V3 behaviour\n", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.CollectionUtils.java"], "label": 1, "es_results": []}, {"bug_id": 507, "bug_title": "ComparatorUtils.chainedComparator(..) should not force the objects to implement Comparable", "bug_description": "", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.ComparatorUtils.java"], "label": 1, "es_results": []}, {"bug_id": 516, "bug_title": "NullPointerException in MapUtils.toProperties", "bug_description": "calling MapUtils.toProperties with a map having null entries results in a NullPointerException. In this case the Map has only entry <null, null>.\nHowever, javadoc states \"A null input will return an empty\nproperties object.\" so (1) this should be clarified as it would\nonly apply to the map reference itself, not its contents, or (2)\nan empty property object should be generated for null entries in\nthe map as well.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.MapUtils.java"], "label": 1, "es_results": []}, {"bug_id": 531, "bug_title": "Generic Wildcards specified in CollectionUtils#isEqualCollection(Collection, Collection, Equator) may throw ClassCastException in certain cases", "bug_description": "CollectionUtils#isEqualCollection(Collection, Collection, Equator) is defined as\n\n\npublic static boolean isEqualCollection(final Collection<?> a, final Collection<?> b, final Equator<?> equator) {\n...\n}\n\n\nThis makes it possible to invoke it with a code like\n\n\npublic static class IntegerEquator implements Equator<Integer> {\n        public boolean equate(Integer o1, Integer o2) {\n            return o1.intValue() == o2.intValue();\n        }\n        public int hash(Integer o) {\n            return o.intValue();\n        }\n    }\n\n    @Test\n    public void test() {\n        List<Long> longList = Arrays.asList(1L, 2L);\n        List<Integer> intList = Arrays.asList(1, 2);\n        assertTrue(CollectionUtils.isEqualCollection(longList, intList, new IntegerEquator()));\n\n    }\n\n\nwhich compiles perfectly but throws a ClassCastException as Long cannot be cast to an Integer. However, the generics should be defined such that this is stopped at compile time itself.\nIf we modify the method to something like\n\n\npublic static <E> boolean isEqualCollection(final Collection<? extends E> a, final Collection<? extends E> b, final Equator<? super E> equator) {\n...\n}\n\n\nthe above example would give a compile time error. imho we should modify this method with bounded wildcards. I don&apos;t think this change would break any existing binaries if the method is being used correctly, otherwise it is probably throwing ClassCastExceptions anyway.\nTest case attached", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.CollectionUtils.java"], "label": 1, "es_results": []}, {"bug_id": 534, "bug_title": "Performance bug in CollectionBag::retainAll", "bug_description": "Hi,\nThere seems to be a performance bug in the method retainAll in the CollectionBag class.\nThe problem is that the code is checking containment over the parameter collection, which could be expensive for some types of collections like ArrayLists.\nOne solution could be to convert the Collection into a HashSet and check containment in the HashSet.\n If this is done, then running retainAll on a 1,000,000 collection would take less than 2 seconds instead of 27 mins, according to my experiments.\n____________________________________________\nHere&apos;s a function to expose the bug:\n public static void collectionBagRetainAllTest() \n{\n\n\tArrayList<Integer> coll=new ArrayList<Integer>();\n\tfor(int i=0;i<=1000000;++i)\n\t    coll.add(new Integer(i));\n\n\tTreeBag<Integer> treeBag=new TreeBag<Integer>(coll);\n\n\tCollectionBag<Integer> bag = new CollectionBag<Integer>(treeBag);\n\n\tbag.retainAll(coll);\n     }\n_____________________________________\nHere&apos;s a proposed patch:\n  public boolean retainAll(final Collection<?> coll) {\n        if (coll != null) {\n            boolean modified = false;\n            final Iterator<E> e = iterator();\n\t    HashSet<Object> set=new HashSet<Object>(coll);\n            while (e.hasNext()) {\n                if (!set.contains(e.next())) \n{\n                    e.remove();\n                    modified = true;\n                }\n            }\n            return modified;\n        } else \n{\n            // let the decorated bag handle the case of null argument\n            return decorated().retainAll(null);\n        }\n    }\n_____________________________________", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.bag.CollectionBag.java"], "label": 1, "es_results": []}, {"bug_id": 537, "bug_title": "PredicateUtils (all|any)Predicate type misbehaviour Array vs. Collection", "bug_description": "Migrating from collections-generic to collections4 we encountered a type problem with PredicateUtils. When you look at the method anyPredicate(), the signature with array is typed with \"Predicate<? super T>\" whereas the signature with Collection is typed \"? extends Predicate<T>\", so the both methods are not equivalent.\nWe think both methods should have the same types, so it would not break compatibility with a lot of legacy code.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.functors.ChainedClosure.java", "org.apache.commons.collections4.functors.NonePredicate.java", "org.apache.commons.collections4.TransformerUtils.java", "org.apache.commons.collections4.functors.ChainedTransformer.java", "org.apache.commons.collections4.PredicateUtils.java", "org.apache.commons.collections4.functors.AllPredicate.java", "org.apache.commons.collections4.functors.OnePredicate.java", "org.apache.commons.collections4.ClosureUtils.java", "org.apache.commons.collections4.functors.FunctorUtils.java", "org.apache.commons.collections4.functors.AnyPredicate.java"], "label": 1, "es_results": []}, {"bug_id": 525, "bug_title": "PatriciaTrie", "bug_description": " the result of trie tree prefixMap function is inconsistent. it would contain a key but the size is 0;\nsome unittest codes as below: \n  PatriciaTrie<String> aTree =\n        new PatriciaTrie<String> ();\n    aTree.put(\"\", \"\");\n    aTree.put(\"\", \"\");\n    assertTrue(aTree.prefixMap(\"\").containsKey(\"\")); //pass\n    assertEquals(\"\", aTree.prefixMap(\"\").get(\"\")); //pass\n    assertFalse(aTree.prefixMap(\"\").isEmpty());                 //fail\n    assertEquals(1, aTree.prefixMap(\"\").size());                 //fail actural 0\n    assertEquals(1, aTree.prefixMap(\"\").keySet().size());   //fail actural 0\n    assertEquals(1, aTree.prefixMap(\"\").entrySet().size()); //fail actural 0\n    assertEquals(1, aTree.prefixMap(\"\").values().size()); //fail actural 0", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.trie.AbstractPatriciaTrie.java", "org.apache.commons.collections4.trie.PatriciaTrieTest.java"], "label": 1, "es_results": []}, {"bug_id": 512, "bug_title": "equals/hashCode mismatch", "bug_description": "We used Randoop on the collection classes, which found several test cases where two objects are equal but their hash code differs.\nI will attach a file containing two test cases that are different; the other tests seem to be longer versions showing the same issue.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.collection.AbstractCollectionDecorator.java", "org.apache.commons.collections4.comparators.FixedOrderComparator.java", "org.apache.commons.collections4.comparators.TransformingComparator.java", "org.apache.commons.collections4.comparators.TransformingComparatorTest.java"], "label": 1, "es_results": []}, {"bug_id": 558, "bug_title": "ListOrderedSet#remove(int) should return E instead of Object", "bug_description": "Since List#remove(int) returns E the implementation in ListOrderedSet should also return E.\nMinimal example that fails to compile:\n\n\nListOrderedSet<String> los = new ListOrderedSet<String>();\nlos.add(\"foo\");\nString s = los.remove(0);\n\n", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.set.ListOrderedSet.java"], "label": 1, "es_results": []}, {"bug_id": 566, "bug_title": "IteratorUtils.collatedIterator do not use natural ordering if no comparator was provided", "bug_description": "In case a null comparator was provided natural ordering should be used, as stated in the javadoc.\nIn fact an exception is thrown the first time the returned iterator is used.", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.IteratorUtils.java", "org.apache.commons.collections4.IteratorUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 576, "bug_title": "MultiKey subclassing has deserialization problem since COLLECTIONS-266: either declare protected readResolve() or MultiKey must be final", "bug_description": "MultiKey from collections 4 provides a transient hashCode and a private readResolve to resolve COLLECTIONS-266: Issue with MultiKey when serialized/deserialized via RMI.\nUnfortunately the solution does not work in case of subclassing: readResolve in MultiKey should be declared protected readResolve() to be called during deserialization of the subclass. Otherwise MultiKey must be final to avoid such subclassing.\nTestcase:\nMultiKeySerializationTest.java\n\npackage de.ivu.test.common.collections4;\n\nimport static org.junit.Assert.assertEquals;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\nimport java.io.ObjectInputStream;\nimport java.io.ObjectOutputStream;\n\nimport org.apache.commons.collections4.keyvalue.MultiKey;\nimport org.junit.Test;\n\npublic class MultiKeySerializationTest {\n\n    @Test\n    @SuppressWarnings(\"unchecked\")\n    public void testReadResolveEqualHashCode()\n            throws IOException, ClassNotFoundException {\n        class MultiKey2<A, B>\n                extends MultiKey {\n\n            private static final long serialVersionUID = 1928896152249821416L;\n\n            public MultiKey2(A key1, B key2) {\n                super(key1, key2);\n            }\n\n            public A getFirst() {\n                return (A) getKey(0);\n            }\n\n            public B getSecond() {\n                return (B) getKey(1);\n            }\n            \n            // FIXME: MultiKey should either declare protected readResolve() or must be final.\n        }\n        MultiKey2<String, String> one = new MultiKey2<>(\"bla\", \"blub\");\n        System.out.println(one.hashCode());\n        ByteArrayOutputStream byteOut = new ByteArrayOutputStream();\n        ObjectOutputStream out = new ObjectOutputStream(byteOut);\n        out.writeObject(one);\n        out.close();\n        byte[] serialized = byteOut.toByteArray();\n        ByteArrayInputStream byteIn = new ByteArrayInputStream(serialized);\n        ObjectInputStream in = new ObjectInputStream(byteIn);\n        MultiKey2<String, String> two = (MultiKey2<String, String>) in.readObject();\n        System.out.println(two.hashCode());\n        assertEquals(\"hashCode must be equal - please check for protected readResolve in MultiKey*\", one.hashCode(),\n            two.hashCode());\n    }\n}\n\n\nFix:\nMultiKey.java\n\n@@ -274,7 +274,7 @@\n      * only stable for the same process).\n      * @return the instance with recalculated hash code\n      */\n-    private Object readResolve() {\n+    protected Object readResolve() {\n         calculateHashCode(keys);\n         return this;\n     }\n\n", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.keyvalue.MultiKeyTest.java", "org.apache.commons.collections4.keyvalue.MultiKey.java"], "label": 1, "es_results": []}, {"bug_id": 577, "bug_title": "PatriciaTrie bugs when only a few bits change", "bug_description": "I have a bug report for you, for the class AbstractPatriciaTrie.  \nIt has to do with how you handle bits when they are very close to each other.  \nFor example, some of your methods seem to think that if the only difference between a prefix and a longer string, is a single additional bit, then they are actually the same data.  Or if the only difference is some number of zero bits, then it also thinks they are the same data.  \nThere are also MANY situations where the prefixMap does not return all the strings that start with the prefix.\nCan you also make AbstractPatriciaTrie public, and your other package level methods into protected level, that way I don&apos;t have to copy the entire class and subclasse&apos;s code out into another class just to extend it?\nthank you,\nChris Duncan (github user: VEQRYN)", "project": "Commons", "sub_project": "COLLECTIONS", "version": "COLLECTIONS_4_0", "fixed_version": "COLLECTIONS_4_1", "fixed_files": ["org.apache.commons.collections4.trie.PatriciaTrieTest.java"], "label": 1, "es_results": []}, {"bug_id": 81, "bug_title": "TarOutputStream can leave garbage at the end of the archive", "bug_description": "when the last block of the tar archive is incomplete it will contain two EOF blocks potentially followed by partial contents of the next-to-last block.  This causes problems with some \"bad\" untar implementations that read past the EOF blocks.\nhttps://issues.apache.org/bugzilla/show_bug.cgi?id=47421\nhttps://issues.apache.org/bugzilla/show_bug.cgi?id=40195\n", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarBuffer.java"], "label": 1, "es_results": []}, {"bug_id": 86, "bug_title": "tar entries do not set \"magic\" value properly for \"oldgnu\" longname entries", "bug_description": "When using the GNU longfile mode the tar classes create special archive entries containing the file name if the original name was longer than 100 characters.\nEach tar entry header holds a \"magic\" value indicating the format.\nFor the special entries containing the file name the magic value should be \"ustar  \" indicating the \"oldgnu\" format.\nSee also https://issues.apache.org/bugzilla/show_bug.cgi?id=47653 and http://sunsite.ualberta.ca/Documentation/Gnu/tar-1.13/html_chapter/tar_8.html#SEC126", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveEntry.java"], "label": 1, "es_results": []}, {"bug_id": 87, "bug_title": "ZipArchiveInputStream does not report the end of a truncated archive", "bug_description": "If a Zip archive is truncated, (e.g. because it is the first volume in a multi-volume archive) the ZipArchiveInputStream.read() method will not detect that fact. All calls to read() will return 0 bytes read. They will not return -1 (end of stream), nor will they throw any exception (which would seem like a good idea to me because the archive is truncated).\nI have tracked this problem to ZipArchiveInputStream.java, line 239. It contains a check\nif (read == 0 && inf.finished()) \n{\n\n    return -1;\n\n}\n\nFor truncated archives the read is always zero but the inf is never finished(). I suggest adding two lines below:\n\nif (read == 0 && inf.finished()) {\n    return -1;\n}\n else if (read == 0 && lengthOfLastRead == -1) \n{\n\n\tthrow new IOException(\"Truncated ZIP file\");\n\n}\n\nThis solves the problem in my tests.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 94, "bug_title": "Creating zip files with many entries will ocassionally produce corrupted output", "bug_description": "Our application produces large numbers of zip files, often with 1000&apos;s of similarly named files contained within the zip. \nWhen we switched from the standard JDK zip classes to those in commons compress, we would ocassionally produce a zip file that had corrupted index entries and would fail to unzip successfully using 7-zip, winzip, etc.\nDebugging the zip creation showed that the the wrong offsets were being returned from the hashmap in ZipOutputStream for the entries that were being corrupted.  Further analysis revealed that this occurred when the filenames being added had a hash collision with another entry in the same output zip (which appears to happen quite frequently for us).\nThe issue appears to stem from the fact that ZipArchiveEntry can store the entry name either in its superclass if passed in on the ctor or in its own member attribute if set later via setName().  Not sure whether this functionality is really required?  Regardless, the root because of the bug is that the equals() and hashCode() methods in ZipArchiveEntry do not always use the same filename value in their comparisons.  In fact if the filename of the entry is set in the ctor it will always treat two ZipArchiveEntries as equal.  This will break the offset hashmap whenever there is a hash collision as it will overwrite the previous entry, believeing it to be equal.\nPatch to follow.\n", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveEntry.java", "org.apache.commons.compress.archivers.zip.ZipArchiveEntryTest.java"], "label": 1, "es_results": []}, {"bug_id": 73, "bug_title": "ZipArchiveInputStream cannot handle some valid files", "bug_description": "See COMPRESS-62", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveEntry.java", "org.apache.commons.compress.archivers.zip.ZipUtil.java", "org.apache.commons.compress.archivers.zip.ExtraFieldUtilsTest.java", "org.apache.commons.compress.archivers.zip.UnrecognizedExtraField.java", "org.apache.commons.compress.archivers.zip.ExtraFieldUtils.java"], "label": 1, "es_results": []}, {"bug_id": 85, "bug_title": "cpio archive final entry corrupt", "bug_description": "The code below is called with an array of 4 file names. The cpio archive archive.cpio is created with no error messages, but when I then run the Unix command \"cpio -ivct <archive.cpio\" it reports the error \"Can&apos;t read input\" on the last file in the archive. If I run \"cpio -ivcBmu <archive.cpio\" the last file is incomplete, but the other files are extracted correctly. Same result in AIX and Linux.\n{{\n  private void createArchive(String[] outFiles)\n  throws FileNotFoundException, IOException, ArchiveException {\n    short format = CpioArchiveOutputStream.FORMAT_OLD_ASCII;\n    final OutputStream out = new FileOutputStream(\"archive.cpio\");\n    ArchiveOutputStream os = new CpioArchiveOutputStream(out, format);\n    for (int j = 0; j < outFiles.length; j++) \n{\n\n      System.out.println(\"Entry = \" + outFiles[j]);\n\n      File f = new File(outFiles[j]);\n\n      CpioArchiveEntry entry = new CpioArchiveEntry(format);\n\n      entry.setName(outFiles[j]);\n\n      entry.setSize(f.length());\n\n      os.putArchiveEntry(entry);\n\n      IOUtils.copy(new FileInputStream(outFiles[j]), os);\n\n      os.closeArchiveEntry();\n\n    }\n    os.finish();\n    os.close();\n  }\n}}", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.cpio.CpioArchiveEntry.java", "org.apache.commons.compress.archivers.cpio.CpioArchiveOutputStream.java", "org.apache.commons.compress.archivers.cpio.CpioConstants.java"], "label": 1, "es_results": []}, {"bug_id": 74, "bug_title": "ZipArchiveInputStream fails to update count of bytes read", "bug_description": "ZipArchiveInputStream fails to call count() in some cases where it reads (or puts back) data from the input stream.\nNot quite sure how to handle put-back yet, as a count of -1 is currently ignored, so how does one count a single byte that is put back?\nEasy enough to hack this in - count(1);count(-2); - but that&apos;s rather messy.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 101, "bug_title": "ZipArchiveInputStream does not handle data descriptors without signatures", "bug_description": "from http://www.pkware.com/documents/casestudies/APPNOTE.TXT :\n\n      Although not originally assigned a signature, the value \n      0x08074b50 has commonly been adopted as a signature value \n      for the data descriptor record.  Implementers should be \n      aware that ZIP files may be encountered with or without this \n      signature marking data descriptors and should account for\n      either case when reading ZIP files to ensure compatibility.\n      When writing ZIP files, it is recommended to include the\n      signature value marking the data descriptor record.  When\n      the signature is used, the fields currently defined for\n      the data descriptor record will immediately follow the\n      signature.\nThe current code skips over 16 bytes while the descriptor may be using only 12 bytes.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java", "org.apache.commons.compress.archivers.zip.ZipLong.java", "org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 110, "bug_title": "Tar fails to handle ustar \"prefix\" field", "bug_description": "Tar fails to handle the ustar \"prefix\" field, which is used when file paths are longer than 100 (but no longer than 256) characters.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveEntry.java", "org.apache.commons.compress.archivers.tar.TarConstants.java"], "label": 1, "es_results": []}, {"bug_id": 107, "bug_title": "ArchiveStreamFactory does not recognise tar files created by Ant", "bug_description": "ArchiveStreamFactory does not recognise tar files created by Ant.\nThese appear to have magic of  \"ustar\\0\" (i.e. same as MAGIC_POSIX) followed by \"\\0\\0\".\nNote that Compress can process the files OK.\nPatch to follow after checking what Ant writes as the signature.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 114, "bug_title": "TarUtils.parseName does not properly handle characters outside the range 0-127", "bug_description": "if a tarfile contains files with special characters, the names of the tar entries are wrong.\nexample:\ncorrect name: 0302-0601-3F06W220ZBLALALACANDC04060302MOE.model\nname resolved by TarUtils.parseName: 0302-0101-3F06W220ZBHECKMODULECEDC07060302DOERN.model\nplease use: \nresult.append(new String(new byte[] \n{ buffer[i] }\n));\ninstead of: \nresult.append((char) buffer[i]);\nto solve this encoding problem.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarUtilsTest.java", "org.apache.commons.compress.archivers.tar.TarUtils.java"], "label": 1, "es_results": []}, {"bug_id": 75, "bug_title": "ZipArchiveInputStream does not show location in file where a problem occurred", "bug_description": "See COMPRESS-62 - if ExtraFieldUtils.parse() detects an error, it only shows the offset within the local buffer, which is fairly useless in tracking down a problem.\nSomehow the current location in the file needs to be determined and added to the Exception message.\nThe count/bytesRead field would help, but that actually points to the next available byte, i.e. after the problem area.\nAlso, the internal data may have been expanded.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveEntry.java"], "label": 1, "es_results": []}, {"bug_id": 113, "bug_title": "TarArchiveEntry.parseTarHeader() includes the trailing space/NUL when parsing the octal size", "bug_description": "TarArchiveEntry.parseTarHeader() includes the trailing space/NUL when parsing the octal size.\nAlthough the size field in the header is 12 bytes, the last byte is supposed to be space or NUL - i.e. only 11 octal digits are allowed for the size.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.0", "fixed_version": "commons-compress-1.1", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarUtilsTest.java", "org.apache.commons.compress.archivers.tar.TarUtils.java"], "label": 1, "es_results": []}, {"bug_id": 119, "bug_title": "TarArchiveOutputStream#finish does not flush content to underlying stream", "bug_description": "Originally reported against Ant https://issues.apache.org/bugzilla/show_bug.cgi?id=50014\n\nThe finish() method of TarOutputStream does not flush the TarBuffer to the\nunderlying output stream. A subsequent flush() on the TarOutputStream\nunexpectedly does not flush all written data to the underlying stream.\nCase: If an outputStream is tied to a Socket OutputStream, the receiving server\ncan be notified that the stream has finished by calling the\nSocket.shutdownOutput() method while still allowing the server to respond to\nthe client e.g. for sending a status report of the finished data transfer. This\nis in particular usefull if the size of the stream is not known in advance such\nas for a tar stream generated on the fly at the client side.\nThe above case relies on all data being sent to the server before invoking\nSocket.shutdownOutput(). The TarOutputStream finish() method suggests to be\ndoing just this but in practice does not send any remaining data in the\nTarBuffer. Calling the TarOutputStream close() method is not an option in this\ncase since this also closes the Socket connection.\nQuick fix: insert a buffer.flushBlock() statement at the end of the\nTarOutputStream.finish() method.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.1", "fixed_version": "COMPRESS_1.2", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarBuffer.java", "org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 125, "bug_title": "BZip2CompressorInputStream throws IOException if underlying stream returns available() == 0", "bug_description": "BZip2CompressorInputStream,init() will throw an IOException, if the passed stream returns 0 for available():\nBZip2CompressorInputStream.java\n\n\n    private void init() throws IOException {\n\n        ...\n\n        if (in.available() == 0) {\n\n            throw new IOException(\"Empty InputStream\");\n\n        }\n\n        ...\n\n     }\n\n\n\nI think this is not correct, because the underlying stream may indeed be able to only return 0 bytes without blocking but may be able to block a little and then return some more bytes.\nNote also the change in the API documentation from: \"Returns the number of bytes that can be read \" (1.4.2) to \"Returns an estimate of the number of bytes that can be read\".", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.1", "fixed_version": "COMPRESS_1.2", "fixed_files": ["org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 117, "bug_title": "Certain tar files not recognised by ArchiveStreamFactory", "bug_description": "Certain tar files, like this one: http://leo.scruffohio.net/cgi-bin/uploads/lzma912.tar aren&apos;t being recognized by the ArchiveInputStream detector.\nI can open this tar file perfectly well with WinRAR and 7-zip. Neither of these complain. I can also open it with the command tar -xvf. However, I narrowed it down, and it turns out the TarArchiveInputStream.matches() is returning false, even though it is a valid tar file. This glitch should be fixed.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.1", "fixed_version": "COMPRESS_1.2", "fixed_files": ["org.apache.commons.compress.DetectArchiverTestCase.java", "org.apache.commons.compress.archivers.ArchiveStreamFactory.java"], "label": 1, "es_results": []}, {"bug_id": 139, "bug_title": "ZipFile fails to clean up Inflater resources", "bug_description": "This bug has been reported to Ant&apos;s original code as https://issues.apache.org/bugzilla/show_bug.cgi?id=42696 .  It seems as if InflaterInputStream.close() didn&apos;t clean up the Inflater instance if one is passed in to the constructor.  I&apos;m currently testing the patch to Ant and will merge it over once I&apos;m ready.\nThe code in Harmony[1] does invoke inflater.end but calling it again won&apos;t hurt.  OpenJDK&apos;s InflaterInputStream[2] in fact behaves as described in Mounir&apos;s comment to Ant&apos;s Bugzilla.\n[1] http://svn.apache.org/repos/asf/harmony/enhanced/java/trunk/classlib/modules/archive/src/main/java/java/util/zip/InflaterInputStream.java\n[2] http://hg.openjdk.java.net/jdk6/jdk6/jdk/file/506b35a2f558/src/share/classes/java/util/zip/InflaterInputStream.java", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.1", "fixed_version": "COMPRESS_1.2", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipFile.java"], "label": 1, "es_results": []}, {"bug_id": 129, "bug_title": "\"java.io.EOFException: Truncated ZIP entry: <some entry>\"- while extracting a zip file that contains a entry which lager than 2 GB (Integer#MAX_VALUE)", "bug_description": "Issue:\n\"java.io.EOFException: Truncated ZIP entry: <some entry>\" will be threw while extracting a zip file that contains a entry with size larger than Integer#MAX_VALUE bytes (about 2 GB). After the big entry has been read, then try to get next entry by calling ZipArchiveInputStream#getNextZipEntry(), and it throws that EOFException.\nBecause:\nbefore getting next zip entry, ZipArchiveInputStream tries to close the current entry and in the close- method it use the field \"bytesReadFromStream\" to ensure all entry bytes are read, however the field \"bytesReadFromStream\" is a integer, that means it is already overflowed and it leads to a false ensure result.\nSolution suggestion:\ninstead integer using long for \"bytesReadFromStream\" and possibly for \"readBytesOfEntry\" too.", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.1", "fixed_version": "COMPRESS_1.2", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java", "org.apache.commons.compress.archivers.zip.ZipUtil.java", "org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 131, "bug_title": "ArrayOutOfBounds while decompressing bz2", "bug_description": "Decompressing a bz2 file generated by bzip2 utility throws an ArrayIndexOutOfBounds at method recvDecodingTables() line 469.\nI believe it is related to encodings used to generate the original text file (the compressed file).", "project": "Commons", "sub_project": "COMPRESS", "version": "commons-compress-1.1", "fixed_version": "COMPRESS_1.4", "fixed_files": ["org.apache.commons.compress.compressors.BZip2TestCase.java"], "label": 1, "es_results": []}, {"bug_id": 332, "bug_title": "FramedSnappyCompressorInputStream.read returns invalid value at end of stream.", "bug_description": "FramedSnappyCompressorInputStream.read() returns 0 when the end of stream has been reached rather than -1.\nIt appears that this may be caused by SnappyCompressorInputStream.read(byte[], int, int) returning 0 instead of -1 at the end of stream when the exact number of bytes have been read.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.10", "fixed_version": "rel/1.11", "fixed_files": ["org.apache.commons.compress.compressors.snappy.SnappyCompressorInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 331, "bug_title": "Some non TAR files are recognized by ArchiveStreamFactory", "bug_description": "I ran into a case where a PNG file is being recognized as TAR because TarUtils.verifyCheckSum reports it as having a valid checksum (in this case the code thinks the stored checksum is 36936, unsigned is 31155 and signed is 19635). Because the stored checksum value is larger then the unsigned checksum it is treated as a valid TAR.\nI haven&apos;t spent enough time digging into the problem to see if there is a good alternative to the existing check that doesn&apos;t have false positives like this PNG file (which, if anyone is interested comes from an Android download).\nAlso, I noticed a minor thing in the code: the comment in TarUtils.verifyCheckSum has the wrong bug number listed (it says 177 instead of 117).", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.10", "fixed_version": "rel/1.11", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarUtilsTest.java", "org.apache.commons.compress.archivers.tar.TarUtils.java"], "label": 1, "es_results": []}, {"bug_id": 334, "bug_title": "ArArchiveInputStream.getBSDLongName does increment offset", "bug_description": "I have an AR archive which uses the BSD long name convention and I cannot read past the first entry without failing.\nI dug into the issue and it appears the problem is with getBSDLongName which calls readFully with the underlying stream:\n\n\n\nint read = IOUtils.readFully(input, name);\n\ncount(read);\n\n\n\nThis does not increment the offset which is later used by read(byte[], int, int) to compute the value of toRead. Since offset is too small, toRead ends up too big and the first entry contents end up pulling in the first n bytes of the next entry header (n is the BSD long name length).\nI think this can be addressed by adding offset += read or changing the above code to just:\n\n\n\nint read = IOUtils.readFully(this, name);\n\n\n\n(Sorry for posting code in the ticket, I will try and get an environment set up so I can start submitting patches).", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.10", "fixed_version": "rel/1.11", "fixed_files": ["org.apache.commons.compress.archivers.ar.ArArchiveInputStreamTest.java", "org.apache.commons.compress.archivers.ar.ArArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 335, "bug_title": "TAR checksum fails when checksum is right aligned", "bug_description": "The linked TAR has a checksum with zero padding on the left instead of the expected NULL-SPACE terminator on the right. As a result the last two digits of the stored checksum are lost and the otherwise valid checksum is treated as invalid.\nGiven that the code already checks for digits being in range before adding them to the stored sum, is it necessary to only look at the first 6 octal digits instead of the whole field?", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.10", "fixed_version": "rel/1.11", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarUtils.java", "org.apache.commons.compress.DetectArchiverTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 336, "bug_title": "Extended Standard TAR format prefix is 130 characters", "bug_description": "A TAR archive created with star having an artype of \"xstar\" apparently limits the PREFIX to 130 characters to accommodate an access time and a creation time (this much I was able to learn from the star man page). I wasn&apos;t able to track down any specifics about the format, but in at least the first example I found, it appears that the access and creation time are stored as two space terminated ASCII numbers at the end of what would otherwise be the prefix.\nCurrently, the code will read this type of archive and assume the prefix is 131 NULs followed by the two ASCII time stamps. Needless to say, it makes a mess of the entry names.\nI&apos;m not 100% sure of the implementation, but perhaps something like (with XSTAR_PREFIXLEN == 130):\n\n\n\ndefault: {\n\n  final int prefixlen = header[offset + XSTAR_PREFIXLEN + 1] == 0 ? XSTAR_PREFIXLEN : PREFIXLEN;\n\n  String prefix = oldStyle\n\n    ? TarUtils.parseName(header, offset, prefixlen)\n\n    : TarUtils.parseName(header, offset, prefixlen, encoding);\n\n  // ...\n\n}\n\n\n\nMaybe a separate feature request would be appropriate for capturing and exposing the additional timestamps?", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.10", "fixed_version": "rel/1.11", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveEntry.java", "org.apache.commons.compress.archivers.tar.TarConstants.java"], "label": 1, "es_results": []}, {"bug_id": 343, "bug_title": "Native Memory Leak in Sevenz-DeflateDecoder", "bug_description": "The class ...sevenz.Coders.DeflateDecoder does not close (end()) the Deflater and Inflater. This can lead to native memory issues: see https://bugs.openjdk.java.net/browse/JDK-8074108.\nIn our case we create a zip archive with >100000 files. The Java heap is around 300MB (with 2GB max). The native memory is increasing to 8GB and above. Because the Java heap has no pressure - no GC is triggered, therefore the Deflaters are not collected and the native memory is not freed.  \n", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.10", "fixed_version": "rel/1.11", "fixed_files": ["org.apache.commons.compress.archivers.sevenz.Coders.java"], "label": 1, "es_results": []}, {"bug_id": 330, "bug_title": "Tar UnArchive Fails when archive contains directory sizes which are non-zero.", "bug_description": "Tar UnArchive Fails when archive contains directory sizes which are non-zero. I recently came across a set of files which failed to extract with commons-compress but I was able to successfully extract the files with GNU tar. \nThe problem is TarArchiveInputStream.java gets the size of each entry in a tar archive to determine how many bytes to read ahead. Directories are always sized 0 bytes, however its technically possible a tar archive contains a size for a directory. This causes the Input Stream to loose it&apos;s place and eventually results in an exception being thrown.\nI was able to implement a proof-of-concept fix by checking if an entity is a directory and setting the directory size to 0. (A directory with Files in a Tar Archive still has a size of 0 as the directory itself does not have size in the archive.)", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.10", "fixed_version": "rel/1.11", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 344, "bug_title": "Handle NULL terminated GNU AR extended name", "bug_description": "We have an AR archive (a .lib file) whose extended name is terminated by a NULL instead of a line feed which causes an IOException (\"Failed to read entry: 0\"). It looks like ArArchiveInputStream.getExtendedName just needs to check namebuffer[i] for &apos;\\012&apos; or 0.\nThe ar tool in latest GNU binutils seems to be able to handle this.\nI don&apos;t know what to make of the archive itself: it seems to contain 291 different copies of a file with the same name; but it is a Windows lib file and I am not going to pretend like I understand if this is supposed to be normal or not.\nThe file in question is part of the SIGAR project, the sigar-bin/lib/sigar-x86-winnt.lib archive from the 1.6.3 distribution exhibits this behavior. The NULL terminated string only appears in the first file, all subsequent files seem to use the expected line feed terminator.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.10", "fixed_version": "rel/1.11", "fixed_files": ["org.apache.commons.compress.archivers.ar.ArArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 348, "bug_title": "Calling SevenZFile.read() on empty SevenZArchiveEntry throws IllegalStateException", "bug_description": "I&apos;m pretty sure COMPRESS-340 breaks reading empty archive entries. When calling getNextEntry() and that entry has no content, the code jumps into the first block at line 830 (SevenZFile.class), clearing the deferredBlockStreams. When calling entry.read(...) afterwards an IllegalStateException (\"No current 7z entry (call getNextEntry() first).\") is thrown. IMHO, there should be another check for entry.getSize() == 0.\nThis worked correctly up until 1.10.", "project": "Commons", "sub_project": "COMPRESS", "version": "rel/1.11", "fixed_version": "rel/1.12", "fixed_files": ["org.apache.commons.compress.archivers.sevenz.SevenZFile.java", "org.apache.commons.compress.archivers.sevenz.SevenZFileTest.java"], "label": 1, "es_results": []}, {"bug_id": 355, "bug_title": "Parsing PAX headers fails with NegativeArraySizeException", "bug_description": "The TarArchiveInputStream.parsePaxHeaders method fails with a NegativeArraySizeException when there is an empty line at the end of the headers.\nThe inner loop starts reading the length, but it gets a newline (10) and ends up subtracting &apos;0&apos; (48) from it; the result is a negative length that blows up an attempt to allocate the rest array.\nI would say that a check to see if ch is less the &apos;0&apos; and break the loop if it is.\nI used npm pack aws-sdk@2.2.16 to generate a tarball with this issue.", "project": "Commons", "sub_project": "COMPRESS", "version": "rel/1.11", "fixed_version": "rel/1.12", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveInputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveInputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 356, "bug_title": "PAX header entry name ending with / causes problems", "bug_description": "There seems to be a problem when a PAX header entry (link flag is &apos;x&apos;) has a name ending with \"/\". The TarArchiveEntry.isDirectory() check ends up returning true because of the trailing slash which means no content can be read from the entry. PAX header parsing effectively finds nothing and the stream is not advanced; this leaves the stream in a bad state as the next entry&apos;s header is actually read from the header contents.\nIf the name is modified to remove the trailing slash when the link flag indicates a PAX header everything seems to work fine. That would be one potential fix in parseTarHeader. Changing isDirectory to return false if isPaxHeader is true (before the trailing \"/\" check) would probably also fix the issue (though I can&apos;t verify that in the debugger like I can with changing the name).\nSo far I have only seen this when using Docker to save images that contain a yum database. For example:\n\ndocker pull centos:latest && docker save centos:latest | tar x --include \"*/layer.tar\"\n\n\n\nWill produce at least one \"layer.tar\" that exhibits this issue. If I come across a smaller TAR for testing I will attach it.", "project": "Commons", "sub_project": "COMPRESS", "version": "rel/1.11", "fixed_version": "rel/1.12", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveInputStreamTest.java", "org.apache.commons.compress.archivers.tar.TarArchiveEntry.java"], "label": 1, "es_results": []}, {"bug_id": 351, "bug_title": "Defective .zip-archive produces problematic error message", "bug_description": "A truncated .zip-File produces an java.io.EOFException conatining a hughe amount of byte[]-data in the error-message - leading to beeps and crippeling workload in an potential console-logger.\n", "project": "Commons", "sub_project": "COMPRESS", "version": "rel/1.11", "fixed_version": "rel/1.12", "fixed_files": ["org.apache.commons.compress.archivers.cpio.CpioArchiveInputStream.java", "org.apache.commons.compress.ArchiveUtilsTest.java", "org.apache.commons.compress.utils.ArchiveUtils.java", "org.apache.commons.compress.archivers.zip.ZipArchiveInputStreamTest.java", "org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 363, "bug_title": "Overflow in BitInputStream", "bug_description": "in Class BitInputStream.java(\\src\\main\\java\\org\\apache\\commons\\compress\\utils),\nfuncion:\n public long readBits(final int count) throws IOException {\n        if (count < 0 || count > MAXIMUM_CACHE_SIZE) \n{\n\n            throw new IllegalArgumentException(\"count must not be negative or greater than \" + MAXIMUM_CACHE_SIZE);\n\n        }\n        while (bitsCachedSize < count) {\n            final long nextByte = in.read();\n            if (nextByte < 0) \n{\n\n                return nextByte;\n\n            }\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) \n{\n\n                bitsCached |= (nextByte << bitsCachedSize);\n\n            }\n else \n{\n\n                bitsCached <<= 8;\n\n                bitsCached |= nextByte;\n\n            }\n            bitsCachedSize += 8;\n        }\n        final long bitsOut;\n        if (byteOrder == ByteOrder.LITTLE_ENDIAN) \n{\n\n            bitsOut = (bitsCached & MASKS[count]);\n\n            bitsCached >>>= count;\n\n        }\n else \n{\n\n            bitsOut = (bitsCached >> (bitsCachedSize - count)) & MASKS[count];\n\n        }\n        bitsCachedSize -= count;\n        return bitsOut;\n    }\nI think here \"bitsCached |= (nextByte << bitsCachedSize);\" will overflow in some cases. for example, below is a test case:\npublic static void test() {\n        ByteArrayInputStream in = new ByteArrayInputStream(new byte[]\n{87, 45, 66, 15,\n\n                                                                      90, 29, 88, 61, 33, 74}\n);\n        BitInputStream bin = new BitInputStream(in, ByteOrder.LITTLE_ENDIAN);\n        try \n{\n\n            long ret = bin.readBits(5);\n\n            ret = bin.readBits(63);\n\n            ret = bin.readBits(12);\n\n        }\n catch (Exception e) \n{\n\n            e.printStackTrace();\n\n        }\n}\noverflow occur in \"bin.readBits(63);\" , so ,result in wrong result from  \"bin.readBits(12);\" \n", "project": "Commons", "sub_project": "COMPRESS", "version": "rel/1.12", "fixed_version": "rel/1.13", "fixed_files": ["org.apache.commons.compress.utils.BitInputStream.java", "org.apache.commons.compress.utils.BitInputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 366, "bug_title": "TarArchiveEntry: getDirectoryEntries not working", "bug_description": "TarArchiveEntry.getDirectoryEntries() always returns an empty array. This is because entry.getFile() returns null for a directory entry.\nLet folder.tar be a Tar Archive which contains a folder, and that folder contains a file. Consider the following snippet:\n\nimport java.io.FileInputStream;\nimport org.apache.commons.compress.archivers.tar.*;\npublic class GetDirectoryEntriesBug {\n\tpublic static void main(String[] args) throws Exception {\n\t\tTarArchiveInputStream tais = new TarArchiveInputStream(new FileInputStream(\"folder.tar\"));\n\t\tfor(TarArchiveEntry entry; (entry = tais.getNextTarEntry()) != null; ) \n{\n\n\t\t\tSystem.out.println(\"Name: \" + entry.getName() + \", isDirectory: \" + entry.isDirectory() + \", getDirectoryEntries().length: \" + entry.getDirectoryEntries().length);\n\n\t\t}\n\t\ttais.close();\n\t}\n}\n\nOutput:\nName: folder/file, isDirectory: false, getDirectoryEntries().length: 0\nName: folder/, isDirectory: true, getDirectoryEntries().length: 0\nI expected that, for \"folder/\", getDirectoryEntries() will not return an empty array.", "project": "Commons", "sub_project": "COMPRESS", "version": "rel/1.12", "fixed_version": "rel/1.13", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveEntry.java"], "label": 1, "es_results": []}, {"bug_id": 152, "bug_title": "ZiparchiveInputStream and ZiparchiveOutputStream do not clean up Inflater/Deflater instances", "bug_description": "This is similar to COMPRESS-139\nThe Inflater and Deflater instances kept by the streams should be cleaned up by calling end() inside the close method so native resources get freed.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.2", "fixed_version": "COMPRESS_1.3", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java", "org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 160, "bug_title": "TarArchiveOutputStream.getBytesWritten() returns invalid value", "bug_description": "It appears the TarArchiveOutputStream.getBytesWritten()returns zero or invalid value when queried.\nIn the code sample below, it returns zero, even after an sizeable file was processed.\nI&apos;ve printed it twice, once before closing the output stream, and once after, just for the reference.\nIt is also demonstrable on multiple processed files.\nWithin the TarArchiveOutputStream.getBytesWritten() implementation, it appears the call for count(numToWrite) is made after the numToWrite is depleted in the process of actual byte writing. When call for count(numToWrite); is moved up, the returned values for TarArchiveOutputStream.getBytesWritten() are getting equal to the sum of the sizes of processed files. This is much closer to expected value (\"Returns the current number of bytes written to this stream.\") but still not correct, for that number should include the tar header sizes as well.\nAt any rate, please find the proposed patch below, merely moving count(numToWrite); up a few lines. This makes TarArchiveOutputStream.getBytesWritten() closer to true value.\nTest code:\n\n\n\n@Test\n\n\tpublic void tartest() throws Exception {\n\n\t\t\n\n\t\tFileOutputStream myOutputStream = new FileOutputStream(\"C:/temp/tartest.tar\");\n\n\t\t\n\n\t\tArchiveOutputStream sTarOut = new ArchiveStreamFactory().createArchiveOutputStream(ArchiveStreamFactory.TAR, myOutputStream);\n\n\t\t\n\n\t\tFile sSource = new File(\"C:/share/od_l.txt\");\n\n\t\tTarArchiveEntry sEntry = new TarArchiveEntry(sSource);\n\n\t\tsTarOut.putArchiveEntry(sEntry);\n\n\t\t\n\n\t\tFileInputStream sInput = new FileInputStream(sSource);\n\n\t\tbyte[] cpRead = new byte[8192];\n\n\t\t\n\n\t\tint iRead = 0;\n\n\t\twhile ((iRead = sInput.read(cpRead)) > 0) {\n\n\t\t\tsTarOut.write(cpRead, 0, iRead);\n\n\t\t}\n\n\t\t\n\n\t\tsLog.info(\"Processed: \"+sTarOut.getBytesWritten()+\" bytes. File Len: \"+sSource.length());\n\n\t\t\n\n\t\tsInput.close();\n\n\t\tsTarOut.closeArchiveEntry();\n\n\t\tsTarOut.close();\n\n\n\n\t\tsLog.info(\"Processed: \"+sTarOut.getBytesWritten()+\" bytes. File Len: \"+sSource.length());\n\n\n\n\t\t\n\n\t\treturn;\n\n\t\t\t\n\n\t}\n\n\n\nTest Output:\n\n\n\nOct 21, 2011 9:09:28 AM com.cronsult.jndmpd.test.Backup tartest\n\nINFO: Processed: 0 bytes. File Len: 186974208\n\nOct 21, 2011 9:09:28 AM com.cronsult.jndmpd.test.Backup tartest\n\nINFO: Processed: 0 bytes. File Len: 186974208\n\n\n\nProposed Patch:\n\n\n\nIndex: src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java\n\n===================================================================\n\n--- src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java\t(revision 1187150)\n\n+++ src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java\t(working copy)\n\n@@ -276,6 +276,8 @@\n\n             // eliminate some of the buffer copying.\n\n             //\n\n         }\n\n+        \n\n+        count(numToWrite);\n\n \n\n         if (assemLen > 0) {\n\n             if ((assemLen + numToWrite) >= recordBuf.length) {\n\n@@ -325,7 +327,7 @@\n\n             wOffset += num;\n\n         }\n\n         \n\n-        count(numToWrite);\n\n+        \n\n     }\n\n \n\n     /**\n\n\n\n\n", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.2", "fixed_version": "COMPRESS_1.3", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 171, "bug_title": "createArchiveInputStream detects text files less than 100 bytes as tar archives", "bug_description": "The fix for COMPRESS-117 which modified ArchiveStreamFactory().createArchiveInputStream(inputstream) results in short text files (empirically seems to be those <= 100 bytes) being detected as tar archives which obviously is not desirable if one wants to know whether or not the files are archives.\nI&apos;m not an expert on compressed archives but perhaps the heuristic that if a stream is interpretable as a tar file without an exception being thrown should only be applied on archives greater than 100 bytes?", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.2", "fixed_version": "COMPRESS_1.4", "fixed_files": ["org.apache.commons.compress.archivers.ArchiveStreamFactory.java"], "label": 1, "es_results": []}, {"bug_id": 159, "bug_title": "ChangeSetPerformer not reliable for ZipArchiveInputStreams", "bug_description": "ChangeSetPerformer&apos;s perform function takes an ArchiveInputStream as an argument and thus frequently runs into issues described under heading &apos;ZipArchiveInputStream vs ZipFile&apos; at http://commons.apache.org/compress/zip.html \nPersonally for a simple local solution I&apos;ve added a slightly modified performZip function taking a ZipFile argument in place of the ArchiveInputStream argument:\ndifferent perform performZip\n1c1\n<     public ChangeSetResults perform(ArchiveInputStream in, ArchiveOutputStream out)\n\n>     public ChangeSetResults performZip(ZipFile zf, ArchiveOutputStream out)\n17,18c17,18\n<         ArchiveEntry entry = null;\n<         while ((entry = in.getNextEntry()) != null) {\n\n>         ArrayList<ZipArchiveEntry> entries = Collections.list(zf.getEntriesInPhysicalOrder());\n>         for (ZipArchiveEntry entry : entries) {\n46c46\n<                 copyStream(in, out, entry);\n\n>                 copyStream(zf.getInputStream(entry), out, entry);\nA permanent fix may require some re-design, the perform(ArchiveInputStream in, ArchiveOutputStream out) abstraction may be overly general.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.2", "fixed_version": "COMPRESS-1.5", "fixed_files": ["org.apache.commons.compress.changes.ChangeSetPerformer.java", "org.apache.commons.compress.changes.ChangeSetTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 164, "bug_title": "Cannot Read Winzip Archives With Unicode Extra Fields", "bug_description": "I have a zip file created with WinZip containing Unicode extra fields. Upon attempting to extract it with org.apache.commons.compress.archivers.zip.ZipFile, ZipFile.getInputStream() returns null for ZipArchiveEntries previously retrieved with ZipFile.getEntry() or even ZipFile.getEntries(). See UTF8ZipFilesTest.patch in the attachments for a test case exposing the bug. The original test case stopped short of trying to read the entries, that&apos;s why this wasn&apos;t flagged up before. \nThe problem lies in the fact that inside ZipFile.java entries are stored in a HashMap. However, at one point after populating the HashMap, the unicode extra fields are read, which leads to a change of the ZipArchiveEntry name, and therefore a change of its hash code. Because of this, subsequent gets on the HashMap fail to retrieve the original values.\nZipFile.patch contains an (admittedly simple-minded) fix for this problem by reconstructing the entries HashMap after the Unicode extra fields have been parsed. The purpose of this patch is mainly to show that the problem is indeed what I think, rather than providing a well-designed solution.\nThe patches have been tested against revision 1210416.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.3", "fixed_version": "COMPRESS_1.4", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipFile.java", "org.apache.commons.compress.archivers.zip.UTF8ZipFilesTest.java"], "label": 1, "es_results": []}, {"bug_id": 163, "bug_title": "Unable to extract a file larger than 8GB from a Posix-format tar archive", "bug_description": "An attempt to read a posix-format tar archive containing a file in excess of 8^11 bytes in size will fail with a \"Size out of range\" illegal argument exception.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.3", "fixed_version": "COMPRESS_1.4", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveEntryTest.java", "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveEntry.java"], "label": 1, "es_results": []}, {"bug_id": 175, "bug_title": "GNU Tar sometimes uses binary encoding for UID and GID", "bug_description": "Some tar files have UID and GID fields that start with the binary marker 0x80. The TarArchiveEntry is unable to cope with that since it assumes that UID and GID are always ASCII.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.3", "fixed_version": "COMPRESS_1.4", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveEntry.java"], "label": 1, "es_results": []}, {"bug_id": 176, "bug_title": "ArchiveInputStream#getNextEntry(): Problems with WinZip directories with Umlauts", "bug_description": "There is a problem when handling a WinZip-created zip with Umlauts in directories.\nI&apos;m accessing a zip file created with WinZip containing a directory with an umlaut (\"\") with ArchiveInputStream. When creating the zip file the unicode-flag of winzip had been active.\nThe following problem occurs when accessing the entries of the zip:\nthe ArchiveEntry for a directory containing an umlaut is not marked as a directory and the file names for the directory and all files contained in that directory contain backslashes instead of slashes (i.e. completely different to all other files in directories with no umlaut in their path).\nThere is no difference when letting the ArchiveStreamFactory decide which ArchiveInputStream to create or when using the ZipArchiveInputStream constructor with the correct encoding (I&apos;ve tried different encodings CP437, CP850, ISO-8859-15, but still the problem persisted).\nThis problem does not occur when using the very same zip file but compressed by 7zip or the built-in Windows 7 zip functionality.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.3", "fixed_version": "COMPRESS_1.4", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipFileTest.java", "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.java"], "label": 1, "es_results": []}, {"bug_id": 187, "bug_title": "ZipArchiveInputStream and ZipFile do not produce equals ZipArchiveEntry instances", "bug_description": "I&apos;m trying to use a ZipArchiveEntry coming from ZipArchiveInputStream that I stored somwhere for later with a ZipFile and it does not work.\nThe reason is that it can&apos;t find the ZipArchiveEntry in the ZipFile entries map. It is exactly the same zip file but both entries are not equals so the Map#get fail.\nAs far as I can see the main difference is that comment is null in ZipArchiveInputStream while it&apos;s en empty string in ZipFile. I looked at ZipArchiveInputStream and it looks like the comment (whatever it is) is simply not parsed while I can find some code related to the comment at the end of ZIipFile#readCentralDirectoryEntry.\nNote that java.util.zip does not have this issue. Did not checked what they do but the zip entries are equals.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.4", "fixed_version": "COMPRESS-1.5", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveEntryTest.java", "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.java"], "label": 1, "es_results": []}, {"bug_id": 209, "bug_title": "DOC is incorrectly recognized as TAR", "bug_description": "Empty DOC is autodetected as TAR by ArchiveStreamFactory.createArchiveInputStream(InputStream).", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.4.1", "fixed_version": "COMPRESS-1.5", "fixed_files": ["org.apache.commons.compress.archivers.ArchiveStreamFactoryTest.java"], "label": 1, "es_results": []}, {"bug_id": 200, "bug_title": "Round trip conversion with more than 66 US-ASCII characters fails when using TarArchiveOutputStream.LONGFILE_GNU", "bug_description": "When using TarArchiveOutputStream.LONGFILE_GNU with an entry name of more than 66 US-ASCII characters, a round trip conversion (write the entry, then read it back) fails because of several bugs in TarArchiveOutputStream and TarArchiveInputStream.\nThis has been reported as an issue to TrueZIP, which is why you can find a more detailed analysis here: http://java.net/jira/browse/TRUEZIP-286 .", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.4.1", "fixed_version": "COMPRESS-1.5", "fixed_files": ["org.apache.commons.compress.compressors.CompressorStreamFactory.java", "org.apache.commons.compress.archivers.tar.TarUtils.java", "org.apache.commons.compress.archivers.zip.UTF8ZipFilesTest.java", "org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveOutputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 189, "bug_title": "ZipArchiveInputStream may read 0 bytes when reading from a nested Zip file", "bug_description": "When the following code is run an error \"Underlying input stream returned zero bytes\" is produced. If the commented line is uncommented it can be seen that the ZipArchiveInputStream returned 0 bytes. This only happens the first time read is called, subsequent calls work as expected i.e. the following code actually works correctly with that line uncommented!\nThe zip file used to produce this behavious is available at http://wwmm.ch.cam.ac.uk/~dl387/test.ZIP\nIf this is not the correct way of processing a zip file of zip files please let me know. Also I believe whilst ZipFile can iterate over entries fast due to being able to look at the master table whilst ZipArchiveInputStream cannot. Is there anyway of instantiating a ZipFile from a zip file inside another zip file without first extracting the nested zip file?\n    ZipFile zipFile = new ZipFile(\"C:/test.ZIP\");\n    for (Enumeration<ZipArchiveEntry> iterator = zipFile.getEntries(); iterator.hasMoreElements(); ) {\n      ZipArchiveEntry entry = iterator.nextElement();\n      InputStream is = new BufferedInputStream(zipFile.getInputStream(entry));\n      ZipArchiveInputStream zipInput = new ZipArchiveInputStream(is);\n      ZipArchiveEntry innerEntry;\n      while ((innerEntry = zipInput.getNextZipEntry()) != null){\n        if (innerEntry.getName().endsWith(\"XML\"))\n{\n\n          //zipInput.read();\n\n          System.out.println(IOUtils.toString(zipInput));\n\n        }\n      }\n    }", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.4.1", "fixed_version": "COMPRESS-1.5", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveInputStreamTest.java", "org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 201, "bug_title": "No constructor to create a TarArchiveEntry link with leading slash", "bug_description": "I want to create a link with a leading slash and put it in a tar archive (Debian package).\nThe following constructors are provided\n\n\n\npublic TarArchiveEntry(String name, boolean preserveLeadingSlashes)\n\npublic TarArchiveEntry(String name, byte linkFlag)\n\n\n\nbut there is no\n\n\n\npublic TarArchiveEntry(String name, byte linkFlag, boolean preserveLeadingSlashes)\n\n\n\nso I have to overwrite the name using setName(String) which is not very clean.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.4.1", "fixed_version": "COMPRESS-1.5", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveEntryTest.java", "org.apache.commons.compress.archivers.tar.TarArchiveEntry.java"], "label": 1, "es_results": []}, {"bug_id": 212, "bug_title": "TarArchiveEntry getName() returns wrongly encoded name even when you set encoding to TarArchiveInputStream", "bug_description": "I have two file systems. One is Red Hat Linux, the other is MS Windows.\nI created a *.tgz file in Red Hat Linux and tried to decompress it in MS Windows using Commons Compress.\nThe default system encoding are different. UTF-8 in Red Hat Linux and CP949 in MS Windows.\nIt seems that the file name encoding follows the default encoding even though when I use the following to untar it.\nFileInputStream fis = new FileInputStream(new File(*.tgz));\nTarArchiveInputStream zis = new TarArchiveInputStream(new BufferedInputStream(fis),encodingOfRedHatLinux);\nwhile ((entry = (TarArchiveEntry)zis.getNextEntry()) != null)\n{\n\nentry.getName(); // filename is not UTF-8 it is encoded in CP949 and so the filename isn&apos;t consistent\n\n}\n\nBy referring to this\n    /**\n\nConstructor for TarInputStream.\n@param is the input stream to use\n@param encoding name of the encoding to use for file names\n@since Commons Compress 1.4\n     */\n    public TarArchiveInputStream(InputStream is, String encoding) \n{\n\n        this(is, TarBuffer.DEFAULT_BLKSIZE, TarBuffer.DEFAULT_RCDSIZE, encoding);\n\n    }\n\nencoding should be used for file names.\nBut actually this doesn&apos;t seem to work.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.4.1", "fixed_version": "COMPRESS-1.5", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveInputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveInputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 218, "bug_title": "Typo in CompressorStreamFactory Javadoc", "bug_description": "The Javadoc for CompressorStreamFactory contains two examples of \"Compressing a file\". In actuality, the second example is decompressing a file.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.4.1", "fixed_version": "COMPRESS-1.5", "fixed_files": ["org.apache.commons.compress.compressors.CompressorStreamFactory.java"], "label": 1, "es_results": []}, {"bug_id": 219, "bug_title": "ZipArchiveInputStream: ArrayIndexOutOfBoundsException when extracting a STORED zip file entry from within a zip.", "bug_description": "When trying to read out a ZIP file, that has been stored (Method STORE, not DEFLATE!, with DEFLATE it seems OK) in another ZIP file using the ZipArchiveInputStream, I do get an ArrayIndexOutOfBoundsException when doing the arraycopy in ZipArchiveInputStream#readStored(byte[], int, int) (line 362) because the \"toRead\" is not decreased by the buf.offsetInBuffer.\nI will add the zip in question as attachment.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.4.1", "fixed_version": "COMPRESS-1.5", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipArchiveInputStreamTest.java", "org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 229, "bug_title": "incorrect handling of GNU longlink entries", "bug_description": "Apache commons-compress handles GNU LongLink entries for long filenames. But it fails to handle LongLink entries for long linknames[1] correctly.\n[1] http://git.savannah.gnu.org/cgit/tar.git/tree/src/tar.h#n176", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.4.1", "fixed_version": "COMPRESS-1.6", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveInputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveEntry.java", "org.apache.commons.compress.archivers.LongPathTest.java", "org.apache.commons.compress.archivers.tar.TarConstants.java"], "label": 1, "es_results": []}, {"bug_id": 253, "bug_title": "BZip2CompressorInputStream reads fewer bytes from truncated file than CPython's bz2 implementation", "bug_description": "Jython includes support for decompressing bz2 files using commons compress and shares regression tests with CPython. The CPython test test_read_truncated in test_bz2.py passes under CPython but fails under Jython.\nThe BZip2CompressorInputStream is able to read 769 bytes from the truncated data rather than the 770 bytes that the CPython bz2 implementation can read.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS_1.4.1", "fixed_version": "COMPRESS-1.8", "fixed_files": ["org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 146, "bug_title": "BZip2CompressorInputStream always treats 0x177245385090 as EOF, but should treat this as EOS", "bug_description": "BZip2CompressorInputStream always treats 0x177245385090 as EOF, but should treat this as EOS\nThis error occurs mostly on large size files as sudden EOF somwere in the middle of the file.\nAn example of data from archived file:\n$ cat fastq.ax.bz2 | od -t x1 | grep -A 1 &apos;17 72 45&apos;\n22711660 d0 ff b6 01 20 10 ff ff 17 72 45 38 50 90 2e ff\n22711700 b2 d3 42 5a 68 39 31 41 59 26 53 59 84 3c 41 75\n\n24637020 c5 49 ff 19 80 49 20 7f ff 17 72 45 38 50 90 a4\n24637040 a8 ac bd 42 5a 68 39 31 41 59 26 53 59 0d 9a before\n\n40302720 ff b1 24 80 10 ff ff 17 72 45 38 50 90 24 cb c5\n40302740 90 42 5a 68 39 31 41 59 26 53 59 42 05 ae 5e 05\n.....\nSuggested solution:\n    private void initBlock() throws IOException {\n        char magic0 = bsGetUByte();\n        char magic1 = bsGetUByte();\n        char magic2 = bsGetUByte();\n        char magic3 = bsGetUByte();\n        char magic4 = bsGetUByte();\n        char magic5 = bsGetUByte();\n        if( magic0 == 0x17 && magic1 == 0x72 && magic2 == 0x45\n            && magic3 == 0x38 && magic4 == 0x50 && magic5 == 0x90 ) \n        {\n        \tif( complete() ) // end of file);\n\n{\n\n        \t\treturn;\n\n        \t}\n else\n        \t{\n\n        \t\tmagic0 = bsGetUByte();\n\n                magic1 = bsGetUByte();\n\n                magic2 = bsGetUByte();\n\n                magic3 = bsGetUByte();\n\n                magic4 = bsGetUByte();\n\n                magic5 = bsGetUByte();\n\n        \t}\n        } \n        if (magic0 != 0x31 || // &apos;1&apos;\n                   magic1 != 0x41 || // &apos;A&apos;\n                   magic2 != 0x59 || // &apos;Y&apos;\n                   magic3 != 0x26 || // &apos;&&apos;\n                   magic4 != 0x53 || // &apos;S&apos;\n                   magic5 != 0x59 // &apos;Y&apos;\n                   ) \n{\n\n            this.currentState = EOF;\n\n            throw new IOException(\"bad block header\");\n\n        }\n else {\n            this.storedBlockCRC = bsGetInt();\n            this.blockRandomised = bsR(1) == 1;\n            /**\n\nAllocate data here instead in constructor, so we do not allocate\nit if the input file is empty.\n             */\n            if (this.data == null) \n{\n\n                this.data = new Data(this.blockSize100k);\n\n            }\n\n            // currBlockNo++;\n            getAndMoveToFrontDecode();\n            this.crc.initialiseCRC();\n            this.currentState = START_BLOCK_STATE;\n        }\n    }\n    private boolean \n    complete() throws IOException \n    { \n    \tboolean result = false;\n        this.storedCombinedCRC = bsGetInt();\n        try\n        {\n            if (in.available() == 0 ) \n            {\n\n                throw new IOException( \"EOF\" );\n\n            }\n            checkMagicChar(&apos;B&apos;, \"first\");\n            checkMagicChar(&apos;Z&apos;, \"second\");\n            checkMagicChar(&apos;h&apos;, \"third\");\n            int blockSize = this.in.read();\n            if ((blockSize < &apos;1&apos;) || (blockSize > &apos;9&apos;)) \n{\n\n                throw new IOException(\"Stream is not BZip2 formatted: illegal \"\n\n                                      + \"blocksize \" + (char) blockSize);\n\n            }\n\n            this.blockSize100k = blockSize - &apos;0&apos;;\n            this.bsLive = 0;\n            this.bsBuff = 0;\n        } catch( IOException e )\n        {\n\n        \tthis.currentState = EOF;\n\n        \t\n\n        \tresult = true;\n\n        }\n\n        this.data = null;\n        if (this.storedCombinedCRC != this.computedCombinedCRC) \n{\n\n            throw new IOException(\"BZip2 CRC error\");\n\n        }\n        this.computedCombinedCRC = 0;    \n        return result;\n    }", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.5", "fixed_version": "COMPRESS_1.4", "fixed_files": ["org.apache.commons.compress.compressors.XZTestCase.java", "org.apache.commons.compress.compressors.GZipTestCase.java", "org.apache.commons.compress.compressors.BZip2TestCase.java", "org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 228, "bug_title": "ZipException on reading valid zip64 file", "bug_description": "ZipFile zip = new ZipFile(new File(\"ordertest-64.zip\")); throws ZipException \"central directory zip64 extended information extra field&apos;s length doesn&apos;t match central directory data.  Expected length 16 but is 28\".\nThe archive was created by using DotNetZip-WinFormsTool uzing zip64 flag (forces always to make zip64 archives).\nZip file is tested from the console: $zip -T ordertest-64.zip\nOutput:\ntest of ordertest-64.zip OK\nI can open the archive with FileRoller without problem on my machine, browse and extract it.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.5", "fixed_version": "COMPRESS-1.6", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipFileTest.java", "org.apache.commons.compress.archivers.zip.Zip64ExtendedInformationExtraField.java"], "label": 1, "es_results": []}, {"bug_id": 227, "bug_title": "duplicate entries may let ZipFile#getInputStream return null", "bug_description": "Found while investigating https://issues.apache.org/bugzilla/show_bug.cgi?id=54967\nIf an archive contains two entries for the same file readCentralDirectory in ZipFile will only add the first entry to the entries map and only the last to the nameMap map - this is because entries is a LinkedHashMap and nameMap is a \"plain\" HashMap.\nNormally this wouldn&apos;t matter since both ZipArchiveEntry instances are equal and thus it is irrelevant which of the two is used as a key when obtaining the InputStream.\nThings get different, though, if the entry has data inside the local file header as only the first entry is dealt with in resolveLocalFileHeaderData - after this the two instances are no longer equal and nameMap.get(NAME) will return an instance that can no longer be found.\nI intend to modify readCentralDirectory to only add the first entry to nameMap as well and document the behavior.  I&apos;ll also start a discussion thread on the dev list on whether we need to provide an additional API with multiple entries per name.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.5", "fixed_version": "COMPRESS-1.6", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipFileTest.java", "org.apache.commons.compress.archivers.zip.ZipFile.java"], "label": 1, "es_results": []}, {"bug_id": 236, "bug_title": "IllegalArgumentException reading CPIO generated by Redline RPM", "bug_description": "http://redline-rpm.org/ creates CPIO archives with a non-zero file mode on the trailer. This causes an IllegalArgumentException when reading the file. I&apos;ve attached a patch and test archive to fix this.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.5", "fixed_version": "COMPRESS-1.6", "fixed_files": ["org.apache.commons.compress.archivers.cpio.CpioArchiveInputStreamTest.java", "org.apache.commons.compress.archivers.cpio.CpioArchiveEntry.java", "org.apache.commons.compress.archivers.cpio.CpioUtil.java", "org.apache.commons.compress.archivers.cpio.CpioArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 237, "bug_title": "Long link support for TarArchiveOutputStream", "bug_description": "TarArchiveOutputStream doesn&apos;t handle properly long link names, they are truncated silently in most cases.\n\nIn GNU mode a @LongLink entry should be created similarly to long names but with the &apos;K&apos; type (GNU_LONGLINK) instead of &apos;L&apos; (GNU_LONGNAME)\nIn POSIX mode a pax header with the linkpath keyword should be added. This is already implemented for links containing a non ASCII character.\nIn the default ERROR mode no exception is thrown and the link is truncated.\n\nThe logic for adding a pax header on non ASCII characters should probably be reworked, as it seems possible to have pax headers mixed with GNU @LongLink entries. I&apos;m not sure this is desirable.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.5", "fixed_version": "COMPRESS-1.6", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 239, "bug_title": "ArchiveStreamFactory cannot create an ArchiveInputStream from any input stream that is blocking", "bug_description": "Encountered while streaming zip data over a network:\nIn the createArchiveInputStream method of ArchiveStreamFactory, when the provided input stream is read, and it blocks before 12 bytes are available for reading, due to the contract of the java.io.InputStream class, the archive signature will not be completely read, and an ArchiveException will be thrown (\"No Archiver found for the stream signature\").\nIn ZipArchiveInputStream, you have implemented a readFully method, which should solve this issue, but since you are checking the length of the signature read against the expected length, you are never getting to do that.  When you try to read the signature, you should be using readFully.\nFor reference, here is important part of the contract of java.io.InputStream.read():\nThis method blocks until [ANY] input data is available, end of file is detected, or an exception is thrown.\nIf len is zero, then no bytes are read and 0 is returned; otherwise, there is an attempt to read at least one byte. If no byte is available because the stream is at end of file, the value -1 is returned; otherwise, at least one byte is read and stored into b.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.5", "fixed_version": "COMPRESS-1.6", "fixed_files": ["org.apache.commons.compress.compressors.CompressorStreamFactory.java", "org.apache.commons.compress.archivers.ArchiveStreamFactory.java"], "label": 1, "es_results": []}, {"bug_id": 240, "bug_title": "ZipEncodingHelper.isUTF8(String) does not check all UTF-8 aliases", "bug_description": "ZipEncodingHelper.isUTF8(String) does not check all UTF-8 aliases.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.5", "fixed_version": "COMPRESS-1.6", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipEncodingHelper.java"], "label": 1, "es_results": []}, {"bug_id": 244, "bug_title": "7z reading of UINT64 data type is wrong for big values", "bug_description": "Brief description\nlarge values with a first byte indicating at least 4 additional bytes shift an integer by at least 32bits thus leading to an overflow and an incorrect value - the value needs to be casted to long before the bitshift!\n(see the attached patch)\nDetails from the 7z documentation\n\n\nUINT64 means real UINT64 encoded with the following scheme:\n\n  Size of encoding sequence depends from first byte:\n\n  First_Byte  Extra_Bytes        Value\n\n  (binary)   \n\n  0xxxxxxx               : ( xxxxxxx           )\n\n  10xxxxxx    BYTE y[1]  : (  xxxxxx << (8 * 1)) + y\n\n  110xxxxx    BYTE y[2]  : (   xxxxx << (8 * 2)) + y\n\n  ...\n\n  1111110x    BYTE y[6]  : (       x << (8 * 6)) + y\n\n  11111110    BYTE y[7]  :                         y\n\n  11111111    BYTE y[8]  :                         y\n\n\n", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.6", "fixed_version": "COMPRESS-1.7", "fixed_files": ["org.apache.commons.compress.archivers.sevenz.SevenZFile.java"], "label": 1, "es_results": []}, {"bug_id": 252, "bug_title": "Writing 7z empty entries produces incorrect or corrupt archive", "bug_description": "I couldn&apos;t find an exact rule that causes this incorrect behavior, but I tried to reduce it to some simple scenarios to reproduce it:\nInput: A folder with certain files -> tried to archive it.\nIf the folder contains more than 7 files the incorrect behavior appears.\nScenario 1: 7 empty files\nResult: The created archive contains a single folder entry with the name of the archive (no matter which was the name of the file)\nScenario 2: 7 files, some empty, some with content\nResult: The created archive contains a folder entry with the name of the archive and a number of file entries also with the name of the archive. The number of the entries is equal to the number of non empty files.\nScenario 3: 8 empty files\nResult: 7zip Manager cannot open archive and stops working.\nScenario 4.1: 8 files: some empty, some with content, last file (alphabetically) with content\nResult: same behavior as described for Scenario 2.\nScenario 4.2: 8 files, some empty, some with content, last file empy\nResult: archive is corrupt, the following message is received: \"Cannot open file &apos;archivename.7z&apos; as archive\" (7Zip Manager does not crash).", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.6", "fixed_version": "COMPRESS-1.7", "fixed_files": ["org.apache.commons.compress.archivers.sevenz.SevenZOutputFileTest.java", "org.apache.commons.compress.archivers.sevenz.SevenZOutputFile.java"], "label": 1, "es_results": []}, {"bug_id": 259, "bug_title": "CompressorStreamFactory.createCompressorInputStream with explicit compression does not honour decompressConcatenated", "bug_description": "When using CompressorStreamFactory.createCompressorInputStream, the decompressConcatenated property is only used when auto-detecting the compressor format for an input stream. The method with explicit compressor specification ignores the decompressConcatenated property, though.\nThe fix is easy: pass the decompressConcatenated property as argument to all constructors.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.7", "fixed_version": "COMPRESS-1.8", "fixed_files": ["org.apache.commons.compress.compressors.CompressorStreamFactory.java"], "label": 1, "es_results": []}, {"bug_id": 264, "bug_title": "ZIP reads correctly with commons-compress 1.6, gives NUL bytes in 1.7", "bug_description": "When running the code below, commons-compress 1.6 writes:\n Content of test.txt:\n data\nBy comparison, commons-compress 1.7 writes\n Content of test.txt:\n@@@@^@\npackage com.example.jrn;\nimport org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\nimport org.apache.commons.compress.archivers.zip.ZipArchiveInputStream;\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.lang.System;\n/**\n\nHello world!\n *\n */\npublic class App {\n  public static void main(String[] args) {\n    byte[] zip = \n{\n\n       (byte)0x50, (byte)0x4b, (byte)0x03, (byte)0x04, (byte)0x0a, (byte)0x00,\n\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x03, (byte)0x7b,\n\n       (byte)0xd1, (byte)0x42, (byte)0x82, (byte)0xc5, (byte)0xc1, (byte)0xe6,\n\n       (byte)0x05, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x05, (byte)0x00,\n\n       (byte)0x00, (byte)0x00, (byte)0x08, (byte)0x00, (byte)0x1c, (byte)0x00,\n\n       (byte)0x74, (byte)0x65, (byte)0x73, (byte)0x74, (byte)0x2e, (byte)0x74,\n\n       (byte)0x78, (byte)0x74, (byte)0x55, (byte)0x54, (byte)0x09, (byte)0x00,\n\n       (byte)0x03, (byte)0x56, (byte)0x62, (byte)0xbf, (byte)0x51, (byte)0x2a,\n\n       (byte)0x63, (byte)0xbf, (byte)0x51, (byte)0x75, (byte)0x78, (byte)0x0b,\n\n       (byte)0x00, (byte)0x01, (byte)0x04, (byte)0x01, (byte)0xff, (byte)0x01,\n\n       (byte)0x00, (byte)0x04, (byte)0x88, (byte)0x13, (byte)0x00, (byte)0x00,\n\n       (byte)0x64, (byte)0x61, (byte)0x74, (byte)0x61, (byte)0x0a, (byte)0x50,\n\n       (byte)0x4b, (byte)0x01, (byte)0x02, (byte)0x1e, (byte)0x03, (byte)0x0a,\n\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x03,\n\n       (byte)0x7b, (byte)0xd1, (byte)0x42, (byte)0x82, (byte)0xc5, (byte)0xc1,\n\n       (byte)0xe6, (byte)0x05, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x05,\n\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x08, (byte)0x00, (byte)0x18,\n\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x01,\n\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0xa0, (byte)0x81, (byte)0x00,\n\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x74, (byte)0x65, (byte)0x73,\n\n       (byte)0x74, (byte)0x2e, (byte)0x74, (byte)0x78, (byte)0x74, (byte)0x55,\n\n       (byte)0x54, (byte)0x05, (byte)0x00, (byte)0x03, (byte)0x56, (byte)0x62,\n\n       (byte)0xbf, (byte)0x51, (byte)0x75, (byte)0x78, (byte)0x0b, (byte)0x00,\n\n       (byte)0x01, (byte)0x04, (byte)0x01, (byte)0xff, (byte)0x01, (byte)0x00,\n\n       (byte)0x04, (byte)0x88, (byte)0x13, (byte)0x00, (byte)0x00, (byte)0x50,\n\n       (byte)0x4b, (byte)0x05, (byte)0x06, (byte)0x00, (byte)0x00, (byte)0x00,\n\n       (byte)0x00, (byte)0x01, (byte)0x00, (byte)0x01, (byte)0x00, (byte)0x4e,\n\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x47, (byte)0x00, (byte)0x00,\n\n       (byte)0x00, (byte)0x00, (byte)00\n\n    }\n;\n\n    ByteArrayInputStream bin = new ByteArrayInputStream(zip);\n    try {\n      ZipArchiveInputStream in = new ZipArchiveInputStream(bin);\n      try {\n        while (true) {\n          ZipArchiveEntry entry = in.getNextZipEntry();\n          if (entry == null) \n{\n\n            break;\n\n          }\n          byte[] buf = new byte[(int) entry.getSize()];\n          in.read(buf);\n          System.out.println(\"Content of \" + entry.getName() + \":\");\n          System.out.write(buf);\n        }\n      } finally \n{\n\n        in.close();\n\n      }\n    } catch (IOException e) \n{\n\n      System.err.println(\"IOException: \" + e);\n\n    }\n  }\n}", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.7", "fixed_version": "COMPRESS-1.8", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipFileTest.java", "org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java", "org.apache.commons.compress.archivers.zip.ZipArchiveInputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 265, "bug_title": "PAX headers with \"strange\" names can not be written", "bug_description": "as noted by Patrik Burkhalter in COMPRESS-203 if a non-ASCII character is \"7-bit-cleaned\" to &apos;\\&apos; this may lead to a file name for the PAX header that looks like a directory name to TarArchiveEntry - which leads to a bug very similar to COMPRESS-203.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.7", "fixed_version": "COMPRESS-1.8", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveOutputStreamTest.java", "org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 267, "bug_title": "ArchiveStreamFactory throws \"No Archiver found for the stream signature\" on 7z files", "bug_description": "Apache Tika makes use of ArchiveStreamFactory to handle a wide range of archive formats (Zip, AR, CPIO, Tar etc)\nWe&apos;ve just upgraded to Commons Compress 1.7, and tried to make use of the new 7z support to add in 7z handling too. However, when you try to call:\n            ArchiveStreamFactory factory = new ArchiveStreamFactory();\n            ArchiveInputStream ais = factory.createArchiveInputStream(stream);\nWith a 7z file it fails with:\n    new ArchiveException(\"No Archiver found for the stream signature\");", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.7", "fixed_version": "COMPRESS-1.8", "fixed_files": ["org.apache.commons.compress.archivers.ArchiveStreamFactoryTest.java", "org.apache.commons.compress.archivers.ArchiveStreamFactory.java", "org.apache.commons.compress.archivers.sevenz.SevenZFile.java", "org.apache.commons.compress.archivers.sevenz.SevenZFileTest.java"], "label": 1, "es_results": []}, {"bug_id": 268, "bug_title": "Buffer not read first time for entries with STORED compression method", "bug_description": "Trying to extract a ZIP archive that contains entries with STORED compression method, the ZipArchiveInputStream.readStored(byte[], int, int) method is called. At this point, because the buf array has not had a chance to be populated with values from the underlying input stream and because there&apos;s no condition to detect this, the resulting content is prefixed with the buffer&apos;s length (512) of 0 bytes.\nI&apos;ve found that chancing:\nif (buf.position() >= buf.limit()) {\nwith\nif (buf.position() >= buf.limit() || current.bytesReadFromStream == 0) {\nsolves the issue.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.7", "fixed_version": "COMPRESS-1.8", "fixed_files": ["org.apache.commons.compress.archivers.zip.ZipFileTest.java", "org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java", "org.apache.commons.compress.archivers.zip.ZipArchiveInputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 272, "bug_title": "CompressorStreamFactory fails to autodetect files using Unix compress (.Z files)", "bug_description": "I use the factory classes quite extensively to guess the correct implementation of a file that needs to be unpacked.  The current doc does list that for lzma and 7zip files, the auto detect will not work.  I have worked around this by looking at the file extension, and hope that its correct.\nFor .Z files, I can only uncompress the file if I explicitly tell the factory that its using .Z compression, the auto detect never works.  I&apos;m using 1.7, but I do not think its fixed in 1.8 either (after looking at the bug fix list).\nEither its a bug in the doc, or in the auto detect of the compressor factory.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.7", "fixed_version": "COMPRESS-1.8.1", "fixed_files": ["org.apache.commons.compress.compressors.ZTestCase.java", "org.apache.commons.compress.compressors.CompressorStreamFactory.java", "org.apache.commons.compress.compressors.z.ZCompressorInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 279, "bug_title": "TarArchiveInputStream silently finished when unexpected EOF occured", "bug_description": "I just found the following test case didn&apos;t raise an IOException as it used to be for a tar trimmed on purpose \n@Test\n  public void testCorruptedBzip2() throws IOException {\n    String archivePath = PathUtil.join(testdataDir, \"test.tar.bz2\");\n    TarArchiveInputStream input = null;\n    input = new TarArchiveInputStream(new BZip2CompressorInputStream(\n        GoogleFile.SYSTEM.newInputStream(archivePath), true));\n    ArchiveEntry nextMatchedEntry = input.getNextEntry();\n    while (nextMatchedEntry != null) \n{\n\n      logger.infofmt(\"Extracting %s\", nextMatchedEntry.getName());\n\n      String outputPath = PathUtil.join(\"/tmp/\", nextMatchedEntry.getName());\n\n      OutputStream out = new FileOutputStream(outputPath);\n\n      ByteStreams.copy(input, out);\n\n      out.close();\n\n      nextMatchedEntry = input.getNextEntry();\n\n    }\n  }", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.7", "fixed_version": "COMPRESS-1.8.1", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveInputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveInputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 270, "bug_title": "TarArchiveInputStream fails to read PAX header from InputStream", "bug_description": "We have a scenario with a \"slow\" InputStream and are facing IOExceptions with TarArchiveEntry#getNextTarEntry().\nIf the InputStream does not deliver fast enough, TarArchiveEntry#parsePaxHeaders(InputStream i) fails at this location:\nTarArchiveInputStream.java\n\n\n// Get rest of entry\n\nbyte[] rest = new byte[len - read];\n\nint got = i.read(rest);\n\nif (got != len - read){\n\n\tthrow new IOException(\"Failed to read \"\n\n\t\t+ \"Paxheader. Expected \"\n\n\t\t+ (len - read)\n\n\t\t+ \" bytes, read \"\n\n\t\t+ got);\n\n}\n\n\n\nWe would suggest to change the code to something like this:\nTarArchiveInputStream.java\n\n\n// Get rest of entry\n\nbyte[] rest = new byte[len - read];\n\nint got = 0;\n\nwhile((ch = i.read()) != -1) {\n\n\trest[got] = (byte) ch;\n\n\tgot++;\n\n\tif(got == len - read) {\n\n\t\tbreak;\n\n\t}\n\n}\n\nif (got != len - read){\n\n\tthrow new IOException(\"Failed to read \"\n\n\t\t+ \"Paxheader. Expected \"\n\n\t\t+ (len - read)\n\n\t\t+ \" bytes, read \"\n\n\t\t+ got);\n\n}\n\n\n\nThis would make sure, that it gets all bytes of the PAX header value.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.8", "fixed_version": "COMPRESS-1.8.1", "fixed_files": ["org.apache.commons.compress.archivers.dump.TapeInputStream.java", "org.apache.commons.compress.archivers.ar.ArArchiveInputStream.java", "org.apache.commons.compress.compressors.snappy.SnappyCompressorInputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 277, "bug_title": "IOUtils.skip does not work as advertised", "bug_description": "I am trying to feed a TarInputStream from a CipherInputStream.\nIt does not work, because IOUtils.skip() does not adhere to the contract it claims in javadoc:\n\"     * <p>This method will only skip less than the requested number of\n\nbytes if the end of the input stream has been reached.</p>\"\n\nHowever it does:\n            long skipped = input.skip(numToSkip);\n            if (skipped == 0) \n{\n\n                break;\n\n            }\n\nAnd the input stream javadoc says:\n\"     * This may result from any of a number of conditions; reaching end of file\n\nbefore <code>n</code> bytes have been skipped is only one possibility.\"\n\nIn the case of CipherInputStream, it stops at the end of each byte buffer.\nIf you check the IOUtils from colleagues at commons-io, they have considered this case in IOUtils.skip() where they use a read to skip through the stream.\nAn optimized version could combine trying to skip, then read then trying to skip again.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.8", "fixed_version": "COMPRESS-1.8.1", "fixed_files": ["org.apache.commons.compress.utils.IOUtils.java"], "label": 1, "es_results": []}, {"bug_id": 276, "bug_title": "NullPointerException in ZipArchiveOutputStream with invalid entries", "bug_description": "Writing raw data seems to cause problems in multiple ways, because an internal field is not set. Is this a wrong API usage?\n    java.io.ByteArrayOutputStream var0 = new java.io.ByteArrayOutputStream();\n    org.apache.commons.compress.archivers.jar.JarArchiveOutputStream var1 = new org.apache.commons.compress.archivers.jar.JarArchiveOutputStream((java.io.OutputStream)var0);\n    var1.write(25843);\nOther tests (see attachment) are very similar and because the same problem. They can probably be ignored because the first test is the shortest one.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.8", "fixed_version": "COMPRESS-1.8.1", "fixed_files": ["org.apache.commons.compress.archivers.sevenz.SevenZFile.java", "org.apache.commons.compress.archivers.sevenz.SevenZOutputFile.java", "org.apache.commons.compress.archivers.dump.DumpArchiveInputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.java", "org.apache.commons.compress.archivers.arj.ArjArchiveInputStream.java", "org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 274, "bug_title": "NullPointerException in ChangeSet.addDeletion when using bogus data", "bug_description": "When adding some bogus data and then trying to call deleteDir with a bogus name, a NullPointerException results. See attached test.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.8", "fixed_version": "COMPRESS-1.8.1", "fixed_files": ["org.apache.commons.compress.changes.ChangeSet.java"], "label": 1, "es_results": []}, {"bug_id": 273, "bug_title": "NullPointerException when creation fields/entries from scratch", "bug_description": "The API has public default constructors for many data types. However, when these 0-argument constructors are used, certain internal references are null, resulting in a NullPointerException soon after.\nThis also applies to some 1-argument constructors where two references should be set before get... is used later.\nEither (1) these constructors should be non-public, (2) there should be documentation that certain fields need to be set later for an instance to be usable. In the latter case, there must be public set methods for the missing data.\nThe attachment contains a number of similar test cases that show the same issue in a couple of classes.\nAn example:\n    org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField var0 = new org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField();\n    org.apache.commons.compress.archivers.zip.ZipShort var1 = var0.getLocalFileDataLength();", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.8", "fixed_version": "COMPRESS-1.8.1", "fixed_files": ["org.apache.commons.compress.archivers.cpio.CpioArchiveEntry.java", "org.apache.commons.compress.archivers.zip.UnrecognizedExtraField.java", "org.apache.commons.compress.archivers.zip.ExtraFieldUtils.java", "org.apache.commons.compress.archivers.zip.AbstractUnicodeExtraField.java"], "label": 1, "es_results": []}, {"bug_id": 289, "bug_title": "TarArchiveOutputStream includes timestamp in long link headers", "bug_description": "When I create a Tar Archive Entry with a long name, the Long Link Entry contains a default modification date of the current Date.\nThis results in two archives with the same contents having different MD5 checksums.\n", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.8.1", "fixed_version": "COMPRESS-1.9", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveOutputStreamTest.java", "org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 303, "bug_title": "Restore immutability/thread-safety to CompressorStreamFactory", "bug_description": "CompressorStreamFactory was immutable prior to 1.5; r1453945 broke immutability. It&apos;s also no longer thread-safe because the field decompressConcatenated is not safely published.\nIt would be possible to make it thread-safe by making the field volatile.\nIt could be made conditionally immutable by using the same technique as suggested in COMPRESS-302, in preparation for removing the method setDecompressConcatenated in a future API-break release.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.9", "fixed_version": "COMPRESS-1.10", "fixed_files": ["org.apache.commons.compress.compressors.CompressorStreamFactory.java"], "label": 1, "es_results": []}, {"bug_id": 302, "bug_title": "Retore immutability/thread-safety to ArchiveStreamFactory", "bug_description": "COMPRESS-180 added support for encoding.\nUnfortunately this was done in a way that broke immutability.\nAlso the factory is no longer thread-safe as the encoding field is not synch/volatile.\nConsider whether to restore immutability, e.g. by adding a constuctor which takes the encoding setting. The setEntryEncoding method could be deprecated intitially and eventually dropped.\nOne way to support immutability now would be to add a second final encoding field which is set by a new ctor. See patch to follow.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.9", "fixed_version": "COMPRESS-1.10", "fixed_files": ["org.apache.commons.compress.archivers.ArchiveStreamFactory.java"], "label": 1, "es_results": []}, {"bug_id": 306, "bug_title": "ArchiveStreamFactory fails to pass on the encoding when creating some streams", "bug_description": "ArchiveStreamFactory fails to pass on the encoding when creating the following streams (in some or all cases):\n\nArjArchiveInputStream\nCpioArchiveInputStream\nDumpArchiveInputStream\nJarArchiveInputStream\nJarArchiveOutputStream\n\n", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.9", "fixed_version": "COMPRESS-1.10", "fixed_files": ["org.apache.commons.compress.archivers.dump.DumpArchiveInputStream.java", "org.apache.commons.compress.archivers.ArchiveStreamFactory.java", "org.apache.commons.compress.archivers.jar.JarArchiveInputStream.java", "org.apache.commons.compress.archivers.cpio.CpioArchiveOutputStream.java", "org.apache.commons.compress.archivers.ArchiveStreamFactoryTest.java", "org.apache.commons.compress.archivers.jar.JarArchiveOutputStream.java", "org.apache.commons.compress.archivers.cpio.CpioArchiveInputStream.java", "org.apache.commons.compress.archivers.zip.ZipArchiveInputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.java", "org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 312, "bug_title": "public TarArchiveEntry(File file, String fileName) does not normalize the file name.", "bug_description": "Tar entry names are normalized most of the times (like replacing back slashes with forward slashes). But TarArchiveEntry(File file, String fileName) constructor does not do so.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.9", "fixed_version": "COMPRESS-1.10", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveEntry.java"], "label": 1, "es_results": []}, {"bug_id": 315, "bug_title": "tar can not write uid or gid >= 0x80000000", "bug_description": "This is related to COMPRESS-314 - TarArchiveEntry doesn&apos;t allow gid/uid bigger than a signed 32bit int - the POSIX spec does.", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.9", "fixed_version": "COMPRESS-1.10", "fixed_files": ["org.apache.commons.compress.archivers.tar.TarArchiveEntry.java", "org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 357, "bug_title": "BZip2CompressorOutputStream can affect output stream incorrectly", "bug_description": "BZip2CompressorOutputStream has an unsynchronized finished() method, and an unsynchronized finalize method. Finish checks to see if the output stream is null, and if it is not it calls various methods, some of which write to the output stream. \nNow, consider something like this sequence.\nBZip2OutputStream s = ...\n...\ns.close();\ns = null;\nAfter the s = null, the stream is garbage. At some point the garbage collector call finalize(), which calls finish(). But, since the GC may be on a different thread, there is no guarantee that the assignment this.out = null in finish() has actually been made visible to the GC thread, which results in bad data in the output stream.\nThis is not a theoretical problem; In a part of a large project I&apos;m working on, this happens about 2% of the time. \nThe fixes are simple\n1) synchronize finish() or\n2) don&apos;t call finish from finalize().\nA workaround is to derive a class and override the finalize() method. \n", "project": "Commons", "sub_project": "COMPRESS", "version": "COMPRESS-1.9", "fixed_version": "rel/1.12", "fixed_files": ["org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 220, "bug_title": "getDateArray(String key, Date[] defaultValue, String format) ignores format argument", "bug_description": "getDateArray(String key, Date[] defaultValue, String format) in DataConfiguration takes a format argument, but it is not used in the call to getDateList(key). This call should probably be getDateList(key, format).", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_2", "fixed_version": "CONFIGURATION_1_3", "fixed_files": ["org.apache.commons.configuration.TestDataConfiguration.java", "org.apache.commons.configuration.DataConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 228, "bug_title": "XMLConfiguration.subset() with FileChangedReloadingStrategy does *not* 'see' file changes", "bug_description": "XMLConfiguration.subset() with FileChangedReloadingStrategy does not &apos;see&apos; file changes\nConfiguration.xml:\n<conf>\n    <parent-item>\n        <sub-item>old_value</sub-item>\n    </parent-item>\n</conf>\n1. XMLConfiguration is loaded:\n            config = new XMLConfiguration(\"c:conf.xml\");\n            config.setReloadingStrategy(new FileChangedReloadingStrategy());\n2. Now, <sub-item> node value changes to \"new_value\",\nand the code:\n            Configuration parentItemConfig = config.subset(\"parent-item\"); [AA]\n            String ss2 = parentItemConfig.getString(\"sub-item\");\nreturns \"old_value\" !!!\nwhile:  config.getString(\"parent-item.sub-item\"); returns new refreshed value.\nIt is a bug, we have FileChangedReloadingStrategy, we create new subset in [AA] but receive old values.\n[for now, workaround for config users is to call config.reload() when strategy.hasChanged()]\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_2", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.AbstractHierarchicalFileConfiguration.java", "org.apache.commons.configuration.TestXMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 227, "bug_title": "ConfigurationDynaBean does not work well with all types of configurations", "bug_description": "The current implementation of ConfigurationDynaBean makes some assumptions about the underlying Configuration object that are not met by all configuration implementations. Especially setting values for indexed properties by calling getList() and manipulating mapped properties through a subset configuration do not work for configurations derived from HierarchicalConfiguration.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_2", "fixed_version": "CONFIGURATION_1_3", "fixed_files": ["org.apache.commons.configuration.beanutils.ConfigurationDynaBean.java"], "label": 1, "es_results": []}, {"bug_id": 230, "bug_title": "XPathExpressionEngine nodeKey method create a wrong key for attribute node", "bug_description": "In org.apache.commons.configuration.tree.xpath.XPathExpressionEngine line 178\n            if (node.isAttribute())\n            {\n                buf.append(ATTR_DELIMITER);\n            }\n\nshould be changed to \n            if (node.isAttribute()) \n{\n\tbuf.append(NODE_PATH_DELIMITERS);\n            }\n\nUsing ATTR_DELIMITER will create key like element@attribute rather than element/@attribute and make config reload fail.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.tree.xpath.TestXPathExpressionEngine.java", "org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.tree.xpath.XPathExpressionEngine.java"], "label": 1, "es_results": []}, {"bug_id": 233, "bug_title": "Incomplete interpolation in CompositeConfiguration.getList()", "bug_description": "Interpolsation is mostly done by AbstractConfiguration. All classes that override get methods of AbstractConfiguration need to be careful not to break interpolsation.\nThe class CompositeConfiguration overrides getList() to easily chain the results of getList for its subconfigurations. The calls to getList() in the subconfigurations does interpolsation but only inside the subconfiguration. The combined lists never get interpolated in the whole CompositeConfiguration. So whenever a reference in the value refers to a property in another subconfiguration, interpolation breaks.\nSince all the scalar get methods correctly interpolate between different sub configurations, I assume, that this special behavior in getList() is not intended.\nA simple test case would be using the ConfigurationFactory with a config.xml, that uses two property files x.properties:\nx.1=a\nx.2=b\nand y.properties:\ny=$\n{x.1}\n, $\n{x.2}\n\nand then calling getList for the key \"y\".\nThe attached patch fixes the problem locally in CompositeConfiguration. It&apos;s not very elegant, but tries to keep the fix local.\nThe patch should work for 1.2, 1.3 and trunk, since getList() in CompositeConfiguration ist the same in these versions.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.TestCompositeConfiguration.java", "org.apache.commons.configuration.CompositeConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 235, "bug_title": "variable interpolation problem", "bug_description": "interpolation of variables in Subsets doesn&apos;t work, if they are not accessed via getString(). The following\nexample illustrates the problem:\nBaseConfiguration oBase = new BaseConfiguration();\noBase.addProperty( \"a.b\", new Integer(2) );\noBase.addProperty( \"a.c\", \"$\n{a.b}\n\" );\n// as expected: 2\nint iValueBaseConfig = oBase.getInt( \"a.c\" );\nString strValueBaseConfig = oBase.getString(\"a.c\");\n// as expected: 2\nString strValueSubset = oBase.subset(\"a\").getString(\"c\");\n// ConversionException, &apos;c&apos; doesn&apos;t map to an Integer object\nint iValueSubset = oBase.subset(\"a\").getInt(\"c\");\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.TestBaseConfiguration.java", "org.apache.commons.configuration.SubsetConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 229, "bug_title": "config : load for a configuration node is called before parsing is complete : attributes ignored", "bug_description": "Hi there\ni have noticed the following issue which appears to be a bug. (Tested on 1.3 however the version doesn&apos;t appear above)\ncreateObject from FileConfigurationFactory(DigesterConfigurationFactory) set fileName and fires load of properties : \npublic Object createObject(Attributes attributes) throws Exception\n        {\n            FileConfiguration conf = createConfiguration(attributes);\n            conf.setBasePath(getBasePath());\n            conf.setFileName(attributes.getValue(ATTR_FILENAME));\n            try\n            {\n                log.info(\"Trying to load configuration \" + conf.getFileName());\n                conf.load();\n            }\n\nHowever digester invokes createObject when the object is instancied and before setting any attributes. \nAll other attributes beside fileName and basePath are not read in time and therefore are ignored during load.\nI guess load should be called when node initialization is complete, possibly using a proper digester rule. \nRegards,\nandr\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.TestConfigurationFactory.java", "org.apache.commons.configuration.ConfigurationFactory.java"], "label": 1, "es_results": []}, {"bug_id": 242, "bug_title": "The configuration returned by HierarchicalConfiguration.subset() does not globally interpolate", "bug_description": "Interpolation is only performed in the subset configuration itself. If the referenced property is not in the subset (but somewhere else in the parent configuration), it cannot be resolved. Interpolation should take all properties of the parent configuration into account.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.TestSubnodeConfiguration.java", "org.apache.commons.configuration.TestHierarchicalConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 241, "bug_title": "clearProperty() does not generate events", "bug_description": "I am loading configuration information from multiple sources and have registered a listener with the resulting configuration object. Unfortunately the listener does not receive \"clear property\" events. I&apos;ve confirmed that it can properly receive other events (like \"set property\"), and that calls to \"clearProperty()\" do actually clear the property, so I believe this may be a bug in commons-configuration. I&apos;ve tried setting \"details\" to true, which had no effect. Below is a watered down version of what I am doing (note, my configuration file simply pulls in a property file containing this property: name.first=Mike):\nConfigurationFactory configurationFactory = new ConfigurationFactory();\nURL configFileURL = ... get the config file ...\nconfigurationFactory.setConfigurationURL(configFileURL);\nConfiguration configuration = ConfigurationFactory.getConfiguration();\nconfiguration.addConfigurationListener(new ConfigurationListener() {\n    public void configurationChanged(ConfigurationEvent e) \n{\n        System.out.println(e.getPropertyName() + \": \" + e.getPropertyValue());\n    }\n});\nSystem.out.println(configuration.getProperty(\"name.first\")); // prints \"Mike\"\nconfiguration.claerProperty(\"name.first\")); // no output whatsoever\nSystem.out.println(configuration.getProperty(\"name.first\")); // prints \"null\"", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.TestCompositeConfiguration.java", "org.apache.commons.configuration.CompositeConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 232, "bug_title": "ConfigurationUtils spews directly on std.err when exceptions", "bug_description": "ConfigurationUtils:\npublic static URL locate(String base, String name) {\n  3 x :\n  catch ([Malformed|IO]URLException e) \n{\n    e.printStackTrace();\n  }\n}\n.. say no more.\nBut while we&apos;re at it, the log-message (which, for some reason, actually goes to a logger, not uglily to the system) states \"Configuration loaded from ..\", which isn&apos;t true: The thing isn&apos;t loaded yet; it will be loaded.  \"Loading configuration from ..\" would be better, or maybe \"Will be loading configuration from ..\". In addition, these log-lines could state what was the \"base\", and in particular the case where it loads from \"base path\", the whole line reads wrong.\nFor example: \"Loading configuration [\"+name+\"] from base path [\"+base+\"]..\" and similar for the other lines.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.AbstractFileConfiguration.java", "org.apache.commons.configuration.ConfigurationUtils.java", "org.apache.commons.configuration.plist.XMLPropertyListConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 252, "bug_title": "Detection of absolute fileNames when configuration sources are defined in a JAR file", "bug_description": "When using several configuration sources, the sources are defined in a an xml attribute like \nfileName=\"usergui.properties\"\nThe method org.apache.commons.configuration.ConfigurationUtils.getFile(String basePath, String fileName) converts the basePath and fileName into a File.\nThe JavaDoc says: \"The parameter strings can be relative files, absolute files and URLs as well.\"\nThe file containing the definition of the configuration sources will become the basePath of this method.\nAfter a log of debugging I found, that if the basePath start with \"jar:\" the method assumes that the fileName is relative, evene if it&apos;s absolute. This happens, because there&apos;s a ProtocolHandler for Jar files and \"jar:\" is a valid protocol. So the statement \"new URL(new URL(basePath), fileName)\" doesn&apos;t throw a MalformedUrlException.\nSince the URL is valid, it&apos;s never checked, if the fileName may be absolute. \nAttention: this is only the case on Unix/Linux, since this is a valid URL\njar:file:/C:/myjar.jar!/my-config.xml/someprops.properties\nwhile under Windows, a MalformedUrlException will be thrown, when the fileName is absolute: \njar:file:/C:/myjar.jar!/my-config.xml/c:/someprops.properties\nI attached a patch that checks, whether the URL protocol is \"jar\" and the fileName is absolute. If so, the absolute file will be used.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.ConfigurationUtils.java", "org.apache.commons.configuration.TestConfigurationUtils.java"], "label": 1, "es_results": []}, {"bug_id": 254, "bug_title": "Wrong creation of XMLFileConfigurationDelegate in XMLConfiguration.clone()", "bug_description": "The clone() operation on a XMLConfiguration does not work correctly due to the following problem:\nprivate class XMLFileConfigurationDelegate extends FileConfigurationDelegate\n{\n     public void load(InputStream in) throws ConfigurationException\n     {\n            XMLConfiguration.this.load(in);\n      }\n}\nObviously the delegate references the XMLConfiguration instance it is created in. Thus when calling\npublic Object clone()\n{\n      XMLConfiguration copy = (XMLConfiguration) super.clone();\n       // clear document related properties\n       copy.document = null;\n       copy.setDelegate(createDelegate());\n       // clear all references in the nodes, too\n       copy.getRoot().visit(new NodeVisitor()\n       {\n            public void visitBeforeChildren(Node node, ConfigurationKey key)\n            {\n                node.setReference(null);\n             }\n        }, null);\n        return copy;\n}\nthe delegate still references the original XMLConfiguration, thus will save the content of the original one\nto file instead of the clone as expected! Changing the code like this\n        copy.setDelegate(copy.createDelegate());\nsolves the problem! Now the cloned XMLConfiguration with all its  applied changes can be saved!", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 255, "bug_title": "DatabaseConfiguration does not support list delimiters in property values", "bug_description": "The getList() and getStringArray() methods always return a one element list/array, when used with DatabaseConfiguration, even if the value of the queried property contains the list delimiter character.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.DatabaseConfiguration.java", "org.apache.commons.configuration.TestDatabaseConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 256, "bug_title": "MapConfiguration does not decode escaped Delimiters", "bug_description": "Hello, I don&apos;t know how to reopen resolved bug. See: https://issues.apache.org/jira/browse/CONFIGURATION-30\nThere is BAD implementation in MapConfiguration.getProperty(String key) method:\nAs \"return value\" should be list.get(0) instead of \"value\".\nPropertyConverter.split removes escape chars....\nSuggested correction:\n    public Object getProperty(String key)\n    {\n        Object value = map.get(key);\n        if ((value instanceof String) && (!isDelimiterParsingDisabled()))\n        {\n            List list = PropertyConverter.split((String) value, getListDelimiter());\n            //MP: return list.size() > 1 ? list : value;\n            return list.size() > 1 ? list : list.get(0); //MP: split removes escape chars\n        }\n        else\n        {\n            return value;\n        }\n    }\nTest code:\nMap m = new HashMap();\nm.put(\"foo\", \"bar,baz\");\nm.put(\"bar\", \"bar, baz\");\nMapConfiguration mc = new MapConfiguration(m);\nmc.setDelimiterParsingDisabled(false);\nmc.setListDelimiter(&apos;,&apos;);\nConfiguration c = mc;\nString bad = c.getString(\"foo\"); //<-- returns \"bar, baz\" expected \"bar, baz\"\nString ok = c.getString(\"bar\"); // <-- returns \"bar\"\nSystem.err.println(\"Bad: \" + bad);\nSystem.err.println(\"OK: \" + ok);\nCurrent result is:\nBad: bar\\,baz\nOK: bar", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_4", "fixed_files": ["org.apache.commons.configuration.web.ServletConfiguration.java", "org.apache.commons.configuration.web.TestServletFilterConfiguration.java", "org.apache.commons.configuration.web.AppletConfiguration.java", "org.apache.commons.configuration.TestAbstractConfiguration.java", "org.apache.commons.configuration.web.ServletContextConfiguration.java", "org.apache.commons.configuration.MapConfiguration.java", "org.apache.commons.configuration.web.ServletFilterConfiguration.java", "org.apache.commons.configuration.TestMapConfiguration.java", "org.apache.commons.configuration.web.BaseWebConfiguration.java", "org.apache.commons.configuration.web.TestServletContextConfiguration.java", "org.apache.commons.configuration.web.TestAppletConfiguration.java", "org.apache.commons.configuration.web.TestServletRequestConfiguration.java", "org.apache.commons.configuration.web.TestServletConfiguration.java", "org.apache.commons.configuration.web.ServletRequestConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 253, "bug_title": "FileConfiguration getFile(), getFileName(), getBasePath() are not always consistent", "bug_description": "Taken from the maillinglist:\n> I have a properties configuration which is loaded without a problem.\n> Later in my application I will access the file which is aligned to \n> this configuration.\n> \n>  \n> \n> final IFileConfiguration _productConf = \n> _conf.getProductConfigurations().get(_productId);\n> \n> log.debug(\"product conf file \" + _productConf.getFile());\n> \n> log.debug(\"product conf filename \" + _productConf.getFileName());\n> \n> log.debug(\"product conf base path \" + _productConf.getBasePath());\n> \n>  \n> \n> The methods _productConf.getFile() and _productConf.getFileName() \n> returning null but the getBasePath() returns a path which is correct \n> (like file:/C:/Projects/workspace/myProject/project.properties). Seems \n> for me like a bug because the PropertiesConfiguration is loaded \n> correct and works.\n> \n>  \n> \n> By side: I have also set a file reloading strategy for this \n> configuration.\n> \n>  \n> \n> Any ideas what&apos;s happen in this case or where I can find the problem? \n> It would be nicer to get the File() instead the BasePath which has to \n> be converted into a URL before I can access the whole properties file.\n> \n>  \n> \n> Thanks in advance,\n> \n>  \n> \n> - Thomas Wabner\n> \n> \nThomas,\nyou are right, the conversions between a base path, a file name, and a File are not always consistent. How did you load the configuration (this determines, which internal fields are set)?\nI would recommend to work with URLs, i.e. the method getURL(). A file-based configuration&apos;s URL is always defined.\nIf you like, you can open a Jira ticket for this problem.\nThanks.\nOliver\nThe file is loaded in this way:\n_productConf = new ProductConfiguration();\n_productConf.load(FileTools.getPathForList(_propductPathList).getPath());\nmeans the load method gets an String and not an File.\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_3", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.TestFileConfiguration.java", "org.apache.commons.configuration.AbstractFileConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 263, "bug_title": "XMLConfiguration drops attributes if a property value is a list", "bug_description": "When the following test is run with the following xml the second assertEquals statement fails:\nXML:\n<?xml version=\"1.0\" encoding=\"ISO-8859-1\" ?>\n<test>\n<a name=\"X\">ABC</a>\n<a name=\"Y\">1,2,3</a>\n</test\nTEST:\npublic void testXMLConfig() throws Exception {\n  File file = new File(\"/xml/xmlConfigTest.xml\");\n  XMLConfiguration xmlConfig = new XMLConfiguration(file);\n  xmlConfig.load();\n  assertEquals(\"X\",xmlConfig.getProperty(\"a(0)[@name]\"));\n  assertEquals(\"Y\",xmlConfig.getProperty(\"a(1)[@name]\"));\n}", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 264, "bug_title": "SubnodeConfiguration does not see reloads of its parent configuration", "bug_description": "The problem can be reproduced as follows:\n1 config = new XMLConfiguration(\"c:conf.xml\");\n2 config.setReloadingStrategy(new FileChangedReloadingStrategy());\n3 SubnodeConfiguration parentItemConfig = config.configurationAt(\"parent-item\");\n4 String ss2 = parentItemConfig.getString(\"sub-item\");\n5 //Now, <sub-item> node value changes to \"new_value\"\n6 ss2 = parentItemConfig.getString(\"sub-item\"); // still returns old_value", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.TestSubnodeConfiguration.java", "org.apache.commons.configuration.SubnodeConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 265, "bug_title": "XMLConfiguration with setAutoSave(true) does not save if SubnodeConfiguration is changed", "bug_description": "config.xml file\n<conf>\n    <parent-item>\n        <sub-item>old_value</sub-item>\n    </parent-item>\n</conf>\nThe problem can be reproduced as follows: \n 1 XMLConfiguration config = new XMLConfiguration(\"c:conf.xml\"); \n 2 config.setAutoSave(true) ;\n 3 SubnodeConfiguration parentItemConfig = config.configurationAt(\"parent-item\"); \n 4 parentItemConfig.setProperty(\"sub-item\",\"new_value\");\n 5. System.out.println(config.getString(\"parent-item.sub-item\");  // will print new_value\n // if you look at the config.xml sub-item still has old_value\n // also if you try to do \n 5 XMLConfiguration config2 = new XMLConfiguration(\"c:conf.xml\");\n 6. System.out.println(config2.getString(\"parent-item.sub-item\");  // will print old_value", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.event.TestHierarchicalConfigurationEvents.java", "org.apache.commons.configuration.AbstractHierarchicalFileConfiguration.java", "org.apache.commons.configuration.event.AbstractTestConfigurationEvents.java", "org.apache.commons.configuration.SubnodeConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 267, "bug_title": "INIConfiguration.save() does not save file correctly", "bug_description": "The PrintWriter used by save() method implemented in INIConfiguration is not flushed so that configurations maybe not saved completely.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.INIConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 268, "bug_title": "XMLConfiguration does not fully support disabled delimiter parsing", "bug_description": "A call to setDelimiterParsingDisabled(true) should completely turn off the mechanism for searching for list delimiters and splitting property values.\nHowever XMLConfiguration.save() escapes list delimiters even in this mode. When later such a configuration file is loaded and delimiter parsing is turned off, the values of affected properties will contain the escape character.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java", "org.apache.commons.configuration.PropertyConverter.java", "org.apache.commons.configuration.TestPropertyConverter.java"], "label": 1, "es_results": []}, {"bug_id": 270, "bug_title": "INIConfiguration Does not support multi value keys", "bug_description": "The INIConfiguration save(Writer writer) uses PrintWriter instead of using a class like PropertiesConfiguration.PropertiesWriter.\nBecause of that the method uses getString(String key) to get the keys value (which return onlt the first element of the list) and in doing so there is no support for multi value keys.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.TestINIConfiguration.java", "org.apache.commons.configuration.INIConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 269, "bug_title": "PropertiesConfiguration.save() generates superfluous escaping character when delimiter parsing is disabled", "bug_description": "PropertiesConfiguration.save() ignores the delimiter parsing disabled flag and escapes all delimiter characters it encounters. When the configuration is loaded again (with delimiter parsing disabled) the values of affected properties contain the escaping character.\nThis bug is very similar to CONFIGURATION-268, but for PropertiesConfiguration.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.PropertiesConfigurationLayout.java", "org.apache.commons.configuration.TestPropertiesConfiguration.java", "org.apache.commons.configuration.PropertiesConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 274, "bug_title": "PropertiesConfiguration.save() does not support escaping the escape character", "bug_description": "When a new property is added to a PropertiesConfiguration, it is possible to escape the escaping character for list delimiters, e.g.\nconf.addProperty(\"test.dirs\", \"C:\\\\Temp\\\\\\\\,D:\\\\Data\");\nHere the Backslash after Temp must be escaped, otherwise the list delimiter won&apos;t be recognized. This works, but when the configuration is saved and loaded again, the backslash that escapes the escape character is dropped. The property is then treated as a single value property with an escaped list delimiter.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.TestPropertiesConfiguration.java", "org.apache.commons.configuration.PropertiesConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 279, "bug_title": "PropertiesConfiguration dysfunctional if constructed with nonexistent File", "bug_description": "When PropertiesConfiguration is constructed around a File that does not exist, everything seemingly goes well, but saving the configuration only results in an empty file. This differs from constructing a PropertiesConfiguration around a file name passed as String, which appropriately fails at construction-time, since it tries to load() the configuration, which fails.\nWhen the nonexistent configuration file is passed as File, this does not happen, since the File constructor in AbstractFileConfiguration specifically tests if the file exists before calling load() and succeeds even if it wasn&apos;t called. However, if load() is not called, no PropertiesConfigurationLayout is created or registered to the EventSource, and thus nothing is saved when save() is called on the PropertiesConfiguration. Calling save() actually calls getLayout(), which creates a layout and registers it, but the properties set between construction of the PropertiesConfiguration and calling save() on it are still lost.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.TestPropertiesConfiguration.java", "org.apache.commons.configuration.PropertiesConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 280, "bug_title": "autoSave and FileChangedReloadingStrategy corrupts configuration", "bug_description": "First of all, sorry if this is not a bug. I may have missed some information on how to make autoSave and FileChangedReloadingStrategy work.\nWhen a change has been detected by the FileChangedReloadingStrategy code, a clear() followed by a load() is executed in the AbstractFileConfiguration class (around line 809 of version 1.4). The clear() method leads to a possiblySave() call which overwrites the configuration. At the end of the clear() method, the configuration file is empty and the following load() method loads this empty configuration. The possiblySave() call is invoked via the work-around (according to the comment in the code) in AbstractConfiguration (line 538).\nI&apos;m using the following code:\nCompositeConfiguration config = new CompositeConfiguration();\nXMLConfiguration xmlconfig = new XMLConfiguration(\"config.xml\");\nFileChangedReloadingStrategy fcrs = new FileChangedReloadingStrategy();\nxmlconfig.setReloadingStrategy(fcrs);\nconfig.addConfiguration(new SystemConfiguration());\nconfig.addConfiguration(xmlconfig);\nxmlconfig.setAutoSave(true);\n...wait for config changes...\t\t\nA workaround for the problem is to deactivate autoSave in the reload() method of the AbstractFileConfiguration class. After the configuration is cleared and loaded, the original autoSave is restored. See the different below:\n806a807,808\n>                         boolean autoSaveBak = this.isAutoSave(); // save the current state\n>                         this.setAutoSave(false); // deactivate autoSave to prevent information loss\n813a816\n>                               this.setAutoSave(autoSaveBak); // set autoSave to previous value\nThe code fragment looks like:\n--------------------------\n                    if (strategy.reloadingRequired())\n                    {\n                        if (getLogger().isInfoEnabled())\n                        {\n                            getLogger().info(\"Reloading configuration. URL is \" + getURL());\n                        }\n                        fireEvent(EVENT_RELOAD, null, getURL(), true);\n                        setDetailEvents(false);\n                        boolean autoSaveBak = this.isAutoSave(); // save the current state\n                        this.setAutoSave(false); // deactivate autoSave to prevent information loss\n                        try\n                        {\n                            clear();\n                            load();\n                        }\n                        finally\n                        {\n                                this.setAutoSave(autoSaveBak); // set autoSave to previous value\n                            setDetailEvents(true);\n                        }\n                        fireEvent(EVENT_RELOAD, null, getURL(), false);\n                        // notify the strategy\n                        strategy.reloadingPerformed();\n                    }\n--------------------------\nI hope this is a valid fix. ", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.AbstractFileConfiguration.java", "org.apache.commons.configuration.TestXMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 277, "bug_title": "AbstractConfiguration should handle UnsupportedOperationExceptions in Iterator.remove() gracefully", "bug_description": "Hi,\nIn AbstractConfiguration method clear() on line 533 (see code below) a possible UnsupportedOperationException is not caught resulting in the malfunctioning of the whole method. However, the documentation of the getKeys() method itself warns about relying on the remove() method of interface Iterator. The clear() method should not propagate that exception, it should catch it and try the clearProperty(String) approach if the remove() method is not supported.\n            Iterator it = getKeys();\n            while (it.hasNext())\n            {\n                String key = (String) it.next();\n                it.remove();                                          <------- EVIL!\n                if (containsKey(key))\n                {\n                    // workaround for Iterators that do not remove the property on calling remove()\n                    clearProperty(key);\n                }\n\nBest regards,\nMichael", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.AbstractConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 283, "bug_title": "CombinedConfiguration does not take escaped characters into account.", "bug_description": "Hi, \nI&apos;ve tried to used CombinedConfiguration but it seems escaped characters are not taken into account :\nimport org.apache.commons.configuration.CombinedConfiguration;\nimport org.apache.commons.configuration.ConfigurationException;\nimport org.apache.commons.configuration.PropertiesConfiguration;\nimport junit.framework.TestCase;\npublic class TestProp extends TestCase {\n\tpublic void testprop() throws ConfigurationException \n{\n\t\t// test.properties contains :\n\t\t//    without_escape=aa,bb\n\t\t//    with_escape=aa\\,bb\n\t\t//    with_2escapes=aa\\\\,bb\n\t\t\n\t\tString prop_filename = \"c:\\\\tmp\\\\test.properties\";\n\t\tPropertiesConfiguration properties_config = new PropertiesConfiguration(prop_filename);\n\t\tCombinedConfiguration   combined_config   = new CombinedConfiguration();\n\t\tcombined_config.addConfiguration(properties_config);\n\t\t\n\t\tSystem.out.println(\"Properties config\");\n\t\tSystem.out.println(properties_config.getString(\"without_escape\"));\n\t\tSystem.out.println(properties_config.getString(\"with_escape\"));\n\t\tSystem.out.println(properties_config.getString(\"with_2escapes\"));\n\n\t\tSystem.out.println(\"\\nCombined config\");\n\t\tSystem.out.println(combined_config.getString(\"without_escape\"));\n\t\tSystem.out.println(combined_config.getString(\"with_escape\"));\n\t\tSystem.out.println(combined_config.getString(\"with_2escapes\"));\n\t\t\n\t}\n}\nResult : \n---------\nProperties config\naa\naa,bb\naa,bb\nCombined config\naa\naa\naa\nThanks !\nFranck", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.ConfigurationUtils.java", "org.apache.commons.configuration.TestConfigurationUtils.java", "org.apache.commons.configuration.TestCombinedConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 285, "bug_title": "<configuration fileName=\"local-configuration.xml\" config-optional=\"true\" /> does not report parsing errors in local-configuration.xml", "bug_description": "I&apos;m trying to build a configuration system to be shared between our Java applications.  I therefore want to have optional configurations included from the main configuration file read by DefaultConfigurationBuilder\n<configuration fileName=\"local-configuration.xml\" config-optional=\"true\" />\nI have found that if the sub-configuration file is invalid the error is silently ignored.  The same without config-options=\"true\" results in a ConfigurationException to be thrown (which wraps a ConfigurationRuntimeException which wraps the ConfigurationException which wraps the SAXException).\nI believe that the behaviour is incorrect.  The sub-configuration file should be skipped if not found, but any errors found when parsing the existing file should be logged.\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.DefaultConfigurationBuilder.java", "org.apache.commons.configuration.TestDefaultConfigurationBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 287, "bug_title": "XMLConfiguration.addNodes() problem using other configuration Nodes", "bug_description": "Trying to use the method XMLConfiguration.addNodes() to add a tree of configuration properties to a node in other tree fails. \nExample:\npublic static void main(String ...args){            \ntry\n{\n           configDestination = new XMLConfiguration(\"output.xml\");\n           configSource = new XMLConfiguration(\"input.xml\");\n       }\n       catch(ConfigurationException cex)\n{\n           System.out.println(\"File not found\");\n       }\n             Collection collection = new ArrayList();\n       collection = configSource.getRoot().getChildren();\n             configDestination.addNodes(\"newNodes\", collection);                  \ntry \n{\n           configDestination.save();\n            }\n catch (ConfigurationException e) \n{\n           System.out.println(\"Error saving\");\n       }\n}\nThe XML files:\ninput.xml\n<rootNode>\n   <newNodeChild>\n       <newNodeChildChild>child value</newNodeChildChild>\n       <newNodeChildChild>child value 2</newNodeChildChild>\n       <newNodeChildChild>child value 3</newNodeChildChild>\n   </newNodeChild>\n</rootNode>\noutput.xml\n<testRootNode>\n   <test>TEST</test>\n</testRootNode>\noutput.xml after running the code:\n<testRootNode>\n   <test>TEST</test>\n   <newNodes/>\n</testRootNode>\nExpected output.xml:\n<testRootNode>\n   <test>TEST</test>\n   <newNodes>\n       <newNodeChild>\n           <newNodeChildChild>child value</newNodeChildChild>\n           <newNodeChildChild>child value 2</newNodeChildChild>\n           <newNodeChildChild>child value 3</newNodeChildChild>\n       </newNodeChild>\n   <newNodes/>\n</testRootNode> \nApparently \"the copied nodes still contain a reference to their old configuration (because you directly fetched them from the root node of the source configuration). Because of this reference they are not detected as new nodes when the destination configuration is saved, and hence not written to disk.\nI think addNodes() should reset this reference, so that the added nodes can be detected as new nodes. (But then you have to be aware that you break the source configuration because a node can only be contained in exactly one configuration.) \" ", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.TestHierarchicalConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 291, "bug_title": "addNodes() does not trigger auto saving", "bug_description": "When I add nodes the XMLConfiguration with autoSave enabled the file doen&apos;t change to reflect new nodes. Here is an example\nXMLConfiguration config = new XMLConfiguration(\"c:conf.xml\"); \nconfig.setAutoSave(true) ;\nConfigurationNode childNode = new HierarchicalConfiguration.Node(\"ChildNodeName\",\"ChildNodeValue\");\nList nodes = new ArrayList();\nnodes.add(childNode);\nconfig.addNodes(null,nodes);\nconfig.xml didn&apos;t change to include child node.\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.AbstractHierarchicalFileConfiguration.java", "org.apache.commons.configuration.TestXMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 294, "bug_title": "Trying to set values in the newly added node (using XMLConfiguration) does not work", "bug_description": "I&apos;m using XMLConfiguration to manipulate my xml configuration file. Here is what I did,\n1) Add a new node using \"addNodes\"\n2) Save the configuration (using save method)\n3) The new node gets added to the configuration file   <--- Good\n4) Now try setting a value in the just added new node using \"setProperty\" method.\n5) Save the configuration (using save method)\n6) The save method still writes the configuration that was in step 3        <------- Bug\nLooks like for some reason the save still holds the old configuration in cache that after the add nodes. If I call \"reload\" after each save then everything works fine. \nJust a setProperty on its own (without addNodes) work fine. It&apos;s only when you try to addNodes and then do setProperty that this fails.\nI&apos;ve tried this with the latest nightly build (1.5), released 1.4 and 1.3 --> all of them fail in this situation.\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 295, "bug_title": "HierarchicalConfiguration.subset(...) misses an entry with the key equal to the subset prefix.", "bug_description": "Having a HierarchicalConfiguration with entries alike:\n    test.sample = subset title\n    test.sample.boolean = true\n    test.sample.int = 123456\n    test.sample.text = I wish it worked!\nWhen performing subset(\"test.sample\"), I get a configuration of only three entries, the \"subset title\" value is not present (should have an empty string key).\nThe behaviour is in contradiction to Javadoc of the Configuration interface http://commons.apache.org/configuration/apidocs/org/apache/commons/configuration/Configuration.html#subset(java.lang.String)\nSeems to me, that the bug is hidden in iterating the cloned configuration, where only children of the clone are processed, but not the root itself.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.TestHierarchicalConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 296, "bug_title": "XMLConfiguration and attributes on the root node", "bug_description": "With XMLConfiguration it is not possible to change the value of an attribute of the root element.\nNew attributes on the root level can be created, e.g.:\nXMLConfiguration config = new XMLConfiguration();\nconfig.addProperty(\"[@test]\", \"true\");\nwould create a new \"test\" attribute of the root element. However if this configuration is saved and loaded again, a\nconfig.setProperty(\"[@test]\", \"false\");\nonly temporarily changes the value: getProperty() returns the new value, but when the configuration is saved, the old value is written.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 299, "bug_title": "Interpolation of ${const:xxx} variables can cause a ClassCastException", "bug_description": "If ConstantLookup is used for resolving a variable that is of a different type than String, a ClassCastException can be thrown:\nThe first access to this variable obtains the value and stores it in an internal cache. If this variable is requested again, the value is fetched from the cache and a type cast to String is performed. This will fail with a ClassCastException.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_5", "fixed_files": ["org.apache.commons.configuration.interpol.TestConstantLookup.java", "org.apache.commons.configuration.interpol.ConstantLookup.java"], "label": 1, "es_results": []}, {"bug_id": 300, "bug_title": "Can not create a PropertiesConfiguration if the filename contains a '#'", "bug_description": "It is not possible to load a properties file if there is a &apos;#&apos; in the filename.\nTo reproduce:\n    public static void main(String[] args) throws Exception\n    {\n        File file = new File(\"myProperties #1.properties\");\n        try\n        {\n            file.createNewFile();\n            \n            Configuration configuration = new PropertiesConfiguration(file);\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        finally\n        {\n            file.delete();\n        }\n}\nThe method that drops the &apos;#&apos; is : ConfigurationUtils.fileFromURL...", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_4", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.AbstractFileConfiguration.java", "org.apache.commons.configuration.ConfigurationUtils.java", "org.apache.commons.configuration.TestPropertiesConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 302, "bug_title": "FileChangedReloadingStrategy.reloadingRequired() can fail", "bug_description": "If reloadingRequired() returns true and you call it again before calling reloadingPerformed(), the 2nd time it can return false (but you have not yet reloaded!) because it doesn&apos;t check the file system again until the refresh delay is expired.\nOf course this is a very unusual test case (usually you reload immediately) but the behaviour of the method should be consistent in this case too: if reloadingRequired() returns true any subsequent call to this method should return true until reloadingPerformed() is called.\nIn my project I have fixed the method by promoting the flag called \"reloading\" to class scope so I that can check whether the previous call returned true or false:\nprotected boolean reloading = false;\npublic boolean reloadingRequired()\n{\n\tif (!reloading)\n\t{\n\t\tlong now = System.currentTimeMillis();\n\t\tif (now > lastChecked + refreshDelay)\n\t\t{\n\t\t\tlastChecked = now;\n\t\t\tif (hasChanged())\n\t\t\t{\n\t\t\t\treloading = true;\n\t\t\t}\n\t\t}\n\t}\n\treturn reloading;\n}\nOf course I reset this flag in init() and reloadingPerformed().", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.reloading.FileChangedReloadingStrategy.java", "org.apache.commons.configuration.reloading.TestFileChangedReloadingStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 306, "bug_title": "INIConfiguration: Quoted values should not be trimmed", "bug_description": "I am reading in .ini files with quoted strings.  Here is an example of a line:\nCmdPrompt=\"[sdog@rosco ~]$ \"\nIn Commons Configuration 1.4, this incorrectly reads in as \"[sdog@rosco ~]$ \".  When I saw that this had been addressed in 1.5, I was excited... until I started using 1.5.  Now I don&apos;t get quotes, but I also don&apos;t get my trailing space, which is the reason I&apos;m using quotes in the first place.  I expect the offending line is the final line in the parseValue(String) method of INIConfiguration:\nline 403:\n        return result.toString().trim();\nExpected behavior: INIConfiguration should read in the exact string between the quotes, not a trimmed version.\nI&apos;m going back to 1.4 and my own code to strip the quotes.  Please address this as I cannot move to future versions without it.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.TestINIConfiguration.java", "org.apache.commons.configuration.INIConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 315, "bug_title": "CombinedConfiguration fire EVENT_COMBINED_INVALIDATE even before the contained configuration is updated", "bug_description": "CombinedConfiguration.configurationChanged doesn&apos;t check the !event.isBeforeUpdate() and trigger the invalidate call. The invalidate call fire the EVENT_COMBINED_INVALIDATE. so now when a contained config is changed, EVENT_COMBINED_INVALIDATE is fired twice by the CombinedConfiguration. However it should only fire it after the contained FileConfiguration is updated.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.TestCombinedConfiguration.java", "org.apache.commons.configuration.CombinedConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 316, "bug_title": "Constructor XMLConfiguration(HierarchicalConfiguration) invalidates text in singular top element", "bug_description": "When using the XMLConfiguration(HierarchicalConfiguration) constructor for copying an XMLConfiguration e.g. originally read from the XML string <e a=\"v\">example</e>, the \"example\" text can still be retrieved from the copy using getString(\"\"). When saving the copy to an XML file, however, the text is not written; the resulting XML string is just <configuration a=\"v\"/>. I will attach sample code that illustrates this.\nWhether it is intended that the name of the top-level element gets lost as well, I don&apos;t know (but that is not my focus here).", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 318, "bug_title": "Constructor XMLConfiguration(HierarchicalConfiguration) loses the name of the root element", "bug_description": "When creating an XMLConfiguration from another one using the copy constructor the name of the root element (i.e. the document element) is dropped. When the configuration is saved, the default root name (\"configuration\") is used.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.TestHierarchicalConfigurationXMLReader.java", "org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 320, "bug_title": "XMLPropertyListConfiguration is limited to 32 bits integers", "bug_description": "XMLPropertyListConfiguration parses integers as 32 bits values, but the plist spec doesn&apos;t limit the size of the integers. It should at least support Long values, and maybe BigIntegers.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.plist.XMLPropertyListConfiguration.java", "org.apache.commons.configuration.plist.TestXMLPropertyListConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 322, "bug_title": "ConfigurationDynaBean does not work with indexed properties stored internally as arrays", "bug_description": "ConfigurationDynaBean is not fully tested for properties stored in the configuration as arrays. This case is rare because any array added to a configuration is automatically transformed into a List. To reproduce this issue the configuration must be a wrapper of a source that already contains an array, such as a MapConfiguration or a JNDIConfiguration.\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.AbstractConfiguration.java", "org.apache.commons.configuration.beanutils.ConfigurationDynaBean.java", "org.apache.commons.configuration.beanutils.TestConfigurationDynaBean.java"], "label": 1, "es_results": []}, {"bug_id": 332, "bug_title": "PropertiesConfiguration.save() does not persist properties added through a DataConfiguration", "bug_description": "There is a regression in Commons Configuration with PropertiesConfiguration wrapped into a DataConfiguration. The properties added through a DataConfiguration aren&apos;t persisted when the configuration is saved, but they can be queried normally. Commons Configuration 1.4 wasn&apos;t affected by this issue.\nThe following test fails on the last assertion :\n\npublic void testSaveWithDataConfiguration() throws ConfigurationException\n{\n    File file = new File(\"target/testsave.properties\");\n    if (file.exists()) {\n        assertTrue(file.delete());\n    }\n\n    PropertiesConfiguration config = new PropertiesConfiguration(file);\n\n    DataConfiguration dataConfig = new DataConfiguration(config);\n\n    dataConfig.setProperty(\"foo\", \"bar\");\n    assertEquals(\"bar\", config.getProperty(\"foo\"));\n    config.save();\n\n    // reload the file\n    PropertiesConfiguration config2 = new PropertiesConfiguration(file);\n    assertFalse(\"empty configuration\", config2.isEmpty());\n}\n\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.DataConfiguration.java", "org.apache.commons.configuration.TestPropertiesConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 334, "bug_title": "Inconsistent parent nodes in HierarchicalConfiguration when using setRootNode()", "bug_description": "HierarchicalConfiguration allows setting a root node explicitly using either setRootNode() or setRoot(). The latter exists for backwards compatibility only and operates on the type HierarchicalConfiguration.Node rather than ConfigurationNode. To support a corresponding getRoot() method a new instance of HierarchicalConfiguration.Node is created, and the child nodes of the root node are added to it. Thus these nodes become child nodes of this new node.\nIf now addProperty() is called for adding new properties to the configuration, the nodes created for the new properties are added to the node passed to the setRootNode() method. So they have a different parent node than the existing nodes.\nAs long as only methods of the Configuration interface are used for querying or manipulating the configuration, this does not seem to have any strange effects. But when working with the nodes directly it is certainly confusing.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.TestHierarchicalConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 339, "bug_title": "ConfigurationConverter does not handle CompositeConfiguration correctly", "bug_description": "ConfigurationConverter doesn&apos;t seem to respect the ordering of a composite configuration. I am having problems describing the behavior, but I thinks the following test case illustrates it best:\n\t@Test\n\tpublic void showBug() {\n\t\tPropertiesConfiguration p = new PropertiesConfiguration();\n\t\tp.addProperty(\"foo\", \"initial\");\n\t\tp.addProperty(\"bar\", \"$\n{foo}\n\");\n\t\tp.addProperty(\"prefix.foo\", \"override\");\n\t\tCompositeConfiguration cfg = new CompositeConfiguration();\n\t\tcfg.addConfiguration(p.subset(\"prefix\"));\n\t\tcfg.addConfiguration(p);\n\t\t// this assertion passes as expected since the subset\n\t\t// was added first to the composite configuration\n\t\tAssert.assertEquals(\"override\", cfg.getString(\"bar\"));\n\t\t// after converting to properties, this assertion fails and\n\t\t// reports that the value is &apos;initial&apos;\n\t\tProperties properties = ConfigurationConverter.getProperties(cfg);\n\t\tAssert.assertEquals(\"override\", properties.getProperty(\"bar\"));\n\t}", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.TestCompositeConfiguration.java", "org.apache.commons.configuration.CompositeConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 341, "bug_title": "FileChangedReloadingStrategy not working for SubnodeConfiguration with a CombinedConfiguration parent", "bug_description": "A SubnodeConfiguration that has a CombinedConfiguration as its parent, doesn&apos;t \"hot\" reload properties even if the combined configuration consists of XMLConfigurations that have the FileChangedReloadingStrategy set, the combined configuration has the setForceReloadCheck flag set and the SubnodeConfiguration was obtained by calling configurationAt method and passing in true for the \"supportUpdates\" flag.  \nHere&apos;s some code that shows the problem where the final assertion will fail.\n @Test\n  public void testSubnodeReadload() throws ConfigurationException \n{\n    XMLConfiguration xmlConfiguration = new XMLConfiguration(\"service.xml\");\n    FileChangedReloadingStrategy fileReloadStrategy = new FileChangedReloadingStrategy();\n    fileReloadStrategy.setRefreshDelay(1000);\n    xmlConfiguration.setReloadingStrategy(fileReloadStrategy);\n    XMLConfiguration config2 = new XMLConfiguration(\"environment.xml\");\n    FileChangedReloadingStrategy fileReloadStrategy2 = new FileChangedReloadingStrategy();\n    fileReloadStrategy2.setRefreshDelay(1000);\n    xmlConfiguration.setReloadingStrategy(fileReloadStrategy2);\n\n    CombinedConfiguration combinedConfig = new CombinedConfiguration();\n    combinedConfig.setForceReloadCheck(true);\n    combinedConfig.addConfiguration(xmlConfiguration);\n    combinedConfig.addConfiguration(config2);\n    int queue0threads2 = xmlConfiguration.getInt(\"messaging-new.queue(0).threads\");\n    assert 2 == queue0threads2;\n\n    //change the value in service.xml to 4\n    int queue0threads4 = xmlConfiguration.getInt(\"messaging-new.queue(0).threads\");\n    assert 4 == queue0threads4;\n\n\n    SubnodeConfiguration subConfigHier = xmlConfiguration.configurationAt(\"messaging-new.queue(0)\", true);\n    int queue0threadsCombinedSub4 = subConfigHier.getInt(\"threads\");\n    assert queue0threadsCombinedSub4 == 4;\n\n    //change the value service.xml to 8\n    int queue0threadsCombinedSub8 = subConfigHier.getInt(\"threads\");\n    assert queue0threadsCombinedSub8 == 8;\n\n\n    SubnodeConfiguration subNodeConfigParentIsCombinedConfig = combinedConfig.configurationAt(\"messaging-new.queue(0)\", true);\n    int queue0threadsSub8 = subNodeConfigParentIsCombinedConfig.getInt(\"threads\");\n    assert queue0threadsSub8 == 8;\n\n    //change the value service.xml file to 16\n    int queue0threadsSub16 = subNodeConfigParentIsCombinedConfig.getInt(\"threads\");\n    assert queue0threadsSub16 == 16;         //THIS TEST FAILS\n\n  }\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.TestCombinedConfiguration.java", "org.apache.commons.configuration.CombinedConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 346, "bug_title": "ConfigurationUtils.convertToHierarchical() does not correctly handle properties with multiple values", "bug_description": "If a property has multiple values, the resulting hierarchical configuration will contain a node whose value is a list with these values. There should, however, be a single node for each value. Some queries, especially those containing indices, do not work on such structures.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.ConfigurationUtils.java", "org.apache.commons.configuration.TestConfigurationUtils.java"], "label": 1, "es_results": []}, {"bug_id": 347, "bug_title": "Iterating over the keys of a file-based configuration can cause a ConcurrentModificationException", "bug_description": "Some implementations of FileConfiguration return an iterator in their getKeys() method that is directly connected to the underlying data store. When now a reload is performed (which can happen at any time) the data store is modified, and the iterator becomes invalid.\nThis behavior is very confusing because ConcurrentModificationExceptions are typically related to multi-threading access. But even if the code performing the iteration is the only instance that accesses the configuration, the exception can be thrown.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.TestFileConfiguration.java", "org.apache.commons.configuration.AbstractFileConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 348, "bug_title": "AbstractHierarchicalFileConfiguration does not trigger a reload for the getKeys() method", "bug_description": "In getKeys() the obligatory call to reload() was forgotten.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.AbstractHierarchicalFileConfiguration.java", "org.apache.commons.configuration.TestXMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 345, "bug_title": "PropertiesConfiguration does not use the default encoding to load files", "bug_description": "The piece of code\nPropertiesConfiguration.java\n    // initialization block to set the encoding before loading the file in the constructors\n    {\n        setEncoding(DEFAULT_ENCODING);\n    }\n\n\nseems to set correctly the default encoding, but this block is called after \"super()\" in constructors.\nSo when using either PropertiesConfiguration(java.io.File file), PropertiesConfiguration(java.lang.String fileName) or PropertiesConfiguration(java.net.URL url), the super() statement is called, and it loads the file without the default encoding.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.TestPropertiesConfiguration.java", "org.apache.commons.configuration.PropertiesConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 349, "bug_title": "XMLConfigurationProvider cannot create a class that extends XMLConfiguration", "bug_description": "XMLConfigurationProvider in DefaultConfigurationBuilder has default scope. When configuring a new ConfigurationProvider to use a class that extends XMLConfiguration an Exception is raised because the setConfigurationClass method cannot be called from the bean utility classes.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.DefaultConfigurationBuilder.java", "org.apache.commons.configuration.TestDefaultConfigurationBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 335, "bug_title": "XMLConfiguration: Cannot disable attribute splitting ", "bug_description": "My XML configuration has the following attribute:\n<some-element some-attribute=\"\n\n\" />\nBut XML Configuration is trying to split this string and trims it after splitting. I don&apos;t need this behaviour, but setting setDelimiterParsingDisabled() just changing delimeter to \"|\" and not disables attribute trimming.\nNeed either to disable trimming/splitting if setDelimiterParsingDisabled() is set to TRUE (incompatible change), or add something like setParseAttributesAsIs() that will prevent attributes to be trimmed and splitted", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 321, "bug_title": "AbstractHierarchicalConfiguration.getKeys(key) does not return the key", "bug_description": "The getKeys(String prefix) implementation of AbstractHierarchicalConfiguration doesn&apos;t return anything if the prefix used is the key of an existing property. The iterator returned should at least contain the key used as the prefix.\nHere is the test method for TestHierarchicalConfiguration, currently the first assertion fails:\n\npublic void testGetKeysWithKeyAsPrefix()\n{\n    Iterator<?> it = config.getKeys(\"order.key1\");\n    assertTrue(\"no key found\", it.hasNext());\n    assertEquals(\"1st key\", \"order.key1\", it.next());\n    assertFalse(\"more keys than expected\", it.hasNext());\n}\n\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_6", "fixed_files": ["org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.TestHierarchicalConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 357, "bug_title": "ConversionException message is incorrect for AbstractConfiguration.getBigInteger(String,BigInteger)", "bug_description": "The message in the ConfigurationException thrown from AbstractConfiguration.getBigInteger(String,BigInteger) reads \"BigDecimal\" but should read \"BigInteger\".", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_5", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.AbstractConfiguration.java", "org.apache.commons.configuration.TestAbstractConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 368, "bug_title": "CombinedConfiguration: java.util.NoSuchElementException after reload of enclosed SubnodeConfiguration/XMLConfiguration", "bug_description": "Steps to repeat:\n\ncreate a XMLConfiguration based on a XML config file (xml file content e.g.:  <config><foo><bar>0</bar></for></config>)\nassign file reloading strategy to the XMLConfiguration\ncreate a SubnodeConfiguration based on this XMLConfiguration (prefix e.g.: &apos;foor&apos;)\ncreate a CombinedConfiguration\nadd the SubnodeConfiguration to this CombinedConfiguration\nget a configuration value from the CombinedConfiguration (e.g. &apos;bar&apos;) -> OK, this works\ntouch the underlying xml configuration\ntry to get a configuration value from the CombinedConfiguration again (e.g. &apos;bar&apos;) ->  java.util.NoSuchElementException\n\nSee also attached TestCase.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.TestSubnodeConfiguration.java", "org.apache.commons.configuration.SubnodeConfiguration.java", "org.apache.commons.configuration.TestCombinedConfiguration.java", "org.apache.commons.configuration.CombinedConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 369, "bug_title": "SubsetConfiguration ignores local StrLookups", "bug_description": "For an AbstractConfiguration it is normally possible to register local StrLookup instances. These are simply ignored by the SubsetConfiguration.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.interpol.ConfigurationInterpolator.java", "org.apache.commons.configuration.TestSubnodeConfiguration.java", "org.apache.commons.configuration.SubnodeConfiguration.java", "org.apache.commons.configuration.TestSubsetConfiguration.java", "org.apache.commons.configuration.SubsetConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 385, "bug_title": "clearProperty does not generate events in DatabaseConfiguration", "bug_description": "clearProperty does not generate a EVENT_CLEAR_PROPERTY event because it overrides AbstractConfiguration.clearProperty instead of AbstractConfiguration.clearPropertyDirect as recommended by the superclass.  ", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.DatabaseConfiguration.java", "org.apache.commons.configuration.TestDatabaseConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 362, "bug_title": "XMLPropertyListConfiguration does not handle empty dictionary correctly", "bug_description": "Empty \"dict\" elements are not handled properly.   During printing of a configuration, configuration nodes that have no children and no value are assumed to be \"strings\" (see XMLPropertyListConfiguration.java:printValue()&apos;s last \"else\" clause).   <dict/> is parsed in such a way (see startElement() and endElement() of that same file) that it ends up creating a configuration node with no children and no value.  Thus printing out a parsed property list will print <string>null</string> everywhere an empty dictionary was in the input.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.plist.XMLPropertyListConfiguration.java", "org.apache.commons.configuration.plist.TestXMLPropertyListConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 388, "bug_title": "When updating an element or attribute that contains the list delimiter the character is escaped even if delimiting is disabled.", "bug_description": "When updating an attribute or element using an XMLConfiguration, if the new value contains a delimiter the delimiter will be escaped with a &apos;\\&apos; even if delimiters have been disabled.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 393, "bug_title": "BaseConfiguration.clone() does not work with list properties", "bug_description": "In the clone() implementation the map storing the configuration data is also cloned. For properties with multiple values this map contains lists. Because no deep clone is performed these lists are simply copied into the cloned map. So if the corresponding properties are changed on either the original or the clone, the other object is affected, too.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.BaseConfiguration.java", "org.apache.commons.configuration.TestBaseConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 396, "bug_title": "In HierarchicalConfiguration$Node visits the ConfigurationKey  points to the parent path in visitAfterChildren call", "bug_description": "When visiting a child the key is aggregated to have the child node name, and a call to visitBeforeChildren and grandChildren&apos;s visits are made.\nHowever, before calling  visitAfterChildren,  the resetting of the key to that of it&apos;s parent&apos;s, seems inconsistent.\nLast lines in visit(NodeVisitor visitor, ConfigurationKey key) \n            if (key != null)\n            {\n\n                key.setLength(length);\n\n            }\n            visitor.visitAfterChildren(this, key);", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.TestHierarchicalConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 399, "bug_title": "interpolator for reading environment variables", "bug_description": "Hello there,\nThis is an extension of issue Id CONFIGURATION-284\nThe \"env\" interpolator prefix still does not seem to work. On investigation, I noticed that we have a new java class to support reading environment variables from different OSes, but there is no interpolator class (extending StrLookup) that supports the \"env\" prefix.\nCould someone look into this? For the moment, I have put in a class myself locally. Will add to the repository once I find some time.. \nRegards.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.interpol.ConfigurationInterpolator.java", "org.apache.commons.configuration.TestAbstractConfigurationBasicFeatures.java"], "label": 1, "es_results": []}, {"bug_id": 403, "bug_title": "XMLConfiguration isEmpty has altered behaviour between 1.4  and 1.6", "bug_description": "Given xml configuration file\n\n \n\n<?xml version=\"1.0\" encoding=\"ISO-8859-1\" ?>\n\n<empty></empty>\n\n\n\n\n\n\nXMLConfiguration xml = new XMLConfiguration(file);\n\nassertTrue(xml.isEmpty());\n\n\n\nisEmpty() returned true on version 1.4 ?, now it returns false, and the configuration contains a single empty string as key.\nNot sure if this should be considered a bug, but the documnetation reads: Checks if this configuration is empty. Empty means that there are no keys with any values, though there can be some (empty) nodes. \n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 407, "bug_title": "HierarchicalINIConfiguration can throw an exception if the global section is accessed concurrently", "bug_description": "If the global section of a hierarchical INI configuration is requested, a new ViewNode is created which becomes the root node of a new configuration for the global section. Nodes representing properties of the global section are added to this ViewNode. This operation temporarily changes the parent node of these nodes which may cause problems if the method is called by multiple threads concurrently. Because access to a section is a read-only operation, this should be thread-safe.\nThe probability that this error happens is pretty low I AM GOING TO. Therefore it should be hard to create a unit test.\nThe issue can be fixed by synchronizing the add operation to the ViewNode.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.HierarchicalINIConfiguration.java", "org.apache.commons.configuration.TestHierarchicalINIConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 409, "bug_title": "Key containing escapedDelimiter does not save() properly", "bug_description": "It seems that a key containing the escapedDelimiter is stored without the delimiter and the escape is not recreated when saving the configuration. \n\ntest.java\n\n\nHierarchicalINIConfiguration inicfg = new HierarchicalINIConfiguration();\n\ninicfg.setProperty( \"Andrew L.. Cooper.first\", \"Andrew\" );\n\ninicfg.setProperty( \"Andrew L.. Cooper.last\", \"Cooper\" );\n\ninicfg.setProperty( \"Andrew L.. Cooper.mail\", \"andrew.cooper@example.com\" );\n\ninicfg.save( System.out );\n\nSystem.out.println( inicfg.get( \"Andrew L..Cooper.mail\" );\n\n\n\nExpected Output\n\n\n[Andrew L. Cooper]\n\nfirst = Andrew\n\nlast = Cooper\n\nmail = andrew.cooper@example.com\n\n\n\nandrew.cooper@example.com\n\n\n\nActual Output\n\n\n[Andrew L. Cooper]\n\n\n\nandrew.cooper@example.com\n\n\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.HierarchicalINIConfiguration.java", "org.apache.commons.configuration.TestHierarchicalINIConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 408, "bug_title": "When I save a URL as a property value, the forward slashes are getting escaped", "bug_description": "When I save a URL as a property value, the forward slashes are getting escaped.\nie: \nfoo = http:\\/\\/www.google.com\\/\nExample Code : \npublic static void main(String[] args)\n  {\n    try\n    {\n\n      PropertiesConfiguration config = new PropertiesConfiguration();     \n\n      File newProps = new File(\"foo.properties\");\n\n      config.setProperty(\"foo\", \"http://www.google.com/\");     \n\n      config.save(newProps);\n\n      \n\n    }\n    catch (Exception e){}\n  }", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.TestPropertiesConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 413, "bug_title": "Subset configuration does not support events", "bug_description": "", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.SubsetConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 415, "bug_title": "ConfigurationUtils.fileFromURL mangles files with '+' (plus sign) in the name", "bug_description": "Attached is a sample program that demonstrates the problem.  The following is the method in ConfigurationUtils in question:\npublic static File fileFromURL(URL url)\n{\n\n...\n\n        return new File(URLDecoder.decode(url.getPath()));\n\n...\n\n}\n\nURLDecoder (poorly named) decodes data encoded in the application/x-www-form-urlencoded MIME format.  This format is commonly used to encode HTML form data.  It is not intended for encoding URLs, though the formats are similar.\nThe operative difference is that the MIME format allows the use of the plus sign (&apos;+&apos;) to represent spaces, whereas URLs must have spaces hex encoded (&apos;%20&apos;).  Files may have plus signs in the name, and therefore, decoding the plus sign as a space produces a different path.\nSee attached code demonstrating the problem.\nReference:  http://www.w3.org/MarkUp/html-spec/html-spec_8.html#SEC8.2.1\nand http://www.ietf.org/rfc/rfc1738.txt", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.TestFileConfiguration.java", "org.apache.commons.configuration.ConfigurationUtils.java", "org.apache.commons.configuration.TestConfigurationUtils.java"], "label": 1, "es_results": []}, {"bug_id": 423, "bug_title": "TestFileChangedReloadingStrategy fails incorrectly in testFromClasspath() ", "bug_description": "testFromClassPath() can fail when it should not because of inconsistent escaping of output from PropertiesConfiguration.getURL() and FileChangedReloadingStrategy.getFile().toURL().", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.reloading.TestFileChangedReloadingStrategy.java"], "label": 1, "es_results": []}, {"bug_id": 428, "bug_title": "The Windows file path cannot be saved correctly as expected in XMLConfiguration", "bug_description": "I want to generate a XML as:\n\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n\n<Test>\n\n    <Cluster>\n\n        <Server location=\"C:\\Server92\"/>\n\n    </Cluster>\n\n</Test>\n\n\n\nJava Code:\nTest.java\n\n\nXMLConfiguration config = new XMLConfiguration();\n\nconfig.setRootElementName(\"Test\");\n\nconfig.addProperty(\"Cluster.Server[@location]\",  \"C:\\\\Server92\");\n\nconfig.save(\"C:\\\\NEW.xml\");\n\n\n\nBUT after running the Java Code, the generated XML looks like:\n\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n\n<Test>\n\n<Cluster>\n\n<Server location=\"C:\\\\Server92\"/>\n\n</Cluster>\n\n</Test>\n\n\n\nYou will find that the location is \"C:\\ \\Server92\", BUT what I expected is \"C:\\Server92\".", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.PropertyConverter.java", "org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java", "org.apache.commons.configuration.TestPropertyConverter.java"], "label": 1, "es_results": []}, {"bug_id": 433, "bug_title": "ConfigurationDynaBean does not work as advertised regarding indexed properties", "bug_description": "The Javadoc of ConfigurationDynaBean says: \"Setting an indexed property always throws an exception.\" However, there is a reasonable implementation for setting indexed properties.\nThe get() method for indexed properties tries to figure out whether a property is indexed and throws an exception if not. However, it does not discover all cases of invalid access to non-indexed properties.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.beanutils.ConfigurationDynaBean.java", "org.apache.commons.configuration.beanutils.TestConfigurationDynaBean.java"], "label": 1, "es_results": []}, {"bug_id": 434, "bug_title": "In HierarchicalINIConfiguration, after saving a property by calling setProperty() and save() methods, the contents after semicolon disappeared in the saved file.", "bug_description": "Hi!\nI am developing an application using Commons Configuration API.\nAnd I got this problem.\nIn the ini file like this:\n[Environment]\nApplication Type=any\nClass Path=C:\\Program Files\\jar\\manage.jar;C:\\Program Files\\jar\\guiLauncher.jar;C:\\Program Files\\jar\\appStart.jar;%USERPROFILE%;\nI changed the value of &apos;Application Type&apos; from &apos;any&apos; to &apos;gui&apos; by using class HierarchicalINIConfiguration.\nThe value was successfully modified, but instead the value of &apos;Class Path&apos; was cut in the middle.\nIt is reduced like this :  &apos;Class Path=C:\\Program Files\\jar\\manage.jar&apos;\nIn my opinion, the Configuration System regards the contents after &apos;;&apos; as comments, which disappeared from the file.\nIs this a kind of bug? Or is there a way to show all the contents after &apos;;&apos; properly?\nI appreciate if you give comments on this.\nThank you.\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.HierarchicalINIConfiguration.java", "org.apache.commons.configuration.TestHierarchicalINIConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 446, "bug_title": "XMLConfiguration removes empty attributes on update", "bug_description": "Consider an XML configuration file config.xml containing:\n<configuration>\n<foo attr=\"aValue\">\n</foo>\n</configuration>\nUpdate the configuration to set an attribute to empty:\n    XMLConfiguration config = new XMLConfiguration(\"config.xml\");\n    config.setExpressionEngine(new XPathExpressionEngine());\n    config.setProperty(\"foo/@attr\", \"\");\n    config.save();\nThe file has been modified as follows:\n<configuration>\n<foo/>\n</configuration>\nThe attribute shouldn&apos;t be removed as an empty attribute is different from a missing attribute.\nMethod removing the attribute: XMLConfiguration$XMLBuilderVisitor.updateAttribute(Node node, Element elem, String name, char listDelimiter)", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.TestXMLConfiguration.java", "org.apache.commons.configuration.XMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 448, "bug_title": "INI config cannot have = in the value when using : seperators", "bug_description": "If I load an INI file with these values:\nusername: identity\npassword: abc=123\nIt actually considers the second line to have the key \"password: abc\" and the value \"123:G\". ", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.HierarchicalINIConfiguration.java", "org.apache.commons.configuration.TestHierarchicalINIConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 458, "bug_title": "SubnodeConfiguration.clear() does not work correctly", "bug_description": "If clear() is called on a SubnodeConfiguration obtained through the configurationAt() method of HierarchicalConfiguration, the node structure of the parent configuration (outside of the area of the sub configuration) may also be manipulated. Properties added to the SubnodeConfiguration after a clear() are not visible from the parent configuration.\nThe problem is caused by the implementation of the clear() method inherited from AbstractConfiguration. This will call clearProperty() for each property found in the configuration. clearProperty() in turn clears the value of nodes and recursively clears the parent node if it does not contain any data. Here the structure owned by the SubnodeConfiguration can be left so that the parent gets manipulated.\nA possible solution would be to provide a specific implementation of clear() for hierarchical configurations. This could also be more efficient than the base implementation.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.TestHierarchicalConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 455, "bug_title": "HierachicalINIConfiguration section access without dotted notation", "bug_description": "Setting section properties of previously non-existent Sections using getSection().setProperty() fails.\nUsing a non existing filename for f, the following produces an empty file.\nini = HierarchicalINIConfiguration(f)\nsection = ini.getSection(\"section\")\nsection.setProperty(\"foo\", \"bar\")\nini.save()\nAccessing SubnodeConfigurations after clearing them fails.\nusing an existing file with an existing section, produces an empty file.\nini = new HierarchicalINIConfiguration(f);\nsubnode = ini.getSection(\"section\");\nif (! subnode.isEmpty() ) \n{\n\n   subnode.clear();\n\n}\nsubnode.setProperty(\"foo\", \"bar\");\nini.save();\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.HierarchicalINIConfiguration.java", "org.apache.commons.configuration.TestHierarchicalINIConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 456, "bug_title": "AbstractConfiguration.getKeys(String prefix) docs does not say about the point ('.')", "bug_description": "For the method org.apache.commons.configuration.AbstractConfiguration.getKeys(String prefix) the documentation doesn&apos;t say that to the prefix will be added a point character (&apos;.&apos;) to filter the keys list (http://commons.apache.org/configuration/apidocs/org/apache/commons/configuration/AbstractConfiguration.html#getKeys%28java.lang.String%29). I discovered it only reading the source.\nMaybe I should have known this behaviour, but I think a lot of occasional users could face this unpredictable behaviour. Specifying this would be very helpfull.\nMany thanks for your wonderful libraries!", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.AbstractConfiguration.java", "org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.Configuration.java"], "label": 1, "es_results": []}, {"bug_id": 460, "bug_title": "reloadStrategy does not work for files inside <additional> tag using DefaultConfigurationBuilder", "bug_description": "In the configuration file that DefaultConfigurationBuilder reads to build a CombinedConfiguration, it&apos;s possible to include configuration file either inside <override> or <additional> xml elements.\nEach such declaration, of a file, allows a realodStrategy to be specified (see example below). It appears that the reload occurs only for the files inside <override> and not for the ones inside <additional>.\nExample:\n<configuration>\n  <header>\n    <result forceReloadCheck=\"true\">\n      <expressionEngine config-class=\"org.apache.commons.configuration.tree.xpath.XPathExpressionEngine\"/>\n    </result>\n  </header>\n  <override>\n    <properties fileName=\"user.properties\" config-optional=\"true\">\n      <reloadingStrategy refreshDelay=\"100\"\n                         config-class=\"org.apache.commons.configuration.reloading.FileChangedReloadingStrategy\"/>\n    </properties>\n  </override>\n  <additional>\n    <properties fileName=\"application.properties\">\n      <reloadingStrategy refreshDelay=\"100\"\n                         config-class=\"org.apache.commons.configuration.reloading.FileChangedReloadingStrategy\"/>\n    </properties>\n  </additional>\n</configuration>\nIn above example, both user.properties and application.properties are supposed to reload upon change. However, as tested by the following code, one user.properties gets reloaded:\n\t\tDefaultConfigurationBuilder dcb = new DefaultConfigurationBuilder(\"example.xml\");\n\t\tConfiguration conf = dcb.getConfiguration();\n\t\tSystem.out.println(\"user: \" + conf.getBoolean(\"user\"));\n\t\tSystem.out.println(\"application: \" + conf.getBoolean(\"application\"));\n\t\tSystem.out.println(\"Change files and then press  to continue...\");\n\t\tSystem.in.read();\n\t\tSystem.out.println(\"user: \" + conf.getBoolean(\"user\"));\n\t\tSystem.out.println(\"application: \" + conf.getBoolean(\"application\"));\nOutput from above code:\nuser: true\napplication: true\nChange files and then press  to continue...\n0 [main] INFO org.apache.commons.configuration.PropertiesConfiguration  - Reloading configuration. URL is file:<snipped>/user.properties\nuser: false\napplication: true\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_7", "fixed_files": ["org.apache.commons.configuration.DefaultConfigurationBuilder.java", "org.apache.commons.configuration.TestDefaultConfigurationBuilder.java", "org.apache.commons.configuration.TestCombinedConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 463, "bug_title": "AbstractFileConfiguration.load(String filename) returns with current directory changed", "bug_description": "\n\n\n    PropertiesConfiguration pc = new PropertiesConfiguration();\n\n    ...\n\n    for (String f : cl.getArgs())\n\n    {\n\n        pc.load(f);\n\n    }\n\n\n\nInvoked in directory /home/me/test with several relative paths, i.e. ./sub1/a.cfg ./sub2/b.cfg\nFirst file loads successfully. Upon return from the first invocation of pc.load(f), the current directory has been changed to the absolute path of the loaded file (/home/me/test/sub1).  Thus, subsequent paths given as relative references to the original user.dir fail to be found.\nIf this is the expected behavior, it should be documented.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_8", "fixed_files": ["org.apache.commons.configuration.AbstractFileConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 483, "bug_title": "DatabaseConfiguration unclosed resultset", "bug_description": "An error ocurred when invoking method DatabaseConfiguration.close because the resultset is not closed.\nERROR: [PoolConnection@19859608] close() : Got 1 unclosed ResultSets !\n\tuk.org.primrose.pool.core.PoolConnection.checkAndCloseResultSets(PoolConnection.java:46)\n\tuk.org.primrose.pool.core.PoolConnection.close(PoolConnection.java:133)\n\torg.apache.commons.configuration.DatabaseConfiguration.close(DatabaseConfiguration.java:596)\n\torg.apache.commons.configuration.DatabaseConfiguration.containsKey(DatabaseConfiguration.java:394)\n\torg.apache.commons.configuration.CompositeConfiguration.getProperty(CompositeConfiguration.java:190)\n\torg.apache.commons.configuration.AbstractConfiguration.resolveContainerStore(AbstractConfiguration.java:1160)\n\torg.apache.commons.configuration.AbstractConfiguration.getString(AbstractConfiguration.java:1035)\n\torg.apache.commons.configuration.AbstractConfiguration.getString(AbstractConfiguration.java:1018)", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_6", "fixed_version": "CONFIGURATION_1_9", "fixed_files": ["org.apache.commons.configuration.DatabaseConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 476, "bug_title": "Possible ClassCastException in CompositeConfiguration if a special in-memory configuration is used", "bug_description": "Some methods of CompositeConfiguration expect that the in-memory configuration is of type BaseConfiguration (a configuration of this type is created if no specific in-memory configuration was provided).\nHowever, there are constructors accepting an arbitrary Configuration object as in-memory configuration. If here a configuration is specified which does not extend BaseConfiguration, the casts performed by these methods will fail. The casts should only be performed if possible.\nThe following methods are affected:\n\nsetListDelimiter()\nsetDelimiterParsingDisabled()\n\n", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_7", "fixed_version": "CONFIGURATION_1_8", "fixed_files": ["org.apache.commons.configuration.TestCompositeConfiguration.java", "org.apache.commons.configuration.CompositeConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 471, "bug_title": "CompositeConfiguration delivers value from wrong child", "bug_description": "I create a composite configuration with two PropertyConfigurations: a user configuration and a default configuration, with the userConfiguration as inMemoryConfiguration.\nIf both configurations contain the same key, the value from defaultConfiguration is returned although the user configuration was added first. If no inMemoryConfiguration is specified, it&apos;s working ok:\nConfiguration defaultConf = new PropertiesConfiguration();\ndefaultConf.addProperty(\"prop1\", \"fromDefaultConfig\");\nConfiguration userConf = new PropertiesConfiguration();\nuserConf.addProperty(\"prop1\", \"fromUserConfig\");\nCompositeConfiguration noMemoryConfig = new CompositeConfiguration();\nnoMemoryConfig.addConfiguration(userConf);\nnoMemoryConfig.addConfiguration(defaultConf);\nSystem.out.println(\"noMemoryConfig: \" + noMemoryConfig.getString(\"prop1\")); // Shows \"fromUserConfig\" \nCompositeConfiguration withMemoryConfig = new CompositeConfiguration(userConf);\nwithMemoryConfig.addConfiguration(userConf);\nwithMemoryConfig.addConfiguration(defaultConf);\nSystem.out.println(\"withMemoryConfig: \" + withMemoryConfig.getString(\"prop1\")); // Shows \"fromDefaultConfig\"", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_7", "fixed_version": "CONFIGURATION_1_8", "fixed_files": ["org.apache.commons.configuration.TestCompositeConfiguration.java", "org.apache.commons.configuration.CompositeConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 474, "bug_title": "Lists do not seem to work whean loading INI files", "bug_description": "After loading an INI file with HierarchicalINIConfiguration, getList() and getStringArray() alaways return 1-size sets even if the parameter is in the right form in the file. To reproduce one only need to create a small INI file with the following content :\nkey=val1,val2,val3\nThe following code returns a 1-sized list :\n\n\n\nHierarchicalINIConfiguration c = new HierarchicalINIConfiguration();\n\nc.load(\"test.ini\");\n\nList<?> l = c.getList(\"val\");\n\n\n\n=> l = [\"val1,val2,val3\"] instead of [\"val1\",\"val2\",\"val3\"]\nI tried to change the list delimiter, but it didn&apos;t work. Maybe it&apos;s expected, but I haven&apos;t found anything in the doc.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_7", "fixed_version": "CONFIGURATION_1_8", "fixed_files": ["org.apache.commons.configuration.TestHierarchicalINIConfiguration.java", "org.apache.commons.configuration.HierarchicalINIConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 465, "bug_title": "OSGI issue - configuration depends on Jexl 1.0 which does not have OSGI release", "bug_description": "I updated to use commons-configuration 1.7 (from 1.6) and am running into the following resolution error under Eclipse Equinox OSGi framework.\n        An Import-Package could not be resolved. Caused by missing constraint in bundle <org.apache.commons.configuration_1.7.0>\n             constraint: <Import-Package: org.apache.commons.jexl; version=\"0.0.0\">\nAccording to the Import-Package header in 1.7&apos;s MANIFEST.MF, commons-configuration now has a required dependency on org.apache.commons.jexl.  Problem is, there is no OSGi enabled version of Apache Commons jexl that exports this package.\nIn jexl 2.0 the packages were renamed and org.apache.commons.jexl2 is exported.  The 1.x codeline, although using org.apache.commons.jexl, does not have the appropriate MANIFEST.MF headers to be used as an OSGi bundle.\nAs such, you cannot consume commons-configuration 1.7 in an OSGi environment.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_7", "fixed_version": "CONFIGURATION_1_8", "fixed_files": ["org.apache.commons.configuration.interpol.ExprLookup.java"], "label": 1, "es_results": []}, {"bug_id": 481, "bug_title": "Variable interpolation across files broken in 1.7 & 1.8", "bug_description": "With Commons Configuration 1.6, I was able to declare a variable in a properties file, and then reference it in a XML file using the ${myvar} syntax.\nFor example:\nglobal.properties:\n\nmyvar=abc\n\ntest.xml:\n\n\n\n<products>\n\n  <product name=\"abc\">\n\n    <desc>${myvar}-product</desc>\n\n  </product>\n\n</products>\n\n\n\nconfig.xml:\n\n\n\n<properties fileName=\"global.properties\"/>\n\n<xml fileName=\"test.xml\" config-name=\"test\">\n\n  <expressionEngine config-class=\"org.apache.commons.configuration.tree.xpath.XPathExpressionEngine\"/>\n\n</xml>\n\n\n\nWhen I try to retrieve the value, like so:\n\ncombinedConfig.getConfiguration(\"test\").configurationAt(\"products/product[@name=&apos;abc&apos;]\", true).getString(\"desc\")\n\nI get \"${myvar}-product\" instead of \"abc-product\".\nThis was working in Commons Configuration 1.6, but seems to be broken in 1.7 and 1.8.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_7", "fixed_version": "CONFIGURATION_1_9", "fixed_files": ["org.apache.commons.configuration.TestDefaultConfigurationBuilder.java", "org.apache.commons.configuration.DefaultConfigurationBuilder.java", "org.apache.commons.configuration.interpol.ConfigurationInterpolator.java"], "label": 1, "es_results": []}, {"bug_id": 487, "bug_title": "DataConfiguration.get() cannot handle a trivial conversion", "bug_description": "A call to DataConfiguration.get() eventually invokes the PropertyConverter.to() method. Here a number of supported data types are checked and corresponding conversions are done.\nHowever, the case that the value does not need to be converted at all - because it already has the expected type - is not taken into account. This is especially a problem for string values: there is not conversion to string, so the get() method fails even if the value is already a string.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_8", "fixed_version": "CONFIGURATION_1_9", "fixed_files": ["org.apache.commons.configuration.PropertyConverter.java", "org.apache.commons.configuration.TestDataConfiguration.java", "org.apache.commons.configuration.TestPropertyConverter.java"], "label": 1, "es_results": []}, {"bug_id": 495, "bug_title": "If delimiter parsing is disabled, adding of list properties to an AbstractConfiguration does not work correctly", "bug_description": "When calling addProperty() or setProperty() with a collection or an array as value it is expected that the single elements are added rather than the container. However, if delimiter parsing is disabled, this check is not performed, and the complex value is directly added. This causes strange effects, for instance when the configuration is saved: then the complex value is just converted to a string. Loading the same configuration again will then produce different values for those properties.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_8", "fixed_version": "CONFIGURATION_1_9", "fixed_files": ["org.apache.commons.configuration.HierarchicalConfiguration.java", "org.apache.commons.configuration.TestPropertiesConfiguration.java", "org.apache.commons.configuration.TestXMLConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 501, "bug_title": "XMLPropertyListConfiguration swallows exceptions about invalid date properties", "bug_description": "XMLPropertyListConfiguration.PListNode in its setDateValue() method just ignores parsing exceptions caused by invalid date property values. The values are not added to the newly created configuration object.\nIt makes sense that parsing is robust and error-tolerant. However, rather than simply swallowing exceptions, there should at least be a log output, so that users have a chance to determine that something went wrong during loading of a configuration.\nMaybe we can later add a strict mode which fails on loading configuration files with invalid properties.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_1_8", "fixed_version": "CONFIGURATION_1_9", "fixed_files": ["org.apache.commons.configuration.plist.XMLPropertyListConfiguration.java", "org.apache.commons.configuration.plist.TestXMLPropertyListConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 610, "bug_title": "Improve Configuration Variable Documentation", "bug_description": "The JavaDoc for Class PropertiesConfiguration  lists a very handy feature of Configuration, which is variable substitution.  The behavior can be seen in the example file on the JavaDoc (good).\nBut it&apos;s not in the preceding list of features.  It would be nice to have a little writeup describing the intended behavior:  What is supposed to happen if you have an undefined ref?  What about nested refs?  What about circular refs? \nAnd it was interesting to find that the substitution occurred during get time rather than load (unexpected), and that if one calls the wrong get() method then the substitution will not happen.  (Is that intended behavior?)  This should be called out in the description as well as the individual method JavaDocs.", "project": "Commons", "sub_project": "CONFIGURATION", "version": "CONFIGURATION_2_0_alpha1", "fixed_version": "CONFIGURATION_2_0", "fixed_files": ["org.apache.commons.configuration2.XMLConfiguration.java", "org.apache.commons.configuration2.INIConfiguration.java", "org.apache.commons.configuration2.PropertiesConfiguration.java"], "label": 1, "es_results": []}, {"bug_id": 132, "bug_title": "Incorrect Javadoc referencing org.apache.commons.csv.CSVFormat withQuote()", "bug_description": "Hi folks,\nthe Javadoc for org.apache.commons.csv.CSVFormat seems to be inconsistent since there are some references to a non-existing method withQuoteChar. The method name should be replaced by withQuote.", "project": "Commons", "sub_project": "CSV", "version": "CSV_1.0", "fixed_version": "1.1", "fixed_files": ["org.apache.commons.csv.CSVFormat.java"], "label": 1, "es_results": []}, {"bug_id": 140, "bug_title": "QuoteMode.NON_NUMERIC does not work with CSVPrinter.printRecords(ResultSet)", "bug_description": " CSVPrinter.printRecords(final ResultSet resultSet) retrieves all fields from the ResultSet using ResultSet.getString(), which makes QuoteMode.NON_NUMERIC unnecessarily quote numbers.\nResultSet.getObject() could be used instead. Any reason why it wasn&apos;t?", "project": "Commons", "sub_project": "CSV", "version": "CSV_1.0", "fixed_version": "1.1", "fixed_files": ["org.apache.commons.csv.CSVPrinter.java"], "label": 1, "es_results": []}, {"bug_id": 145, "bug_title": "CSVFormat.with* methods clear the header comments", "bug_description": "Some of the CSVFormat.with* methods clear the header comments by just passing null to the constructor. Using header comments works only with set at last.", "project": "Commons", "sub_project": "CSV", "version": "1.1", "fixed_version": "csv-1.2", "fixed_files": ["org.apache.commons.csv.CSVFormat.java", "org.apache.commons.csv.CSVPrinterTest.java"], "label": 1, "es_results": []}, {"bug_id": 156, "bug_title": "Incorrect Javadoc on QuoteMode.NONE", "bug_description": "The JavaDoc for QuoteMode.NONE says:\n\nNever quotes fields. When the delimiter occurs in data, it is preceded by the current escape character. If the escape character is not set, printing will throw an exception if any characters that require escaping are encountered.\nHowever, the CSVFormat.validate() method will throw an IllegalArgumentException if there is no escape character.\nI was expecting the documented behaviour. I guess I will need to use QuoteMode.MINIMAL instead.", "project": "Commons", "sub_project": "CSV", "version": "1.1", "fixed_version": "csv-1.2", "fixed_files": ["org.apache.commons.csv.QuoteMode.java"], "label": 1, "es_results": []}, {"bug_id": 169, "bug_title": "The null string should be case-sensitive when reading records", "bug_description": "The null string should be case-sensitive when reading records. In 1.2 the null string is checked with String.equalsIgnoreCase(), it should use plain equals()", "project": "Commons", "sub_project": "CSV", "version": "csv-1.2", "fixed_version": "csv-1.3", "fixed_files": ["org.apache.commons.csv.CSVParser.java"], "label": 1, "es_results": []}, {"bug_id": 170, "bug_title": "CSVFormat.MYSQL nullString should be \"\\N\"", "bug_description": "CSVFormat.MYSQL&apos;s nullString should be:\n\n\"\\N\"\n\n\n", "project": "Commons", "sub_project": "CSV", "version": "csv-1.2", "fixed_version": "csv-1.3", "fixed_files": ["org.apache.commons.csv.CSVFormat.java", "org.apache.commons.csv.CSVPrinter.java", "org.apache.commons.csv.CSVPrinterTest.java"], "label": 1, "es_results": []}, {"bug_id": 168, "bug_title": "CsvFormat.nullString should not be escaped", "bug_description": "Hello,\nUse case: I&apos;m generating MySQL dump files (text format) - for more details check this - http://dev.mysql.com/doc/refman/5.7/en/select-into.html. \nIssue: The value null is represented as \"\\N\". Also by default the escape char is &apos;\\N&apos;. The CsvPrinter.printAndEscape method will convert this value into \n\n\"\\\\N\"\n\nI suggest to modify the CsvPrinter in order to not escape the nullString value  - it should be written as it is. I can create a pull request if you want.\nI consider it a minor issue because it can be mitigated by making sure that the escape character is not a part of the nullString - however in my case it means that the LOAD commands should be modified accordingly.", "project": "Commons", "sub_project": "CSV", "version": "csv-1.2", "fixed_version": "csv-1.3", "fixed_files": ["org.apache.commons.csv.CSVFormat.java", "org.apache.commons.csv.CSVPrinter.java", "org.apache.commons.csv.CSVPrinterTest.java"], "label": 1, "es_results": []}, {"bug_id": 161, "bug_title": "Fix Javadoc to say CSVFormat with() methods return a new CSVFormat", "bug_description": "The quoting mode depends on the order of format declaration.", "project": "Commons", "sub_project": "CSV", "version": "csv-1.2", "fixed_version": "csv-1.3", "fixed_files": ["org.apache.commons.csv.CSVFormat.java"], "label": 1, "es_results": []}, {"bug_id": 167, "bug_title": "Comment line hides next record", "bug_description": "1. First CSV record after the comment line is not processed at all (record #2 and #7)\n2. Second/Third line after the first comment line are not recognized as comment lines (record #5 and #6)\nSee attached example!", "project": "Commons", "sub_project": "CSV", "version": "csv-1.2", "fixed_version": "csv-1.3", "fixed_files": ["org.apache.commons.csv.CSVRecord.java"], "label": 1, "es_results": []}, {"bug_id": 243, "bug_title": "SwappedDataInputStream readBoolean is inverted", "bug_description": "The method readBoolean in SwappedDataInputStream returns true when the byte is zero, false otherwise. In accordance with the contract in java.io.DataInput, true should indicate a non-zero byte. SwappedDataInputStream is for reading Little Endian formats, it should not change the boolean value of individual bytes.", "project": "Commons", "sub_project": "IO", "version": "IO_1_0", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.input.SwappedDataInputStreamTest.java", "org.apache.commons.io.input.SwappedDataInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 83, "bug_title": "Bug with 'df' command on AIX 5.3", "bug_description": "Reported via commons-owner@\n--------------------------------------------\nFrom: James Urie\nSent: Tuesday, July 11, 2006 8:51 AM\nTo: &apos;commons-dev-subscribe@jakarta.apache.org&apos;\nSubject: New AIX fix?\nHello,\nI had to change the \"commons-io\" code to allow for usage on AIX 5.3.\nAttached is the file with changes.\nThe long and short of the change is that the \"df\" command used in the\nFile System Utils classes requires\nA \"P\" to be added to the switch to enable \"POSIX\" capability.\nJames Urie", "project": "Commons", "sub_project": "IO", "version": "IO_1_2", "fixed_version": "IO_1_3", "fixed_files": ["org.apache.commons.io.FileSystemUtilsTestCase.java", "org.apache.commons.io.FileSystemUtils.java"], "label": 1, "es_results": []}, {"bug_id": 93, "bug_title": "FileSystemUtils needs to call Process.destroy() on exec'd processes", "bug_description": "Calling the FileSystemUtils.getFreeSpace() method multiple times (~3000) will generate an IOException with the following text: \"Too many open files\". Documentation from Sun says this problem is due to not destroying the java.lang.Process object returned from the System.exec() call.\nSome sample code I wrote confirms that calling destroy prevents this error from occurring.", "project": "Commons", "sub_project": "IO", "version": "IO_1_2", "fixed_version": "IO_1_3", "fixed_files": ["org.apache.commons.io.FileSystemUtilsTestCase.java", "org.apache.commons.io.FileSystemUtils.java"], "label": 1, "es_results": []}, {"bug_id": 84, "bug_title": "Many classes are limited to length of stream < 2 GB, and behave incorrectly on larger streams", "bug_description": "java int.MAX_VALUE is 2 GB. Classes that handle streams larger than 2 GB will behave incorrectly.\nFor example, see \nhttp://svn.apache.org/viewvc/jakarta/commons/proper/io/trunk/src/java/org/apache/commons/io/IOUtils.java?view=markup\nMethod: int copy(InputStream input, OutputStream output).\nThe correct method would be: long copy(InputStream input, OutputStream output).\nThis issue may affect many classes and routines.", "project": "Commons", "sub_project": "IO", "version": "IO_1_2", "fixed_version": "IO_1_3", "fixed_files": ["org.apache.commons.io.IOUtils.java", "org.apache.commons.io.IOUtilsCopyTestCase.java", "org.apache.commons.io.input.CountingInputStreamTest.java", "org.apache.commons.io.output.CountingOutputStreamTest.java", "org.apache.commons.io.input.CountingInputStream.java", "org.apache.commons.io.output.CountingOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 100, "bug_title": "FileUtils.touch should raise an IOException if I may not modify the file", "bug_description": "The documentation states, that FileUtils.touch implements the UNIX-touch command. However I may successfully FileUtils.touch files like /etc/passwd, which is not allowed on the she will as normal user. \nLooking at the implementation, you should propably raise an IOException if the returnvalue of `file.setLastModified(System.currentTimeMillis());` is `false`.", "project": "Commons", "sub_project": "IO", "version": "IO_1_2", "fixed_version": "IO_1_3", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 101, "bug_title": "The method EndianUtils.writeSwappedDouble() and EndianUtils.readSwappedDouble() do not match!", "bug_description": "Code:\npublic static void main(String[] args) {\n\t\tdouble[] tests = new double[] \n{34.345, -345.5645, 545.12, 10.043, 7.123456789123}\n;\n\t\tfor (int i = 0; i< tests.length ;i++) \n{\n\t\t\tbyte[] buffer = new byte[8];\t\t\t\n\t\t\tEndianUtils.writeSwappedDouble(buffer, 0, tests[i]);\n\t\t\tdouble val = EndianUtils.readSwappedDouble(buffer, 0);\n\t\t\tSystem.out.println(val);\t\n\t\t}\n\n}\nResult:\n34.344969482421874\n-345.5645\n545.11951171875\n10.043\n7.123456789123\nNote:\nIn my opinion the values shouldn&apos;t be changed at all.", "project": "Commons", "sub_project": "IO", "version": "IO_1_2", "fixed_version": "IO_1_3", "fixed_files": ["org.apache.commons.io.EndianUtils.java", "org.apache.commons.io.EndianUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 99, "bug_title": "FileCleaner thread never ends and because memory leak in AS", "bug_description": "FileCleaner opens a thread and no solution is given to the user to end it. So when an application is undeployed\nin an Application Server, a thread is still alive. The WebApp can&apos;t be undeployed and this results in a classloader\nleak that will cause an OutOfMemoryError.\nI think the API should be extended so that a user can end the thread. A better way would be to provide a class that\ncleans everything for commons IO.", "project": "Commons", "sub_project": "IO", "version": "IO_1_2", "fixed_version": "IO_1_3", "fixed_files": ["org.apache.commons.io.FileCleanerTestCase.java", "org.apache.commons.io.FileCleaner.java"], "label": 1, "es_results": []}, {"bug_id": 187, "bug_title": "FileSystemUtils.freeSpaceKb does not work with relative paths on Linux", "bug_description": "Calling FileSystemUtils.freeSpaceKb with \".\", \"./\", \"../\", \"../foo\" etc. will result in an empty string being passed to df.\nfreeSpaceKb calls FileNameUtils.normalize on the path which destroys relative paths.\nI don&apos;t see any need to normalize the path so the fix is simply to remove that call.", "project": "Commons", "sub_project": "IO", "version": "IO_1_2", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileSystemUtils.java"], "label": 1, "es_results": []}, {"bug_id": 112, "bug_title": "NPE in FileUtils.openOutputStream(File) when file has no parent in path.", "bug_description": "----Original Message----\nFrom: deng xinzi xinzi6388@gmail.com \nSent: Sunday, February 04, 2007 6:19 AM\nTo: commons-dev@jakarta.apache.org\nSubject: [bug]commons-io 1.3 FileUtils.openOutputStream(File file) NullPointException\nFileUtils.openOutputStream(File file)\nWhen the file = new File( \"abc.txt\" );\nThere will be a NullPointerException throw.\nBecause\nfile = new File(\"abc.txt\")\nfile.getParentFile() returns null.\nSo I suggest adding the null check code like this.\n            File parent = file.getParentFile();\n            if( parent Unable to render embedded object: File (= null ) {   // ADD THIS) not found.!!\n              if (parent.exists() == false) {\n                if (parent.mkdirs() == false) \n{\n                    throw new IOException(\"File &apos;\" + file + \"&apos; could not be\ncreated\");\n                }\n              }\n            }\n                                       Xinzi ...", "project": "Commons", "sub_project": "IO", "version": "IO_1_3", "fixed_version": "IO_1_3_1", "fixed_files": ["org.apache.commons.io.FileUtils.java", "org.apache.commons.io.FileUtilsTestCase.java", "org.apache.commons.io.testtools.FileBasedTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 113, "bug_title": "FileUtils.readFileToString is not static", "bug_description": "FileUtils.readFileToString isn&apos;t static.  It should be; since the constructor for FileUtils says \"Instances should NOT be constructed in standard programming\", this makes readFileToString unusable.  Right now I&apos;m using FileUtils.readBytesToByteArray(file).toString().", "project": "Commons", "sub_project": "IO", "version": "IO_1_3", "fixed_version": "IO_1_3_1", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 223, "bug_title": "IOUtils.copy Javadoc inconsistency (return -1 vs. throw ArithmeticException)", "bug_description": "The javadoc for IOUtils.copy says:\n\"Large streams (over 2GB) will return a bytes copied value of -1 after the copy has completed since the correct number of bytes cannot be returned as an int.\"\nbut then it says:\n\"Throws:\nArithmeticException - if the byte count is too large\"\nThis is an inconsistency since -1 is always returned if the byte count is too large. ArithmeticException is never actually thrown.\nSee IO-84 for discussion on solving the \"too large byte count\" problem.", "project": "Commons", "sub_project": "IO", "version": "IO_1_3", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.IOUtils.java"], "label": 1, "es_results": []}, {"bug_id": 117, "bug_title": "EndianUtils.readSwappedUnsignedInteger() may return a negative number", "bug_description": "Methods about reading unsigned-integer in class EndianUtils may return a negative number, due to casting int to long.\nCalculations with operator & etc. are under integer in these methods so its results are integer,\nthen implicit casting the results to long keeps its positive/negative sign.", "project": "Commons", "sub_project": "IO", "version": "IO_1_3_1", "fixed_version": "commons-io-1.3.2", "fixed_files": ["org.apache.commons.io.EndianUtilsTest.java", "org.apache.commons.io.EndianUtils.java"], "label": 1, "es_results": []}, {"bug_id": 136, "bug_title": "HexDump's use of static StringBuffers is not thread-safe", "bug_description": "HexDump has two private static dump() method&apos;s that alter static instances of StringBuffer:\ndump(long) modifies static StringBuffer variable _lbuffer returning _lbuffer \ndump(char) modifies static StringBuffer variable _cbuffer returning _cbuffer\nBoth these methods are called by the public static dump(byte[], long, OutputStream, int) method. Multiple threads calling the public dump method at the same time could cause these StringBuffer to contain mixed up data and result in a bug.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.3.2", "fixed_version": "commons-io-1.4", "fixed_files": ["org.apache.commons.io.HexDump.java"], "label": 1, "es_results": []}, {"bug_id": 147, "bug_title": "Deletion of orphaned Softlinks does not work", "bug_description": "If there is an orphaned softlink a -> b, but b does not exists anymore than the softlink will not be removed. This happens when you call FileUtils.deleteDirectory() and when the linked target b is deleted earlier than the link.\nThis is caused by the \"ugly\" file.exists() call in forceDelete()\nif (!file.exists()) {\n         throw new FileNotFoundException(\"File does not exist: \" + file);\n}\nif this check is not done, everything works as expected. I think this test is not neccessery, because file.delete will test this better.\nPlease discuss and change this.\nthanx\nStefan ", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.3.2", "fixed_version": "commons-io-1.4", "fixed_files": ["org.apache.commons.io.FileUtils.java", "org.apache.commons.io.FileUtilsTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 141, "bug_title": "Infinite loop on FileUtils.copyDirectory when the destination directory is within the source directory", "bug_description": "When you attempt to copy a directory and the destination directory is inside the source directory an inifinite loop occurs in the copyDirectory causing Commons-IO to create a folder w/o stopping until its reaches OS limitation.\nThis code will recreate the bug:\nFileUtils.copyDirectory(new File(\"C:\\\\temp\\\\test-io\\\\a.\"), new File(\"C:\\\\temp\\\\test-io\\\\a.\" + File.separator + new Date().getTime()));\nMake sure C:\\temp\\test-io\\a exists", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.3.2", "fixed_version": "commons-io-1.4", "fixed_files": ["org.apache.commons.io.FileUtils.java", "org.apache.commons.io.FileUtilsTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 163, "bug_title": "FileUtils.toURLs() uses deprecated (and bad) method of conversion to URL", "bug_description": "The method FileUtils.toURLs() uses the following method to convert from File to URL:\n    File.toURL();\nThis method has scary warnings that it&apos;s a bad way to do the conversion because characters will not be escaped as required in URL strings.  In Java 1.6, this method has actually been deprecated.  All recent JDK versions recommend instead using:\n    File.toURI().toURL();\nas the URI code will properly perform the escaping.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileUtils.java", "org.apache.commons.io.FileUtilsTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 167, "bug_title": "Fix case-insensitive string handling", "bug_description": "Case-insensitive operations are currently platform-dependent, please see Common Bug #3 for details.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.IOCase.java", "org.apache.commons.io.IOCaseTestCase.java", "org.apache.commons.io.FilenameUtilsWildcardTestCase.java", "org.apache.commons.io.FilenameUtils.java", "org.apache.commons.io.FileSystemUtilsTestCase.java", "org.apache.commons.io.FileSystemUtils.java"], "label": 1, "es_results": []}, {"bug_id": 180, "bug_title": "LineIterator documentation ", "bug_description": "In the Javadoc for rg.apache.commons.io.LineIterator (in Commons IO 1.4),\nthis code snippet is incorrect:  the last instance of \"iterator\" should be\n\"it\".\n  LineIterator it = FileUtils.lineIterator(file, \"UTF-8\");\n   try {\n     while (it.hasNext()) \n{\n\n       String line = it.nextLine();\n\n       /// do something with line\n\n     }\n   } finally \n{\n\n     LineIterator.closeQuietly(iterator);\n\n   }", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.LineIterator.java"], "label": 1, "es_results": []}, {"bug_id": 161, "bug_title": "FileCleaningTrackerTestCase hangs", "bug_description": "The following code in FileCleaningTrackerTestCase never exits, hanging the whole compilation/testing process:\n    private void waitUntilTrackCount() {\n        while (theInstance.getTrackCount() != 0) {\n            int total = 0;\n            while (theInstance.getTrackCount() != 0) \n{\n                byte[] b = new byte[1024 * 1024];\n                b[0] = (byte) System.currentTimeMillis();\n                total = total + b[0];\n                System.gc();\n            }\n        }\n    }\nIt is clear that in theory this code might loop forever, as the allocation of the byte arrays might never unleash a garbage collection complete enough to deallocate all marker objects, so to bring the track count to zero. Believe me, it&apos;s not only theory .", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileCleaningTrackerTestCase.java", "org.apache.commons.io.FileCleaningTracker.java"], "label": 1, "es_results": []}, {"bug_id": 202, "bug_title": "NotFileFilter documentation is incorrect", "bug_description": "The documentation for NotFileFilter (http://commons.apache.org/io/api-release/index.html) incorrectly states that it, \"Checks to see if both filters are true.\"  It looks to be the result of a hasty copy-and-paste from an old version of AndFileFilter (http://svn.apache.org/viewvc/commons/proper/io/trunk/src/java/org/apache/commons/io/filefilter/AndFileFilter.java?revision=140357&view=markup).  It should say something like, \"Returns the logical NOT of the underlying filter&apos;s return value for the same arguments.\"\nPatch is attached.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.filefilter.NotFileFilter.java"], "label": 1, "es_results": []}, {"bug_id": 212, "bug_title": "Incorrect ProxyInputStream.skip() javadoc", "bug_description": "The ProxyInputStream.skip() method documents the return value as \"the number of bytes to skipped or -1 if the end of stream\" when the underlying InputStream.skip() method returns \"the actual number of bytes skipped\", i.e. never -1.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.input.ProxyInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 231, "bug_title": "FileUtils generate wrong exception message in isFileNewer method", "bug_description": "\n\n\nif (!reference.exists()) {\n\n    throw new IllegalArgumentException(\"The reference file &apos;\" + file + \"&apos; doesn&apos;t exist\");\n\n}\n\n\n\nIf second argument file does not exist isFileNewer method generates exception with message about first argument file does not exist.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileUtils.java", "org.apache.commons.io.FileUtilsTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 207, "bug_title": "Race condition in forceMkdir", "bug_description": "If two processes or threads call forceMkdir() with the same directory there is a chance that one will throw an IOException even though a directory was correctly created (by the other process or thread). ", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 216, "bug_title": "Delete files quietly when an exception is thrown during initialization", "bug_description": "LockableFileWriter fails to report lock file deletion failure - it calls lockFile.delete() several times but fails to check the return code.\nN.B. IIRC, file.delete() returns false if there was no file to delete, so any fix needs to take this into account.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.output.LockableFileWriter.java"], "label": 1, "es_results": []}, {"bug_id": 168, "bug_title": "Symbolic links (symlinks) followed when deleting directory.", "bug_description": "If &apos;dlink&apos; is a symbolic link to a directory &apos;dir&apos;, and FileUtils.forceDelete is called on dlink, then here is what happens:\n1) the contents of &apos;dir&apos; are emptied (the link is followed).\n2) &apos;dir&apos; continues to exist (but is empty).\n3) &apos;dlink&apos; is removed.\nThe correct behavior is to simply remove &apos;dlink&apos; without following it and thus without altering the contents of &apos;dir&apos; (or &apos;dir&apos; itself).", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 217, "bug_title": "FileUtils.copyDirectoryToDirectory makes infinite loops", "bug_description": "When using FileUtils.copyDirectoryToDirectory, infinite loops has occurred.\n detail \nsrc directory :  D:\\a\ndest directory : D:\\a\ncall : FileUtils.copyDirectoryToDirectory(new File(\"D:\\a\"), new File(\"D:\\a\"));\nexpected result : directory D:\\a\\a will be created\nactual result      : D:\\a\\a\\a\\a.......   was created\ni guess FileUtils.copyDirectoryToDirectory causes of this result.\n(\"destDir.mkdir()\" is done before \"srcDir.listFiles()\")\nam i calling wrong method?\nthank you.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileUtils.java", "org.apache.commons.io.FileUtilsTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 166, "bug_title": "Fix URL decoding in FileUtils.toFile()", "bug_description": "The sequence \"%2520\" should decode to \"%20\".", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileUtilsTestCase.java", "org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 246, "bug_title": "FilenameUtils.wildcardMatch gives incorrect results", "bug_description": "This wildcard pattern \"*?\" does not match correctly. The command:\nSystem.out.println(FilenameUtils.wildcardMatch(\"aaa\", \"*?\"));\nprints out \"false\", even though it matches. The wildcard mask is a bit unusal, but not incorrect. It should match any input with at least one character.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FilenameUtilsWildcardTestCase.java", "org.apache.commons.io.FilenameUtils.java"], "label": 1, "es_results": []}, {"bug_id": 185, "bug_title": "FileSystemUtils.freeSpaceWindows blocks", "bug_description": "Hi,\nin my Application I have the problem that FileSystemUtils.freeSpaceWindows works properly for hours and then the function call blocks and does not return anymore. Any Ideas? Thanks.\nThe stacktrace:\n-------------\n\"Thread-16\" daemon prio=5 tid=33 RUNNABLE\n    java.io.FileInputStream.readBytes(Native Method)\n    java.io.FileInputStream.read(Unknown Source)\n    java.io.BufferedInputStream.read1(Unknown Source)\n    java.io.BufferedInputStream.read(Unknown Source)\n    sun.nio.cs.StreamDecoder.readBytes(Unknown Source)\n    sun.nio.cs.StreamDecoder.implRead(Unknown Source)\n    sun.nio.cs.StreamDecoder.read(Unknown Source)\n    java.io.InputStreamReader.read(Unknown Source)\n    java.io.BufferedReader.fill(Unknown Source)\n    java.io.BufferedReader.readLine(Unknown Source)\n    java.io.BufferedReader.readLine(Unknown Source)\n    org.apache.commons.io.FileSystemUtils.performCommand(FileSystemUtils.java:413)\n    org.apache.commons.io.FileSystemUtils.freeSpaceWindows(FileSystemUtils.java:225)\n    org.apache.commons.io.FileSystemUtils.freeSpaceOS(FileSystemUtils.java:194)\n    org.apache.commons.io.FileSystemUtils.freeSpaceKb(FileSystemUtils.java:166)\n-------------\nRegards,\nMartin", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileSystemUtilsTestCase.java", "org.apache.commons.io.FileSystemUtils.java"], "label": 1, "es_results": []}, {"bug_id": 190, "bug_title": "FileUtils.copyDirectory preserves destination subdirectories, rather than overwriting with the source subdirectories", "bug_description": "When using FileUtils.copyDirectory to copy directories with subdirectories, the source will overwrite all files that exist in the destination directory, but not the subdirectories themselves. The files inside the subdirectories will be overwritten. The only difference that I&apos;ve noticed thus far is that this preserves the old file dates of the subdirectories rather than using the dates from the source or the current date, if preserveFileDate is set to &apos;false.&apos;", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileUtils.java", "org.apache.commons.io.FileUtilsTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 248, "bug_title": "getFullPathNoEndSeparator return empty while path is one level directory", "bug_description": "the getFullPathNoEndSeparator method in FilenameUtils.java (Revision 736890) \nif filename=\"/\" return \"/\" <<==right\nif filename=\"/abc\" return empty <<==bug\nif filename=\"/abc/xyz\" return \"/abc\" <<==right \n\n\n\n885 \tint index = indexOfLastSeparator(filename);\n\n886 \tif (index < 0) {\n\n887 \t\treturn filename.substring(0, prefix);\n\n888 \t}\n\n889 \tint end = index + (includeSeparator ? 1 : 0);\n\n================\n\n                if(end==0) return \"/\";\n\n>>>>>>>>>>>>>>>>\n\n890 \treturn filename.substring(0, end);\n\n\n", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FilenameUtilsTestCase.java", "org.apache.commons.io.FilenameUtils.java"], "label": 1, "es_results": []}, {"bug_id": 209, "bug_title": "FileSystemUtils.freeSpaceKb fails to return correct size for a windows mount point", "bug_description": "FileSystemUtils.freeSpaceKb fails to return correct result for a NTFS mount point or junction.\nSuppose I have a NTFS partition mounted at C:\\Data\\partition1.\nNow assume that the free space on Partition mounted as C: is 1GB and that mounted on \"C:\\Data\\partition1\" is 2GB. A call to FileSystemUtils.freeSpaceKb(\"C:\\Data\\partition1\") will return the free space on C: and not on \"C:\\Data\\partition1\".\nThis is because while running the \"dir /-c\" with the given path, the code just retains first 2 chars i.e. for any path under \"C:\\blah\\de\\blah\", \"dir /-c\" will be called with \"C:\" which will return incorrect result.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.FileSystemUtils.java", "org.apache.commons.io.FileSystemUtilsTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 201, "bug_title": "CountingInputStream/CountingOutputStream only partially synchronized", "bug_description": "CountingInputStream is only partially synchronized.\nThe count is not synchronized when it is updated in read operations, so is not guaranteed to be published correctly\nThe synchronization could be removed without loss of functionality.\nNot sure it makes sense to share a stream between threads anyway, as the underlying stream is unlikely to be thread-safe.\nIf only one thread reads the stream, then the count field could be made volatile.\nThis would allow other threads to read the count safely.", "project": "Commons", "sub_project": "IO", "version": "commons-io-1.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.input.CountingInputStream.java", "org.apache.commons.io.output.CountingOutputStream.java"], "label": 1, "es_results": []}, {"bug_id": 258, "bug_title": "XmlStreamReader consumes the stream during encoding detection", "bug_description": "XmlStreamReader reads the underlying InputStream to try and detect the encoding. However once that process is done the bytes read from the stream should still be available to be read - this was accidentally broken in r1004882 by creating the underlying reader with the original InputStream, rather than the wrapped streams used to detect encoding.", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.0", "fixed_version": "commons-io-2.0.1", "fixed_files": ["org.apache.commons.io.input.XmlStreamReaderTest.java", "org.apache.commons.io.input.XmlStreamReader.java"], "label": 1, "es_results": []}, {"bug_id": 264, "bug_title": "FileUtils.moveFile() JavaDoc should specify FileExistsException thrown", "bug_description": "FileUtils.moveFile() JavaDoc does specify the behaviour of the method in a case when the destFile exists. It would be helpful to know from the JavaDoc that in such case an exception is thrown as it&apos;s not immediately obvious.", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.0", "fixed_version": "commons-io-2.1", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 260, "bug_title": "ClassLoaderObjectInputStream does not handle Proxy classes", "bug_description": "ObjectInputSteam has 2 methods that need to be overloaded for proper behavior in this case.\nresolveClass is ok, but resolveProxyClass doesn&apos;t attempt to look in the passed class loader to resolve the interfaces.", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.0.1", "fixed_version": "commons-io-2.1", "fixed_files": ["org.apache.commons.io.input.ClassLoaderObjectInputStream.java", "org.apache.commons.io.input.ClassLoaderObjectInputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 263, "bug_title": "FileSystemUtils.freeSpaceKb throws exception for Windows volumes with no visible files.", "bug_description": "Doing a FileSystemUtils.freeSpaceKb(\"D:/\") where the drive (in this example d:) is an empty drive (A drive with no non-hidden files on it yet) results in an exception being thrown.\n     \"Command line returned OS error code &apos;1&apos; for command [cmd.exe, /C, dir /-c \"D:\\\"]\nPerhaps it could do a \"dir /a /-c\" to work in more cases? (Since hidden file \"System Volume Information\" will usually be available)", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.0.1", "fixed_version": "commons-io-2.1", "fixed_files": ["org.apache.commons.io.FileSystemUtilsTestCase.java", "org.apache.commons.io.FileSystemUtils.java"], "label": 1, "es_results": []}, {"bug_id": 274, "bug_title": "Tailer returning partial lines when reaching EOF before EOL", "bug_description": "As reported here: http://mail-archives.apache.org/mod_mbox/commons-user/201105.mbox/%3cBANLkTim6hA-xGjn8cA6FfcPkVa6ax6KGag@mail.gmail.com%3e", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.0.1", "fixed_version": "commons-io-2.1", "fixed_files": ["org.apache.commons.io.input.TailerTest.java", "org.apache.commons.io.input.Tailer.java"], "label": 1, "es_results": []}, {"bug_id": 277, "bug_title": "ReaderInputStream enters infinite loop when it encounters an unmappable character", "bug_description": "The ReaderInputStream.read(byte[] b, int off, int len) method enters an infinite loop when its CharsetEncoder encounters an unmappable character in the input buffer.\nWhen its CharsetEncoder encounters an unmappable character, the value of CoderResult lastCoderResult.isUnmappable() == true, and Reader.read() is not invoked on the underlying Reader ever again.\nAttaching source file that reproduces this behavior.\nOne fix to consider is to call CharsetEncoder.onUnmappableCharacter(CodingErrorAction) in the ReaderInputStream constructor with a value other than the default CodingErrorAction.REPORT. e.g.:\npublic ReaderInputStream(Reader reader, Charset charset, int bufferSize) {\n            this.reader = reader;\n            encoder = charset.newEncoder();\n            encoder.onUnmappableCharacter(CodingErrorAction.REPLACE);\n...\nBy replacing the unmappable character with encoder&apos;s default replacement character, this effectively prevents the infinite loop from occurring. I&apos;m not sure if that&apos;s the ideal behavior, but it seems fairly consistent with what org.apache.commons.io.output.WriterOutputStream does.", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.0.1", "fixed_version": "commons-io-2.1", "fixed_files": ["org.apache.commons.io.input.ReaderInputStream.java", "org.apache.commons.io.input.ReaderInputStreamTest.java", "org.apache.commons.io.output.WriterOutputStream.java", "org.apache.commons.io.input.XmlStreamReaderTest.java"], "label": 1, "es_results": []}, {"bug_id": 299, "bug_title": "getPrefixLength returns null if filename has leading slashes", "bug_description": "Situation:\nFilenameUtils.getPrefixLength is used in FilenameUtils.doNormalize.\nFilenameUtils.normalize(\"////I don&apos;t want to become null!\") returns null.\nProblem:\nExpected was: \"I don&apos;t want to become null!\"\nThe method FilenameUtils.getPrefixLength returns -1 for the mentioned string.\nThe root problem is found in following lines of code:\nFilenameUtils.getPrefixLength\n\n\n...\n\n                int posUnix = filename.indexOf(UNIX_SEPARATOR, 2);\n\n                int posWin = filename.indexOf(WINDOWS_SEPARATOR, 2);\n\n                if ((posUnix == -1 && posWin == -1) || posUnix == 2 || posWin == 2) {\n\n                    return -1;\n\n                }\n\n...\n\n\n\nSolution:\nAll leading slashes should be ignored at all, but considering the rest of the string.", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.0.1", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.FilenameUtilsTestCase.java", "org.apache.commons.io.FilenameUtils.java"], "label": 1, "es_results": []}, {"bug_id": 428, "bug_title": "BOMInputStream.skip returns wrong count if stream contains no BOM", "bug_description": "If the skip method of BOMInputStream is used on a stream without a BOM, skip returns the wrong number of bytes (n - max(BOM-length)). This can lead to problems if the return value is evaluated for example from guava ByteStreams.skipFully.\nBomTest.java\n\n\npublic class BomTest {\n\n\n\n\tprivate static InputStream createInputStream(boolean addBOM) {\n\n\t\tByteBuffer bb = ByteBuffer.allocate(64);\n\n\t\tif (addBOM) {\n\n\t\t\t// UTF-8 BOM\n\n\t\t\tbb.put(new byte[] { (byte) 0xEF, (byte) 0xBB, (byte) 0xBF });\n\n\t\t}\n\n\t\tbb.put((byte) 0x31);\n\n\t\tbb.put((byte) 0x32);\n\n\t\tbb.put((byte) 0x33);\n\n\t\treturn new ByteArrayInputStream(bb.array());\n\n\t}\n\n\t\n\n\tpublic static void main(String[] args) throws IOException {\n\n\t\tBOMInputStream is1 = new BOMInputStream(createInputStream(true));\n\n\t\tassertEquals(2, is1.skip(2));\n\n\t\tassertEquals((byte) 0x33, is1.read());\n\n\t\t\n\n\t\tBOMInputStream is2 = new BOMInputStream(createInputStream(false));\n\n\t\tassertEquals(2, is2.skip(2)); // fails here - skip returns 0\n\n\t\tassertEquals((byte) 0x33, is2.read());\n\n\t}\n\n\t\n\n}\n\n\n\nI catched this bug in 2.0.1, but as far as I can see on the source 2.5 is still affected.\nI suggest the following change to the skip method:\nBOMInputStream.java\n\n\n    public long skip(long n) throws IOException {\n\n    \tint skipped = 0;\n\n        while ((n > skipped) && (readFirstBytes() >= 0)) {\n\n            skipped++;\n\n        }\n\n        return in.skip(n - skipped) + skipped;\n\n    }\n\n\n", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.0.1", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.input.BOMInputStream.java", "org.apache.commons.io.input.BOMInputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 302, "bug_title": "ArrayIndexOutOfBoundsException in BOMInputStream when reading a file without BOM multiple times", "bug_description": "Resetting the BOMInputStream doesn&apos;t reset the fbLength member variable. This causes fbLength to grow bigger than the firstBytes array (when the file doesn&apos;t contain a BOM), which leads to an ArrayIndexOutOfBoundsException in the readFirstBytes method.\nThe attached test case reveals the problem.", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.1", "fixed_version": "2.2", "fixed_files": ["org.apache.commons.io.input.BOMInputStreamTest.java", "org.apache.commons.io.input.BOMInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 304, "bug_title": "The second constructor of Tailer  class does not pass 'delay' to the third one", "bug_description": "Here is the second contructor of Tailer class:\n\n\n\n    public Tailer(File file, TailerListener listener, long delay) {\n\n        this(file, listener, 1000, false);\n\n    }\n\n\n", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.1", "fixed_version": "2.2", "fixed_files": ["org.apache.commons.io.input.Tailer.java"], "label": 1, "es_results": []}, {"bug_id": 298, "bug_title": "Various methods of class 'org.apache.commons.io.FileUtils' incorrectly suppress 'java.io.IOException's.", "bug_description": "", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.1", "fixed_version": "2.2", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 306, "bug_title": "ReaderInputStream#read(byte[] b, int off, int len) should always return 0 for length == 0", "bug_description": "The method read(byte[] b, int off, int len) should always return 0 if the requested length == 0, even if the stream is empty or at EOF, as that is how the overridden java.io.InputStream method is documented to behave.", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.1", "fixed_version": "2.2", "fixed_files": ["org.apache.commons.io.input.ReaderInputStream.java", "org.apache.commons.io.input.ReaderInputStreamTest.java"], "label": 1, "es_results": []}, {"bug_id": 307, "bug_title": "ReaderInputStream#read(byte[] b, int off, int len) should check for valid parameters", "bug_description": "If the buffer is null, the method should throw NPE immediately (rather than letting it occur later)\nIf the offset or length are < 0 or would overflow the buffer, then throw IndexOutOfBoundsException with details of the values.", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.1", "fixed_version": "2.2", "fixed_files": ["org.apache.commons.io.input.ReaderInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 453, "bug_title": "Regression in FileUtils.readFileToString from 2.0.1", "bug_description": "FileUtils.readFileToString has changed it&apos;s behavior to make a call from IOUtils.toByteArray(in) to IOUtils.toByteArray(in, file.length()) in FileUtils.readFileToString. This is a regression because if the file.length = 0, then it will return 0. According to the javadocs for File#length, it is possible to return 0 if it is a System dependent entities, so even though the File.length might return 0, the stream is still open and  \nSteps to reproduce (Ubuntu):\n1. Execute nohup sleep 10000 & in a terminal, and get the process id of the sleep command (ps -ef | grep sleep).\n2. Call FileUtils.readFileToString(new File(\"/proc/$PID/environ\")); where $PID is the process ID from step 1.\nYou will notice that in 2.0.1 it returns several elements, however in 2.1 it will return nothing.\nSee nicolas de loof&apos;s comment in https://github.com/apache/commons-io/commit/53a40a6d9dcaaa616b404255406edc30fe2d524c.", "project": "Commons", "sub_project": "IO", "version": "commons-io-2.1", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 329, "bug_title": "FileUtils.writeLines uses unbuffered IO", "bug_description": "FileUtils.writeLines looks like next:\n out = openOutputStream(file, append);\n IOUtils.writeLines(lines, lineEnding, out, encoding);\nopenOutputStream opens plain FileOutputStream without any buffering and IOUtils.writeLines does not add abyt buffering. This means each line require 2 write syscalls (one for the line and one for line separator). This makes call very slow, especially for short lines", "project": "Commons", "sub_project": "IO", "version": "2.3", "fixed_version": "2.4", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 331, "bug_title": "BOMInputStream wrongly detects UTF-32LE_BOM files as UTF-16LE_BOM files in method getBOM()", "bug_description": "Hi,\nThe BOMInputStream works great for most UTF encoded files when detecting Byte Order Marks. However, if a file is UTF-32LE encoded with BOM the class takes it as UTF-16LE instead. This is not expected behavior.\nThe problem comes from method getBOM(). And the first two bytes for UTF-16LE and UTF-32LE are the same, which might be the root because of the problem.\nThe following lists the bytes for UTF encodings for reference. The content is a BOM followed by letter &apos;t&apos;.\n\n\nEncoding\nByte 1\nByte 2\nByte 3\nByte 4\n\n\n\n\n\n\nUTF8\nEF\nBB\nBF\n74\n\n\n\n\n\n\nUTF16-LE\nFF\nFE\n74\n00\n\n\n\n\n\n\nUTF16-BE\nFE\nFF\n00\n74\n\n\n\n\n\n\nUTF32-LE\nFF\nFE\n00\n00\n74\n00\n00\n00\n\n\nUTF32-BE\n00\n00\nFE\nFF\n00\n00\n00\n74\n\n\nI personally used the following code to work around this problem at the moment. Hope it helps.\n\n\n\n\tprivate void detectBOM(InputStream in) throws IOException{\n\n\t\tList<ByteOrderMark> all=availableBOMs();\n\n\t\tint max=0;\n\n        for (ByteOrderMark bom : all) {\n\n            max = Math.max(max, bom.length());\n\n        }\n\n\t\tbyte[] firstBytes=new byte[max];\n\n\t\tfor (int i = 0; i < max; i++) {\n\n\t\t\tfirstBytes[i]=(byte) in.read();\n\n\t\t\tSystem.out.print(Integer.toHexString(firstBytes[i] & 0xff).toUpperCase()+\" \");\n\n\t\t}\n\n\t\t\n\n\t\tboolean found=false;\n\n\t\tfor (int j = max; j >1; j--) {\n\n\t\t\tbyte[] _copy=Arrays.copyOf(firstBytes, j);\n\n\t\t\tfor (ByteOrderMark mark : all) {\n\n\t\t\t\tfound=Arrays.equals(_copy, mark.getBytes());\n\n\t\t\t\tif (found) {\n\n\t\t\t\t\tSystem.out.println(\"\\nBOM is: \"+mark.getCharsetName());\n\n\t\t\t\t\tbreak;\n\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t\tif (found) break;\n\n\t\t}\n\n\t}\n\n\t\n\n\tprivate static List<ByteOrderMark> availableBOMs(){\n\n\t\tList<ByteOrderMark> all=new ArrayList<ByteOrderMark>();\n\n\t\tall.add(ByteOrderMark.UTF_8);\n\n\t\tall.add(ByteOrderMark.UTF_16BE);\n\n\t\tall.add(ByteOrderMark.UTF_16LE);\n\n\t\tall.add(ByteOrderMark.UTF_32BE);\n\n\t\tall.add(ByteOrderMark.UTF_32LE);\n\n\t\treturn all;\n\n\t}\n\n\n", "project": "Commons", "sub_project": "IO", "version": "2.3", "fixed_version": "2.4", "fixed_files": ["org.apache.commons.io.input.compatibility.XmlStreamReaderUtilitiesCompatibilityTest.java", "org.apache.commons.io.input.BOMInputStream.java", "org.apache.commons.io.input.XmlStreamReaderUtilitiesTest.java", "org.apache.commons.io.input.XmlStreamReaderTest.java", "org.apache.commons.io.input.XmlStreamReader.java", "org.apache.commons.io.input.compatibility.XmlStreamReader.java"], "label": 1, "es_results": []}, {"bug_id": 336, "bug_title": "Yottabyte (YB) incorrectly defined in FileUtils", "bug_description": "In FileUtils, a yottabyte is currently defined as follows:\npublic static final BigInteger ONE_YB = ONE_ZB.multiply(BigInteger.valueOf(ONE_EB));\nI believe this should be:\npublic static final BigInteger ONE_YB = ONE_ZB.multiply(BigInteger.valueOf(ONE_KB));", "project": "Commons", "sub_project": "IO", "version": "2.3", "fixed_version": "2.4", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 355, "bug_title": "IOUtils copyLarge() and skip() methods are performance hogs", "bug_description": "IOUtils.skip(InputStream, long) and IOUtils.skip(Reader, long) have the worst possible performance as they always use read() on the input instead of using skip(). In many cases, using skip() from a subclass of InputStream is much faster than read(), as the skip() can be implemented via a disk seek.\nThe IOUtils.skip() methods are also used in the copyLarge() methods that involve a skip.\nCase in point: I have observed this performance degradation with Java 7 on Windows 7. A series of consecutive copyLarge() invocations on a large file on disk that involved skips changed my performance from 30 secs as my baseline to 10 minutes after starting to use IOUtils.copyLarge().", "project": "Commons", "sub_project": "IO", "version": "2.3", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.IOUtils.java"], "label": 1, "es_results": []}, {"bug_id": 323, "bug_title": "What should happen in FileUtils.sizeOf[Directory] when an overflow takes place?", "bug_description": "FileUtils.sizeOf[Directory] adds longs. What should happen when an overflow happens?", "project": "Commons", "sub_project": "IO", "version": "2.3", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 354, "bug_title": "Commons IO Tailer does not respect UTF-8 Charset", "bug_description": "I just realized there is a defect in the source code of \"org.apache.commons.io.input.Tailer.java\". Basically, the current implementation does not work for multi-byte encoded files. See the following snippet,\n448    private long readLines(RandomAccessFile reader) throws IOException {\n449        StringBuilder sb = new StringBuilder();\n450\n451        long pos = reader.getFilePointer();\n452        long rePos = pos; // position to re-read\n453\n454        int num;\n455        boolean seenCR = false;\n456        while (run && ((num = reader.read(inbuf)) != -1)) {\n457            for (int i = 0; i < num; i++) {\n458                byte ch = inbuf[i];\n459                switch (ch) {\n460                case &apos;\\n&apos;:\n461                    seenCR = false; // swallow CR before LF\n462                    listener.handle(sb.toString());\n463                    sb.setLength(0);\n464                    rePos = pos + i + 1;\n465                    break;\n466                case &apos;\\r&apos;:\n467                    if (seenCR) \n{\n\n468                        sb.append(&apos;\\r&apos;);\n\n469                    }\n470                    seenCR = true;\n471                    break;\n472                default:\n473                    if (seenCR) \n{\n\n474                        seenCR = false; // swallow final CR\n\n475                        listener.handle(sb.toString());\n\n476                        sb.setLength(0);\n\n477                        rePos = pos + i + 1;\n\n478                    }\n479                    sb.append((char) ch); // add character, not its ascii value\n480                }\n481            }\n482\n483            pos = reader.getFilePointer();\n484        }\n485\n486        reader.seek(rePos); // Ensure we can re-read if necessary\n487        return rePos;\n488    }\nAt line 479, the conversion of byte to char type breaks the encoding.", "project": "Commons", "sub_project": "IO", "version": "2.3", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.input.TailerTest.java", "org.apache.commons.io.input.Tailer.java"], "label": 1, "es_results": []}, {"bug_id": 197, "bug_title": "BoundedInputStream", "bug_description": "Apache Jackrabbit has an interesting InputStream implementation that reads up to a specified amount of bytes from an underlying stream, and then acts as if the end of the stream was reached:\nhttp://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/BoundedInputStream.java\nSimilar classes are also found in other projects:\nhttp://svn.mucommander.com/cgi-bin/viewvc.cgi/trunk/source/com/mucommander/io/BoundedInputStream.java?content-type=text%2Fplain&view=co\nhttps://www.sunspotworld.com/docs/Purple/javadoc/com/sun/spot/peripheral/BoundedInputStream.html", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "commons-io-2.0", "fixed_files": ["org.apache.commons.io.input.BoundedInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 362, "bug_title": "IOUtils.contentEquals* methods returns false if input1 == input2, should return true", "bug_description": "The fix should be relatively simple, just add an identity check to the beginning of the method:\nif(is1 == is2) \n{\n\n  return true;\n\n}\n\nThe methods affected are:\n\norg.apache.commons.io.IOUtils.contentEquals(InputStream, InputStream)\norg.apache.commons.io.IOUtils.contentEquals(Reader, Reader)\norg.apache.commons.io.IOUtils.contentEqualsIgnoreEOL(Reader, Reader)\n\n", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.IOUtilsTestCase.java", "org.apache.commons.io.IOUtils.java"], "label": 1, "es_results": []}, {"bug_id": 374, "bug_title": "WildcardFileFilter ctors should not use null to mean IOCase.SENSITIVE when delegating to other ctors", "bug_description": "WildcardFileFilter ctors should not use null to mean IOCase.SENSITIVE when delegating to other ctors.\nJust because null happens to mean case-sensitive, does not mean that internal calls to ctors should use that feature. It makes the code harder to read.", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.filefilter.WildcardFileFilter.java"], "label": 1, "es_results": []}, {"bug_id": 343, "bug_title": "org.apache.commons.io.comparator Javadoc is inconsistent with real code", "bug_description": "Package org.apache.commons.io.comparator has a lot of inconsistent JavaDocs. \nFor example this class org.apache.commons.io.comparator.NameFileComparator\nhttp://svn.apache.org/viewvc/commons/proper/io/trunk/src/main/java/org/apache/commons/io/comparator/NameFileComparator.java?view=markup\nhas JavaDocs\n List<File> list = ...\n NameFileComparator.NAME_COMPARATOR.sort(list); ....\n File[] array = ...\n NameFileComparator.NAME_INSENSITIVE_REVERSE.sort(array);\nbut this will not work because all static members of NameFileComparator declared as Comparator<File> for example \n public static final Comparator<File> NAME_REVERSE = new ReverseComparator(NAME_COMPARATOR);\npublic static final Comparator<File> NAME_INSENSITIVE_REVERSE = new ReverseComparator(NAME_INSENSITIVE_COMPARATOR);\nand Comparator class doesn&apos;t have the sort() method.", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.comparator.SizeFileComparator.java", "org.apache.commons.io.comparator.PathFileComparator.java", "org.apache.commons.io.comparator.LastModifiedFileComparator.java", "org.apache.commons.io.comparator.NameFileComparator.java", "org.apache.commons.io.comparator.CompositeFileComparator.java", "org.apache.commons.io.comparator.DefaultFileComparator.java", "org.apache.commons.io.comparator.ExtensionFileComparator.java", "org.apache.commons.io.comparator.DirectoryFileComparator.java", "org.apache.commons.io.comparator.DefaultFileComparatorTest.java", "org.apache.commons.io.comparator.ExtensionFileComparatorTest.java", "org.apache.commons.io.comparator.ComparatorAbstractTestCase.java", "org.apache.commons.io.comparator.CompositeFileComparatorTest.java", "org.apache.commons.io.comparator.NameFileComparatorTest.java", "org.apache.commons.io.comparator.SizeFileComparatorTest.java", "org.apache.commons.io.comparator.DirectoryFileComparatorTest.java", "org.apache.commons.io.comparator.PathFileComparatorTest.java", "org.apache.commons.io.comparator.LastModifiedFileComparatorTest.java"], "label": 1, "es_results": []}, {"bug_id": 356, "bug_title": "CharSequenceInputStream#reset() behaves incorrectly in case when buffer size is not dividable by data size", "bug_description": "The size effect happens when buffer size of input stream is not dividable by requested data size. The bug is hidden in CharSequenceInputStream#reset() method which should also call (I think) bbuf.limit(0) otherwise next call to CharSequenceInputStream#read() will return the remaining tail which bbuf has accumulated.\nIn the attached test case the test fails, if dataSize = 13 (not dividable by 10) and runs OK if dataSize = 20 (dividable by 10).", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.input.CharSequenceInputStreamTest.java", "org.apache.commons.io.input.CharSequenceInputStream.java"], "label": 1, "es_results": []}, {"bug_id": 383, "bug_title": "FileUtils.doCopyFile caches the file size; needs to be documented", "bug_description": "FileUtils.doCopyFile saves the input file size before starting the copy.\nThe copy is considered complete when the original file size is reached.\nThe method then checks the new input file size against the destination size.\nThis will fail if the file has changed in size since the copy started.\nThis behaviour should be documented; also it would help if the two sizes were shown in the exception message.", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 390, "bug_title": "FileUtils.sizeOfDirectoryAsBigInteger can overflow", "bug_description": "FileUtils.sizeOfDirectoryAsBigInteger can overflow.\nThis is because it calls FileUtils.sizeOf(file) which calls sizeOfDirectory() for processing subdirectories.\nAs it stands, the method only works properly at the top level.\nA possible solution would be to create private \"Big\" versions of the called methods; these would not need all the checks so should be faster.", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.FileUtils.java", "org.apache.commons.io.FileUtilsTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 436, "bug_title": "Improper JavaDoc comment for FilenameUtils.indexOfExtension", "bug_description": "The method FilenameUtils.indexOfExtension contains this JavaDoc comment:\n  * @param filename  the filename to find the last path separator in, null returns -1\n  * @return the index of the last separator character, or -1 if there\n  * is no such character\nThis comment was obviously copied from the FilenameUtils.indexOfLastSeparator method, where it makes perfect sense.\nThe JavaDoc comment for FilenameUtils.indexOfExtension should rather read e.g. as follows:\n  * @param filename  the filename to find the last extension separator in, null returns -1\n  * @return the index of the last extension separator character, or -1 if there\n  * is no such character", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.FilenameUtils.java"], "label": 1, "es_results": []}, {"bug_id": 462, "bug_title": "IOExceptionWithCause no longer needed", "bug_description": "The class IOExceptionWithCause is no longer needed in Java 6, because IOException now includes a constructor that takes a Throwable.\nThe class should be deprecated and internal usage can revert to using IOException directly.", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.IOExceptionWithCause.java", "org.apache.commons.io.IOExceptionWithCauseTestCase.java", "org.apache.commons.io.FileSystemUtils.java", "org.apache.commons.io.TaggedIOException.java"], "label": 1, "es_results": []}, {"bug_id": 481, "bug_title": "org.apache.commons.io.FileUtils#waitFor waits too long", "bug_description": "The timing algorithm is basically broken, since Thread.sleep is imprecise. There is also a counter error in the looping code. \nThe following testcase will never run in less than 4 seconds on my machine\n  public void testRealWallTime() \n{\n\n        long start = System.currentTimeMillis();\n\n        FileUtils.waitFor(new File(\"\"), 2);\n\n        System.out.println(\"elapsed = \" + (System.currentTimeMillis() - start));\n\n    }", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.FileUtils.java"], "label": 1, "es_results": []}, {"bug_id": 484, "bug_title": "FilenameUtils should handle embedded null bytes", "bug_description": "embedding nulls in filenames exposes injection vectors if the application passes unsanitized data to some functions in FileNameUtils", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.FilenameUtilsTestCase.java", "org.apache.commons.io.FilenameUtils.java"], "label": 1, "es_results": []}, {"bug_id": 492, "bug_title": "Typo: In an IOUtils.java comment it says \"focussed\" instead of \"focused\"", "bug_description": "See here: https://github.com/apache/commons-io/blob/trunk/src/main/java/org/apache/commons/io/IOUtils.java#L101", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.IOUtils.java"], "label": 1, "es_results": []}, {"bug_id": 494, "bug_title": "Mismatch return type in api docs of class DirectoryWalker.", "bug_description": "In the api docs of class org.apache.commons.io.DirectoryWalker<T>, topic of 3.1 External / Multi-threaded.Method handleIsCancelled in demo code returns boolean but with a void return type.", "project": "Commons", "sub_project": "IO", "version": "2.4", "fixed_version": "2.5", "fixed_files": ["org.apache.commons.io.DirectoryWalker.java"], "label": 1, "es_results": []}, {"bug_id": 261, "bug_title": "Error in an example in the javadoc of the StringUtils.splitPreserveAllTokens() method", "bug_description": "There is an error in the javadoc of the org.apache.commons.lang.StringUtils.splitPreserveAllTokens(String str,char separatorChar) method.\nHere the original line :\nStringUtils.splitPreserveAllTokens(\"a..b.c\", &apos;.&apos;)   = [\"a\", \"b\", \"c\"]\nBut it should be :\nStringUtils.splitPreserveAllTokens(\"a..b.c\", &apos;.&apos;)   = [\"a\", \"\", \"b\", \"c\"]", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_1", "fixed_version": "LANG_2_2", "fixed_files": ["org.apache.commons.lang.StringUtilsTest.java", "org.apache.commons.lang.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 100, "bug_title": "[lang] RandomStringUtils.random() family of methods create invalid unicode sequences", "bug_description": "Problem are surrogate pairs: \nE.g. RandomStringUtils.random(int) may create strings with a high surrogate not\nfollowed by a low surrogate character.\nWhen processing them, we get errors in string-conversion-functions later on.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_1", "fixed_version": "LANG_2_2", "fixed_files": ["org.apache.commons.lang.RandomStringUtils.java", "org.apache.commons.lang.RandomStringUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 59, "bug_title": "[lang] DateUtils.truncate method is buggy when dealing with DST switching hours", "bug_description": "Try to truncate 2004-10-31 01:00:00 MDT by hour and you&apos;ll actually get 2004-10-\n31 01:00:00 MST, which is one hour after the input hour.\n    // truncate 2004-10-31 01:00:00 MDT\n    Date oct31_01MDT = new Date(1099206000000L);    \n    Date result = DateUtils.truncate(oct31_01MDT, Calendar.HOUR_OF_DAY);\n    assertEquals(oct31_01MDT, result);", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_1", "fixed_version": "LANG_2_2", "fixed_files": ["org.apache.commons.lang.time.DateUtilsTest.java", "org.apache.commons.lang.time.DateUtils.java"], "label": 1, "es_results": []}, {"bug_id": 259, "bug_title": "ValuedEnum.compareTo(Object other) not typesafe - it easily could be...", "bug_description": "int org.apache.commons.lang.enums.ValuedEnum.compareTo(Object other)\n is not typesafe - if the int-values are the same, it will return \"0\" even for two totally different sub-classes of ValuedEnum", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_1", "fixed_version": "LANG_2_2", "fixed_files": ["org.apache.commons.lang.enums.ValuedEnumTest.java", "org.apache.commons.lang.enums.EnumEqualsTest.java", "org.apache.commons.lang.enums.ValuedEnum.java"], "label": 1, "es_results": []}, {"bug_id": 277, "bug_title": "Javadoc errors on StringUtils.splitPreserveAllTokens(String, char)", "bug_description": "In the Javadoc for StringUtils.splitPreserveAllTokens(String, char) there are a couple of mistakes.  I didn&apos;t check for similar mistakes in the similar functions with different signatures.\nStringUtils.splitPreserveAllTokens(\"a..b.c\", &apos;.&apos;)   = [\"a\", \"b\", \"c\"]\nshould read\nStringUtils.splitPreserveAllTokens(\"a..b.c\", &apos;.&apos;)   = [\"a\", \"\", \"b\", \"c\"]\nThese two lines have the same input giving different outputs.  I think that the input string on the second call should be \"a b c  \".\nStringUtils.splitPreserveAllTokens(\"a b c \", &apos; &apos;)   = [\"a\", \"b\", \"c\", \"\"]\nStringUtils.splitPreserveAllTokens(\"a b c \", &apos; &apos;)   = [\"a\", \"b\", \"c\", \"\", \"\"]", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_1", "fixed_version": "LANG_2_2", "fixed_files": ["org.apache.commons.lang.StringUtilsTest.java", "org.apache.commons.lang.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 278, "bug_title": "javadoc for StringUtils.removeEnd is incorrect", "bug_description": "The javadoc for StringUtils.removeEnd(String, String) lists a number of examples, one of which is wrong:\nStringUtils.removeEnd(\"www.domain.com\", \".com.\")  = \"www,domain\"\nThe actual result of this is just \"www.domain.com\"", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_1", "fixed_version": "LANG_2_2", "fixed_files": ["org.apache.commons.lang.StringUtilsTest.java", "org.apache.commons.lang.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 281, "bug_title": "DurationFormatUtils returns wrong result", "bug_description": "DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005\nThe following code will result in a String of -2 which is way off.\nI&apos;ve tested against 2.1 and 2.2.\n        Calendar cal = Calendar.getInstance();\n        cal.set(Calendar.MONTH, Calendar.DECEMBER);\n        cal.set(Calendar.DAY_OF_MONTH, 31);\n        cal.set(Calendar.YEAR, 2005);\n        cal.set(Calendar.HOUR_OF_DAY, 0);\n        cal.set(Calendar.MINUTE, 0);\n        cal.set(Calendar.SECOND, 0);\n        cal.set(Calendar.MILLISECOND, 0);\n        String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), \"MM\");\n        System.out.println(result);", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_1", "fixed_version": "LANG_2_3", "fixed_files": ["org.apache.commons.lang.time.DurationFormatUtilsTest.java", "org.apache.commons.lang.time.DurationFormatUtils.java"], "label": 1, "es_results": []}, {"bug_id": 313, "bug_title": "Wrong behavior of Entities.unescape", "bug_description": "Hi,\n    There&apos;s seems to be a bug at Entities.unescape. Try for example StringEscapeUtils.unescapeHtml(\"& &\"). It outputs \"& &\" instead of \"& &\". The problem is at this piece of code:\n                if (entityValue == -1) \n{\n                    buf.append(&apos;&&apos;);\n                    buf.append(entityName);\n                    buf.append(&apos;;&apos;);\n                }\n else \n{\n                    buf.append((char) (entityValue));\n                }\n                i = semi;\n     The method always skips to the next \";\", even if it doesn&apos;t finds the entity value and then disregarding any entity that may be actually be referred inside.\nRegards,\nThiago Souza", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_1", "fixed_version": "LANG_2_3", "fixed_files": ["org.apache.commons.lang.StringEscapeUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 69, "bug_title": "[lang] ToStringBuilder throws StackOverflowError when an Object cycle exists", "bug_description": "Hi,\nThe ToStringBuilder throws a StackOverflowError if you have a cycle in the\nobject graph. For instance, the following toString() method will cause a\nStackOverflowError:\npublic class ObjectCycle {\n    Object obj;\n    public String toString() \n{\n        return new ToStringBuilder(this).append(obj).toString();\n    }\n}\npublic void testObjectCycle() {\n    ObjectCycle a = new ObjectCycle();\n    ObjectCycle b = new ObjectCycle();\n    a.obj = b;\n    b.obj = a;\n    a.toString();  // ouch: StackOverflowError    \t\n}\nI&apos;ll submit some patches that fixes this problem...\nregards,\nMaarten", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_1", "fixed_version": "LANG_2_3", "fixed_files": ["org.apache.commons.lang.builder.ToStringStyle.java", "org.apache.commons.lang.builder.ToStringBuilderTest.java", "org.apache.commons.lang.builder.ReflectionToStringBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 299, "bug_title": "Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException", "bug_description": "There&apos;s a bug in method appendFixedWidthPadRight of class StrBuilder:\npublic StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {\n        if (width > 0) {\n            ensureCapacity(size + width);\n            String str = (obj == null ? getNullText() : obj.toString());\n            int strLen = str.length();\n            if (strLen >= width) \n{\n ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);\n            }\n else {\n                int padLen = width - strLen;\n                str.getChars(0, strLen, buffer, size);\n                for (int i = 0; i < padLen; i++) \n{\n                    buffer[size + strLen + i] = padChar;\n                }\n            }\n            size += width;\n        }\n        return this;\n    }\nThis is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width.\nIt&apos;s counterpart method appendFixedWidthPadLeft seems to be ok.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_2", "fixed_version": "LANG_2_3", "fixed_files": ["org.apache.commons.lang.text.StrBuilderAppendInsertTest.java", "org.apache.commons.lang.text.StrBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 300, "bug_title": "NumberUtils.createNumber throws NumberFormatException for one digit long", "bug_description": "NumberUtils.createNumber throws a NumberFormatException when parsing \"1l\", \"2l\" .. etc...\nIt works fine if you try to parse \"01l\" or \"02l\".  The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for \"1l\"", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_2", "fixed_version": "LANG_2_3", "fixed_files": ["org.apache.commons.lang.math.NumberUtils.java", "org.apache.commons.lang.NumberUtils.java", "org.apache.commons.lang.math.NumberUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 304, "bug_title": "NullPointerException in isAvailableLocale(Locale)", "bug_description": "FindBugs pointed out:\n   UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet\ncAvailableSet is used directly once in the source - and if availableLocaleSet() hasn&apos;t been called it will cause a NullPointerException.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_2", "fixed_version": "LANG_2_3", "fixed_files": ["org.apache.commons.lang.LocaleUtils.java", "org.apache.commons.lang.LocaleUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 303, "bug_title": "FastDateFormat.mRules is not transient or serializable", "bug_description": "Reported by FindBugs.\nEither we need to make the Rule interface Serializable, or make mRules transient and add deserializing code to kick off init().", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_2", "fixed_version": "LANG_2_3", "fixed_files": ["org.apache.commons.lang.time.FastDateFormatTest.java", "org.apache.commons.lang.time.FastDateFormat.java"], "label": 1, "es_results": []}, {"bug_id": 102, "bug_title": "[lang] Refactor Entities methods", "bug_description": "The pairs of escape and unescape methods in Entities need to be modified so that\nthey call each other (one escape to the other escape etc). Otherwise there&apos;s a\nlarge chunk of repeated code that gives us a high chance of errors.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_2", "fixed_version": "LANG_2_3", "fixed_files": ["org.apache.commons.lang.Entities.java"], "label": 1, "es_results": []}, {"bug_id": 346, "bug_title": "Dates.round() behaves incorrectly for minutes and seconds", "bug_description": "Get unexpected output for rounding by minutes or seconds.\npublic void testRound()\n{\n    Calendar testCalendar = Calendar.getInstance(TimeZone.getTimeZone(\"GMT\"));\n    testCalendar.set(2007, 6, 2, 8, 9, 50);\n    Date date = testCalendar.getTime();\n    System.out.println(\"Before round() \" + date);\n    System.out.println(\"After round()  \" + DateUtils.round(date, Calendar.MINUTE));\n}\n--2.1 produces\nBefore round() Mon Jul 02 03:09:50 CDT 2007\nAfter round()  Mon Jul 02 03:10:00 CDT 2007  this is what I would expect\n--2.2 and 2.3 produces\nBefore round() Mon Jul 02 03:09:50 CDT 2007\nAfter round()  Mon Jul 02 03:01:00 CDT 2007  this appears to be wrong", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_2", "fixed_version": "LANG_2_4", "fixed_files": ["org.apache.commons.lang.time.DateUtilsTest.java", "org.apache.commons.lang.time.DateUtils.java"], "label": 1, "es_results": []}, {"bug_id": 367, "bug_title": "FastDateFormat thread safety", "bug_description": "FastDateFormat has several static HashMaps. These are currently not final, so there is no guarantee that they will be visible to all threads.\nPatch to follow.\nAlso, as far as I can make out, the class shares SimpleDateFormat instances between threads.\nIt does not document why this is OK.\nI&apos;m guessing that it assumes that instances of the SimpleDateFormat class are thread-safe provided that they have the same attributes, but this is not documented. If this is the case, it&apos;s not clear that it is a valid assumption.\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_2", "fixed_version": "LANG_2_4", "fixed_files": ["org.apache.commons.lang.time.FastDateFormat.java"], "label": 1, "es_results": []}, {"bug_id": 302, "bug_title": "StrBuilder does not implement clone()", "bug_description": "As reported by FindBugs.\nDoes StrBuilder need to be Cloneable?", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_2", "fixed_version": "LANG_2_6", "fixed_files": ["org.apache.commons.lang.text.StrBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 328, "bug_title": "LocaleUtils.toLocale() rejects strings with only language+variant", "bug_description": "LocaleUtils.toLocale() throws an exception on strings containing a language and a variant but no country code. For example : fr__POSIX\nThis string can be produced with the JDK by instanciating a Locale with an empty string for the country : new Locale(\"fr\", \"\", \"POSIX\").toString(). According to the javadoc for the Locale class a variant is allowed with just a language code or just a country code.\nCommons Configuration handles this case in its PropertyConverter.toLocale() method. I&apos;d like to replace our implementation by the one provided by LocaleUtils, but our tests fail due to this case.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_3", "fixed_version": "LANG_2_4", "fixed_files": ["org.apache.commons.lang.LocaleUtils.java", "org.apache.commons.lang.LocaleUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 361, "bug_title": "BooleanUtils toBooleanObject javadoc does not match implementation", "bug_description": "The javadoc description for the BooleanUtils method toBooleanObject(String, String, String, String) states that it returns null if the input string does not match the true, false, or null string values.  However, the implementation throws and IllegalArgumentException when the input string does not match the other string values.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_3", "fixed_version": "LANG_2_4", "fixed_files": ["org.apache.commons.lang.BooleanUtils.java"], "label": 1, "es_results": []}, {"bug_id": 364, "bug_title": "Documentation bug for ignoreEmptyTokens accessors in StrTokenizer", "bug_description": "The javadoc for the accessors of the ignoreEmptyTokens property currently states that the default value is false, when it is actually true.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_3", "fixed_version": "LANG_2_4", "fixed_files": ["org.apache.commons.lang.text.StrTokenizer.java"], "label": 1, "es_results": []}, {"bug_id": 363, "bug_title": "StringEscapeUtils.escapeJavaScript() method did not escape '/' into '\\/', it will make IE render page uncorrectly", "bug_description": "If Javascripts including&apos;/&apos;, IE will parse the scripts uncorrectly, actually &apos;/&apos; should be escaped to &apos;\\/&apos;.\nFor example, document.getElementById(\"test\").value = &apos;<script>alert(\\&apos;aaa\\&apos;);</script>&apos;;this expression will make IE render page uncorrect, it should be document.getElementById(\"test\").value = &apos;<script>alert(\\&apos;aaa\\&apos;);<\\/script>&apos;;\nBy The Way, Spring&apos;s JavascriptEscape behavor is correct.\nTry  to run below codes, you will find the difference:\n  String s = \"<script>alert(&apos;aaa&apos;);</script>\";\n  String str = org.springframework.web.util.JavaScriptUtils.javaScriptEscape(s);\n  System.out.println(\"Spring JS Escape : \"+str);\n  str = org.apache.commons.lang.StringEscapeUtils.escapeJavaScript(s);\n  System.out.println(\"Apache Common Lang JS Escape : \"+ str);", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_3", "fixed_version": "LANG_2_4", "fixed_files": ["org.apache.commons.lang.StringEscapeUtilsTest.java", "org.apache.commons.lang.StringEscapeUtils.java"], "label": 1, "es_results": []}, {"bug_id": 380, "bug_title": "infinite loop in Fraction.reduce when numerator == 0", "bug_description": "Summary pretty much says it all.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_3", "fixed_version": "LANG_2_4", "fixed_files": ["org.apache.commons.lang.math.Fraction.java", "org.apache.commons.lang.math.FractionTest.java"], "label": 1, "es_results": []}, {"bug_id": 381, "bug_title": "NumberUtils.min(floatArray) returns wrong value if floatArray[0] happens to be Float.NaN", "bug_description": "The min() method of NumberUtils returns the wrong result if  the first value of the array happens to be Float.NaN. The following code snippet shows the behaviour:\n        float a[] = new float[] \n{(float) 1.2, Float.NaN, (float) 3.7, (float) 27.0, (float) 42.0, Float.NaN}\n;\n        float b[] = new float[] \n{Float.NaN, (float) 1.2, Float.NaN, (float) 3.7, (float) 27.0, (float) 42.0, Float.NaN}\n;\n        float min = NumberUtils.min(a);\n        System.out.println(\"min(a): \" + min); // output: 1.2\n        min = NumberUtils.min(b);\n        System.out.println(\"min(b): \" + min); // output: NaN\nThis problem may exist for double-arrays as well. \nProposal: Use Float.compare(float, float) or NumberUtils.compare(float, float) to achieve a consistent result.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_3", "fixed_version": "LANG_2_4", "fixed_files": ["org.apache.commons.lang.math.NumberUtils.java", "org.apache.commons.lang.math.NumberUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 393, "bug_title": "EqualsBuilder do not compare BigDecimals correctly ", "bug_description": "When comparing a BigDecimal, the comparing is made using equals, not compareTo, which is more appropriate in the case of BigDecimal.  ", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_3", "fixed_version": "LANG_2_4", "fixed_files": ["org.apache.commons.lang.builder.EqualsBuilderTest.java", "org.apache.commons.lang.builder.EqualsBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 412, "bug_title": "StrBuilder appendFixedWidth does not handle nulls", "bug_description": "Appending a null value with fixed width causes a null pointer exception if getNullText() has not been set.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_3", "fixed_version": "LANG_2_4", "fixed_files": ["org.apache.commons.lang.text.StrBuilderTest.java", "org.apache.commons.lang.text.StrBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 421, "bug_title": "StringEscapeUtils.escapeJava(String) escapes '/' characters", "bug_description": "Commons Lang 2.4 StringEscapeUtils.escapeJava(String) now escapes &apos;/&apos; characters, which is not a valid \"escapable\" character in Java strings.  I haven&apos;t tried the other Java escape/unescape methods to see if they have a similar problem, or that only Java \"escapable\" characters are escaped by escapeJava(String).\nThis bug may have appeared as an unintended side-effect of the fix for LANG-363.\nAlso the javadoc for escapeJava is now a little off, in that &apos;/&apos; should now be included in the sentence describing the differences between Java and Javascript strings, with respect to escaping rules.\nThe following is a JUnit3 test demonstrating the bug.\nimport junit.framework.TestCase;\nimport org.apache.commons.lang.StringEscapeUtils;\npublic class StringEscapeUtilsTest extends TestCase {\n    public void testEscapeJavaWithSlash() \n{\n\n        final String input = \"String with a slash (/) in it\";\n\n        \n\n        final String expected = input;\n\n        final String actual   = StringEscapeUtils.escapeJava( input );\n\n\n\n        /**\n\n         * In 2.4 StringEscapeUtils.escapeJava(String) escapes &apos;/&apos; characters,\n\n         * which are not a valid character to escape in a Java string.  \n\n         */\n\n        assertEquals( expected, actual );\n\n    }\n}\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_4", "fixed_version": "LANG_2_5", "fixed_files": ["org.apache.commons.lang.StringEscapeUtils.java", "org.apache.commons.lang.StringEscapeUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 419, "bug_title": "WordUtils.abbreviate bug when lower is greater than str.length", "bug_description": "In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower.\nBut lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too...\nThen, str.substring(0, upper) throw a StringIndexOutOfBoundsException\nThe fix is to adjust lower to the length of the string", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_4", "fixed_version": "LANG_2_5", "fixed_files": ["org.apache.commons.lang.WordUtilsTest.java", "org.apache.commons.lang.WordUtils.java"], "label": 1, "es_results": []}, {"bug_id": 459, "bug_title": "Issue in HashCodeBuilder which only shows up under high load multi-threaded usage.", "bug_description": "We found we were having problems with HashCodeBuilder under multi-threaded high load.\nI narrowed this down to the following attached test case.\nWhen I dug into the code, I found the problem was solved by commenting out the isRegistered method (though this would break the infinite loop problem).\n( I did a lot of other digging that I will not bore you with).\nSo instead I replaced the HashSet with an ArrayList and just added the object, rather than the toIdentityHashCodeInteger(object)\nThis results in about 5 lines of change.  \nMy suspicion is that System.identityHashCode does not return unique values (it is after all a hashcode method).  The code assumes it will return a unique value and this causes the problem at high loads.\nThe downside is a List vs. a Set, but I believe this is necessary.\nI&apos;d like to submit this fix and have it verified (and perhaps improved).  I am convinced it is a necessary fix which we have seen show up under high loads.\nKindest regards, \nAndrew.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_4", "fixed_version": "LANG_2_5", "fixed_files": ["org.apache.commons.lang.builder.HashCodeBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 460, "bug_title": "Wrong, interchanged parameters in Dokumentation of StringUtils.startsWith[IgnoreCase](String, String)", "bug_description": "The parameters in the examples of the JavaDoc of StringUtils.startsWithIgnoreCase(String, String) and StringUtils.startsWith(String, String) are interchanged.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_4", "fixed_version": "LANG_2_5", "fixed_files": ["org.apache.commons.lang.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 477, "bug_title": "ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes", "bug_description": "When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur.\nExample that will cause error:\nExtendedMessageFormatTest.java\n\n\n\n\nprivate static Map<String, Object> formatRegistry = new HashMap<String, Object>();    \n\n    static {\n\n        formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());\n\n    }\n\n    \n\n    public static void main(String[] args) {\n\n        ExtendedMessageFormat mf = new ExtendedMessageFormat(\"it&apos;&apos;s a {dummy} &apos;test&apos;!\", formatRegistry);\n\n        String formattedPattern = mf.format(new String[] {\"great\"});\n\n        System.out.println(formattedPattern);\n\n    }\n\n}\n\n\n\n\n\nThe following change starting at line 421 on the 2.4 release seems to fix the problem:\nExtendedMessageFormat.java\n\n\nCURRENT (Broken):\n\nif (escapingOn && c[start] == QUOTE) {\n\n        return appendTo == null ? null : appendTo.append(QUOTE);\n\n}\n\n\n\nWORKING:\n\nif (escapingOn && c[start] == QUOTE) {\n\n        next(pos);\n\n        return appendTo == null ? null : appendTo.append(QUOTE);\n\n}\n\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_4", "fixed_version": "LANG_2_5", "fixed_files": ["org.apache.commons.lang.text.ExtendedMessageFormatTest.java", "org.apache.commons.lang.text.ExtendedMessageFormat.java"], "label": 1, "es_results": []}, {"bug_id": 509, "bug_title": "OutOfMemory Error caused by ExtendedMessageFormat", "bug_description": "It is possible to let ExtendedMessageFormat because an OutOfMemory Error (Java heap space) - no matter how large you define the memory, because the code produces an endless loop that extends a StringBuffer ad infinitum.\nCode to reproduce:\ninstantiate an ExtendedMessageFormat object using the constructor \npublic ExtendedMessageFormat(String pattern, Locale locale, Map registry) \nlocale and registry (not null) don&apos;t matter actually, but pattern as String looks like this:\n{{The field &apos;&apos;\n{0}\n&apos;&apos; must be completed}}\nnotice the doubled single quotes.\nThe constructor then executes applyPattern(pattern)\nIn applyPattern, line 158 (that is inside the loop over the pattern length)  appendQuotedString(pattern, pos, stripCustom, true); is called, it is the last statement for that case in the loop. \nIn  appendQuotedString, line 422 the quote character gets appended to the return StringBuffer, then return. The problem in fact is, that the pointer (ParsePosition pos) isn&apos;t updated and after return the procedure will check the same character again and again and again. \nPrimitive workaround: no use of single quotes in messages as input of ExtendedMessageFormat.\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_4", "fixed_version": "LANG_2_5", "fixed_files": ["org.apache.commons.lang.text.ExtendedMessageFormatTest.java", "org.apache.commons.lang.text.ExtendedMessageFormat.java"], "label": 1, "es_results": []}, {"bug_id": 535, "bug_title": "ClassUtils.getShortClassName() will not work with an array;  it seems to add a semicolon to the end.", "bug_description": "A semicolon is introduced into the class name at the end for all arrays...\nString sArray[] = new String[2];\nsArray[0] = \"mark\";\nsArray[1] = \"is cool\";\nString simpleString = \"chris\";\nassertEquals(\"String\", ClassUtils.getShortClassName(simpleString, null));\nassertEquals(\"String;\", ClassUtils.getShortClassName(sArray, null));", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_4", "fixed_version": "LANG_2_5", "fixed_files": ["org.apache.commons.lang.ClassUtilsTest.java", "org.apache.commons.lang.ClassUtils.java"], "label": 1, "es_results": []}, {"bug_id": 432, "bug_title": "Fix case-insensitive string handling", "bug_description": "String.to*Case() is locale-sensitive, this is usually not intended for case-insensitive comparisions. Please see Common Bug #3 for details.", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_4", "fixed_version": "LANG_2_5", "fixed_files": ["org.apache.commons.lang.StringUtilsEqualsIndexOfTest.java", "org.apache.commons.lang.StringUtils.java", "org.apache.commons.lang.SystemUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 473, "bug_title": "StringEscapeUtils.escapeJava () not escaping forward slash correctly.", "bug_description": "We are trying to escape forward slash using StringEscapeUtils.escapeJava ()  method. The output returned by this method is no longer a valid Java String. \ne.g. \nString s = \"a/b/c\";\nSystem.out.println(StringEscapeUtils.escapeJava (s));\nThe output returned is a\\/b\\/c which is not a even a valid Java String.\nThis was working fine in 2.0 release of this jar.\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_2_4", "fixed_version": "LANG_2_5", "fixed_files": ["org.apache.commons.lang.StringEscapeUtils.java", "org.apache.commons.lang.StringEscapeUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 720, "bug_title": "StringEscapeUtils.escapeXml(input) outputs wrong results when an input contains characters in Supplementary Planes.", "bug_description": "Hello.\nI use StringEscapeUtils.escapeXml(input) to escape special characters for XML.\nThis method outputs wrong results when input contains characters in Supplementary Planes.\nString str1 = \"\\uD842\\uDFB7\" + \"A\";\nString str2 = StringEscapeUtils.escapeXml(str1);\n// The value of str2 must be equal to the one of str1,\n// because str1 does not contain characters to be escaped.\n// However, str2 is diffrent from str1.\nSystem.out.println(URLEncoder.encode(str1, \"UTF-16BE\")); //%D8%42%DF%B7A\nSystem.out.println(URLEncoder.encode(str2, \"UTF-16BE\")); //%D8%42%DF%B7%FF%FD\nThe because of this problem is that the loop to translate input character by character is wrong.\nIn CharSequenceTranslator.translate(CharSequence input, Writer out),\nloop counter \"i\" moves from 0 to Character.codePointCount(input, 0, input.length()),\nbut it should move from 0 to input.length().", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_0", "fixed_version": "LANG_3_0_1", "fixed_files": ["org.apache.commons.lang3.StringEscapeUtilsTest.java", "org.apache.commons.lang3.text.translate.CharSequenceTranslator.java"], "label": 1, "es_results": []}, {"bug_id": 734, "bug_title": "The CHAR_ARRAY cache in CharUtils duplicates the cache in java.lang.Character", "bug_description": "The CHAR_ARRAY cache in CharUtils duplicates the cache in java.lang.Character.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_0", "fixed_version": "LANG_3_0_1", "fixed_files": ["org.apache.commons.lang3.CharUtils.java"], "label": 1, "es_results": []}, {"bug_id": 727, "bug_title": "ToStringBuilderTest.testReflectionHierarchyArrayList fails with IBM JDK 6", "bug_description": "The unit test fails when running with IBM JDK 6:\n\nFailed tests:\n\ntestReflectionHierarchyArrayList(org.apache.commons.lang3.builder.ToStringBuilderTest):null\n\nexpected:<....ArrayList@115b115b[[elementData={<null>,<null>,<null>,<null>,<null>,<null>,<null>,null>,null>,null>},size=0],modCount=0]>\n\nbut was:<....ArrayList@115b115b[[firstIndex=0,lastIndex=0,array={<null>,<null>,<null>,<null>,<null>,<null>,<null>,null>,null>,null>}],modCount=0]>\n\n\n\nActually the test is wrong, because it makes wrong assumptions about the implementation of ArrayList in the runtime.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_0", "fixed_version": "LANG_3_0_1", "fixed_files": ["org.apache.commons.lang3.builder.ToStringBuilderTest.java"], "label": 1, "es_results": []}, {"bug_id": 736, "bug_title": "CharUtils static final array CHAR_STRING is not needed to compute CHAR_STRING_ARRAY", "bug_description": "CharUtils static final array CHAR_STRING is not needed to compute CHAR_STRING_ARRAY.\nThis:\n\n\n\n    private static final String CHAR_STRING = \n\n        \"\\u0000\\u0001\\u0002\\u0003\\u0004\\u0005\\u0006\\u0007\" +\n\n        \"\\b\\t\\n\\u000b\\f\\r\\u000e\\u000f\" +\n\n        \"\\u0010\\u0011\\u0012\\u0013\\u0014\\u0015\\u0016\\u0017\" +\n\n        \"\\u0018\\u0019\\u001a\\u001b\\u001c\\u001d\\u001e\\u001f\" +\n\n        \"\\u0020\\u0021\\\"\\u0023\\u0024\\u0025\\u0026\\u0027\" +\n\n        \"\\u0028\\u0029\\u002a\\u002b\\u002c\\u002d\\u002e\\u002f\" +\n\n        \"\\u0030\\u0031\\u0032\\u0033\\u0034\\u0035\\u0036\\u0037\" +\n\n        \"\\u0038\\u0039\\u003a\\u003b\\u003c\\u003d\\u003e\\u003f\" +\n\n        \"\\u0040\\u0041\\u0042\\u0043\\u0044\\u0045\\u0046\\u0047\" +\n\n        \"\\u0048\\u0049\\u004a\\u004b\\u004c\\u004d\\u004e\\u004f\" +\n\n        \"\\u0050\\u0051\\u0052\\u0053\\u0054\\u0055\\u0056\\u0057\" +\n\n        \"\\u0058\\u0059\\u005a\\u005b\\\\\\u005d\\u005e\\u005f\" +\n\n        \"\\u0060\\u0061\\u0062\\u0063\\u0064\\u0065\\u0066\\u0067\" +\n\n        \"\\u0068\\u0069\\u006a\\u006b\\u006c\\u006d\\u006e\\u006f\" +\n\n        \"\\u0070\\u0071\\u0072\\u0073\\u0074\\u0075\\u0076\\u0077\" +\n\n        \"\\u0078\\u0079\\u007a\\u007b\\u007c\\u007d\\u007e\\u007f\";\n\n\n\n// snip\n\n\n\n        for (int i = 127; i >= 0; i--) {\n\n            CHAR_STRING_ARRAY[i] = CHAR_STRING.substring(i, i + 1);\n\n        }\n\n\n\nCan be recoded as:\n\n\n\n        for (char c = 0; c < CHAR_STRING_ARRAY.length; c++) {\n\n            CHAR_STRING_ARRAY[c] = String.valueOf(c);\n\n        }\n\n\n\nWith the lang 3.0 code, using the Oracle Java 5 String impl, the 128 Strings share the underlying CHAR_STRING char[] because of the way Sun implemented String#substring(int,int).\nThe proposed implementation does not reply on this private implementation detail but creates one char[1] array per String. \nThoughts?", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_0", "fixed_version": "LANG_3_1", "fixed_files": ["org.apache.commons.lang3.CharUtils.java"], "label": 1, "es_results": []}, {"bug_id": 746, "bug_title": "NumberUtils does not handle upper-case hex: 0X and -0X", "bug_description": "NumberUtils.createNumber() should work equally for 0x1234 and 0X1234; currently 0X1234 generates a NumberFormatException\nInteger.decode() handles both upper and lower case hex.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_0", "fixed_version": "LANG_3_1", "fixed_files": ["org.apache.commons.lang3.math.NumberUtilsTest.java", "org.apache.commons.lang3.math.NumberUtils.java"], "label": 1, "es_results": []}, {"bug_id": 613, "bug_title": "ConstructorUtils.getAccessibleConstructor() Does Not Check the Accessibility of Enclosing Classes", "bug_description": "ConstructorUtils.getAccessibleConstructor() checks if the declaring class is public but not whether it&apos;s a top-level class or an enclosed one.  Consequently, with enclosed declaring classes, the method does not check if the enclosing class is public, or it&apos;s enclosing class, or it&apos;s enclosing class, etc...", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_0", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.reflect.ConstructorUtils.java", "org.apache.commons.lang3.reflect.ConstructorUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 772, "bug_title": "ClassUtils.PACKAGE_SEPARATOR javadoc contains garbage text", "bug_description": "javadoc description of ClassUtils.PACKAGE_SEPARATOR is as follows:\nThe package separator String: \"&#x2e;\".\nIt should be something like the following:\nThe package separator String: \".\".", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_0_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.ClassUtils.java"], "label": 1, "es_results": []}, {"bug_id": 764, "bug_title": "StrBuilder has a serialVersionUID but is not serializable - this is inconsistent", "bug_description": "StrBuilder has a serialVersionUID but is not serializable.\nIs StrBuilder supposed to be Serializable?\nIf so, then add the interface; if not remove the field.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_0_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.text.StrBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 819, "bug_title": "EnumUtils.generateBitVector needs a \"? extends\"", "bug_description": "    public static <E extends Enum<E>> long generateBitVector(Class<E> enumClass, Iterable<E> values) {\nShould be Iterable<? extends E>.\nThis is because although no subclasses of E can exist, the \"? extends\" is a common idiom for marking the collection as readonly, or not \"owned\" by the current object.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_0_1", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.EnumUtils.java"], "label": 1, "es_results": []}, {"bug_id": 773, "bug_title": "ImmutablePair doc contains nonsense text", "bug_description": "Description of this class contains the following nonsense text:\n#ThreadSafe# if the objects are threadsafe", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.tuple.ImmutablePair.java"], "label": 1, "es_results": []}, {"bug_id": 775, "bug_title": "TypeUtils.getTypeArguments() misses type arguments for partially-assigned classes", "bug_description": "failing test code to add to TypeUtilsTest.testGetTypeArguments():\n\n\n\ntypeVarAssigns = TypeUtils.getTypeArguments(Other.class, This.class);\n\nAssert.assertEquals(2, typeVarAssigns.size());\n\nAssert.assertEquals(String.class, typeVarAssigns.get(This.class.getTypeParameters()[0]));\n\nAssert.assertEquals(Other.class.getTypeParameters()[0], typeVarAssigns.get(This.class.getTypeParameters()[1]));\n\n\n\nThese should pass based on:\n\n\n\n\n\npublic interface This<K, V> {\n\n}\n\n\n\npublic class Other<T> implements This<String, T> {\n\n}\n\n\n\nThis case fails because the current code ignores the Other class due to its specifying its own type variables, which is obviously incorrect.  This report is extrapolated from an offline report received by Hen.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.reflect.TypeUtils.java", "org.apache.commons.lang3.reflect.TypeUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 776, "bug_title": "TypeUtilsTest contains incorrect type assignability assertion due to lost/skipped type variable information during the decision process", "bug_description": "TypeUtilsTest originally contained the following under #testIsAssignable():\n\n\n\nAssert.assertTrue(\"WRONG!\", TypeUtils.isAssignable(dingType, disType));\n\n\n\nFor background:\n\n\n\npublic interface This<K, V> {\n\n}\n\n\n\npublic class Other<T> implements This<String, T> {\n\n}\n\n\n\npublic class Thing<Q> extends Other<B> {\n\n}\n\n\n\n<B> refers to a type parameter on the TypeUtilsTest class itself.\ndisType and dingType refer to the generic types of the following fields, respectively:\n\n\n\npublic This<String, String> dis;\n\npublic Thing ding;\n\n\n\nThus the assertion in question declares that type Thing is assignable to This<String, String>.  If we start at This we can see that the implementing class Other maps its T type parameter to the V type parameter of This.  From this point we can proceed down to Thing and see that it maps the B type parameter of the enclosing TypeUtilsTest class to the T type parameter of Other.  Thus it is fairly obvious that only a TypeUtilsTest<String>.Thing is assignable to This<String, String>.  From this we can determine that the intent of the message in the original test assertion must indeed have been to flag an incorrect assertion.  This is the associated bug report.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.reflect.TypeUtils.java", "org.apache.commons.lang3.reflect.TypeUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 783, "bug_title": "Documentation bug: StringUtils.split", "bug_description": "The documentation for StringUtils.split(String, String, int) contains wrong strings:\nStringUtils.split(\"ab de fg\", null, 0)   = [\"ab\", \"cd\", \"ef\"]\nStringUtils.split(\"ab   de fg\", null, 0) = [\"ab\", \"cd\", \"ef\"]\nThis should read:\nStringUtils.split(\"ab cd ef\", null, 0)   = [\"ab\", \"cd\", \"ef\"]\nStringUtils.split(\"ab   cd ef\", null, 0) = [\"ab\", \"cd\", \"ef\"]", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 788, "bug_title": "SerializationUtils throws ClassNotFoundException when cloning primitive classes", "bug_description": "If a serializable object contains a reference to a primitive class, e.g. int.class or int[].class, the SerializationUtils throw a ClassNotFoundException when trying to clone that object.\n\nimport org.apache.commons.lang3.SerializationUtils;\n\nimport org.junit.Test;\n\n\n\n\n\npublic class SerializationUtilsTest {\n\n\n\n\t\n\n\t@Test\n\n\tpublic void primitiveTypeClassSerialization(){\n\n\t\tClass<?> primitiveType = int.class;\n\n\t\t\n\n\t\tClass<?> clone = SerializationUtils.clone(primitiveType);\n\n\t\tassertEquals(primitiveType, clone);\n\n\t}\n\n}\n\n\n\nThe problem was already reported as a java bug http://bugs.sun.com/view_bug.do?bug_id=4171142 and ObjectInputStream is fixed since java version 1.4.\nThe SerializationUtils problem arises because the SerializationUtils internally use the ClassLoaderAwareObjectInputStream that overrides the ObjectInputStream&apos;s\nresoleClass method without delegating to the super method in case of a ClassNotFoundException.\nI understand the intention of the ClassLoaderAwareObjectInputStream, but this implementation should also implement a fallback to the original implementation.\nFor example:\n\n        protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {\n\n            String name = desc.getName();\n\n            try {\n\n                return Class.forName(name, false, classLoader);\n\n            } catch (ClassNotFoundException ex) {\n\n            \ttry {\n\n            \t     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());\n\n            \t} catch (Exception e) {\n\n\t\t     return super.resolveClass(desc);\n\n\t\t}\n\n            }\n\n        }\n\n\n\nHere is the code in ObjectInputStream that fixed the java bug.\n\n    protected Class<?> resolveClass(ObjectStreamClass desc)\n\n\tthrows IOException, ClassNotFoundException\n\n    {\n\n\tString name = desc.getName();\n\n\ttry {\n\n\t    return Class.forName(name, false, latestUserDefinedLoader());\n\n\t} catch (ClassNotFoundException ex) {\n\n\t    Class cl = (Class) primClasses.get(name);\n\n\t    if (cl != null) {\n\n\t\treturn cl;\n\n\t    } else {\n\n\t\tthrow ex;\n\n\t    }\n\n\t}\n\n    }\n\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.SerializationUtilsTest.java", "org.apache.commons.lang3.SerializationUtils.java"], "label": 1, "es_results": []}, {"bug_id": 800, "bug_title": "Javadoc bug in DateUtils#ceiling for Calendar and Object versions.", "bug_description": "The documentation has only been corrected for the ceiling method that takes a Date, but not for those that take a Calendar or an Object, respectively.\n\nFor example, if you had the datetime of 28 Mar 2002\n\n     * 13:45:01.231, if you passed with HOUR, it would return 28 Mar\n\n     * 2002 13:00:00.000.  If this was passed with MONTH, it would\n\n     * return 1 Mar 2002 0:00:00.000. \n\nLooks like copy-paste from truncate. (the dates are incorrect)", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.time.DateUtils.java"], "label": 1, "es_results": []}, {"bug_id": 802, "bug_title": "LocaleUtils - unnecessary recursive call in SyncAvoid class", "bug_description": "The SyncAvoid class calls availableLocaleList() which is a method in the containing class that returns SyncAvoid.AVAILABLE_LOCALE_LIST.\nThis is unnecessarily complex. The code should use the local variable \"list\" instead.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.LocaleUtils.java"], "label": 1, "es_results": []}, {"bug_id": 813, "bug_title": "StringUtils.equalsIgnoreCase does not check string reference equality", "bug_description": "This looks like a regression from .lang versions prior to 3. If the 2 given CharSequences are strings (and both not null) you should check reference equality before delegating to regionMatches()", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsEqualsIndexOfTest.java"], "label": 1, "es_results": []}, {"bug_id": 814, "bug_title": "[Method|Constructor]Utils.invoke*(*, Object... args) variants cannot handle null values", "bug_description": "throws NPE", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.reflect.ConstructorUtils.java", "org.apache.commons.lang3.reflect.ConstructorUtilsTest.java", "org.apache.commons.lang3.reflect.MethodUtilsTest.java", "org.apache.commons.lang3.reflect.MethodUtils.java"], "label": 1, "es_results": []}, {"bug_id": 817, "bug_title": "Add org.apache.commons.lang3.SystemUtils.IS_OS_WINDOWS_8", "bug_description": "Add org.apache.commons.lang3.SystemUtils.IS_OS_WINDOWS_8 to check for version \"6.2\".", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.SystemUtils.java"], "label": 1, "es_results": []}, {"bug_id": 810, "bug_title": "StringUtils.join() endIndex, bugged for loop", "bug_description": "endIndex is described as index, but for loop still checks it as \"array length\".\nBasically missing equal sign\ncommons-lang3-3.1-sources.jar, StringUtils.java lines 3309, 3394:\n        for (int i = startIndex; i < endIndex; i++) {\nshould be:\n        for (int i = startIndex; i <= endIndex; i++) {\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 799, "bug_title": "DateUtils#parseDate uses default locale; add Locale support", "bug_description": "Similar issue as https://issues.apache.org/jira/browse/HTTPCLIENT-471\nFollowing line throws an ParseException on a German system:\nd = DateUtils.parseDate(\"Wed, 09 Apr 2008 23:55:38 GMT\", new String[] \n{\"EEE, dd MMM yyyy HH:mm:ss zzz\"}\n);\nReason: parseDate internally calls SimpleDateFormat without providing a locale. This causes \"MMM\" to be interpreted using the system locale. If the system is German, the date is trying to be interpreted as German date.\nI see following solutions:\n A) Always instantiate SimpleDateFormat with Locale.ENGLISH\n B) Make two instances of SimpleDateFormat. One without providing a locale and one with Locale.ENGLISH. Try two parsings\n C) Make as many SimpleDateFormat instances as locales are availble iterate over all instances at the parsing attempts.\n D) provide an additional (optional) parameter to parseDate for providing a Locale\nI would prefer B) as this seems the best trade-off between internationalization and local usage.\nWhat do you think?", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.time.DateUtilsTest.java", "org.apache.commons.lang3.time.DateUtils.java"], "label": 1, "es_results": []}, {"bug_id": 838, "bug_title": "ArrayUtils removeElements methods clone temporary index arrays unnecessarily", "bug_description": "The private method removeAll(Object array, int... indices) sorts the indices array, so arrays passed in by application code need to be cloned first.\nHowever, where the index array is generated locally, that is unnecessary.\nThe removeElements() methods currently call the public removeAll(<type>[] array, int... indices) methods, which clone the indices before calling the private removeAll() method.\nThe removeElements() methods should call the private method directly, avoiding the unnecessary clone call.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.ArrayUtils.java"], "label": 1, "es_results": []}, {"bug_id": 822, "bug_title": "NumberUtils#createNumber - bad behaviour for leading \"--\"", "bug_description": "NumberUtils#createNumber checks for a leading \"--\" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal.\nReturning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException.\nIt&apos;s not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.math.NumberUtilsTest.java", "org.apache.commons.lang3.math.NumberUtils.java"], "label": 1, "es_results": []}, {"bug_id": 865, "bug_title": "LocaleUtils.toLocale does not parse strings starting with an underscore", "bug_description": "Hi,\nJavadocs of Locale.toString() states that \"If the language is missing, the string will begin with an underbar.\". This is not handled in the LocaleUtils.toLocale method if it is meant to be the inversion method of Locale.toString().\nThe fix for the ticket 328 does not handle well the case \"fr__P\", which I found out during fixing the first bug.\nI am attaching the patch for both problems.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.LocaleUtilsTest.java", "org.apache.commons.lang3.LocaleUtils.java"], "label": 1, "es_results": []}, {"bug_id": 881, "bug_title": " NumberUtils.createNumber() Javadoc says it does not work for octal numbers", "bug_description": "The javadoc for NumberUtils.createNumber() states:\n\"Values with leading 0&apos;s will not be interpreted as octal.\"\nHowever,\nassertEquals(25, NumberUtils.createNumber(\"0025\"));\nfails, because NumberUtils.createNumber(\"0025\") returns 21.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.math.NumberUtils.java", "org.apache.commons.lang3.math.NumberUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 882, "bug_title": "LookupTranslator accepts CharSequence as input, but fails to work with implementations other than String", "bug_description": "The core of org.apache.commons.lang3.text.translate is a HashMap<CharSequence, CharSequence> lookupMap.\nFrom the Javadoc of CharSequence (emphasis mine):\n\nThis interface does not refine the general contracts of the equals and hashCode methods. The result of comparing two objects that implement CharSequence is therefore, in general, undefined. Each object may be implemented by a different class, and there is no guarantee that each class will be capable of testing its instances for equality with those of the other. It is therefore inappropriate to use arbitrary CharSequence instances as elements in a set or as keys in a map.\nThe current implementation causes code such as the following to not work as expected:\n\n\n\nCharSequence cs1 = \"1 < 2\";\n\nCharSequence cs2 = CharBuffer.wrap(\"1 < 2\".toCharArray());\n\n\n\nSystem.out.println(StringEscapeUtils.ESCAPE_HTML4.translate(cs1));\n\nSystem.out.println(StringEscapeUtils.ESCAPE_HTML4.translate(cs2));\n\n\n\n... which gives the following results (but should be identical):\n\n1 &lt; 2\n\n1 < 2\n\n\n\nThe problem, at a minimum, is that CharBuffer.equals is even documented in the Javadoc that:\n\nA char buffer is not equal to any other type of object.\n... so a lookup on a CharBuffer in the Map will always fail when compared against the String implementations that it contains.\nAn obvious work-around is to instead use something along the lines of either of the following:\n\n\n\nSystem.out.println(StringEscapeUtils.ESCAPE_HTML4.translate(cs2.toString()));\n\nSystem.out.println(StringEscapeUtils.escapeHtml4(cs2.toString()));\n\n\n\n... which forces everything back to a String.  However, this is not practical when working with large sets of data, which would require significant heap allocations and garbage collection concerns.  (As such, I was actually trying to use the translate method that outputs to a Writer - but simplified the above examples to omit this.)\nAnother option that I&apos;m considering is to use a custom CharSequence wrapper around a char[] that implements hashCode() and equals() to work with those implemented on String.  (However, this will be interesting due to the symmetric assumption - which is further interesting that String.equals is currently implemented using instanceof - even though String is final...)", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.text.translate.LookupTranslatorTest.java", "org.apache.commons.lang3.text.translate.LookupTranslator.java"], "label": 1, "es_results": []}, {"bug_id": 920, "bug_title": "Add ArrayUtils#nullToEmpty(Class<?>[])", "bug_description": "Class is a ubiquitous array component type. This method can be immediately consumed by [lang]&apos;s own reflection code.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.ArrayUtils.java", "org.apache.commons.lang3.ArrayUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 921, "bug_title": "BooleanUtils.xor(boolean...) produces wrong results", "bug_description": "BooleanUtils.xor(true, true, false, true) returns false, although \ntrue ^ true ^ false ^ true is true. This is because the implementation only checks the count of true in the provided array.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.BooleanUtils.java", "org.apache.commons.lang3.BooleanUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 902, "bug_title": "RandomStringUtils.random (count, letters=true, number=true) may not use numerics", "bug_description": "Either there is a bug in an implementation or misunderstanding in docs.\nRandomStringUtils.random (count, letters, numbers) is documented so that:\nletters  if true, generated string will include alphabetic characters\nnumbers  if true, generated string will include numeric characters\nBut apparently the current implementation supports only that generated string may include either only letters, only numbers or both.\nThis is current implementation:\n if (letters && Character.isLetter(ch) || numbers && Character.isDigit(ch) || !letters && !numbers)\nSo there may be situation when generated string is not containing numbers at all which is in contrary with what the docs say. ", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_2", "fixed_files": ["org.apache.commons.lang3.RandomStringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 915, "bug_title": "Wrong locale handling in LocaleUtils.toLocale()", "bug_description": "The static method LocaleUtils.toLocale() fails, at least, to parse 3-char locale strings, which are completelly valid BCP47 locales.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.LocaleUtilsTest.java", "org.apache.commons.lang3.LocaleUtils.java"], "label": 1, "es_results": []}, {"bug_id": 951, "bug_title": "Fragments are wrong by 1 day when using fragment YEAR or MONTH", "bug_description": "When one trys to get the fragments of a calendar object and uses the fragment YEAR or MONTH, the returned value is wrong by 1 day in the targeted timeunit. The bug resides in the class DateUtils in function \n\n\n\nprivate static long getFragment(Calendar calendar, int fragment, int unit)\n\n\n\nThere is an initial recalculation if the fragment is YEAR or MONTH. So if one would like to have the minutes for the fragment YEAR for the date 2000-01-01 00:00:00 this would return 1440 which is actually wrong. The error can be found on lines 1635 - 1643.\nSuggested fix:\n\n\n\n// Fragments bigger than a day require a breakdown to days\n\n        switch (fragment) {\n\n            case Calendar.YEAR:\n\n                result += ((calendar.get(Calendar.DAY_OF_YEAR)-1) * MILLIS_PER_DAY) / millisPerUnit;\n\n                break;\n\n            case Calendar.MONTH:\n\n                result += ((calendar.get(Calendar.DAY_OF_MONTH)-1) * MILLIS_PER_DAY) / millisPerUnit;\n\n                break;\n\n        }\n\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.time.DateUtilsFragmentTest.java", "org.apache.commons.lang3.time.DateUtils.java"], "label": 1, "es_results": []}, {"bug_id": 943, "bug_title": "Test DurationFormatUtilsTest.testEdgeDuration fails in JDK 1.6, 1.7 and 1.8, BRST time zone", "bug_description": "While helping testing LANG-942 with JDK1.8 I always got three tests failing, while others had 2. After @britter fixed the issues related to JDK1.8, I continue getting errors with JDK1.8.\nI decided to vote for [lang] 3.2.1 and test the tag with JDK 1.8 and a few others. However, I&apos;m getting errors with any JDK, 1.6, 1.7 and 1.8 (build 121). Always the same error:\nFailed tests: \n  DurationFormatUtilsTest.testEdgeDurations:467->assertEqualDuration:562->assertEqualDuration:575 expected:<7[7]> but was:<7[6]>\nI get the same error with JDK 1.6 and the tag 3.1\nSince the test is somewhat related to Time Zones (there are some Calendar&apos;s, TimeZone.getDefault(), etc), here&apos;s my locale and time zone:\nkinow@chuva:~/java/apache/commons-lang-31$ locale\nLANG=en_US.UTF-8\nLANGUAGE=en_US\nLC_CTYPE=\"en_US.UTF-8\"\nLC_NUMERIC=\"en_US.UTF-8\"\nLC_TIME=\"en_US.UTF-8\"\nLC_COLLATE=\"en_US.UTF-8\"\nLC_MONETARY=\"en_US.UTF-8\"\nLC_MESSAGES=\"en_US.UTF-8\"\nLC_PAPER=\"en_US.UTF-8\"\nLC_NAME=\"en_US.UTF-8\"\nLC_ADDRESS=\"en_US.UTF-8\"\nLC_TELEPHONE=\"en_US.UTF-8\"\nLC_MEASUREMENT=\"en_US.UTF-8\"\nLC_IDENTIFICATION=\"en_US.UTF-8\"\nLC_ALL=\nkinow@chuva:~/java/apache/commons-lang-31$ date\nSun Jan  5 21:23:05 BRST 2014", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.time.DurationFormatUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 936, "bug_title": "StringUtils.getLevenshteinDistance with too big of a threshold returns wrong result", "bug_description": "StringUtils.getLevenshteinDistance(CharSequence s, CharSequence t, int threshold) specifies:\n\nFind the Levenshtein distance between two Strings if it&apos;s less than or equal to a given threshold.\nWhen passing a threshold > Integer.MAX_VALUE - max(s.length(), t.length()) the method always returns -1.\nThe simplest use case is passing Integer.MAX_VALUE (a common practice if one would want to find the min/max LD of a string to several other strings in an iterative fashion.\nThe code should be fixed to consider the threshold in relation to the source/target lengths, or alternatively the javadoc should be fixed to pronounce the current limit.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 883, "bug_title": "Add StringUtils.containsAny(CharSequence, CharSequence... ) method", "bug_description": "Presently there is a: public static boolean containsAny(CharSequence cs, char... searchChars). It would be useful to have the: public static boolean containsAny(CharSequence cs, CharSequence... searchCharSequences) which will return true if any of the searchCharSequences are contained within the cs. If you decide to implement it, it would be nice to have an alias method for collections, e.g., public static boolean containsAny(CharSequence cs, Collection<CharSequence> searchCharSequences)", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsEqualsIndexOfTest.java"], "label": 1, "es_results": []}, {"bug_id": 794, "bug_title": "SystemUtils.IS_OS_WINDOWS_2008, VISTA are incorrect", "bug_description": "On Windows Server 2008 R2 (a Rackspace Cloud Instance), the values of SystemUtils.IS_OS_WINDOWS_2008 and SystemUtils.IS_OS_WINDOWS_VISTA are incorrect: the former is false and the latter is true.\nI&apos;m not sure how to fix the VISTA flag (as I don&apos;t have an instance to test against), but the because of WINDOWS_2008 being set to false is that the match explicitly requires the version to be 6.1, where 6.0 can apparently also be the version number for 2008 R2.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.SystemUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1205, "bug_title": "NumberUtils.createNumber() behaves inconsistently with NumberUtils.isNumber()", "bug_description": "The NumberUtils.createNumber() method fails to check for multiple trailing characters, and as a result, it returns a value even though NumberUtils.isNumber() indicates that it should not.  For example:\n\n\n\nboolean isNumber = NumberUtils.isNumber(\"81.5514DD\");   // returns false\n\n\n\nNumber numValue = NumberUtils.createNumber(\"81.5514DD\");  // returns a Double value, 81.5514\n\n\n\nI would expect the createNumber() method to throw a NumberFormatException in this case.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.math.NumberUtilsTest.java", "org.apache.commons.lang3.math.NumberUtils.java"], "label": 1, "es_results": []}, {"bug_id": 901, "bug_title": "endsWithAny is case sensitive - documented as case insensitive", "bug_description": "endsWithAny was added in response to this task: LANG-614\nDocumentation says that the method returns \"true if the CharSequence starts with any of the the prefixes, case insensitive, or both null\" \nStringUtils.endsWithAny(\"MIME/TYPE\", \"TYPE\") true\nStringUtils.endsWithAny(\"MIME/TYPE\", \"type\") false", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_1", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsStartsEndsWithTest.java"], "label": 1, "es_results": []}, {"bug_id": 961, "bug_title": "org.apache.commons.lang3.reflect.FieldUtils.removeFinalModifier(Field) does not clean up after itself", "bug_description": "FieldUtils.removeFinalModifier(Field) calls setAccessible(true) on the Field it is working on but does not reset it to false when done, after the final modifier has been removed. Also the method does not need to call setAccessible(true) if the field is already accessible.\n\nOnly call setAccessible when needed\nRefactor and add a new method removeFinalModifier(Field field, boolean forceAccess)\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_2", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.reflect.MemberUtils.java", "org.apache.commons.lang3.reflect.FieldUtilsTest.java", "org.apache.commons.lang3.reflect.FieldUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1251, "bug_title": "SerializationUtils.ClassLoaderAwareObjectInputStream should use static initializer to initialize primitiveTypes map.", "bug_description": "SerializationUtils.ClassLoaderAwareObjectInputStream should use static initializer to initialize primitiveTypes map because initializing the map in the constructor of ClassLoaderAwareObjectInputStream would break thread safety. java.util.HashMap is not thread safe.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_2", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.SerializationUtils.java"], "label": 1, "es_results": []}, {"bug_id": 946, "bug_title": "ConstantInitializerTest fails when building with IBM JDK 7", "bug_description": "Found by Jrg Schaible during the vote for 3.2.1 RC1 [1]:\n\n\n\n========================== %< ====================\n\nFailed tests:\n\n  ConstantInitializerTest.testToString:122 Wrong string:\n\nConstantInitializer@-669671219 [ object = 42 ]\n\n========================== %< ====================\n\n\n\n[1] http://markmail.org/message/7exdutk2cktec2yy", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_2_1", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.concurrent.ConstantInitializerTest.java"], "label": 1, "es_results": []}, {"bug_id": 969, "bug_title": "StringUtils.toEncodedString(byte[], Charset) needlessly throws UnsupportedEncodingException", "bug_description": "This new method throws UnsupportedEncodingException when passed in a Charset object. Why?\nnew String(byte[], Charset) does not throw this exception.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_2_1", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 972, "bug_title": "NumberUtils#isNumber does not allow for hex 0XABCD", "bug_description": "isNumber() does not allow for 0XABCD whereas createNumber() allows for both 0xABCD and 0XABCD", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_2_1", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.math.NumberUtilsTest.java", "org.apache.commons.lang3.math.NumberUtils.java"], "label": 1, "es_results": []}, {"bug_id": 971, "bug_title": "NumberUtils#isNumber(String) fails to reject invalid Octal numbers", "bug_description": "When trying to convert \"0085\" with NumberUtils.createInteger(String) an NumberFormatException is thrown. \nThis is because the leading 0 causes the String to be evaluated as an Octal, 8 is not a valid octal.\nHowever NumberUtils#isNumber(String) evaluates to true, even so it cannot be converted.  \n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_2_1", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.math.NumberUtilsTest.java", "org.apache.commons.lang3.math.NumberUtils.java"], "label": 1, "es_results": []}, {"bug_id": 977, "bug_title": "NumericEntityEscaper incorrectly encodes supplementary characters", "bug_description": "NumericEntityEscaper will incorrectly encode supplementary unicode characters depending on the char length of the first code point in the string.\nTo reproduce, run:\n\n\n\nString escaped = NumericEntityEscaper.between(0x7f, Integer.MAX_VALUE).translate(\"a \\uD83D\\uDC14 \\uD83D\\uDCA9\");\n\n\n\nExpected:\n\n\n\nescaped == \"a &#128020; &#128169;\"\n\n\n\nActual:\n\n\n\nescaped == \"a &#128020;&#56340; &#128169;&#56489;\"\n\n\n\nThe issue lies in CharSequenceTranslator.translate() and the way it checks code points to figure out how many characters it needs to consume.  Specifically, the issue is on line 95:\n\n\n\n// contract with translators is that they have to understand codepoints \n\n// and they just took care of a surrogate pair\n\nfor (int pt = 0; pt < consumed; pt++) {\n\n    pos += Character.charCount(Character.codePointAt(input, pt));\n\n}\n\n\n\nThe point of this code is to check the charCount of the character that was just translated and move ahead by that many characters in the input string.  The bug is that it&apos;s indexing into the string using &apos;pt&apos;, which is always 0 at the beginning of the loop.  It&apos;s effetively checking the charCount of first character in the string every time.\nA patch is attached that fixes the issue and includes supporting unit tests.  Fixing this issue in CharSequenceTranslator uncovered an issue in CsvEscaper/CsvUnescaper caused by the fact that it wasn&apos;t respecting the \"code point contract\" described in CharSequenceTranslator.translate.  The fix there was to have the translate methods return the string&apos;s code point count rather than character count.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_2_1", "fixed_version": "LANG_3_3", "fixed_files": ["org.apache.commons.lang3.StringEscapeUtilsTest.java", "org.apache.commons.lang3.StringEscapeUtils.java", "org.apache.commons.lang3.text.translate.CharSequenceTranslator.java"], "label": 1, "es_results": []}, {"bug_id": 978, "bug_title": "Failing tests with Java 8 b128", "bug_description": "Gary Gregory has reported failing tests during the vote for Lang 3.3 RC1, when building with:\n\n\n\njava version \"1.8.0\"\n\nJava(TM) SE Runtime Environment (build 1.8.0-b128)\n\nJava HotSpot(TM) 64-Bit Server VM (build 25.0-b69, mixed mode)\n\n\n\nApache Maven 3.2.1 (ea8b2b07643dbb1b84b6d16e1f08391b666bc1e9;\n\n2014-02-14T12:37:52-05:00)\n\nMaven home: C:\\Java\\apache-maven-3.2.1\\bin\\..\n\nJava version: 1.8.0, vendor: Oracle Corporation\n\nJava home: C:\\Program Files\\Java\\jdk1.8.0\\jre\n\nDefault locale: en_US, platform encoding: Cp1252\n\nOS name: \"windows 7\", version: \"6.1\", arch: \"amd64\", family: \"dos\"\n\n\n\n\n\n\nFastDateFormat_ParserTest>FastDateParserTest.testParses:242->FastDateParserTest.validateSdfFormatFdpParseEquality:219  Parse FastDateParserTest.testParses:242->validateSdfFormatFdpParseEquality:219  Parse\n\n\n\nRunning org.apache.commons.lang3.time.FastDateFormat_ParserTest /1/2/10/5/??/0/0/?/+0000 ja_JP_JP_#you-ca-japanese 1867 G/y/M/d/h/a/m/s/E/Z America/New_York Tests run: 29, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.491 sec <<< FAILURE! - in org.apache.commons.lang3.time.FastDateFormat_ParserTest testParses(org.apache.commons.lang3.time.FastDateFormat_ParserTest) Time elapsed: 1.492 sec <<< ERROR! java.text.ParseException: (The ja_JP_JP_#you-ca-japanese locale does not support dates before 1868 AD)\n\n\n\nSee: http://markmail.org/message/suvorq3xrqmimnui", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3", "fixed_version": "LANG_3_3_1", "fixed_files": ["org.apache.commons.lang3.time.DurationFormatUtils.java"], "label": 1, "es_results": []}, {"bug_id": 987, "bug_title": "DateUtils.getFragmentInDays(Date, Calendar.MONTH) returns wrong days", "bug_description": "Commons lang3 screwed up my system after upgraded to 3.3 last night!\nWe use DateUtils.getFragmentInDays(Date, Calendar.MONTH) to extract days for later use. Basically (in 3.2), &apos;today&apos; (March 13) returns 13, but, it returns 12 in 3.3!\nI compared the underlying method org.apache.commons.lang3.time.DateUtils.getFragment(Calendar, int, int) between 3.2 and 3.3:\n3.2\n\n\n\n        // Fragments bigger than a day require a breakdown to days\n\n        switch (fragment) {\n\n            case Calendar.YEAR:\n\n                result += (calendar.get(Calendar.DAY_OF_YEAR) * MILLIS_PER_DAY) / millisPerUnit;\n\n                break;\n\n            case Calendar.MONTH:\n\n                result += (calendar.get(Calendar.DAY_OF_MONTH) * MILLIS_PER_DAY) / millisPerUnit;\n\n                break;\n\n        }\n\n\n\n3.3\n\n\n\n        // Fragments bigger than a day require a breakdown to days\n\n        switch (fragment) {\n\n            case Calendar.YEAR:\n\n                result += ((calendar.get(Calendar.DAY_OF_YEAR) -1) * MILLIS_PER_DAY) / millisPerUnit;\n\n                break;\n\n            case Calendar.MONTH:\n\n                result += ((calendar.get(Calendar.DAY_OF_MONTH) -1) * MILLIS_PER_DAY) / millisPerUnit;\n\n                break;\n\n            default:\n\n                break;\n\n        }\n\n\n\nIs there ANY ANY reason for adding &apos;-1&apos; in 3.3?! Plus, do you have any unit test for this method?", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3", "fixed_version": "LANG_3_3_1", "fixed_files": ["org.apache.commons.lang3.time.DateUtils.java", "org.apache.commons.lang3.time.DateUtilsFragmentTest.java"], "label": 1, "es_results": []}, {"bug_id": 995, "bug_title": "Fix bug with stripping spaces on last line in WordUtils.wrap() ", "bug_description": "Via github: https://github.com/apache/commons-lang/pull/18", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.text.WordUtilsTest.java", "org.apache.commons.lang3.text.WordUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1001, "bug_title": "ISO 8601 misspelled throughout the Javadocs", "bug_description": "The Javadocs say: ISO8601 but the correct format is ISO 8601. Note the space.\nA patch can be provided.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.time.DurationFormatUtils.java", "org.apache.commons.lang3.time.FastDateFormat.java", "org.apache.commons.lang3.time.DurationFormatUtilsTest.java", "org.apache.commons.lang3.time.FastDatePrinter.java", "org.apache.commons.lang3.time.StopWatch.java", "org.apache.commons.lang3.time.DateFormatUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1003, "bug_title": "DurationFormatUtils are not able to handle negative durations/periods", "bug_description": "It spits out complete garbage.\n\n\n\nSystem.out.println(DurationFormatUtils.formatDurationHMS(-3454));\n\nSystem.out.println(DurationFormatUtils.formatPeriodISO(4000, 3000));\n\n\n\n\n0:00:-3.-454\n\nP-1Y11M30DT23H59M59.000S\n\n\n\nIt should throw an IllegalArgumentException if duration is < 0 or period different is < 0.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.time.DurationFormatUtils.java", "org.apache.commons.lang3.time.DurationFormatUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 1027, "bug_title": "org.apache.commons.lang3.SystemUtils#isJavaVersionAtLeast should return true by default", "bug_description": "Hi\nlast [lang] release doesn&apos;t support java 9 (not in JavaVersion enum). However it shouldn&apos;t break the org.apache.commons.lang3.SystemUtils#isJavaVersionAtLeast method which should return true when java version is not known (not known = older)\nMy proposal is to add UNKNOWN to the enum and override atLeast for this particular value to return true.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.JavaVersionTest.java", "org.apache.commons.lang3.JavaVersion.java"], "label": 1, "es_results": []}, {"bug_id": 1004, "bug_title": "DurationFormatUtils#formatDurationHMS implementation does not correspond to Javadoc and vice versa", "bug_description": "This method has several flaws:\n1. Javadoc says: \"The format used is ISO8601-like: H:m:s.S.\" but the method call supplies \"H:mm:ss.SSS\"\n2. ISO time never omits leading zeros, so the proper pattern must be \"HH:mm:ss.SSS\"\n3. The method name says: \"HMS\" but includes the second fraction.\nSince the use of fractions is optional, the method should use \"HH:mm:ss\" and update the Javadoc as well.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.time.DurationFormatUtils.java", "org.apache.commons.lang3.time.DurationFormatUtilsTest.java", "org.apache.commons.lang3.time.StopWatchTest.java"], "label": 1, "es_results": []}, {"bug_id": 1000, "bug_title": "ParseException when trying to parse UTC dates with Z as zone designator using DateFormatUtils.ISO_DATETIME_TIME_ZONE_FORMAT", "bug_description": "I have the String 2013-11-18T12:48:05Z which I want to parse into a Date using DateFormatUtils.ISO_DATETIME_TIME_ZONE_FORMAT. \nAccording to http://en.wikipedia.org/wiki/ISO_8601#UTC the Z at the end should be a valid abbreviation for UTC+00:00 (or UTC) and so all should be fine.\nBut when I am trying to do so I get the following exception:\n\n\n\njava.text.ParseException: Unparseable date: \"2013-11-18T12:48:05Z\" does not match\n\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.time.DateFormatUtilsTest.java", "org.apache.commons.lang3.time.FastDatePrinter.java", "org.apache.commons.lang3.time.FastDatePrinterTest.java", "org.apache.commons.lang3.time.FastDateParser.java"], "label": 1, "es_results": []}, {"bug_id": 1041, "bug_title": "Fix MethodUtilsTest so it does not depend on JDK method ordering", "bug_description": "Placeholder for https://github.com/apache/commons-lang/pull/30.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.reflect.MethodUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 1072, "bug_title": "Duplicated \"0x\" check in createBigInteger in NumberUtils", "bug_description": "I think there is typo in below line:\norg.apache.commons.lang3.math.NumberUtils.java\n\n\n if (str.startsWith(\"0x\", pos) || str.startsWith(\"0x\", pos)) { // hex\n\n            radix = 16;\n\n            pos += 2;\n\n}\n\n\n\nThe second \"0x\" should be \"0X\"\norg.apache.commons.lang3.math.NumberUtils.java\n\n\n if (str.startsWith(\"0x\", pos) || str.startsWith(\"0X\", pos)) { // hex\n\n            radix = 16;\n\n            pos += 2;\n\n}\n\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.math.NumberUtilsTest.java", "org.apache.commons.lang3.math.NumberUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1073, "bug_title": "Read wrong component type of array in add in ArrayUtils", "bug_description": "org.apache.commons.lang3.ArrayUtils.java\n\n\n    public static <T> T[] add(final T[] array, final T element) {\n\n        Class<?> type;\n\n        if (array != null){\n\n            type = array.getClass();\n\n        } else if (element != null) {\n\n            type = element.getClass();\n\n        } else {\n\n            throw new IllegalArgumentException(\"Arguments cannot both be null\");\n\n        }\n\n        .......\n\n    }\n\n\n\nI think it should be:\norg.apache.commons.lang3.ArrayUtils.java\n\n\n    public static <T> T[] add(final T[] array, final T element) {\n\n        Class<?> type;\n\n        if (array != null){\n\n            type = array.getClass().getComponentType();\n\n        } else if (element != null) {\n\n            type = element.getClass();\n\n        } else {\n\n            throw new IllegalArgumentException(\"Arguments cannot both be null\");\n\n        }\n\n        .......\n\n    }\n\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.ArrayUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1071, "bug_title": "Fix wrong examples in JavaDoc of StringUtils.replaceEachRepeatedly(...), StringUtils.replaceEach(...)", "bug_description": "StringUtils.replaceEachRepeatedly(, *, *) has javadoc with examples of invokation of StringUtils.replaceEach(, *, *):\nhttps://svn.apache.org/viewvc/commons/proper/lang/trunk/src/main/java/org/apache/commons/lang3/StringUtils.java?revision=1639624&view=markup#l4588\nStringUtils.replaceEach(, *, *, *, *) has javadoc with examples of invokation of StringUtils.StringUtils.replaceEach(, *, *, *). Note different number of arguments.\nhttps://svn.apache.org/viewvc/commons/proper/lang/trunk/src/main/java/org/apache/commons/lang3/StringUtils.java?revision=1639624&view=markup#l4639\nPlease fix javadoc.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1055, "bug_title": "StrSubstitutor.replaceSystemProperties does not work consistently", "bug_description": "StrSubsitutor caches a references to SystemProperties statically on first class references.  This does not work properly with System.setProperties().  For example the following code:\n\n\n\npackage test.utilities;\n\n\n\nimport java.util.Properties;\n\n\n\nimport org.apache.commons.lang.text.StrSubstitutor;\n\n\n\npublic class TestStrSubstitutor {\n\n\n\n    public static void main(String[] args) {\n\n        System.out.println(StrSubstitutor.replaceSystemProperties(\"os.name=${os.name}\"));\n\n        Properties testProps = new Properties();\n\n        testProps.put(\"test_key\",  \"test_value\");\n\n        testProps.putAll(System.getProperties());\n\n        System.setProperties(testProps);\n\n        System.out.println(StrSubstitutor.replace(\"test_key=${test_key}\", System.getProperties()));\n\n        System.out.println(StrSubstitutor.replaceSystemProperties(\"test_key=${test_key}\"));\n\n    }\n\n\n\n}\n\n\n\nproduces the following output:\n\n\n\nos.name=Windows 7\n\ntest_key=test_value\n\ntest_key=${test_key}\n\n\n\nThe code linked here shows the static caching of the System Properties reference: http://commons.apache.org/proper/commons-lang/apidocs/src-html/org/apache/commons/lang3/text/StrLookup.html", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.text.StrSubstitutorTest.java", "org.apache.commons.lang3.text.StrLookupTest.java", "org.apache.commons.lang3.text.StrLookup.java"], "label": 1, "es_results": []}, {"bug_id": 1081, "bug_title": "DiffBuilder.append(String, Object left, Object right) does not do a left.equals(right) check", "bug_description": "Only testing == without testing equals() seems wrong.\nShould an equals() test be added after determining that the parameters are not arrays?", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.builder.DiffBuilder.java", "org.apache.commons.lang3.builder.DiffBuilderTest.java"], "label": 1, "es_results": []}, {"bug_id": 1092, "bug_title": "Wrong formating of time zones with daylight saving time in FastDatePrinter", "bug_description": "At work we&apos;re getting build issues with Lang 3.3.2 (and any since 3.2 when the test code was introduced in LANG-818).  The test org.apache.commons.lang3.time.FastDatePrinterTest.testCalendarTimezoneRespected picks a timezone and runs a test on it. One assumes that timezones usually work, but some are not - so it depends on the order of timezones returned by TimeZone.getAvailableIDs().\nThis would seem to imply a daylight savings time bug in FastDateFormat. This may be the same issue as LANG-916.\nIf you adjust the for loop such that the test is within the loop and happens on every timezone, you will hit timezones that fail.  e.g.:\n\n\n\nIndex: FastDatePrinterTest.java\n\n===================================================================\n\n--- FastDatePrinterTest.java\t(revision 1665715)\n\n+++ FastDatePrinterTest.java\t(working copy)\n\n@@ -269,8 +269,6 @@\n\n         for (final String zone : availableZones) {\n\n             if (!zone.equals(currentZone.getID())) {\n\n                 anotherZone = TimeZone.getTimeZone(zone);\n\n-            }\n\n-        }\n\n         \n\n         assertNotNull(\"Cannot find another timezone\", anotherZone);\n\n         \n\n@@ -282,6 +280,8 @@\n\n         final String expectedValue = sdf.format(cal.getTime());\n\n         final String actualValue = FastDateFormat.getInstance(pattern).format(cal);\n\n         assertEquals(expectedValue, actualValue);\n\n+            }\n\n+        }\n\n     }\n\n     \n\n     @Test\n\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_4", "fixed_files": ["org.apache.commons.lang3.time.FastDatePrinter.java", "org.apache.commons.lang3.time.FastDatePrinterTest.java"], "label": 1, "es_results": []}, {"bug_id": 1069, "bug_title": "CharSet.getInstance documentation does not clearly explain how to include negation character in set", "bug_description": "As discussed in this Stack Overflow question, the documentation for CharSet.getInstance() don&apos;t explain clearly how to include the negation character (^) as a literal character.\nThe two solutions suggested in the SO question are:\n\n\n\n// Add the &apos;^&apos; on its own\n\nCharSet.getInstance(\"^\", \"otherlettershere\");\n\n\n\n// Add the &apos;^&apos; as the last character\n\nCharSet.getInstance(\"otherlettershere^\")\n\n\n\nIf those are the best options, we should add a line to the Javadoc to indicate this. If there is a better way, clearly that should be documented instead.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.CharSet.java", "org.apache.commons.lang3.CharSetTest.java"], "label": 1, "es_results": []}, {"bug_id": 1002, "bug_title": "Several predefined ISO FastDateFormats in DateFormatUtils are incorrect", "bug_description": "Formats ISO_TIME_FORMAT, ISO_TIME_TIME_ZONE_FORMAT prepend a T but this is not correct. Sole time is never prepended by defintion. T is used only when date and time are given.\nThe Javadocs of ISO_TIME_NO_T_FORMAT, ISO_TIME_NO_T_FORMAT are in correct too because they say: \"This pattern does not comply with the formal ISO8601 specification as the standard requires the &apos;T&apos; prefix for times.\"\nYou might want to read Markus Kuhn&apos;s reference on that.\nA solution would be remove the first two and rename the second two by dropping the NO_T in the name.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.time.FastDatePrinter.java", "org.apache.commons.lang3.time.FastDateFormat.java", "org.apache.commons.lang3.time.DateFormatUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1018, "bug_title": "NumberUtils.createNumber(final String str)  Precision will be lost", "bug_description": "With commons-lang 3.2.2:\nNumberUtils.createNumber(\"-160952.54\");\nThe result is \"-160952.55\".\nShould not be based on the length of the decimal point number to judge whether the floating point number.\nUsing the method (createFloat(str)) of dealing with the valid number greater than seven Numbers will cause accuracy loss.\nThe source code is as follows:\n\n\n\ntry {\n\n            if(numDecimals <= 7){// If number has 7 or fewer digits past the decimal point then make it a float\n\n                final Float f = createFloat(str);\n\n                if (!(f.isInfinite() || (f.floatValue() == 0.0F && !allZeros))) {\n\n                    return f;\n\n                }\n\n            }\n\n        } catch (final NumberFormatException nfe) { // NOPMD\n\n            // ignore the bad number\n\n        }\n\n        try {\n\n            if(numDecimals <= 16){// If number has between 8 and 16 digits past the decimal point then make it a double\n\n                final Double d = createDouble(str);\n\n                if (!(d.isInfinite() || (d.doubleValue() == 0.0D && !allZeros))) {\n\n                    return d;\n\n                }\n\n            }\n\n        } catch (final NumberFormatException nfe) { // NOPMD\n\n            // ignore the bad number\n\n        }\n\n\n\n        return createBigDecimal(str);\n\n    }\n\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_3_2", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.math.NumberUtilsTest.java", "org.apache.commons.lang3.math.NumberUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1112, "bug_title": "MultilineRecursiveToStringStyle largely unusable due to being package-private", "bug_description": "The new MultilineRecursiveToStringStyle which was added under LANG-1052 is a package-private scoped class (no specific scope is specified for the class). I&apos;m assuming, like RecursiveToStringStyle, it should be public instead?", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.builder.MultilineRecursiveToStringStyle.java"], "label": 1, "es_results": []}, {"bug_id": 1132, "bug_title": "ReflectionToStringBuilder does not throw IllegalArgumentException when the constructor's object param is null", "bug_description": "Placeholder for https://github.com/apache/commons-lang/pull/85\n\nAs described in it&apos;s javadoc, ReflectionToStringBuilder constructor will throw IllegalArgumentException if the Object to build a toStringfor is null, while in fact it won&apos;t.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.builder.ReflectionToStringBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 1147, "bug_title": "EnumUtils *BitVector issue with more than 32 values Enum", "bug_description": "In EnumUtils all BitVector related methods fail in handling Enums with more than 32 values.\nThis is due to a implicit int -> long conversion in generating the Enum value long mask.\nBad code : here 1 is an int value so the << operation is done into an int context and then, the result is converted to a long value\n\n\n\nlong mask = 1 << 32;    // -> mask = 1 and not 4294967296 (0x100000000)\n\n\n\nGood code : here 1L is a long value so the << operation is done into an long context\n\n\n\nlong mask = 1L << 32;    // -> mask = 4294967296 (0x100000000)\n\n\n\nSee PR#97 : https://github.com/apache/commons-lang/pull/97", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.EnumUtils.java", "org.apache.commons.lang3.EnumUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 1141, "bug_title": "StrLookup.systemPropertiesLookup() no longer reacts on changes on system properties", "bug_description": "In versions of Commons Lang before 3.4 a Lookup object created from the systemPropertiesLookup() method offered a live view of system properties. So if a system property was changed after the creation of the lookup object, the new value was visible. In version 3.4 this is no longer the case.\nThe change seems to be related to LANG-1055. Here a fix was implemented which initializes a lookup object for system properties with a snapshot copy of the current properties. Changes made later on system properties are no longer reflected. I do not understand the background for this change because this is not really related to the original bug report.\nI would propose an implementation which fixes the reported problem in LANG-1055 and allows a live view on system properties. Maybe the snapshot use case could still be supported by an overloaded method.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.text.StrLookupTest.java", "org.apache.commons.lang3.text.StrLookup.java"], "label": 1, "es_results": []}, {"bug_id": 1142, "bug_title": "StringUtils#capitalize: Javadoc says toTitleCase; code uses toUpperCase", "bug_description": "The capitalize Javadoc says the code uses  Character#toTitleCase, however the code actually uses Character#toUpperCase.\nGenerally these produce the same result, but some charsets may have different characters for upper and title case - see for example the Javadoc [1] for Character#isTitleCase.\nThe way I read this, the character that looks like \"lj\" is lower-case, \"LJ\" is upper case and \"Lj\" is title case - i.e. not the same.\nThe question here is: should the code be corrected to use TitleCase or should the Javadoc be corrected to use UpperCase?\n[1] http://docs.oracle.com/javase/7/docs/api/java/lang/Character.html#isTitleCase%28char%29", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 1162, "bug_title": "StringUtils#equals fails with Index OOBE on non-Strings with identical leading prefix", "bug_description": "From the Commons User mailing list:\n\nStringUtils.equals(cs1,cs2) delegates to CharSequence.regionMatches(...) in a way that causes IndexOutOfBounds when either of cs1/cs2 isn&apos;t a String.\nSpecifically, comparing \"foo\" and \"foobar\" for non-String CharSequences bombs due to CharSequenceUtils.regionMatches(cs1, false, 0, cs2, 0, Math.max(cs1.length(), cs2.length())) because regionMatches doesn&apos;t check for input exhaustion.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsEqualsIndexOfTest.java"], "label": 1, "es_results": []}, {"bug_id": 1163, "bug_title": "There are no tests for CharSequenceUtils.regionMatches", "bug_description": "There are no tests for CharSequenceUtils.regionMatches.\nIt ought to behave the same way as the String version.\nThis includes not failing with Index OOBE if the CharSequences are not long enough.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.CharSequenceUtilsTest.java", "org.apache.commons.lang3.CharSequenceUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1182, "bug_title": "Clarify JavaDoc of StringUtils.containsAny()", "bug_description": "(1) The javadoc for StringUtils.containsAny(CharSequence cs, CharSequence... searchCharSequences) confusingly says \n\nStringUtils.containsAny(\"abcd\", \"ab\", \"cd\") = false\n\nYou can verify this actually returns true by running this:\n        if (!StringUtils.containsAny(\"abcd\", \"ab\", \"cd\")) \n            throw new AssertionError(\"Third sample from StringUtils 3.4 javadoc\");\n(2) The javadoc for containsAny(final CharSequence cs, final CharSequence searchChars) is inadequate, and could easily mislead naive readers to believe this containsAny() looks for a matching sequence (substring) rather than characters in a set:\n\nStringUtils.containsAny(\"zzabyycdxx\", \"za\") = true\nStringUtils.containsAny(\"zzabyycdxx\", \"by\") = true\n\nIn other words, both examples would be equally true for StringUtils.contains().  I suggest adding clarifying examples, like:\n\nStringUtils.containsAny(\"zzabyycdxx\", \"\\tx\") = true\nStringUtils.containsAny(\"zzabyycdxx\", \"$.#yF\") = true\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1193, "bug_title": "ordinalIndexOf(\"abc\", \"ab\", 1) gives incorrect answer of -1 (correct answer should be 0)", "bug_description": "In Apache Commons Lang 3.4, StringUtils.ordinalIndexOf(\"abc\", \"ab\", 1) gives incorrect answer of -1 (correct answer should be 0)but StringUtils.ordinalIndexOf(\"abc\", \"a\", 1) gives correct answer of 0.\nBased on the above mentioned observation, the bug occurrs if the searchStr is of length > 1, and locates at the index 0 of the str.\nIn Apache Commons Lang 2.6, this bug is not observed.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsEqualsIndexOfTest.java"], "label": 1, "es_results": []}, {"bug_id": 1200, "bug_title": "StringUtils.ordinalIndexOf: Add missing right parenthesis in JavaDoc example", "bug_description": "https://github.com/apache/commons-lang/pull/120", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.StringUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1191, "bug_title": "Incorrect Javadoc StringUtils.containsAny(CharSequence, CharSequence...) ", "bug_description": "Javadoc for boolean org.apache.commons.lang3.StringUtils.containsAny(CharSequence cs, CharSequence... searchCharSequences) says:\nStringUtils.containsAny(\"abcd\", \"ab\", \"cd\") = false\nwhich is not true. It should be:\nStringUtils.containsAny(\"abcd\", \"ab\", \"cd\") = true", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsEqualsIndexOfTest.java"], "label": 1, "es_results": []}, {"bug_id": 1207, "bug_title": "StringUtils.equals with CharSequence - IndexOutOfBoundsException", "bug_description": "Good day,\nThis is my first report here, so I&apos;m really sorry if I did not fill in the form right .\nI just ran into a bug with the method \npublic static boolean equals(final CharSequence cs1, final CharSequence cs2).\nIf one of the object is not the String object, the method use the CharSequenceUtils to check the equality. \nThe problem is that using Math.max(cs1.length(), cs2.length()) give the max length of the 2 objects. Then 1 of the object throw IndexOutOfBoundsException.\nI think it will be better to check the size before using CharSequenceUtils as the method equalsIgnoreCase.\nMaybe this code could correct the bug :\nif (cs1 == cs2) \n{\n\n    return true;\n\n}\n else if (cs1 == null || cs2 == null) \n{\n\n    return false;\n\n} else if (cs1.length() != cs2.length()) {\n    return false;\n}\n else if (cs1 instanceof String && cs2 instanceof String) \n{\n\n    return cs1.equals(cs2);\n\n}\n else \n{\n\n    return CharSequenceUtils.regionMatches(cs1, false, 0, cs2, 0, cs1.length());\n\n}\n\nKind regards,", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsEqualsIndexOfTest.java"], "label": 1, "es_results": []}, {"bug_id": 1232, "bug_title": "DiffBuilder: Add null check on fieldName when appending Object or Object[]", "bug_description": "The other append methods throw an IllegalArgumentException if fieldName is null. So, append(Object) and append(Object[]) should also do this.\nsource: https://github.com/apache/commons-lang/pull/121", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.builder.DiffBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 1178, "bug_title": "ArrayUtils.removeAll(Object array, int... indices) should do the clone, not its callers", "bug_description": "The method ArrayUtils.removeAll(Object array, int... indices) currently sorts the input indices array. Therefore the array needs to be cloned; this is currently done by the callers.\nHowever the sort is an implementation detail of the method, so should be done by the method itself, not by the callers, which is fragile (easy to overlook when creating a new method) and unnecessary.\nThis would also allow the method to be more easily changed to a different implementation that does not need to sort the array (e.g. using BitSet)", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.ArrayUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1242, "bug_title": " \"\\u2284\":\"\u00e2\u0160\u201e\" mapping missing from EntityArrays#HTML40_EXTENDED_ESCAPE", "bug_description": "see: https://github.com/apache/commons-lang/pull/159", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.text.translate.EntityArrays.java"], "label": 1, "es_results": []}, {"bug_id": 1199, "bug_title": "Fix implementation of StringUtils.getJaroWinklerDistance()", "bug_description": "The current implementation of StringUtils.getJaroWinklerDistance() does not compute the correct result in some cases. See #LANG-944 for the initial code contribution.\nStringUtils.getJaroWinklerDistance(\"Haus Ingeborg\", \"Ingeborg Esser\") == 0.0\nThis is due to the incorrect computation of common characters, which causes the algorithm to exit prematurely.\nIn contrast, the implementation in Lucene gives ~0.63, which is about right.\n    JaroWinklerDistance d = new JaroWinklerDistance();\n    getDistance(\"Haus Ingeborg\", \"Ingeborg Esser\");\nSee https://lucene.apache.org/core/3_0_3/api/contrib-spellchecker/org/apache/lucene/search/spell/JaroWinklerDistance.html", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 1248, "bug_title": "FastDatePrinter Memory allocation regression", "bug_description": "when the code was migrated from StringBuffer to Appendable in LANG-1152.\nWe&apos;ve lost the ability to modify the buffer (setCharAt) \nThe new implementation of appendFullDigits allocate a temporary char array to work around that limitation.\nThis is a major source of memory allocation which is not present in version 3.4.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.time.FastDatePrinter.java"], "label": 1, "es_results": []}, {"bug_id": 1202, "bug_title": "parseDateStrictly does't pass specified locale", "bug_description": "LANG-799 added support for specifying a locale, but parseDateStrictly() doesn&apos;t pass it to the final parseDateWithLeniency() method.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.time.DateUtils.java", "org.apache.commons.lang3.time.DateUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 1226, "bug_title": "StringUtils#normalizeSpace does not trim the string anymore", "bug_description": "These work with 3.3.2, but fail with 3.4:\n\n\n\nassertEquals(\"b\", StringUtils.normalizeSpace(\"\\u0000b\"));\n\nassertEquals(\"b\", StringUtils.normalizeSpace(\"b\\u0000\"));\n\n\n\nJava doc still says \"... Additionally #trim(String)} removes control characters (char <= 32) from both ends of this String.\"", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.StringUtils.java", "org.apache.commons.lang3.StringUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 1190, "bug_title": "TypeUtils.isAssignable throws NullPointerException when fromType has type variables and toType generic superclass specifies type variable", "bug_description": "\n\n\nimport static org.apache.commons.lang3.reflect.TypeUtils.*;\n\n\n\nimport java.lang.reflect.InvocationTargetException;\n\nimport java.lang.reflect.Type;\n\nimport java.util.ArrayList;\n\n\n\npublic class Demonstration {\n\n\n\n    public static <YOU> Iterable<YOU> someMethod() { return null; }\n\n    \n\n    public static class WorkingClass extends ArrayList { }\n\n    public static class FailingClass extends ArrayList<Object> { }\n\n    \n\n    \n\n    public static void main(String[] args) throws NoSuchMethodException, SecurityException, IllegalAccessException, IllegalArgumentException, InvocationTargetException {\n\n        Type fromType = Demonstration.class.getDeclaredMethod(\"someMethod\").getGenericReturnType();\n\n        Type workingToType = wildcardType().withLowerBounds(WorkingClass.class).build();\n\n        Type failingToType = wildcardType().withLowerBounds(FailingClass.class).build();\n\n        \n\n        System.out.println(fromType);\n\n        System.out.println(workingToType);\n\n        System.out.println(failingToType);\n\n        \n\n        System.out.println(isAssignable(fromType, workingToType));\n\n        System.out.println(isAssignable(fromType, failingToType));\n\n    }\n\n}\n\n\n", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.reflect.TypeUtilsTest.java", "org.apache.commons.lang3.reflect.TypeUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1214, "bug_title": "ClassUtils.getClass(ClassLoader, String) fails for \"void\"", "bug_description": "ClassUtils.getClass(ClassUtils.class.getClassLoader(), \"void\") throws \"ClassNotFoundException: [V\".\nRoot because: ClassUtils contains an abbreviationMap for use in dealing with abbreviated primitive types like \"[I\" and \"[J\". However, this commit introduces a \"void -> V\" mapping for \"completeness\".\nThis seems wholly erroneous, since the maps are used only for primitive array abbreviations, and \"void/V\" is not among them.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.ClassUtilsTest.java", "org.apache.commons.lang3.ClassUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1262, "bug_title": "CompareToBuilder.append(Object,Object,Comparator) method is too big to be inlined", "bug_description": "Issue is the same as in LANG-1218: CompareToBuilder.append(Object,Object,Comparator) is quite big, due to in-place arrays processing, and thus breaching inlining threshold (325 bytecodes):\n\n....\n\no.a.c.l.b.CompareToBuilder::append (346 bytes)   hot method too big\n\n....\n\n\n\nThis prevents CompareToBuilder object itself from being scalarized.\nFix may be the same, as in LANG-1218: extract arrays processing into separate method.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.builder.CompareToBuilder.java"], "label": 1, "es_results": []}, {"bug_id": 1252, "bug_title": "NumberUtils.isNumber and NumberUtils.createNumber resolve inconsistently", "bug_description": "In considering the issues LANG-1060, LANG-1040, LANG-1038, and LANG-992, it seems that there are times when NumberUtils.isNumber resolves to false while NumberUtils.createNumber, given the same input, does not throw an exception, returning a valid java.lang.Number. This inconsistency should be resolved either by making isNumber more lenient or createNumber more stringent.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.math.NumberUtils.java", "org.apache.commons.lang3.math.NumberUtilsTest.java", "org.apache.commons.lang3.math.package-info.java"], "label": 1, "es_results": []}, {"bug_id": 1197, "bug_title": "Prepare Java 9 detection", "bug_description": "In anticipation of Java 9 and JEP 223, the versioning scheme will change completely. We need to reflect that in SystemUtils, JavaVersion, etc.:\n\nSystemUtils: Deprecate IS_JAVA_1_9 and replace with IS_JAVA_9\nJavaVersion: Deprecate JAVA_1_9 in JavaVersion and replace with JAVA_9\n\nand other places in the library.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.JavaVersionTest.java", "org.apache.commons.lang3.SystemUtilsTest.java", "org.apache.commons.lang3.JavaVersion.java", "org.apache.commons.lang3.SystemUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1261, "bug_title": "ArrayUtils.contains returns false for instances of subtypes", "bug_description": "ArrayUtils.contains(Object[] array, Object objectToFind) wrongly returns false.\nSTEPS TO REPRODUCE\n=========================================================\n-Create a superclass \"Parent\" and override equals and hashcode based on some member id variable.\n-Create a class \"Child\" extending \"Parent\". Do not override equals nor hashcode.\n-Let \"childrens\" be an array of type Child[] containing several instances.\nCreate an instance of Parent \"p\" with the same id as childrens[0], such that childrens[0].equals(p) returns true and p.equals(childrens[0]) returns true as well.\nBecause they are equals, ArrayUtils.contains(childrens, p) should return true. However it returns false.\nWHERE THE BUG IS LOCATED\n=====================================================\n-Go to ArrayUtils.class, line 1917. In the \"indexOf\" method implementation, before going into calling equals for each element of the input array, there is some sort of optimization check to make sure the instance to be found is an instance of the array type:\n} else if (array.getClass().getComponentType().isInstance(objectToFind)) {\nThat line is wrong. In our case, the array contains elements of type \"Child\", whereas the object to be found is of type \"Parent\". They are equals according to the equals implementation of \"Parent\", but obviously Children.class.isInstance(p) is false.\nEXPECTED BEHAVIOR\n================================================\nSince the method signature accepts an array of Object[] and an instance of Object, it should ignore the classes of the arguments. It should be possible to call \"ArrayUtils.contains(Child[] children, Parent p)\", in fact it should be possible to do this with any combination of classes, not only the ones assignable from the class hierarchy.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.ArrayUtils.java", "org.apache.commons.lang3.ArrayUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 1216, "bug_title": "NumberUtils.isNumber bug", "bug_description": "I found that when using NumberUtilsorg.apache.commons.lang3.math.NumberUtils.isNumber(version3.4) isNumber method the following phenomenon, \nwhen the parameter is 1.0 is true, but when the parameter is 0.1 is displayed is false.\nWhen I use 0.1 as the parameter tuning isNumber method, \nview the source code in the method of the method of line 1370 discovered a problem.\nWhen the analytical parameter is \". \", this time the chars [I] value is 46, direct return false.\nI think to do so is not very reasonable, please give me your guidance.", "project": "Commons", "sub_project": "LANG", "version": "LANG_3_4", "fixed_version": "LANG_3_5", "fixed_files": ["org.apache.commons.lang3.math.NumberUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 197, "bug_title": "RandomDataImpl.nextPoisson() is extreme slow for large lambdas", "bug_description": "The RandomDataImpl.nextPoisson() is extreme slow for large lambdas:\nE.g. drawing 100 random numbers with lambda = 1000 takes around 10s on my dual core with 2.2GHz.\nWith lambda smaller than 500 everything is fine. Any ideas?\n    RandomDataImpl are = new RandomDataImpl();\n    r.reSeed(101);\n    int d = 100;\n    long poissonLambda = 1000;\n    long st = System.currentTimeMillis();\n    for (int row = 0; row < d; row++) \n{\n      long nxtRnd = r.nextPoisson(poissonLambda);\n    }\n    System.out.println(\"delta \" + (System.currentTimeMillis() - st));", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_0", "fixed_version": "MATH_2_0", "fixed_files": ["org.apache.commons.math.random.RandomDataTest.java", "org.apache.commons.math.random.RandomDataImpl.java"], "label": 1, "es_results": []}, {"bug_id": 298, "bug_title": "EmpiriicalDisributionImpl.getUpperBounds does not return upper bounds on data bins", "bug_description": "Per the javadoc, the getUpperBounds method in the EmpiricalDistribution should return upper bounds for the bins used in computing the empirical distribution and the bin statistics.  What the method actually returns is the upper bounds of the subintervals of [0,1] used in generating data following the empirical distribution.", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.random.EmpiricalDistributionImpl.java"], "label": 1, "es_results": []}, {"bug_id": 309, "bug_title": "nextExponential parameter check bug - patch supplied", "bug_description": "Index: src/main/java/org/apache/commons/math/random/RandomDataImpl.java\n===================================================================\n src/main/java/org/apache/commons/math/random/RandomDataImpl.java\t(revision 830102)\n+++ src/main/java/org/apache/commons/math/random/RandomDataImpl.java\t(working copy)\n@@ -462,7 +462,7 @@\n\n@return the random Exponential value\n      */\n     public double nextExponential(double mean) {\n\n\nif (mean < 0.0) {\n+        if (mean <= 0.0) \nUnknown macro: {             throw MathRuntimeException.createIllegalArgumentException(                   \"mean must be positive ({0})\", mean);         } \n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.random.RandomDataImpl.java", "org.apache.commons.math.random.RandomDataTest.java"], "label": 1, "es_results": []}, {"bug_id": 282, "bug_title": "ChiSquaredDistributionImpl.cumulativeProbability > 1", "bug_description": "Calling \nnew ChiSquaredDistributionImpl(1.0).cumulativeProbability(66.41528551683048)\nreturns 1.000000000000004, which is bogus (should never be > 1)", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.random.RandomDataTest.java", "org.apache.commons.math.distribution.PoissonDistributionTest.java"], "label": 1, "es_results": []}, {"bug_id": 153, "bug_title": "RandomDataImpl nextInt(int, int) nextLong(long, long)", "bug_description": "RandomDataImpl.nextInt(Integer.MIN_VALUE, Integer.MAX_VALUE) suffers from overflow errors.\nchange\nreturn lower + (int) (rand.nextDouble() * (upper - lower + 1));\nto\nreturn (int) (lower + (long) (rand.nextDouble()*((long) upper - lower + 1)));\nadditional checks must be made for the nextlong(long, long) method.\nAt first I thought about using MathUtils.subAndCheck(long, long) but there is only an int version avalible, and the problem is that subAndCheck internaly uses long values to check for overflow just as my previous channge proposes.  The only thing I can think of is using a BigInteger to check for the number of bits required to express the difference.", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_1", "fixed_version": "MATH_1_2", "fixed_files": ["org.apache.commons.math.random.RandomDataTest.java", "org.apache.commons.math.random.RandomDataImpl.java"], "label": 1, "es_results": []}, {"bug_id": 166, "bug_title": "Special functions not very accurate", "bug_description": "The Gamma and Beta functions return values in double precision but the default epsilon is set to 10e-9. I think that the default should be set to the highest possible accuracy, as this is what I&apos;d expect to be returned by a double precision routine. Note that the erf function already uses a call to Gamma.regularizedGammaP with an epsilon of 1.0e-15.", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_1", "fixed_version": "MATH_1_2", "fixed_files": ["org.apache.commons.math.special.Gamma.java", "org.apache.commons.math.special.GammaTest.java", "org.apache.commons.math.special.Beta.java", "org.apache.commons.math.special.BetaTest.java"], "label": 1, "es_results": []}, {"bug_id": 175, "bug_title": "chiSquare(double[] expected, long[] observed) is returning incorrect test statistic", "bug_description": "ChiSquareTestImpl is returning incorrect chi-squared value. An implicit assumption of public double chiSquare(double[] expected, long[] observed) is that the sum of expected and observed are equal. That is, in the code:\nfor (int i = 0; i < observed.length; i++) \n{\n            dev = ((double) observed[i] - expected[i]);\n            sumSq += dev * dev / expected[i];\n        }\nthis calculation is only correct if sum(observed)==sum(expected). When they are not equal then one must rescale the expected value by sum(observed) / sum(expected) so that they are.\nIronically, it is an example in the unit test ChiSquareTestTest that highlights the error:\nlong[] observed1 = \n{ 500, 623, 72, 70, 31 }\n;\n        double[] expected1 = \n{ 485, 541, 82, 61, 37 }\n;\n        assertEquals( \"chi-square test statistic\", 16.4131070362, testStatistic.chiSquare(expected1, observed1), 1E-10);\n        assertEquals(\"chi-square p-value\", 0.002512096, testStatistic.chiSquareTest(expected1, observed1), 1E-9);\n16.413 is not correct because the expected values do not make sense, they should be: 521.19403 581.37313  88.11940  65.55224  39.76119 so that the sum of expected equals 1296 which is the sum of observed.\nHere is some R code (r-project.org) which proves it:\n> o1\n[1] 500 623  72  70  31\n> e1\n[1] 485 541  82  61  37\n> chisq.test(o1,p=e1,rescale.p=TRUE)\n        Chi-squared test for given probabilities\ndata:  o1 \nX-squared = 9.0233, df = 4, p-value = 0.06052\n> chisq.test(o1,p=e1,rescale.p=TRUE)$observed\n[1] 500 623  72  70  31\n> chisq.test(o1,p=e1,rescale.p=TRUE)$expected\n[1] 521.19403 581.37313  88.11940  65.55224  39.76119\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_1", "fixed_version": "MATH_1_2", "fixed_files": ["org.apache.commons.math.stat.inference.ChiSquareTestTest.java", "org.apache.commons.math.stat.inference.TestUtilsTest.java", "org.apache.commons.math.stat.inference.ChiSquareTestImpl.java"], "label": 1, "es_results": []}, {"bug_id": 184, "bug_title": "cumulativeProbability((double)n, (double)n) returns 0 for integer distributions", "bug_description": "cumulativeProbability((double)n, (double)n) returns 0 for\ndiscrete/integer distributions\nI suppose AbstractIntegerDistribution.cumulativeProbability(double,\ndouble) should be overridden to call its (int, int) version instead of\nusing default one from AbstractDistribution", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_1", "fixed_version": "MATH_1_2", "fixed_files": ["org.apache.commons.math.distribution.AbstractIntegerDistribution.java", "org.apache.commons.math.distribution.IntegerDistributionAbstractTest.java"], "label": 1, "es_results": []}, {"bug_id": 306, "bug_title": "Method 'divide' in class 'Complex' uses a false formula for a special case resulting in erroneous division by zero.", "bug_description": "The formula that &apos;divide&apos; wants to implement is\n( a + bi )  /  ( c + di )  =  ( ac + bd + ( because - ad ) i )  /  ( c^2 + d^2 )\nas correctly written in the description.\nWhen c == 0.0 this leads to the special case\n( a + bi )  /  di  = ( b / d ) - ( a / d ) i\nBut the corresponding code is:\nif (c == 0.0) \n{\n\n    return createComplex(imaginary/d, -real/c);\n\n}\n\nThe bug is the last division -real/c, which should obviously be -real/d.", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_1", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.complex.Complex.java", "org.apache.commons.math.complex.ComplexTest.java"], "label": 1, "es_results": []}, {"bug_id": 201, "bug_title": "T-test p-value precision hampered by machine epsilon", "bug_description": "The smallest p-value returned by TTestImpl.tTest() is the machine epsilon, which is 2.220446E-16 with IEEE754 64-bit double precision floats.\nWe found this bug porting some analysis software from R to java, and noticed that the p-values did not match up.  We believe we&apos;ve identified why this is happening in commons-math-1.2, and a possible solution.\nPlease be gentle, as I am not a statistics expert!\nThe following method in org.apache.commons.math.stat.inference.TTestImpl currently implements the following method to calculate the p-value for a 2-sided, 2-sample t-test:\nprotected double tTest(double m1, double m2, double v1, double v2,  double n1, double n2)\nand it returns:\n        1.0 - distribution.cumulativeProbability(-t, t);\nat line 1034 in version 1.2.\ndouble cumulativeProbability(double x0, double x1) is implemented by org.apache.commons.math.distribution.AbstractDisstribution, and returns:\n        return cumulativeProbability(x1) - cumulativeProbability(x0);\nSo in essence, the p-value returned by TTestImpl.tTest() is:\n1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t))\nFor large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0.  When cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because:\n1.0 - 1.0 + 0.0 = 0.0\nAn alternative calculation for the p-value of a 2-sided, 2-sample t-test is:\np = 2.0 * cumulativeProbability(-t)\nThis calculation does not suffer from the machine epsilon problem, and we are now getting p-values much smaller than the 2.2E-16 limit we were seeing previously.", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_2", "fixed_version": "MATH_2_0", "fixed_files": ["org.apache.commons.math.stat.inference.TTestImpl.java"], "label": 1, "es_results": []}, {"bug_id": 252, "bug_title": "Fraction.comparTo returns 0 for some differente fractions", "bug_description": "If two different fractions evaluate to the same double due to limited precision,\nthe compareTo methode returns 0 as if they were identical.\n\n\n\n// value is roughly PI - 3.07e-18\n\nFraction pi1 = new Fraction(1068966896, 340262731);\n\n\n\n// value is roughly PI + 1.936e-17\n\nFraction pi2 = new Fraction( 411557987, 131002976);\n\n\n\nSystem.out.println(pi1.doubleValue() - pi2.doubleValue()); // exactly 0.0 due to limited IEEE754 precision\n\nSystem.out.println(pi1.compareTo(pi2)); // display 0 instead of a negative value\n\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_1_2", "fixed_version": "MATH_2_0", "fixed_files": ["org.apache.commons.math.fraction.Fraction.java", "org.apache.commons.math.fraction.FractionTest.java"], "label": 1, "es_results": []}, {"bug_id": 283, "bug_title": "MultiDirectional optimzation loops forver if started at the correct solution", "bug_description": "MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution.\nsee the attached test case (testMultiDirectionalCorrectStart) as an example.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.optimization.direct.MultiDirectional.java", "org.apache.commons.math.optimization.direct.MultiDirectionalTest.java"], "label": 1, "es_results": []}, {"bug_id": 290, "bug_title": "NullPointerException in SimplexTableau.initialize", "bug_description": "SimplexTableau throws a NullPointerException when no solution can be found instead of a NoFeasibleSolutionException\nHere is the code that causes the NullPointerException:\nLinearObjectiveFunction f = new LinearObjectiveFunction(new double[] \n{ 1, 5 }\n, 0 );\nCollection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();\nconstraints.add(new LinearConstraint(new double[] \n{ 2, 0 }\n, Relationship.GEQ, -1.0));\nRealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MINIMIZE, true);\nNote: Tested both with Apache Commons Math 2.0 release and SVN trunk", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.optimization.linear.SimplexSolverTest.java", "org.apache.commons.math.optimization.linear.SimplexTableau.java"], "label": 1, "es_results": []}, {"bug_id": 292, "bug_title": "TestUtils.assertRelativelyEquals() generates misleading error on failure", "bug_description": "TestUtils.assertRelativelyEquals() generates misleading error on failure.\nFor example:\nTestUtils.assertRelativelyEquals(1.0, 0.10427661385154971, 1.0e-9)\ngenerates the error message:\njunit.framework.AssertionFailedError: expected:<0.0> but was:<0.8957233861484503>\nwhich is not very helpful.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.TestUtils.java"], "label": 1, "es_results": []}, {"bug_id": 286, "bug_title": "SimplexSolver not working as expected?", "bug_description": "I guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints...\nConsider this LP:\nmax: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5;\nr1: x0 + x2 + x4 = 23.0;\nr2: x1 + x3 + x5 = 23.0;\nr3: x0 >= 10.0;\nr4: x2 >= 8.0;\nr5: x4 >= 5.0;\nLPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0;\nThe same LP expressed in Apache commons math is:\nLinearObjectiveFunction f = new LinearObjectiveFunction(new double[] \n{ 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 }\n, 0 );\nCollection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();\nconstraints.add(new LinearConstraint(new double[] \n{ 1, 0, 1, 0, 1, 0 }\n, Relationship.EQ, 23.0));\nconstraints.add(new LinearConstraint(new double[] \n{ 0, 1, 0, 1, 0, 1 }\n, Relationship.EQ, 23.0));\nconstraints.add(new LinearConstraint(new double[] \n{ 1, 0, 0, 0, 0, 0 }\n, Relationship.GEQ, 10.0));\nconstraints.add(new LinearConstraint(new double[] \n{ 0, 0, 1, 0, 0, 0 }\n, Relationship.GEQ, 8.0));\nconstraints.add(new LinearConstraint(new double[] \n{ 0, 0, 0, 0, 1, 0 }\n, Relationship.GEQ, 5.0));\nRealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);\nthat returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0;\nIs it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 >= 5.0) is not satisfied...\nAm I using the interface wrongly?", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.optimization.linear.SimplexSolverTest.java", "org.apache.commons.math.optimization.linear.SimplexTableauTest.java", "org.apache.commons.math.optimization.linear.SimplexTableau.java", "org.apache.commons.math.optimization.linear.SimplexSolver.java"], "label": 1, "es_results": []}, {"bug_id": 318, "bug_title": "wrong result in eigen decomposition", "bug_description": "Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0\n\n\n\n    public void testMathpbx02() {\n\n\n\n        double[] mainTridiagonal = {\n\n        \t  7484.860960227216, 18405.28129035345, 13855.225609560746,\n\n        \t 10016.708722343366, 559.8117399576674, 6750.190788301587, \n\n        \t    71.21428769782159\n\n        };\n\n        double[] secondaryTridiagonal = {\n\n        \t -4175.088570476366,1975.7955858241994,5193.178422374075, \n\n        \t  1995.286659169179,75.34535882933804,-234.0808002076056\n\n        };\n\n\n\n        // the reference values have been computed using routine DSTEMR\n\n        // from the fortran library LAPACK version 3.2.1\n\n        double[] refEigenValues = {\n\n        \t\t20654.744890306974412,16828.208208485466457,\n\n        \t\t6893.155912634994820,6757.083016675340332,\n\n        \t\t5887.799885688558788,64.309089923240379,\n\n        \t\t57.992628792736340\n\n        };\n\n        RealVector[] refEigenVectors = {\n\n        \t\tnew ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),\n\n        \t\tnew ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),\n\n        \t\tnew ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),\n\n        \t\tnew ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),\n\n        \t\tnew ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),\n\n        \t\tnew ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),\n\n        \t\tnew ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})\n\n        };\n\n\n\n        // the following line triggers the exception\n\n        EigenDecomposition decomposition =\n\n            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);\n\n\n\n        double[] eigenValues = decomposition.getRealEigenvalues();\n\n        for (int i = 0; i < refEigenValues.length; ++i) {\n\n            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);\n\n            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {\n\n                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);\n\n            } else {\n\n                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);\n\n            }\n\n        }\n\n\n\n    }\n\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.linear.EigenDecompositionImpl.java", "org.apache.commons.math.linear.EigenDecompositionImplTest.java"], "label": 1, "es_results": []}, {"bug_id": 322, "bug_title": "during ODE integration, the last event in a pair of very close event may not be detected", "bug_description": "When an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let&apos;s say this step spans from 90.0 to 153.0. The switching function switches once again in this step.\nIf the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative.\nThis bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.ode.events.EventState.java"], "label": 1, "es_results": []}, {"bug_id": 260, "bug_title": "Inconsistent API in Frequency", "bug_description": "The overloaded Frequency methods are not consistent in the parameter types that they handle.\naddValue() has an Integer version which converts the parameter to a Long, and then calls addValue(Object).\nThe various getxxx() methods all handle Integer parameters as an Object.\nSeems to me that it would be better to treat Integer consistently.\nBut perhaps there is a good reason for having an addValue(Integer) method but no getxxx(Integer) methods?\nIf so, then it would be helpful to document this.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.stat.Frequency.java"], "label": 1, "es_results": []}, {"bug_id": 320, "bug_title": "NaN singular value from SVD", "bug_description": "The following jython code\nStart code\nfrom org.apache.commons.math.linear import *\nAlist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]\nA = Array2DRowRealMatrix(Alist)\ndecomp = SingularValueDecompositionImpl(A)\nprint decomp.getSingularValues()\nEnd code\nprints\narray(&apos;d&apos;, [11.218599757513008, 0.3781791648535976, nan])\nThe last singular value should be something very close to 0 since the matrix\nis rank deficient.  When i use the result from getSolver() to solve a system, i end \nup with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.\nDoes this SVD implementation require that the matrix be full rank?  If so, then i would expect\nan exception to be thrown from the constructor or one of the methods.\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.linear.SingularValueSolverTest.java", "org.apache.commons.math.linear.SingularValueDecompositionImpl.java", "org.apache.commons.math.linear.SingularValueDecomposition.java"], "label": 1, "es_results": []}, {"bug_id": 307, "bug_title": "BigReal/Fieldelement divide without setting a proper scale -> exception: no exact representable decimal result", "bug_description": "BigReal implements the methode divide of Fieldelement. The problem is that there is no scale defined for the BigDecimal so the class will throw an error when the outcome is not a representable decimal result. \n(Exception: no exact representable decimal result)\nThe workaround for me was to copy the BigReal and set the scale and roundingMode the same as version 1.2.\nMaybe is it possible to set the scale in FieldMatrix and implements it also a divide(BigReal b, int scale, int roundMode) ?? \n", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.util.BigReal.java"], "label": 1, "es_results": []}, {"bug_id": 297, "bug_title": "Eigenvector computation incorrectly returning vectors of NaNs", "bug_description": "As reported by Axel Kramer on commons-dev, the following test case succeeds, but should fail:\n\n\n\npublic void testEigenDecomposition() {\n\n    double[][] m = { { 0.0, 1.0, -1.0 }, { 1.0, 1.0, 0.0 }, { -1.0,0.0, 1.0 } };\n\n    RealMatrix rm = new Array2DRowRealMatrix(m);\n\n    assertEquals(rm.toString(),\n\n        \"Array2DRowRealMatrix{{0.0,1.0,-1.0},{1.0,1.0,0.0},{-1.0,0.0,1.0}}\");\n\n    EigenDecompositionImpl ed = new EigenDecompositionImpl(rm,\n\n        MathUtils.SAFE_MIN);\n\n    RealVector rv0 = ed.getEigenvector(0);\n\n    assertEquals(rv0.toString(), \"{(NaN); (NaN); (NaN)}\");\n\n  }\n\n\n\ned.getRealEigenvalues() returns the correct eigenvalues (2, 1, -1), but all three eigenvectors contain only NaNs.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.linear.EigenDecompositionImpl.java"], "label": 1, "es_results": []}, {"bug_id": 296, "bug_title": "LoessInterpolator.smooth() not working correctly", "bug_description": "I have been comparing LoessInterpolator.smooth output with the loessFit output from ARE (R-project.org, probably the most widely used loess implementation) and have had strangely different numbers. I have created a small set to test the difference and something seems to be wrong with the smooth method but I do no know what and I do not understand the code.\nExample 1\n\n\nx-input: \n1.5\n 3.0\n 6\n 8\n 12\n13\n 22\n 24\n28\n31\n\n\ny-input: \n3.1\n6.1\n3.1\n2.1\n1.4\n5.1\n5.1\n6.1\n7.1\n7.2\n\n\nOutput LoessInterpolator.smooth():\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nOutput from loessFit() from R: \n3.191178027520974\n3.0407201231474037\n2.7089538903778636\n2.7450823274490297\n4.388011000549519\n4.60078952381848\n5.2988217587114805\n5.867536388457898\n6.7797794777879705\n7.444888598397342\n\n\nExample 2 (same x-values, y-values just floored)\n\n\nx-input: \n1.5\n 3.0\n 6\n 8\n 12\n13\n 22\n 24\n28\n31\n\n\ny-input: \n3\n6\n3\n2\n1\n5\n5\n6\n7\n7\n\n\nOutput LoessInterpolator.smooth(): \n3\n6\n3\n2\n0.9999999999999005\n5.0000000000001705\n5\n5.999999999999972\n7\n6.999999999999967\n\n\nOutput from loessFit() from R: \n3.091423927353068\n2.9411521572524237\n2.60967950675505\n2.7421759322272248\n4.382996912300442\n4.646774316632562\n5.225153658563424\n5.768301917477015\n6.637079139313073\n7.270482144410326\n\n\nAs you see the output is practically the replicated y-input.\nAt this point this funtionality is critical for us but I could not find any other suitable java-implementation. Help. Maybe this strange behaviour gives someone a clue?\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.analysis.interpolation.LoessInterpolator.java", "org.apache.commons.math.analysis.interpolation.LoessInterpolatorTest.java", "org.apache.commons.math.MessagesResources_fr.java"], "label": 1, "es_results": []}, {"bug_id": 338, "bug_title": "Wrong parameter for first step size guess for Embedded Runge Kutta methods", "bug_description": "In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator.\nHere, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...)\nThe problem comes from the array \"scale\" that is used as a parameter in the call off initializeStep(..)\nFollowing the theory described by Hairer in his book \"Solving Ordinary Differential Equations 1 : Nonstiff Problems\", the scaling should be :\nsci = Atol i + |y0i| * Rtoli\nWhereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli\nNote that the Gragg-Bulirsch-Stoer integrator uses the good implementation \"sci = Atol i + |y0i| * Rtoli  \" when he performs the call to the same method initializeStep(..)\nIn the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user.\nBut in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...)\nTo fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator\nFor exemple :\n final double[] scale= new double[y0.length];;\n          if (vecAbsoluteTolerance == null) {\n              for (int i = 0; i < scale.length; ++i) \n{\n\n                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));\n\n                scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;\n\n              }\n            } else {\n              for (int i = 0; i < scale.length; ++i) \n{\n\n                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));\n\n                scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;\n\n              }\n            }\n          hNew = initializeStep(equations, forward, getOrder(), scale,\n                           stepStart, y, yDotK[0], yTmp, yDotK[1]);\nSorry for the length of this message, looking forward to hearing from you soon\nVincent Morand\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator.java", "org.apache.commons.math.ode.nonstiff.HighamHall54IntegratorTest.java", "org.apache.commons.math.ode.nonstiff.AdamsMoultonIntegratorTest.java", "org.apache.commons.math.ode.nonstiff.HighamHall54StepInterpolatorTest.java", "org.apache.commons.math.ode.nonstiff.DormandPrince853IntegratorTest.java"], "label": 1, "es_results": []}, {"bug_id": 343, "bug_title": "Brent solver does not throw IllegalArgumentException when initial guess has the wrong sign", "bug_description": "Javadoc for \"public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)\" claims that \"if the values of the function at the three points have the same sign\" an IllegalArgumentException is thrown. This case isn&apos;t even checked.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.analysis.solvers.BrentSolver.java", "org.apache.commons.math.analysis.solvers.BrentSolverTest.java"], "label": 1, "es_results": []}, {"bug_id": 358, "bug_title": "ODE integrator goes past specified end of integration range", "bug_description": "End of integration range in ODE solving is handled as an event.\nIn some cases, numerical accuracy in events detection leads to error in events location.\nThe following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.\n\n\n\n  public void testMissedEvent() throws IntegratorException, DerivativeException {\n\n          final double t0 = 1878250320.0000029;\n\n          final double t =  1878250379.9999986;\n\n          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {\n\n            \n\n            public int getDimension() {\n\n                return 1;\n\n            }\n\n            \n\n            public void computeDerivatives(double t, double[] y, double[] yDot)\n\n                throws DerivativeException {\n\n                yDot[0] = y[0] * 1.0e-6;\n\n            }\n\n        };\n\n\n\n        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,\n\n                                                                               1.0e-10, 1.0e-10);\n\n\n\n        double[] y = { 1.0 };\n\n        integrator.setInitialStepSize(60.0);\n\n        double finalT = integrator.integrate(ode, t0, y, t, y);\n\n        Assert.assertEquals(t, finalT, 1.0e-6);\n\n    }\n\n\n\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.ode.nonstiff.ClassicalRungeKuttaIntegratorTest.java", "org.apache.commons.math.ode.nonstiff.DormandPrince853IntegratorTest.java", "org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator.java", "org.apache.commons.math.ode.nonstiff.AdamsMoultonIntegrator.java", "org.apache.commons.math.ode.nonstiff.AdamsBashforthIntegrator.java", "org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator.java"], "label": 1, "es_results": []}, {"bug_id": 371, "bug_title": "PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon", "bug_description": "Similar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that.\nIn MATH-201, the problem was described as such:\n> So in essence, the p-value returned by TTestImpl.tTest() is:\n> \n> 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t))\n> \n> For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When \n> cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because:\n> \n> 1.0 - 1.0 + 0.0 = 0.0\nThe solution in MATH-201 was to modify the p-value calculation to this:\n> p = 2.0 * cumulativeProbability(-t)\nHere, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():\n  p = 2 * (1 - tDistribution.cumulativeProbability(t));\nDirectly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:\n  p = 2 * (tDistribution.cumulativeProbability(-t));\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_1", "fixed_files": ["org.apache.commons.math.stat.correlation.PearsonsCorrelation.java", "org.apache.commons.math.stat.correlation.PearsonsCorrelationTest.java"], "label": 1, "es_results": []}, {"bug_id": 362, "bug_title": "LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it", "bug_description": "LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it. This makes it hard to specify custom stopping criteria for the optimizer.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer.java", "org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizerTest.java", "org.apache.commons.math.optimization.general.MinpackTest.java"], "label": 1, "es_results": []}, {"bug_id": 352, "bug_title": "Jacobian rank determination in LevenbergMarquardtOptimizer is not numerically robust", "bug_description": "LevenbergMarquardtOptimizer is designed to handle singular jacobians,  i.e. situations when some of the fitted parameters depend on each other. The check for that condition is in LevenbergMarquardtOptimizer.qrDecomposition uses precise comparison to 0.\n    if (ak2 == 0 ) \n{\n\n                rank = k;\n\n                return;\n\n        }\n\nA correct check would be comparison with a small epsilon. Hard coded 2.2204e-16 is used elsewhere in the same file for similar purpose.\n\nfinal double QR_RANK_EPS = Math.ulp(1d); //2.220446049250313E-16\n....\n    if (ak2  < QR_RANK_EPS) {\n                rank = k;\n                return;\n        }\n\nCurrent exact equality check is not tolerant of the real world poorly conditioned situations. For example I am trying to fit a cylinder into sample 3d points. Although theoretically cylinder has only 5 independent variables, derivatives for optimizing function (signed distance) for such minimal parametrization are complicated and it  it much easier to work with a 7 variable parametrization (3 for axis direction, 3 for axis origin and 1 for radius). This naturally results in rank-deficient jacobian, but because of the numeric errors the actual ak2 values for the dependent rows ( I am seeing values of 1e-18 and less), rank handling code does not kick in.\nKeeping these tiny values around then leads to huge corrections for the corresponding very slowly changing parameters, and consequently to numeric errors and instabilities. I have noticed the problem because tiny shift in the initial guess (on the order of 1e-12 in the axis component and origins) resulted in significantly different finally converged answers (origins and radii differing by as much as 0.02) which I tracked to loss of precision due to numeric error with root because described above.\nProviding a cutoff as suggested fixes the issue. After the fix, small perturbations in the initial guess had practically no effect to the converged result - as expected from a robust algorithm.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer.java", "org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizerTest.java"], "label": 1, "es_results": []}, {"bug_id": 409, "bug_title": "Multiple Regression API should allow specification of whether or not to estimate intercept term", "bug_description": "The OLS and GLS regression APIs should support estimating models including intercepts using design matrices including only variable data.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.stat.regression.OLSMultipleLinearRegression.java", "org.apache.commons.math.stat.regression.AbstractMultipleLinearRegression.java", "org.apache.commons.math.stat.regression.OLSMultipleLinearRegressionTest.java", "org.apache.commons.math.stat.regression.MultipleLinearRegressionAbstractTest.java"], "label": 1, "es_results": []}, {"bug_id": 408, "bug_title": "GLSMultipleLinearRegression has no nontrivial validation tests", "bug_description": "There are no non-trivial tests verifying the computations for GLSMultipleLinearRegression.  Tests verifying computations against analytically determined models, R or some other reference package / datasets should be added to ensure that the statistics reported by this class are valid.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_0", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.stat.regression.GLSMultipleLinearRegressionTest.java"], "label": 1, "es_results": []}, {"bug_id": 365, "bug_title": "Issue with \"SmoothingBicubicSplineInterpolator\"", "bug_description": "I figured out that the name of this class is misleading as the implementation doesn&apos;t perform the intended smoothing.\nIn order to solve this issue, I propose to:\n\ndeprecate the \"SmoothingBicubicSplineInterpolator\" class\ncreate a \"BicubicSplineInterpolator\" class (similar to the above class but with the useless code removed)\nremove the \"SmoothingBicubicSplineInterpolatorTest\" class\nadd a \"BicubicSplineInterpolatorTest\" with essentially the same contents as the above one\n\nThen I would also add a new \"SmoothingPolynomialBicubicSplineInterpolator\" where I used the \"PolynomialFitter\" class to smooth the input data along both dimensions before the interpolating function is computed.\nDoes someone object to these changes?", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_1", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.optimization.fitting.PolynomialFitter.java", "org.apache.commons.math.analysis.BivariateRealFunction.java", "org.apache.commons.math.analysis.interpolation.SmoothingBicubicSplineInterpolatorTest.java", "org.apache.commons.math.analysis.interpolation.SmoothingBicubicSplineInterpolator.java", "org.apache.commons.math.analysis.interpolation.BivariateRealGridInterpolator.java"], "label": 1, "es_results": []}, {"bug_id": 368, "bug_title": "OpenMapRealVector.getSparcity should be getSparsity", "bug_description": "The term for describing the ratio of nonzero elements to zero elements in a matrix/vector is sparsity, not sparcity.  Suggest renaming getSparcity() to getSparsity()", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_1", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.linear.OpenMapRealVector.java"], "label": 1, "es_results": []}, {"bug_id": 377, "bug_title": "weight versus sigma in AbstractLeastSquares", "bug_description": "In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.\n Once corrected, getRMS() can even reduce\n public double getRMS() \n{return Math.sqrt(getChiSquare()/rows);}", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_1", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizerTest.java", "org.apache.commons.math.optimization.general.AbstractLeastSquaresOptimizer.java", "org.apache.commons.math.linear.SingularValueDecompositionImpl.java", "org.apache.commons.math.linear.EigenDecompositionImpl.java"], "label": 1, "es_results": []}, {"bug_id": 392, "bug_title": "calculateYVariance in OLS/GLSMultipleLinearRegression uses residuals not Y vars", "bug_description": "Implementation of OLS/GLSMultipleLinearRegression is:\n@Override\n173        protected double calculateYVariance() \n{\n\n174            RealVector residuals = calculateResiduals();\n\n175            return residuals.dotProduct(residuals) /\n\n176                   (X.getRowDimension() - X.getColumnDimension());\n\n177        }\n\nThis gives variance of residuals not variance of the dependent (Y) variable as the documentation suggests.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_1", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.stat.regression.AbstractMultipleLinearRegression.java", "org.apache.commons.math.stat.regression.GLSMultipleLinearRegression.java", "org.apache.commons.math.stat.regression.OLSMultipleLinearRegressionTest.java", "org.apache.commons.math.stat.regression.GLSMultipleLinearRegressionTest.java", "org.apache.commons.math.stat.regression.OLSMultipleLinearRegression.java"], "label": 1, "es_results": []}, {"bug_id": 410, "bug_title": "Wrong variable in \"FunctionEvaluationException\"", "bug_description": "Some constructors use both argument and arguments as argument names and, in the body, argument is sometimes used in places where it should have been arguments.", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_1", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.FunctionEvaluationException.java"], "label": 1, "es_results": []}, {"bug_id": 421, "bug_title": "restarting an ODE solver that has been stopped by an event does not work", "bug_description": "If an ODE solver is setup with an EventHandler that return STOP when the even is triggered, the integrators stops (which is exactly the expected behavior).\nIf however the user want to restart the solver from the final state reached at the event with the same configuration (expecting the event to be triggered again at a later time), then the integrator may fail to start. It can get stuck at the previous event.\nThe occurrence of the bug depends on the residual sign of the g function which is not exactly 0, it depends on the convergence of the first event.\nAs this use case is fairly general, event occurring less than epsilon after the solver start in the first step should be ignored, where epsilon is the convergence threshold of the event. The sign of the g function should be evaluated after this initial ignore zone, not exactly at beginning (if there are no event at the very beginning g(t0) and g(t0+epsilon) have the same sign, so this does not hurt ; if there is an event at the very beginning, g(t0) and g(t0+epsilon) have opposite signs and we want to start with the second one. Of course, the sign of epsilon depend on the integration direction (forward or backward).", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_1", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.ode.events.EventStateTest.java", "org.apache.commons.math.ode.events.EventState.java", "org.apache.commons.math.ode.events.CombinedEventsManager.java"], "label": 1, "es_results": []}, {"bug_id": 391, "bug_title": "Inconsistent behaviour of constructors in ArrayRealVector class", "bug_description": "ArrayRealVector(double[] d) allows to construct a zero-length vector, but ArrayRealVector(double[] d, boolean copyArray) doesn&apos;t. Both should allow this as zero-length vectors are mathematically well-defined objects and they are useful boundary cases in many algorithms.\nThis breaks some arithmetic operators (addition) on zero-length real vectors which worked in 2.0 but don&apos;t work in 2.1", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_1", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.linear.ArrayFieldVector.java", "org.apache.commons.math.linear.ArrayFieldVectorTest.java", "org.apache.commons.math.linear.ArrayRealVectorTest.java", "org.apache.commons.math.linear.ArrayRealVector.java"], "label": 1, "es_results": []}, {"bug_id": 484, "bug_title": "events detection in ODE solvers is too complex and not robust", "bug_description": "All ODE solvers support multiple events detection since a long time. Events are specified by users by implementing the EventHandler interface. Events occur when the g(t, y) function evaluates to 0. When an event occurs, the solver step is shortened to make sure the event is located at the end of the step, and the event is triggered by calling the eventOccurred method in the user defined implementation class. Depending on the return value of this method, integration can continue, it can be stopped, or the state vector can be reset.\nSome ODE solvers are adaptive step size solvers. They can modify step size to match an integration error setting, increasing step size when error is low (thus reducing computing costs) or reducing step size when error is high (thus fulfilling accuracy requirements).\nThe step adaptations due to events on one side and due to adaptive step size solvers are quite intricate by now, due to numerous fixes (MATH-161, MATH-213, MATH-322, MATH-358, MATH-421 and also during standard maintenance - see for example r781157). The code is very difficult to maintain. It seems each bug fix introduces new bugs (r781157/MATH-322) or tighten the link between adaptive step size and event detection (MATH-388/r927202).\nA new bug discovered recently on an external library using a slightly modified version of this code could not be retroffitted into commons-math, despite the same problem is present. At the beginning of EventState.evaluateStep, the initial step may be exactly 0 thus preventing root solving, but preventing this size to drop to 0 would reopen MATH-388. I could not fix both bugs at the same time.\nSo it is now time to untangle events detection and adaptive step size, simplify code, and remove some inefficiency (event root solving is always done twice, once before step truncation and another time after truncation, of course with slightly different results, events shortened steps induce high computation load until the integrator recovers its optimal pace again, steps are rejected even when the event does not requires it ...).", "project": "Commons", "sub_project": "MATH", "version": "MATH_2_1", "fixed_version": "MATH_2_2", "fixed_files": ["org.apache.commons.math.ode.AbstractIntegrator.java", "org.apache.commons.math.ode.nonstiff.ThreeEighthesIntegratorTest.java", "org.apache.commons.math.ode.nonstiff.GillStepInterpolatorTest.java", "org.apache.commons.math.ode.nonstiff.EulerIntegratorTest.java", "org.apache.commons.math.ode.sampling.DummyStepInterpolatorTest.java", "org.apache.commons.math.ode.nonstiff.GraggBulirschStoerStepInterpolator.java", "org.apache.commons.math.ode.nonstiff.DormandPrince54StepInterpolatorTest.java", "org.apache.commons.math.ode.nonstiff.HighamHall54IntegratorTest.java", "org.apache.commons.math.ode.nonstiff.GraggBulirschStoerIntegratorTest.java", "org.apache.commons.math.ode.sampling.AbstractStepInterpolator.java", "org.apache.commons.math.ode.events.EventState.java", "org.apache.commons.math.ode.nonstiff.HighamHall54StepInterpolatorTest.java", "org.apache.commons.math.ode.sampling.NordsieckStepInterpolatorTest.java", "org.apache.commons.math.ode.nonstiff.GraggBulirschStoerIntegrator.java", "org.apache.commons.math.ode.nonstiff.AdamsMoultonIntegrator.java", "org.apache.commons.math.ode.TestProblemHandler.java", "org.apache.commons.math.ode.nonstiff.MidpointStepInterpolatorTest.java", "org.apache.commons.math.ode.nonstiff.GraggBulirschStoerStepInterpolatorTest.java", "org.apache.commons.math.ode.nonstiff.DormandPrince54IntegratorTest.java", "org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator.java", "org.apache.commons.math.ode.nonstiff.DormandPrince853StepInterpolatorTest.java", "org.apache.commons.math.ode.ODEIntegrator.java", "org.apache.commons.math.ode.nonstiff.ClassicalRungeKuttaStepInterpolatorTest.java", "org.apache.commons.math.ode.nonstiff.DormandPrince853StepInterpolator.java", "org.apache.commons.math.ode.nonstiff.DormandPrince853IntegratorTest.java", "org.apache.commons.math.ode.nonstiff.AdamsBashforthIntegratorTest.java", "org.apache.commons.math.ode.nonstiff.AdamsBashforthIntegrator.java", "org.apache.commons.math.ode.events.CombinedEventsManager.java", "org.apache.commons.math.ode.TestProblemAbstract.java", "org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator.java", "org.apache.commons.math.ode.nonstiff.MidpointIntegratorTest.java", "org.apache.commons.math.ode.nonstiff.ClassicalRungeKuttaIntegratorTest.java", "org.apache.commons.math.ode.TestProblem4.java", "org.apache.commons.math.ode.nonstiff.ThreeEighthesStepInterpolatorTest.java", "org.apache.commons.math.ode.nonstiff.GillIntegratorTest.java"], "label": 1, "es_results": []}, {"bug_id": 770, "bug_title": "SymmLQ not tested in SymmLQTest", "bug_description": "In SymmLQTest, two test actually create instances of ConjugateGradient instead of SymmLQ. These tests are\n\ntestUnpreconditionedNormOfResidual()\ntestPreconditionedNormOfResidual().\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.linear.SymmLQTest.java", "org.apache.commons.math3.linear.JacobiPreconditioner.java"], "label": 1, "es_results": []}, {"bug_id": 776, "bug_title": "Need range checks for elitismRate in ElitisticListPopulation constructors.", "bug_description": "There is a range check for setting the elitismRate via ElitisticListPopulation&apos;s setElitismRate method, but not via the constructors.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.genetics.ElitisticListPopulation.java", "org.apache.commons.math3.genetics.ElitisticListPopulationTest.java"], "label": 1, "es_results": []}, {"bug_id": 775, "bug_title": "In the ListPopulation constructor, the check for a negative populationLimit should occur first.", "bug_description": "In the ListPopulation constructor, the check to see whether the populationLimit is positive should occur before the check to see if the number of chromosomes is greater than the populationLimit.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.genetics.ListPopulation.java", "org.apache.commons.math3.genetics.ElitisticListPopulation.java", "org.apache.commons.math3.genetics.Population.java", "org.apache.commons.math3.genetics.ListPopulationTest.java", "org.apache.commons.math3.genetics.TournamentSelection.java"], "label": 1, "es_results": []}, {"bug_id": 779, "bug_title": "ListPopulation Iterator allows you to remove chromosomes from the population.", "bug_description": "Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.genetics.ListPopulation.java", "org.apache.commons.math3.genetics.ListPopulationTest.java"], "label": 1, "es_results": []}, {"bug_id": 782, "bug_title": "BrentOptimizer: User-defined check block is badly placed", "bug_description": "The CM implementation of Brent&apos;s original algorithm was supposed to allow for a user-defined stopping criterion (in addition to Brent&apos;s default one).\nHowever, it turns out that this additional block of code is not at the right location, implying an unwanted early exit.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.optimization.univariate.BrentOptimizer.java", "org.apache.commons.math3.optimization.univariate.BrentOptimizerTest.java"], "label": 1, "es_results": []}, {"bug_id": 781, "bug_title": "SimplexSolver gives bad results", "bug_description": "Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0\nin a simple test problem. It works well in commons-math-2.2. ", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.optimization.linear.SimplexTableau.java", "org.apache.commons.math3.optimization.linear.SimplexSolverTest.java"], "label": 1, "es_results": []}, {"bug_id": 802, "bug_title": "RealVector.subtract(RealVector) returns wrong answer.", "bug_description": "The following piece of code\n\n\n\nimport org.apache.commons.math3.linear.ArrayRealVector;\n\nimport org.apache.commons.math3.linear.OpenMapRealVector;\n\nimport org.apache.commons.math3.linear.RealVectorFormat;\n\n\n\npublic class DemoMath {\n\n\n\n    public static void main(String[] args) {\n\n        final double[] data1 = {\n\n            0d, 1d, 0d, 0d, 2d\n\n        };\n\n        final double[] data2 = {\n\n            3d, 0d, 4d, 0d, 5d\n\n        };\n\n        final RealVectorFormat format = new RealVectorFormat();\n\n        System.out.println(format.format(new ArrayRealVector(data1)\n\n            .subtract(new ArrayRealVector(data2))));\n\n        System.out.println(format.format(new OpenMapRealVector(data1)\n\n            .subtract(new ArrayRealVector(data2))));\n\n    }\n\n}\n\n\n\nprints\n\n{-3; 1; -4; 0; -3}\n\n{3; 1; 4; 0; -3}\n\n\n\nthe second line being wrong. In fact, when subtracting mixed types, OpenMapRealVector delegates to the default implementation in RealVector which is buggy.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.linear.RealVector.java"], "label": 1, "es_results": []}, {"bug_id": 790, "bug_title": "Mann-Whitney YOU Test Suffers From Integer Overflow With Large Data Sets", "bug_description": "When performing a Mann-Whitney YOU Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations.\nAttached is a patch, including a test, and a fix, which modifies the affected code to use doubles", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.stat.inference.MannWhitneyUTest.java", "org.apache.commons.math3.stat.inference.MannWhitneyUTestTest.java"], "label": 1, "es_results": []}, {"bug_id": 798, "bug_title": "PolynomialFitter.fit() stalls", "bug_description": "Hi, in certain cases I ran into the problem that the PolynomialFitter.fit() method stalls, meaning that it does not return, nor throw an Exception (even if it runs for 90 min). Is there a way to tell the PolynomialFitter to iterate only N-times to ensure that my program does not stall?", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.optimization.fitting.PolynomialFitterTest.java", "org.apache.commons.math3.optimization.fitting.PolynomialFitter.java", "org.apache.commons.math3.optimization.fitting.CurveFitterTest.java"], "label": 1, "es_results": []}, {"bug_id": 812, "bug_title": "In RealVector, dotProduct and outerProduct return wrong results due to misuse of sparse iterators", "bug_description": "In class RealVector, the default implementation of RealMatrix outerProduct(RealVector) uses sparse iterators on the entries of the two vectors. The rationale behind this is that 0d * x == 0d is true for all double x. This assumption is in fact false, since 0d * NaN == NaN.\nProposed fix is to loop through all entries of both vectors. This can have a significant impact on the CPU cost, but robustness should probably be preferred over speed in default implementations.\nSame issue occurs with double dotProduct(RealVector), which uses sparse iterators for this only.\nAnother option would be to through an exception if isNaN() is true, in which case caching could be used for both isNaN() and isInfinite().", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.linear.RealVector.java", "org.apache.commons.math3.linear.OpenMapRealVector.java", "org.apache.commons.math3.linear.ArrayRealVector.java"], "label": 1, "es_results": []}, {"bug_id": 805, "bug_title": "Percentile calculation is very slow when input data are constants", "bug_description": "I use the Percentile class to calculate quantile on a big array (10^6 entries). When I have to test the performance of my code, I notice that the calculation of quantile is at least 100x slower when my data are constants (10^6 of the same nomber). Maybe the Percentile calculation can be improved for this special case.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.stat.descriptive.rank.Percentile.java"], "label": 1, "es_results": []}, {"bug_id": 835, "bug_title": "Fraction percentageValue rare overflow", "bug_description": "The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value.\nThe patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.fraction.FractionTest.java", "org.apache.commons.math3.fraction.Fraction.java"], "label": 1, "es_results": []}, {"bug_id": 836, "bug_title": "Fraction(double, int) constructor strange behaviour", "bug_description": "The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100&apos;000s), two distinct bugs can manifest:\n1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value\n2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.\nI have, as of yet, not found a solution. The constructor looks like this:\npublic Fraction(double value, int maxDenominator)\n        throws FractionConversionException\n    {\n\n       this(value, 0, maxDenominator, 100);\n\n    }\n\nIncreasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest. \nThe problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find.\nThis bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.\n\nIt is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that \"since fractions are always in lowest terms, numerators and can be compared directly for equality\", so it seems like this is the intention.\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.fraction.FractionTest.java", "org.apache.commons.math3.fraction.Fraction.java"], "label": 1, "es_results": []}, {"bug_id": 840, "bug_title": "Failures in \"FastMathTestPerformance\" when testRuns >= 10,000,002", "bug_description": "Tests for methods \"asin\" and \"acos\" fail because they use\n\n\n\ni / 10000000.0\n\n\n\nas the argument to those methods, where \"i\" goes from 0 to the value of \"testRuns\" minus one (if \"testRuns\" is defined).\nA solution is to replace the above with\n\n\n\ni / (double) RUNS\n\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.util.FastMathTestPerformance.java"], "label": 1, "es_results": []}, {"bug_id": 828, "bug_title": "Not expected UnboundedSolutionException", "bug_description": "SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables.\nIn order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you&apos;ll get a massive of unbounded exceptions.\nFirst iteration is runned with predefined set of input data with which the Solver gives back an appropriate result.\nThe problem itself is well tested by it&apos;s authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values.\nWhat is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem.\nThe problem is formulated as\nmin(1*t + 0*L) (for every r-th subject)\ns.t.\n-q(r) + QL >= 0\nx(r)t - XL >= 0\nL >= 0\nwhere \nare = 1..R, \nL = \n{l(1), l(2), ..., l(R)}\n (vector of R rows and 1 column),\nQ - coefficients matrix MxR\nX - coefficients matrix NxR ", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.optimization.linear.SimplexSolver.java", "org.apache.commons.math3.optimization.linear.SimplexSolverTest.java", "org.apache.commons.math3.optimization.linear.SimplexTableau.java"], "label": 1, "es_results": []}, {"bug_id": 844, "bug_title": "\"HarmonicFitter.ParameterGuesser\" sometimes fails to return sensible values", "bug_description": "The inner class \"ParameterGuesser\" in \"HarmonicFitter\" (package \"o.a.c.m.optimization.fitting\") fails to compute a usable guess for the \"amplitude\" parameter.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.optimization.fitting.HarmonicFitterTest.java", "org.apache.commons.math3.optimization.fitting.HarmonicFitter.java", "org.apache.commons.math3.exception.util.LocalizedFormats.java"], "label": 1, "es_results": []}, {"bug_id": 843, "bug_title": "Precision.EPSILON: wrong documentation", "bug_description": "The documentation of the Field EPSILON in class org.apache.commons.math3.util.Precision states, that EPSILON is the smallest positive number such that 1 - EPSILON is not numerically equal to 1, and its value is defined as 1.1102230246251565E-16.\nHowever, this is NOT the smallest positive number with this property.\nConsider the following program:\n\n\n\npublic class Eps {\n\n  public static void main(String[] args) {\n\n    double e = Double.longBitsToDouble(0x3c90000000000001L);\n\n\tdouble e1 = 1-e;\n\n\tSystem.out.println(e);\n\n\tSystem.out.println(1-e);\n\n\tSystem.out.println(1-e != 1);\n\n  }\n\n}\n\n\n\nThe output is:\n\n\n\n% java Eps\n\n5.551115123125784E-17\n\n0.9999999999999999\n\ntrue\n\n\n\nThis proves, that there are smaller positive numbers with the property that 1-eps != 1.\nI propose not to change the constant value, but to update the documentation. The value Precision.EPSILON is \nan upper bound on the relative error which occurs when a real number is\nrounded to its nearest Double floating-point number. I propose to update \nthe api docs in this sense.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.util.Precision.java", "org.apache.commons.math3.util.PrecisionTest.java"], "label": 1, "es_results": []}, {"bug_id": 855, "bug_title": "\"BrentOptimizer\" not always reporting the best point", "bug_description": "BrentOptimizer (package \"o.a.c.m.optimization.univariate\") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.optimization.univariate.BrentOptimizer.java", "org.apache.commons.math3.optimization.univariate.BrentOptimizerTest.java"], "label": 1, "es_results": []}, {"bug_id": 865, "bug_title": "Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function", "bug_description": "If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.optimization.direct.CMAESOptimizerTest.java", "org.apache.commons.math3.optimization.direct.CMAESOptimizer.java"], "label": 1, "es_results": []}, {"bug_id": 864, "bug_title": "CMAESOptimizer does not enforce bounds", "bug_description": "The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.optimization.direct.CMAESOptimizerTest.java", "org.apache.commons.math3.optimization.direct.CMAESOptimizer.java"], "label": 1, "es_results": []}, {"bug_id": 778, "bug_title": "Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n)", "bug_description": "In class org.apache.commons.math3.Dfp,  the method multiply(int n) is limited to 0 <= n <= 9999. This is not consistent with the general contract of FieldElement.multiply(int n), where there should be no limitation on the values of n.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.dfp.DfpTest.java", "org.apache.commons.math3.dfp.Dfp.java"], "label": 1, "es_results": []}, {"bug_id": 905, "bug_title": "FastMath.[cosh, sinh] do not support the same range of values as the Math counterparts", "bug_description": "As reported by Jeff Hain:\ncosh(double) and sinh(double):\nMath.cosh(709.783) = 8.991046692770538E307\nFastMath.cosh(709.783) = Infinity\nMath.sinh(709.783) = 8.991046692770538E307\nFastMath.sinh(709.783) = Infinity\n===> This is due to using exp( x )/2 for values of |x|\nabove 20: the result sometimes should not overflow,\nbut exp( x ) does, so we end up with some infinity.\n===> for values of |x| >= StrictMath.log(Double.MAX_VALUE),\nexp will overflow, so you need to use that instead:\nfor x positive:\ndouble t = exp(x*0.5);\nreturn (0.5*t)*t;\nfor x negative:\ndouble t = exp(-x*0.5);\nreturn (-0.5*t)*t;", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.util.FastMath.java", "org.apache.commons.math3.util.FastMathTest.java"], "label": 1, "es_results": []}, {"bug_id": 904, "bug_title": "FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53 ", "bug_description": "As reported by Jeff Hain:\npow(double,double):\nMath.pow(-1.0,5.000000000000001E15) = -1.0\nFastMath.pow(-1.0,5.000000000000001E15) = 1.0\n===> This is due to considering that power is an even\ninteger if it is >= 2^52, while you need to test\nthat it is >= 2^53 for it.\n===> replace\n\"if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)\"\nwith\n\"if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)\"\nand that solves it.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_1", "fixed_files": ["org.apache.commons.math3.util.FastMath.java", "org.apache.commons.math3.util.FastMathTest.java"], "label": 1, "es_results": []}, {"bug_id": 891, "bug_title": "SpearmansCorrelation fails when using NaturalRanking together with NaNStrategy.REMOVED", "bug_description": "As reported by Martin Rosellen on the users mailinglist:\nUsing a NaturalRanking with a REMOVED NaNStrategy can result in an exception when NaN are contained in the input arrays.\nThe current implementation just removes the NaN values where they occur, without taken care to remove the corresponding values in the other array.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_2", "fixed_files": ["org.apache.commons.math3.stat.correlation.SpearmansCorrelation.java", "org.apache.commons.math3.stat.correlation.SpearmansRankCorrelationTest.java"], "label": 1, "es_results": []}, {"bug_id": 821, "bug_title": "SparseRealVectorTest.testMap and testMapToSelf fail because zero entries lose their sign", "bug_description": "Mapping Inverse to an OpenMapRealVector can lead to wrong answers, because 1.0 / 0.0 should return +/-Infinity depending on the sign of the zero entry. Since the sign is lost in OpenMapRealVector, the answer must be wrong if the entry is truly -0.0.\nThis is a difficult bug, because it potentially affects any function passed to OpenMapRealVector.map() or mapToSelf(). I would recommend we relax the requirements in the unit tests of this class, and make people aware of the issue in the class documentation.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_0", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.linear.SparseRealVectorTest.java"], "label": 1, "es_results": []}, {"bug_id": 927, "bug_title": "GammaDistribution cloning broken", "bug_description": "Serializing a GammaDistribution and deserializing it, does not result in a cloned distribution that produces the same samples.\nBecause: GammaDistribution inherits from AbstractRealDistribution, which implements Serializable. AbstractRealDistribution has random, in which we have a Well19937c instance, which inherits from AbstractWell. AbstractWell implements Serializable. AbstractWell inherits from BitsStreamGenerator, which is not Serializable, but does have a private field &apos;nextGaussian&apos;.\nSolution: Make BitStreamGenerator implement Serializable as well.\nThis probably affects other distributions as well.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_1", "fixed_version": "MATH_3_1_1", "fixed_files": ["org.apache.commons.math3.random.BitsStreamGenerator.java", "org.apache.commons.math3.distribution.RealDistributionAbstractTest.java"], "label": 1, "es_results": []}, {"bug_id": 929, "bug_title": "MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd", "bug_description": "To reproduce:\n\n\n\nAssert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);\n\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_1_1", "fixed_version": "MATH_3_2", "fixed_files": ["org.apache.commons.math3.distribution.MultivariateNormalDistributionTest.java", "org.apache.commons.math3.distribution.MultivariateNormalDistribution.java"], "label": 1, "es_results": []}, {"bug_id": 962, "bug_title": "Vector3DFormat.parse does not ignore whitespace", "bug_description": "Vector3DFormat notes it ingores whitespace in the javadoc but in the below example it does not:\n\tVector3DFormat vf = new Vector3DFormat(\"(\", \")\", \",\");\n\tSystem.out.println(vf.parse(\"(1, 2, 3)\")); //prints \n{1; 2; 3}\n\tSystem.out.println(vf.parse(\"(1,2,3)\"));   //prints null", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_1_1", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.geometry.euclidean.twod.Vector2DFormat.java", "org.apache.commons.math3.geometry.VectorFormat.java", "org.apache.commons.math3.geometry.euclidean.oned.Vector1DFormat.java", "org.apache.commons.math3.geometry.euclidean.threed.Vector3DFormat.java"], "label": 1, "es_results": []}, {"bug_id": 996, "bug_title": "Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception", "bug_description": "An overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple\nfraction.  For example:\ndouble d = 0.5000000001;\nFraction f = new Fraction(d, 10);\nPatch with unit test on way.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_1_1", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.fraction.FractionTest.java", "org.apache.commons.math3.fraction.Fraction.java", "org.apache.commons.math3.fraction.BigFraction.java", "org.apache.commons.math3.fraction.BigFractionTest.java"], "label": 1, "es_results": []}, {"bug_id": 993, "bug_title": "GaussNewtonOptimizer convergence on singularity", "bug_description": "I am (ab-)using the GaussNewtonOptimizer as a MultivariateFunctionSolver (as I could not find one in commons.math). Recently I stumbled upon an interesting behavior in one of my test cases: If a function is defined in a way that yields a minimum (a root in my case) at a singular point, the solver crashes. This is because of the following lines in doOptimize():\ncatch (SingularMatrixException e) \n{\n\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n\n            }\n\nI would propose to add a convergence check into the catch-phrase, so the solver returns the solution in that special case.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.optim.nonlinear.vector.jacobian.GaussNewtonOptimizer.java"], "label": 1, "es_results": []}, {"bug_id": 1005, "bug_title": "ArrayIndexOutOfBoundsException in MathArrays.linearCombination", "bug_description": "When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line:\ndouble prodHighNext = prodHigh[1];\nlinearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.util.MathArraysTest.java", "org.apache.commons.math3.util.MathArrays.java"], "label": 1, "es_results": []}, {"bug_id": 1022, "bug_title": "Confused by the API docs for org.apache.commons.math3.analysis.function", "bug_description": "Something is wrong or unclear...\nWe read:\nhttp://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/function/Logistic.Parametric.html\n   \"Parametric function where the input array contains the parameters\n    of the logit function, ordered as follows: \"\n --> But the \"logit\" function is not the \"logistic\" function.\nhttp://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/function/Sigmoid.Parametric.html\n   \"Parametric function where the input array contains the parameters of\n   the logit function, ordered as follows: \"\n --> But the \"logit\" function is not the \"sigmoid\" function, and what is\n     the difference between the Logistic Function snd the Sigmoid function?\nhttp://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/function/Logit.Parametric.html\n   \"Parametric function where the input array contains the parameters of\n    the logit function, ordered as follows: \"\n --> That sounds correct.\nReferences:\nhttp://en.wikipedia.org/wiki/Logistic_function\nhttp://en.wikipedia.org/wiki/Logit\nhttp://en.wikipedia.org/wiki/Sigmoid_function\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.analysis.function.Sigmoid.java", "org.apache.commons.math3.analysis.function.Logistic.java"], "label": 1, "es_results": []}, {"bug_id": 1033, "bug_title": "Kalman filter does not work if covarance matrix is not of dimension 1", "bug_description": "In org.apache.commons.math3.filter.KalmanFilter,\nThe check below doesn&apos;t look right, it reques measNoise&apos;s column dimension to be 1 at all time.\n// row dimension of R must be equal to row dimension of H\n        if (measNoise.getRowDimension() != measurementMatrix.getRowDimension() ||\n            measNoise.getColumnDimension() != 1) \n{\n\n            throw new MatrixDimensionMismatchException(measNoise.getRowDimension(),\n\n                                                       measNoise.getColumnDimension(),\n\n                                                       measurementMatrix.getRowDimension(), 1);\n\n        }", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.filter.KalmanFilter.java"], "label": 1, "es_results": []}, {"bug_id": 1045, "bug_title": "EigenDecomposition.Solver should consider tiny values 0 for purposes of determining singularity", "bug_description": "EigenDecomposition.Solver tests for singularity by comparing eigenvalues to 0 for exact equality. Elsewhere in the class and in the code, of course, very small values are considered 0. This causes the solver to consider some singular matrices as non-singular.\nThe patch here includes a test as well showing the behavior  the matrix is clearly singular but isn&apos;t considered as such since one eigenvalue are ~1e-14 rather than exactly 0.\n(What I am not sure of is whether we should really be evaluating the norm of the imaginary eigenvalues rather than real/imag components separately. But the javadoc says the solver only supports real eigenvalues anyhow, so it&apos;s kind of moot since imag=0 for all eigenvalues.)", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.linear.EigenDecomposition.java", "org.apache.commons.math3.linear.EigenSolverTest.java", "org.apache.commons.math3.linear.EigenDecompositionTest.java"], "label": 1, "es_results": []}, {"bug_id": 1051, "bug_title": "EigenDecomposition may not converge for certain matrices", "bug_description": "Jama-1.0.3 contains a bugfix for certain matrices where the original code goes into an infinite loop.\nThe commons-math translations would throw a MaxCountExceededException, so fails to compute the eigen decomposition.\nPort the fix from jama to CM.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.linear.SchurTransformer.java", "org.apache.commons.math3.linear.EigenDecompositionTest.java"], "label": 1, "es_results": []}, {"bug_id": 1058, "bug_title": "Beta, LogNormalDistribution, WeibullDistribution give slightly wrong answer for extremely small args due to log/exp inaccuracy", "bug_description": "Background for those who aren&apos;t familiar: math libs like Math and FastMath have two mysterious methods, log1p and expm1. log1p = log(1+x) and expm1 = exp-1 mathetmatically, but can return a correct answer even when x was small, where floating-point error due to the addition/subtraction introduces a relatively large error.\nThere are three instances in the code that can employ these specialized methods and gain a measurable improvement in accuracy. See patch and tests for an example  try the tests without the code change to see the error.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.distribution.WeibullDistributionTest.java", "org.apache.commons.math3.distribution.WeibullDistribution.java", "org.apache.commons.math3.distribution.LogNormalDistributionTest.java", "org.apache.commons.math3.special.BetaTest.java", "org.apache.commons.math3.special.Beta.java", "org.apache.commons.math3.distribution.LogNormalDistribution.java"], "label": 1, "es_results": []}, {"bug_id": 1056, "bug_title": "Small error in PoissonDistribution.nextPoisson() algorithm", "bug_description": "Here&apos;s a tiny bug I noticed via static inspection, since it flagged the integer division. PoissonDistribution.java:325 says:\n\n\n\nfinal double a1 = FastMath.sqrt(FastMath.PI * twolpd) * FastMath.exp(1 / 8 * lambda);\n\n\n\nThe \"1 / 8 * lambda\" is evidently incorrect, since this will always evaluate to 0. I rechecked the original algorithm (http://luc.devroye.org/devroye-poisson.pdf) and it should instead be:\n\n\n\nfinal double a1 = FastMath.sqrt(FastMath.PI * twolpd) * FastMath.exp(1 / (8 * lambda));\n\n\n\n(lambda is a double so there is no int division issue.) This matches a later expression.\nI&apos;m not sure how to evaluate the effect of the bug. Better to be correct of course; it may never have made much practical difference.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.distribution.PoissonDistribution.java"], "label": 1, "es_results": []}, {"bug_id": 1067, "bug_title": "Stack overflow in Beta.regularizedBeta", "bug_description": "In org.apache.commons.math3.special.Beta.regularizedBeta(double,double,double,double,int), the case\n } else if (x > (a + 1.0) / (a + b + 2.0)) \n{\n\n      ret = 1.0 - regularizedBeta(1.0 - x, b, a, epsilon, maxIterations);\n\n}\n \nis prone to infinite recursion: If x is approximately the tested value, then 1-x is approximately the tested value in the recursion. Thus, due to loss of precision after the subtraction, this condition can be true for the recursive call as well.\nExample:\ndouble x= Double.longBitsToDouble(4597303555101269224L);\ndouble a= Double.longBitsToDouble(4634227472812299606L);\ndouble b = Double.longBitsToDouble(4642050131540049920L);\nSystem.out.println(x > (a + 1.0) / (a + b + 2.0));\nSystem.out.println(1-x>(b + 1.0) / (b + a + 2.0));\nSystem.out.println(1-(1-x)>(a + 1.0) / (a + b + 2.0));\nPossible solution: change the condition to\nx > (a + 1.0) / (a + b + 2.0) && 1-x<=(b + 1.0) / (b + a + 2.0)", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.special.Beta.java", "org.apache.commons.math3.special.BetaTest.java"], "label": 1, "es_results": []}, {"bug_id": 1070, "bug_title": "Incorrect rounding of float", "bug_description": "package org.apache.commons.math3.util \nexample of usage of round functions of Precision class:\nPrecision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01\nPrecision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01\nPrecision.round((float) 0.0, 2) = 0.0\nPrecision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0\nSeems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.\nI think, same problem will be found at usage of other round modes.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.util.PrecisionTest.java", "org.apache.commons.math3.util.Precision.java"], "label": 1, "es_results": []}, {"bug_id": 1088, "bug_title": "MultidimensionalCounter does not throw \"NoSuchElementException\"", "bug_description": "The iterator should throw when \"next()\" is called even though \"hasNext()\" would return false.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.util.MultidimensionalCounterTest.java", "org.apache.commons.math3.util.MultidimensionalCounter.java"], "label": 1, "es_results": []}, {"bug_id": 1065, "bug_title": "EnumeratedRealDistribution.inverseCumulativeProbability returns values not in the samples set", "bug_description": "The method EnumeratedRealDistribution.inverseCumulativeProbability() sometimes returns values that are not in the initial samples domain...\nI will attach a test to exploit this bug.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.distribution.EnumeratedRealDistribution.java", "org.apache.commons.math3.distribution.EnumeratedRealDistributionTest.java"], "label": 1, "es_results": []}, {"bug_id": 1111, "bug_title": "Spelling mistake in org.apache.commons.math3.fitting ", "bug_description": "in the paragraph containing : \n\"should pass through sample points, and were the objective function is the\" \nat http://commons.apache.org/proper/commons-math/javadocs/api-3.2/org/apache/commons/math3/fitting/package-summary.html\n\"were\" should be \"where\"", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.fitting.package-info.java"], "label": 1, "es_results": []}, {"bug_id": 1110, "bug_title": "OLSMultipleLinearRegression needs a way to specify non-zero singularity threshold when instantiating QRDecomposition", "bug_description": "OLSMultipleLinearRegression uses QRDecomposition to perform a least-squares solution. QRDecomposition has the capability to use a non-zero threshold for detecting when the design matrix is singular (see https://issues.apache.org/jira/browse/MATH-665, https://issues.apache.org/jira/browse/MATH-1024, https://issues.apache.org/jira/browse/MATH-1100, https://issues.apache.org/jira/browse/MATH-1101) but OLSMultipleLinearRegression does not use this capability and therefore always uses the default singularity test threshold of 0. This can lead to bad solutions (see in particular https://issues.apache.org/jira/browse/MATH-1101?focusedCommentId=13909750&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13909750) when a SingularMatrixException should instead be thrown. \nWhen I encountered this situation, I noticed it because the solution values were extremely large (in the range 1e09 - 1e12). Normal values in the domain I am working with are on the order of 1e-3. To find out why the values are so large, I traced through the source and found that an rDiag value was on the order of 1e-15, and that this passed the threshold test. I then noticed that two columns of the design matrix are linearly dependent (one column is all 1&apos;s because I want an intercept value in the solution, and another is also all 1&apos;s because that&apos;s how the data worked out). Thus the matrix is definitely singular. \nIf I could specify a non-zero threshold, this situation would result in  a SingularMatrixException, but without that, the bad solution values would be blindly propagated. That is a problem because this solution is intended for controlling a physical system, and damage could result from a bad solution. \nUnfortunately, I see no way to change the threshold value from outside  I would have to in effect re-implement OLSMultipleLinearRegression to do this as a user of the package. ", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_3", "fixed_files": ["org.apache.commons.math3.stat.regression.OLSMultipleLinearRegressionTest.java", "org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression.java"], "label": 1, "es_results": []}, {"bug_id": 1129, "bug_title": "Percentile Computation errs", "bug_description": "In the following test, the 75th percentile is smaller than the 25th percentile, leaving me with a negative interquartile range.\nBar.java\n\n\n@Test public void negativePercentiles(){\n\n\n\n        double[] data = new double[]{\n\n                -0.012086732064244697, \n\n                -0.24975668704012527, \n\n                0.5706168483164684, \n\n                -0.322111769955327, \n\n                0.24166759508327315, \n\n                Double.NaN, \n\n                0.16698443218942854, \n\n                -0.10427763937565114, \n\n                -0.15595963093172435, \n\n                -0.028075857595882995, \n\n                -0.24137994506058857, \n\n                0.47543170476574426, \n\n                -0.07495595384947631, \n\n                0.37445697625436497, \n\n                -0.09944199541668033\n\n        };\n\n        DescriptiveStatistics descriptiveStatistics = new DescriptiveStatistics(data);\n\n\n\n        double threeQuarters = descriptiveStatistics.getPercentile(75);\n\n        double oneQuarter = descriptiveStatistics.getPercentile(25);\n\n\n\n        double IQR = threeQuarters - oneQuarter;\n\n        \n\n        System.out.println(String.format(\"25th percentile %s 75th percentile %s\", oneQuarter, threeQuarters ));\n\n        \n\n        assert IQR >= 0;\n\n        \n\n    }\n\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.stat.descriptive.DescriptiveStatisticsTest.java", "org.apache.commons.math3.stat.descriptive.rank.Percentile.java"], "label": 1, "es_results": []}, {"bug_id": 1118, "bug_title": "Complex: semantics of equals != Double equals, mismatch with hashCode", "bug_description": "Two complex numbers with real/imaginary parts 0.0d but different signs compare as equal numbers. This is according to their mathematical value; the comparison is done via \n                return (real == c.real) && (imaginary == c.imaginary);\nUnfortunately, two Double values do NOT compare as equal in that case, so real.equals(c.real) would return false if the signs differ.\nThis becomes a problem because for the hashCode, MathUtils.hash is used on the real and imaginary parts, which in turn uses Double.hash.\nThis violates the contract on equals/hashCode, so Complex numbers cannot be used in a hashtable or similar data structure:\n    Complex c1 = new Complex(0.0, 0.0);\n    Complex c2 = new Complex(0.0, -0.0);\n    // Checks the contract:  equals-hashcode on c1 and c2\n    assertTrue(\"Contract failed: equals-hashcode on c1 and c2\", c1.equals(c2) ? c1.hashCode() == c2.hashCode() : true);", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_6", "fixed_files": ["org.apache.commons.math3.complex.ComplexTest.java", "org.apache.commons.math3.complex.Complex.java", "org.apache.commons.math3.util.MathUtils.java", "org.apache.commons.math3.util.MathUtilsTest.java"], "label": 1, "es_results": []}, {"bug_id": 1116, "bug_title": "NullPointerException not advertized in Javadoc", "bug_description": "The following statement produces a NullPointerException:\nnew org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizer().getWeight();\nThe documentation does not seem to indicate that other data must be set before getWeight is used (at least I could not find that information). In this case, weightMatrix is still null because it has not been initialized.\nThis call should probably throw an IllegalStateException, which makes it clear that this API usage is incorrect.\nThis test uses LevenbergMarquardtOptimizer but any instantiable subclass of MultivariateVectorOptimizer probably works the same way.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_2", "fixed_version": "MATH_3_6", "fixed_files": ["org.apache.commons.math3.random.EmpiricalDistribution.java", "org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression.java", "org.apache.commons.math3.random.ValueServer.java"], "label": 1, "es_results": []}, {"bug_id": 1135, "bug_title": "Bug in MonotoneChain: a collinear point landing on the existing boundary should be dropped (patch)", "bug_description": "The is a bug on the code in MonotoneChain.java that attempts to handle the case of a point on the line formed by the previous last points and the last point of the chain being constructed. When `includeCollinearPoints` is false, the point should be dropped entirely. In common-math 3,3, the point is added, which in some cases can cause a `ConvergenceException` to be thrown.\nIn the patch below, the data points are from a case that showed up in testing before we went to production.\n\n\n\nIndex: src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java\n\n===================================================================\n\n--- src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java\t(revision 1609491)\n\n+++ src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java\t(working copy)\n\n@@ -160,8 +160,8 @@\n\n                 } else {\n\n                     if (distanceToCurrent > distanceToLast) {\n\n                         hull.remove(size - 1);\n\n+                        hull.add(point);\n\n                     }\n\n-                    hull.add(point);\n\n                 }\n\n                 return;\n\n             } else if (offset > 0) {\n\nIndex: src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java\n\n===================================================================\n\n--- src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java\t(revision 1609491)\n\n+++ src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java\t(working copy)\n\n@@ -204,6 +204,24 @@\n\n     }\n\n \n\n     @Test\n\n+    public void testCollinnearPointOnExistingBoundary() {\n\n+        final Collection<Vector2D> points = new ArrayList<Vector2D>();\n\n+        points.add(new Vector2D(7.3152, 34.7472));\n\n+        points.add(new Vector2D(6.400799999999997, 34.747199999999985));\n\n+        points.add(new Vector2D(5.486399999999997, 34.7472));\n\n+        points.add(new Vector2D(4.876799999999999, 34.7472));\n\n+        points.add(new Vector2D(4.876799999999999, 34.1376));\n\n+        points.add(new Vector2D(4.876799999999999, 30.48));\n\n+        points.add(new Vector2D(6.0959999999999965, 30.48));\n\n+        points.add(new Vector2D(6.0959999999999965, 34.1376));\n\n+        points.add(new Vector2D(7.315199999999996, 34.1376));\n\n+        points.add(new Vector2D(7.3152, 30.48));\n\n+\n\n+        final ConvexHull2D hull = generator.generate(points);\n\n+        checkConvexHull(points, hull);\n\n+    }\n\n+\n\n+    @Test\n\n     public void testIssue1123() {\n\n \n\n         List<Vector2D> points = new ArrayList<Vector2D>();\n\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.geometry.euclidean.twod.hull.ConvexHullGenerator2DAbstractTest.java", "org.apache.commons.math3.geometry.euclidean.twod.hull.MonotoneChain.java"], "label": 1, "es_results": []}, {"bug_id": 1149, "bug_title": "unsafe initialization in DummyStepInterpolator", "bug_description": "DummyStepInterpolator.java\n\n\n  public DummyStepInterpolator(final DummyStepInterpolator interpolator) {\n\n    super(interpolator);\n\n    currentDerivative = interpolator.currentDerivative.clone();\n\n  }\n\n   @Override\n\n  protected StepInterpolator doCopy() {\n\n    return new DummyStepInterpolator(this);\n\n  }\n\n\n\nA constructor in DummyStepInterpolator dereferences a field of the parameter, but a NullPointerException can occur during a call to doCopy().\nTest.java\n\n\npublic void test1() throws Throwable {\n\n  DummyStepInterpolator var0 = new DummyStepInterpolator();\n\n  var0.copy();\n\n}\n\n\n\nHere in Test.java, a NPE occurs because copy() calls doCopy() which calls  DummyStepInterpolator(final DummyStepInterpolator) that passes var0 as an argument.\nI think this constructor should have a null check for  interpolator.currentDerivative like NordsieckStepInterpolator does.\nNordsieckStepInterpolator.java\n\n\n    public NordsieckStepInterpolator(final NordsieckStepInterpolator interpolator) {\n\n        super(interpolator);\n\n        scalingH      = interpolator.scalingH;\n\n        referenceTime = interpolator.referenceTime;\n\n        if (interpolator.scaled != null) {\n\n            scaled = interpolator.scaled.clone();\n\n        }\n\n        if (interpolator.nordsieck != null) {\n\n            nordsieck = new Array2DRowRealMatrix(interpolator.nordsieck.getDataRef(), true);\n\n        }\n\n        if (interpolator.stateVariation != null) {\n\n            stateVariation = interpolator.stateVariation.clone();\n\n        }\n\n    }\n\n\n\n    @Override\n\n    protected StepInterpolator doCopy() {\n\n        return new NordsieckStepInterpolator(this);\n\n    }\n\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.ode.sampling.DummyStepInterpolator.java"], "label": 1, "es_results": []}, {"bug_id": 1145, "bug_title": "Integer overflows MannWhitneyUTest#mannWhitneyU", "bug_description": "In the calculation of MannWhitneyUTest#mannWhitneyU there are two instances where the lengths of the input arrays are multiplied together. Because Array#length is an integer this means that the effective maximum size of your dataset until reaching overflow is Math.sqrt(Integer.MAX_VALUE).\nThe following is a link to a different, with a test the exposes the issue, and a fix (casting lengths up into doubles before multiplying).\nhttps://gist.github.com/aconbere/4fef56e5182e510aceb3", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.stat.inference.MannWhitneyUTestTest.java", "org.apache.commons.math3.stat.inference.MannWhitneyUTest.java"], "label": 1, "es_results": []}, {"bug_id": 1131, "bug_title": "Kolmogorov-Smirnov Tests takes 'forever' on 10,000 item dataset", "bug_description": "I have code simplified to the following:\n    KolmogorovSmirnovTest kst = new KolmogorovSmirnovTest();\n    NormalDistribution nd = new NormalDistribution(mean,stddev);\n    kst.kolmogorovSmirnovTest(nd,dataset)\nI find that for my dataset of 10,000 items, the call to kolmogorovSmirnovTest takes &apos;forever&apos;. It has not returned after nearly 15minutes and in one my my tests has gone over 150MB in  memory usage. ", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest.java", "org.apache.commons.math3.stat.inference.KolmogorovSmirnovTestTest.java", "org.apache.commons.math3.util.MathUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1152, "bug_title": "Suboptimal implementation of EnumeratedDistribution.sample()", "bug_description": "org.apache.commons.math3.distribution.EnumeratedDistribution.sample() performs a linear search to find the appropriate element in the probability space (called singletons here) given a random double value. For large probability spaces, this is not effective. Instead, we should cache the cumulative probabilities and do a binary search.\nRough implementation:\nEnumeratedDistribution.java\n\n\nvoid computeCumulative() {\n\n  cumulative = new double[size]; \n\n  double sum = 0;\n\n  for (int i = 1; i < weights.length - 1; i++) {\n\n      cumulative[i] = cumulative[i-1] + weights[i-1];\n\n   }\n\n  cumulative[size - 1] = 1;\n\n}\n\n\n\nand then \nEnumeratedDistribution.java\n\n\nint sampleIndex() {\n\n double randomValue = random.nextDouble();\n\n int result = Arrays.binarySearch(cumulative, randomValue);\n\n if (result >= 0) return result;\n\n int insertionPoint = -result-1;\n\n return insertionPoint;\n\n}\n\n\n\n\n", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.distribution.EnumeratedDistribution.java"], "label": 1, "es_results": []}, {"bug_id": 1138, "bug_title": "BicubicSplineInterpolator is returning incorrect interpolated values", "bug_description": "I have encountered a use case with the BicubicSplineInterpolator where the interpolated values that are being returned seem incorrect.  Furthermore, the values do not match those generated by MatLab using the interp2 &apos;cubic&apos; method.\nHere is a snippet of code that uses the interpolator:\n        double[] xValues = new double[] \n{36, 36.001, 36.002}\n;\n        double[] yValues = new double[] {-108.00, -107.999, -107.998};\n        double[][] fValues = new double[][] {{1915, 1906, 1931},\n\n{1877, 1889, 1894}\n,\n                                        {1878, 1873, 1888}};\n        BicubicSplineInterpolator interpolator = new BicubicSplineInterpolator();\n        BicubicSplineInterpolatingFunction interpolatorFunction = interpolator.interpolate(xValues, yValues, fValues);\n        double[][] results = new double[9][9];\n        double x = 36;\n        int arrayIndexX = 0, arrayIndexY = 0;\n        while(x <= 36.002) {\n            double y = -108;\n            arrayIndexY = 0;\n            while (y <= -107.998) \n{\n\n                results[arrayIndexX][arrayIndexY] = interpolatorFunction.value(x,  y);\n\n                System.out.println(results[arrayIndexX][arrayIndexY]);\n\n                y = y + 0.00025;\n\n                arrayIndexY++;\n\n            }\n\n            x = x + 0.00025;\n            arrayIndexX++;\n        }\nAttached is a grid showing x and y values and the corresponding interpolated value from both commons math and MatLab.\nThe values produced by commons math are far off from those created by MatLab.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolator.java", "org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatingFunction.java", "org.apache.commons.math3.analysis.interpolation.TricubicSplineInterpolatorTest.java", "org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatorTest.java", "org.apache.commons.math3.analysis.interpolation.TricubicSplineInterpolator.java", "org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatingFunctionTest.java", "org.apache.commons.math3.analysis.interpolation.SmoothingPolynomialBicubicSplineInterpolator.java", "org.apache.commons.math3.analysis.interpolation.SmoothingPolynomialBicubicSplineInterpolatorTest.java", "org.apache.commons.math3.analysis.interpolation.SplineInterpolatorTest.java"], "label": 1, "es_results": []}, {"bug_id": 1165, "bug_title": "Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer", "bug_description": "The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:\nIf the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.\nSo the if condition:\nif (membershipMatrix[i][j] > maxMembership) \n{\n\n                    maxMembership = membershipMatrix[i][j];\n\n                    newCluster = j;\n\n}\nwill never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line:\nclusters.get(newCluster)\n                    .addPoint(point);\nAdding the following condition can solve the problem:\ndouble d;\nif (sum == 0)\nd = 1;\nelse\nd = 1.0/sum;", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.ml.clustering.FuzzyKMeansClustererTest.java", "org.apache.commons.math3.ml.clustering.FuzzyKMeansClusterer.java"], "label": 1, "es_results": []}, {"bug_id": 1167, "bug_title": "OLSMultipleLinearRegression STILL needs a way to specify non-zero singularity threshold when instantiating QRDecomposition", "bug_description": "A fix was made for this issue in MATH-1110 for the newSampleData method but not for the newXSampleData method.\nIt&apos;s a simple change to propagate the threshold to QRDecomposition:\n237c237\n<         qr = new QRDecomposition(getX());\n\n>         qr = new QRDecomposition(getX(), threshold);", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression.java"], "label": 1, "es_results": []}, {"bug_id": 1174, "bug_title": "Some thin rectangles are not handled properly as PolygonsSet", "bug_description": "If the width of a rectangle is smaller than the close point tolerances, some weird effects appear when vertices are extracted. Typically the size will be set to infinity and barycenter will be forced at origin.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.geometry.euclidean.twod.PolygonsSetTest.java"], "label": 1, "es_results": []}, {"bug_id": 1146, "bug_title": "class Mean returns incorrect result after processing an Infinity value", "bug_description": "1. Create a Mean object.\n2. call increment() with Double.POSITIVE_INFINITY.\n3. Call getResult(). Result is INFINITY as expected.\n4. call increment() with 0.\n5. Call getResult(). Result is NaN; not INFINITY as expected.\nThis is apparently due to the \"optimization\" for calculating mean described in the javadoc. Rather than accumulating a sum, it maintains a running mean value using the formula \"m = m + (new value - m) / (number of observations)\", which unlike the \"definition way\", fails after an infinity.\nI was using Mean within a SummaryStatistics. Other statistics also seem to be affected; for example, the standard deviation also incorrectly gives NaN rather than Infinity. I don&apos;t know if that&apos;s due to the error in Mean or if the other stats classes have similar bugs.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.stat.descriptive.moment.FirstMoment.java", "org.apache.commons.math3.stat.descriptive.moment.SecondMoment.java", "org.apache.commons.math3.stat.descriptive.moment.FourthMoment.java", "org.apache.commons.math3.stat.descriptive.moment.ThirdMoment.java", "org.apache.commons.math3.stat.descriptive.moment.Variance.java", "org.apache.commons.math3.stat.descriptive.moment.Kurtosis.java", "org.apache.commons.math3.stat.descriptive.moment.Mean.java", "org.apache.commons.math3.stat.descriptive.moment.FirstMomentTest.java"], "label": 1, "es_results": []}, {"bug_id": 1134, "bug_title": "unsafe initialization in BicubicSplineInterpolatingFunction", "bug_description": "The lazy initialization of the internal array of partialDerivatives in BicubicSplineInterpolatingFunction is not thread safe. If multiple threads call any of the partialDerivative functions concurrently one thread may start the initialization and others will see the array is non-null and assume it is fully initialized. If the internal array of partial derivatives was initialized in the constructor this would not be a problem.\ni.e. the following check in partialDerivative(which, x, y)\n        if (partialDerivatives == null) \n{\n\n            computePartialDerivatives();\n\n        }\nwill start the initialization. However in computePartialDerivatives()\n        partialDerivatives = new BivariateFunction[5][lastI][lastJ];\nmakes it appear to other threads as the the initialization has completed when it may not have.", "project": "Commons", "sub_project": "MATH", "version": "MATH_3_3", "fixed_version": "MATH_3_4", "fixed_files": ["org.apache.commons.math3.analysis.interpolation.TricubicSplineInterpolator.java", "org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolator.java", "org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatingFunction.java"], "label": 1, "es_results": []}, {"bug_id": 336, "bug_title": "jboss-ejb3.xml ignores metadata-complete", "bug_description": "", "project": "JBoss", "sub_project": "JBMETA", "version": "7.0.0.Beta6", "fixed_version": "7.0.0.Beta11", "fixed_files": ["org.jboss.metadata.ejb.parser.jboss.ejb3.JBossEjb3MetaDataParser.java", "org.jboss.metadata.ejb.test.extension.ExtensionTestCase.java", "org.jboss.metadata.ejb.parser.spec.EjbJarMetaDataParser.java", "org.jboss.metadata.ejb.jboss.ejb3.JBossEjb31MetaData.java", "org.jboss.metadata.ejb.parser.spec.AbstractEjbJarMetaDataParser.java"], "label": 1, "es_results": []}, {"bug_id": 338, "bug_title": "Fix WS ref parsing & merging process", "bug_description": "", "project": "JBoss", "sub_project": "JBMETA", "version": "7.0.0.Beta12", "fixed_version": "7.0.0.Beta13", "fixed_files": ["org.jboss.metadata.merge.javaee.jboss.JBossServiceReferencesMetaDataMerger.java", "org.jboss.metadata.merge.javaee.jboss.JBossPortComponentRefMerger.java"], "label": 1, "es_results": []}, {"bug_id": 348, "bug_title": "WebApp24ValidationUnitTestCase tries to connect to java.sun.com", "bug_description": "The tests must never connect to the outside world.", "project": "JBoss", "sub_project": "JBMETA", "version": "7.0.0.Beta30", "fixed_version": "7.0.0.Beta31", "fixed_files": ["org.jboss.test.metadata.web.WebApp24ValidationUnitTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 351, "bug_title": "Missing parser support for replication-config and passivation-config", "bug_description": "", "project": "JBoss", "sub_project": "JBMETA", "version": "7.0.0.Beta31", "fixed_version": "7.0.0.Beta32", "fixed_files": ["org.jboss.test.metadata.web.JBossWeb60UnitTestCase.java", "org.jboss.metadata.parser.jbossweb.JBossWebMetaDataParser.java"], "label": 1, "es_results": []}, {"bug_id": 355, "bug_title": "Application Exception Class is not trimmed", "bug_description": "All parsers use super.getElementText(XML...), the Application Exception parser does not.", "project": "JBoss", "sub_project": "JBMETA", "version": "7.0.0.Final", "fixed_version": "7.0.2.Final", "fixed_files": ["org.jboss.metadata.ejb.parser.spec.ApplicationExceptionMetaDataParser.java", "org.jboss.test.metadata.ejb.whitespace.WhitespaceUnitTestCase.java"], "label": 1, "es_results": []}, {"bug_id": 354, "bug_title": "Support EJB 2.0 dtd descriptors", "bug_description": "", "project": "JBoss", "sub_project": "JBMETA", "version": "7.0.0.Final", "fixed_version": "8.1.2.Final", "fixed_files": ["org.jboss.metadata.ejb.parser.spec.EjbJarElement.java", "org.jboss.metadata.ejb.parser.spec.EnterpriseBeansMetaDataParser.java", "org.jboss.metadata.ejb.parser.spec.MessageDrivenBean31Parser.java", "org.jboss.metadata.ejb.parser.spec.AbstractMessageDrivenBeanParser.java"], "label": 1, "es_results": []}, {"bug_id": 371, "bug_title": "DefaultPropertyReplacer + PropertyResolver is broken for vault expressions", "bug_description": "The DefaultPropertyReplacer + PropertyResolver algorithm is broken for vault expressions. It&apos;s based on the assumption that the contents of the expression string between \"$\n{\" and \"}\n\" have a fixed format a la the old JBoss AS system properties (e.g. propertyname[: default value]) and then the PropertyResolver tries to resolve \"propertyname\".\nThis is incorrect in the case of vault expressions, which do not follow this format. In particular the \":\" char appears multiple places in a vault expression and does not serve as a delimiter between \"propertyname\" and \"default value\".", "project": "JBoss", "sub_project": "JBMETA", "version": "7.0.4.Final", "fixed_version": "8.0.0.Final", "fixed_files": ["org.jboss.test.metadata.property.PropertyReplacerTest.java", "org.jboss.metadata.property.DefaultPropertyReplacer.java"], "label": 1, "es_results": []}, {"bug_id": 377, "bug_title": "NullPointerException occurs during parsing web.xml with missing context param-name", "bug_description": "[Opening this issue to track down the issue specific to jboss metadata] \nWhile deploying a WAR, If the web.xml file is used which has context <param-value> defined, However it has missing <param-name> then it causes NullPointerException.\nRefer to for more details: https://issues.jboss.org/browse/WFLY-3699 ", "project": "JBoss", "sub_project": "JBMETA", "version": "7.1.1.Final", "fixed_version": "8.1.1.Final", "fixed_files": ["org.jboss.metadata.javaee.spec.ParamValueMetaData.java", "org.jboss.metadata.parser.ee.ParamValueMetaDataParser.java"], "label": 1, "es_results": []}, {"bug_id": 386, "bug_title": "Deployment fails when jboss-web.xml contains <enable-websockets>true</enable-websockets>", "bug_description": "When deploying application with WebSockets used in EAP 6 to EAP 7, deployment will fail due EAP7 not supporting [1] in jboss-web.xml. The <enable-websockets> element is required in EAP 6 to enable the WebSockets functionality in JBoss Web.\nThis element should be also allowed in EAP 7 in order to simplify migration from EAP 6. Functionality of {{ <enable-websockets>true</enable-websockets>}} can be ignored as in EAP 7 and in Undertow there is no need to enable WebSockets explicitly.\n[1]\n\n\n\n\n\n\n<jboss-web version=\"7.2\"\n\n\n\n\n   xmlns=\"http://www.jboss.com/xml/ns/javaee\"\n\n\n\n\n   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\n\n\n\n   xsi:schemaLocation=\"http://www.jboss.com/xml/ns/javaee schema/jboss-web_7_2.xsd\">\n\n\n\n\n    <enable-websockets>true</enable-websockets>\n\n\n\n\n</jboss-web>\n\n\n\n\n\n\n\n\n\n\n", "project": "JBoss", "sub_project": "JBMETA", "version": "8.1.1.Final", "fixed_version": "10.0.0.Beta2", "fixed_files": ["org.jboss.metadata.parser.jbossweb.Element.java", "org.jboss.metadata.parser.jbossweb.JBossWebMetaDataParser.java", "org.jboss.metadata.web.jboss.JBossWebMetaData.java"], "label": 1, "es_results": []}, {"bug_id": 99, "bug_title": "AmqpBrokerAdminIntegrationTests failing with new rabbit server (2.3.1)", "bug_description": "AmqpBrokerAdminIntegrationTests failing with new rabbit server (2.3.1)", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.M2", "fixed_version": "1.0.0.M3", "fixed_files": ["org.springframework.amqp.rabbit.admin.RabbitBrokerAdminIntegrationTests.java", "org.springframework.amqp.rabbit.admin.RabbitBrokerAdmin.java", "org.springframework.amqp.rabbit.admin.RabbitControlErlangConverter.java"], "label": 1, "es_results": []}, {"bug_id": 109, "bug_title": "Tomcat shutdown does not wait for consumers in MessageListenerContainer", "bug_description": "I&apos;m not sure it&apos;s really a problem, but Tomcat should wait for the context to close, so why do I keep seeing ShutdownSignalExceptions?  If you start Tomcat in debug mode from WTP and then shut it down, you can see the exception because the debugger pauses.  Unfortunately it happens quite frequently but not always, so the first task is to get an integration test that mimics this scenario (not sure why the existing *LifecycleIntegrationTests don&apos;t expose it).", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.M3", "fixed_version": "1.0.0.RC1", "fixed_files": ["org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 121, "bug_title": "<listener-container concurrency=\"5\"/> blows up", "bug_description": "<listener-container concurrency=\"5\"/> blows up because it uses the wrong property name in SimpleMessageListenerContainer.", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.M3", "fixed_version": "1.0.0.RC1", "fixed_files": ["org.springframework.amqp.rabbit.config.ListenerContainerParser.java"], "label": 1, "es_results": []}, {"bug_id": 124, "bug_title": "Message.toString() ClasscastException", "bug_description": "Message.toString() ClasscastException when body is Serializable and not a String!", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.M1", "fixed_version": "1.0.0.RC1", "fixed_files": ["org.springframework.amqp.core.MessageTests.java", "org.springframework.amqp.core.Message.java"], "label": 1, "es_results": []}, {"bug_id": 125, "bug_title": "Unreliable lock algorithm in SimpleMessageListenerContainer", "bug_description": "Unreliable lock algorithm in SimpleMessageListenerContainer leads to container not waiting for stop() if some listeners have failed.", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.M3", "fixed_version": "1.0.0.RC1", "fixed_files": ["org.springframework.amqp.rabbit.core.RabbitBindingIntegrationTests.java", "org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java", "org.springframework.amqp.rabbit.listener.MessageListenerRecoveryCachingConnectionIntegrationTests.java", "org.springframework.amqp.rabbit.listener.MessageListenerContainerLifecycleIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 148, "bug_title": "Default number of consumers is 0 on SimpleMessageListenerContainer if afterPropertiesSet() not called", "bug_description": "SimpleMessageListenerContainer has a default number of concurrent consumers set to 0, and the validateConfiguration() method, which checks for it and eventually set it to 1, seems to be never called.\nThis is very annoying, because applications using older versions will stop working with no apparent reason.\nFinally, please note that version M2 had a default number set to 1.", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.M3", "fixed_version": "1.0.0.RC2", "fixed_files": ["org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainerTests.java"], "label": 1, "es_results": []}, {"bug_id": 151, "bug_title": "SingleConnectionFactory can close the wrong target connection on close", "bug_description": "SingleConnectionFactory can close the wrong target connection on close. From zzantozz on the forum:\n\nThe targetConnection is created first, and then \"connection\" is targetConnection wrapped with an adapter. That&apos;s all well and good, but the adapter that the former connection is wrapped with is a SingleConnectionFactory.SharedConnectionProxy. The targetConnection is passed in as a constructor arg and is stored in the instance field SharedConnectionProxy.target. Then in SharedConnectionProxy.createChannel, it has the ability to detect that \"target\" is closed and to create a new connection. The problem here is that it only updates SharedConnectionProxy.target and not SingleConnectionFactory.targetConnection, so when the factory is shut down, it will try to close a connection that is already known to be bad, and the good connection won&apos;t be closed. Given that SharedConnectionProxy is an inner class of SingleConnectionFactory, the correct behavior could be gotten just by eliminating the target field from the former and directly accessing targetConnection in the latter. ", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.RC1", "fixed_version": "1.0.0.RC2", "fixed_files": ["org.springframework.amqp.rabbit.connection.SingleConnectionFactoryTests.java", "org.springframework.amqp.rabbit.connection.SingleConnectionFactory.java"], "label": 1, "es_results": []}, {"bug_id": 150, "bug_title": "RabbitAdmin or CachingConnectionFactory should prevent calls to ConnectionListener when physical connection is not opened", "bug_description": "", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.RC1", "fixed_version": "1.0.0.RC2", "fixed_files": ["org.springframework.amqp.rabbit.connection.SingleConnectionFactoryTests.java", "org.springframework.amqp.rabbit.connection.SingleConnectionFactory.java", "org.springframework.amqp.rabbit.connection.CachingConnectionFactoryTests.java", "org.springframework.amqp.rabbit.test.BrokerRunning.java", "org.springframework.amqp.rabbit.connection.CachingConnectionFactory.java", "org.springframework.amqp.rabbit.core.RabbitAdminIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 165, "bug_title": "AmqpAppender does not die cleanly when webapp shuts down", "bug_description": "Fixed in samples by using Log4jConfigListener (it shuts down the appenders) and also in spring-rabbit by manually closing the connection factory when the appender closes.", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.RC1", "fixed_version": "1.0.0.RC2", "fixed_files": ["org.springframework.amqp.rabbit.log4j.AmqpAppender.java"], "label": 1, "es_results": []}, {"bug_id": 168, "bug_title": "SimpleMessageListenerContainer should only commit every txSize messages", "bug_description": "SimpleMessageListenerContainer should only commit every txSize messages (it looks like it does it for every message right now).", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.RC1", "fixed_version": "1.0.0.RC2", "fixed_files": ["org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainerIntegrationTests.java", "org.springframework.amqp.rabbit.config.StatefulRetryOperationsInterceptorFactoryBean.java", "org.springframework.amqp.rabbit.listener.MessageListenerContainerRetryIntegrationTests.java", "org.springframework.amqp.rabbit.test.BrokerRunning.java", "org.springframework.amqp.rabbit.test.RepeatProcessor.java", "org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.MessageListenerRecoveryCachingConnectionIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 170, "bug_title": "Message header value has type com.rabbitmq.client.impl.LongStringHelper.ByteArrayLongString", "bug_description": "The problem has happened using spring-integration-amqp.\nTrying to read a value from the header of a message I get that this value is of com.rabbitmq.client.impl.LongStringHelper.ByteArrayLongString type, when it was supposed to be String.", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.RC1", "fixed_version": "1.0.0.RC2", "fixed_files": ["org.springframework.amqp.rabbit.connection.RabbitUtils.java"], "label": 1, "es_results": []}, {"bug_id": 179, "bug_title": "RabbitAdmin anonymous queue declaration javadocs inconsistent with Queue properties", "bug_description": "RabbitAdmin anonymous queue declaration javadocs inconsistent with Queue properties: it creates a non-durable, non-exclusive, auto-delete queue (as per the defaults in Channel), but javadocs say it is durable and not auto-delete.", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.RC2", "fixed_version": "1.0.0.RC3", "fixed_files": ["org.springframework.amqp.rabbit.core.RabbitAdmin.java"], "label": 1, "es_results": []}, {"bug_id": 223, "bug_title": "Too Many Threads Running in SimpleListenerContainer", "bug_description": "If doStart() is called twice, we end up with two threads running in each consumer.\nThis can happen, say, if spring-integration-amqp declares an adapter that uses an externally defined listener container.\nThe container itself is started, and then started again by the adapter.\nThis can cause acks to go out asynchronously. Need to protect against running the consumers multiple times.", "project": "Spring", "sub_project": "AMQP", "version": "1.0.0.RELEASE", "fixed_version": "1.1.0.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java"], "label": 1, "es_results": []}, {"bug_id": 231, "bug_title": "Fix Failing Integration Tests", "bug_description": "", "project": "Spring", "sub_project": "AMQP", "version": "1.1.0.RELEASE", "fixed_version": "1.1.1.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.core.RabbitBindingIntegrationTests.java", "org.springframework.amqp.rabbit.core.RabbitTemplatePublisherCallbacksIntegrationTests.java", "org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java", "org.springframework.amqp.rabbit.listener.MessageListenerContainerLifecycleIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 232, "bug_title": "More Failing System Tests", "bug_description": "Placeholder until we clean these all up.\nhttps://build.springsource.org/browse/AMQP-INTEGRATION-422/", "project": "Spring", "sub_project": "AMQP", "version": "1.1.0.RELEASE", "fixed_version": "1.1.1.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.MessageListenerRecoveryCachingConnectionIntegrationTests.java", "org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainerIntegrationTests.java", "org.springframework.amqp.rabbit.listener.MessageListenerContainerLifecycleIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 243, "bug_title": "AmqpAppender and log4j synchronization issues resulting in messages loss", "bug_description": "The Log4j AmqpAppender is subject to synchronization issues because of the Log4j PatternLayout.\nProblem is at the following line :\nString routingKey = routingKeyLayout.format(logEvent);\nWe must either use another pattern like the EnhancedPatternLayoutlike suggested by the PatternLayout JavaDoc or add synchronization code.\nThe consequence is message lost because of the routing key badly generated.", "project": "Spring", "sub_project": "AMQP", "version": "1.1.1.RELEASE", "fixed_version": "v1.1.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.log4j.AmqpAppender.java"], "label": 1, "es_results": []}, {"bug_id": 242, "bug_title": "Log4j AmqpAppender does not use the layout", "bug_description": "The Log4j AmqpAppender does not use the layout.\nThe JavaDoc mentions the property log4j.appender.amqp.layout but the message is rendered using new StringBuffer(String.format(\"%s%n\", logEvent.getRenderedMessage())).\nlayout.format(logEvent) should maybe be used.", "project": "Spring", "sub_project": "AMQP", "version": "1.1.1.RELEASE", "fixed_version": "v1.1.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.log4j.AmqpAppender.java"], "label": 1, "es_results": []}, {"bug_id": 250, "bug_title": "Anonymous reply-queue Does not Work with Send/Reply Operations", "bug_description": "", "project": "Spring", "sub_project": "AMQP", "version": "1.1.1.RELEASE", "fixed_version": "v1.1.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.config.TemplateParser.java", "org.springframework.amqp.rabbit.config.TemplateParserTests.java"], "label": 1, "es_results": []}, {"bug_id": 249, "bug_title": "Issues with RabbitTemplate Temporary Reply Queues", "bug_description": "Temporary Reply Queues are configured autoAck=false, but we don&apos;t ack the reply. If using a cached channel, this causes accounting problems on the Rabbit admin (channel shows unacked count).\nAlso, since the replyHandoff is a SynchronizedQueue, there is a race condition in the case the timeout is exceeded just as the reply is received; the consumer thread hangs on the put() because there is nobody in take().", "project": "Spring", "sub_project": "AMQP", "version": "1.1.1.RELEASE", "fixed_version": "v1.1.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.core.RabbitTemplate.java", "org.springframework.amqp.rabbit.core.RabbitTemplateTests.java"], "label": 1, "es_results": []}, {"bug_id": 252, "bug_title": "Remove System.out Call In JsonMessageConverter", "bug_description": "Line #122, when a ClassMapper is provided.", "project": "Spring", "sub_project": "AMQP", "version": "1.1.1.RELEASE", "fixed_version": "v1.1.2.RELEASE", "fixed_files": ["org.springframework.amqp.support.converter.JsonMessageConverter.java"], "label": 1, "es_results": []}, {"bug_id": 260, "bug_title": "Hanging thread when using RabbitTransactionManager with a listener container where channelTransacted=true", "bug_description": "When using the following configuration, the message will never be acknowledged:\n\n\n\n\n\n\n<bean id=\"connectionFactory\" class=\"org.springframework.amqp.rabbit.connection.CachingConnectionFactory\">\n\n\n\n\n\t    <constructor-arg value=\"localhost\"/>\n\n\n\n\n\t    <property name=\"username\" value=\"guest\"/>\n\n\n\n\n\t    <property name=\"password\" value=\"guest\"/>\n\n\n\n\n\t</bean>\t\n\n\n\n\n\t\t\n\n\n\n\n\t<bean id=\"rabbitTransactionManager\" class=\"org.springframework.amqp.rabbit.transaction.RabbitTransactionManager\">\n\n\n\n\n\t\t<property name=\"connectionFactory\" ref=\"connectionFactory\" />\n\n\n\n\n\t</bean>\n\n\n\n\n\t\n\n\n\n\n\t<bean class=\"org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer\">\n\n\n\n\n\t\t<property name=\"connectionFactory\" ref=\"connectionFactory\" />\n\n\n\n\n\t\t<property name=\"queueNames\" value=\"Queue\" />\n\n\n\n\n\t\t<property name=\"messageListener\">\n\n\n\n\n\t\t\t<bean class=\"org.springframework.amqp.rabbit.listener.adapter.MessageListenerAdapter\">\n\n\n\n\n\t\t\t\t<property name=\"delegate\" ref=\"endpoint\" />\n\n\n\n\n\t\t\t</bean>\n\n\n\n\n\t\t</property>\n\n\n\n\n\t\t<property name=\"transactionManager\" ref=\"rabbitTransactionManager\" />\n\n\n\n\n\t\t<property name=\"channelTransacted\" value=\"true\" />\n\n\n\n\n\t</bean>\n\n\n\n\n\n\n\n\n\n\t<bean id=\"endpoint\" class=\"amqp.Endpoint\" />\n\n\n\n\n\t\n\n\n\n\n\t<rabbit:admin connection-factory=\"connectionFactory\" />\n\n\n\n\n\t\n\n\n\n\n\t<rabbit:queue name=\"Queue\" />\n\n\n\n\n\n\nAttached is a zip file with a sample demo project that reproduces the issue.\nPS: The test was done with both 1.1.1.RELEASE and 1.1.2.BUID-SNAPSHOT.\nThis may be related to AMQP-190.", "project": "Spring", "sub_project": "AMQP", "version": "1.1.1.RELEASE", "fixed_version": "v1.1.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java", "org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils.java", "org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.java"], "label": 1, "es_results": []}, {"bug_id": 282, "bug_title": "AmqpAppender always logs message body with default encoding", "bug_description": "When logging the String message &apos;Tourne&apos; with UTF-8 encoding using AmqpAppender and running the java application with -Dfile.encoding=windows-1252, the message retrieved from rabbit queue holds an incorrect character for character &apos;&apos; in &apos;Tourne&apos;.\nThe following code in AmqpAppender causes the problem:\nrabbitTemplate.send(exchangeName, routingKey, new Message(msgBody.toString().getBytes(), amqpProps));\nusing msgBody.toString().getBytes(\"UTF-8\") fixes the problem.", "project": "Spring", "sub_project": "AMQP", "version": "1.1.1.RELEASE", "fixed_version": "v1.2.0.M1", "fixed_files": ["org.springframework.amqp.rabbit.log4j.TestListener.java", "org.springframework.amqp.rabbit.log4j.AmqpAppender.java", "org.springframework.amqp.rabbit.log4j.AmqpAppenderIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 272, "bug_title": "Misspelled accessor for messageKeyGenerator in StatefulRetryOperationsInterceptorFactoryBean", "bug_description": "In looking at the StatefulRetryOperationsInterceptorFactoryBean from 1.1.2 I noticed that the setter for the MessageKeyGenerator property is misspelled:\n\n\n\n\n\n\npublic void setMessageKeyGeneretor(MessageKeyGenerator messageKeyGeneretor) { ... }\n\n\n\n\n\n", "project": "Spring", "sub_project": "AMQP", "version": "v1.1.2.RELEASE", "fixed_version": "v1.2.0.M1", "fixed_files": ["org.springframework.amqp.rabbit.config.StatefulRetryOperationsInterceptorFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 274, "bug_title": "Listener Container Local Transactions Not Properly Set Up", "bug_description": "When using local transactions (no external tx manager) and exposeListenerChannel is true (default), the synchronized resource does not have the synchronizedWithTransaction property set to true.\nThis causes a downstream RabbitTemplate to close() the channel.\n", "project": "Spring", "sub_project": "AMQP", "version": "v1.1.2.RELEASE", "fixed_version": "v1.1.3.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.connection.RabbitResourceHolder.java", "org.springframework.amqp.rabbit.listener.LocallyTransactedTests.java", "org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils.java", "org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.ExternalTxManagerTests.java"], "label": 1, "es_results": []}, {"bug_id": 268, "bug_title": "MDC properties are not correctly handled in AmqpAppender", "bug_description": "the code :\n// Copy properties in from MDC\n\t\t\t\t\t@SuppressWarnings(\"rawtypes\")\n\t\t\t\t\tMap props = event.getProperties();\n\t\t\t\t\tfor (Object key : event.getProperties().entrySet()) \n{\n\t\t\t\t\t\tamqpProps.setHeader(key.toString(), props.get(key));\n\t\t\t\t\t}\n\n\nshould be:\n\n// Copy properties in from MDC\n\t\t\t\t\t@SuppressWarnings(\"rawtypes\")\n\t\t\t\t\tMap props = event.getProperties();\n\t\t\t\t\tfor (Object key : event.getProperties().keySet()) {\t\t\t\t\t\tamqpProps.setHeader(key.toString(), props.get(key));\t\t\t\t\t}", "project": "Spring", "sub_project": "AMQP", "version": "v1.1.2.RELEASE", "fixed_version": "v1.2.0.M1", "fixed_files": ["org.springframework.amqp.rabbit.log4j.TestListener.java", "org.springframework.amqp.rabbit.log4j.AmqpAppender.java", "org.springframework.amqp.rabbit.log4j.AmqpAppenderIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 290, "bug_title": "Rabbit namespace little bug, when using reference to queueArgumentsMap", "bug_description": "I am using some queues like that:\n\n\n\n\n\n\n  <rabbit:queue name=\"${queue.name.routage.lot.reception}\" durable=\"true\" queue-arguments=\"defaultQueueArguments\"/>\n\n\n\n\n  <rabbit:queue name=\"${queue.name.routage.lot.mail.notification}\" durable=\"true\" queue-arguments=\"defaultQueueArguments\"/>\n\n\n\n\n  <rabbit:queue name=\"${queue.name.routage.lot.mobile.notification}\" durable=\"true\" queue-arguments=\"defaultQueueArguments\"/>\n\n\n\n\n\n\nI wanted to use:\n\n\n\n\n\n\n  <util:map id=\"defaultQueueArgumentsMap\">\n\n\n\n\n    <entry key=\"x-dead-letter-exchange\" value=\"${exchange.name.dead.letters}\"/>\n\n\n\n\n  </util:map>\n\n\n\n\n  <rabbit:queue-arguments id=\"defaultQueueArguments\" ref=\"defaultQueueArgumentsMap\"/>\n\n\n\n\n\n\nBut this doesn&apos;t work: my queues are not created with the dead letter.\nWith a little google search I found:\nhttp://forum.springsource.org/showthread.php?125014-Dead-Lettering\nAnd when using:\n\n\n\n\n\n\n  <rabbit:queue-arguments id=\"defaultQueueArguments\">\n\n\n\n\n    <entry key=\"x-dead-letter-exchange\" value=\"${exchange.name.dead.letters}\" />\n\n\n\n\n  </rabbit:queue-arguments>\n\n\n\n\n\n\nIt works fine (but my Intellij complains: element entry not allowed here)\nI think both way should work.\nXSD documentation says: ref = The bean name of the Map to pass to the broker when this component is declared.\n", "project": "Spring", "sub_project": "AMQP", "version": "v1.1.2.RELEASE", "fixed_version": "v1.2.1.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.config.AbstractExchangeParser.java", "org.springframework.amqp.rabbit.config.ExchangeParserTests.java", "org.springframework.amqp.rabbit.config.QueueParserTests.java", "org.springframework.amqp.rabbit.config.QueueParser.java"], "label": 1, "es_results": []}, {"bug_id": 281, "bug_title": "AmqpAppender is causing the application not to shut down", "bug_description": "if the logging is configured to use AmqpAppender the thread remains alive and does not shut down if the application is started from a main method and if it is a web app then the classloader will not unload on hot redeploy and tomcat shows the following message :\nThe web application [/xxx] appears to have started a thread named [log-event-retry-delay] but has failed to stop it. This is very likely to create a memory leak.", "project": "Spring", "sub_project": "AMQP", "version": "v1.1.3.RELEASE", "fixed_version": "v1.2.0.M1", "fixed_files": ["org.springframework.amqp.rabbit.log4j.AmqpAppender.java", "org.springframework.amqp.rabbit.log4j.AmqpAppenderIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 286, "bug_title": "Specify which connection factory to use when creating exchanges and queues", "bug_description": "Add attributes to <rabbit:queue> and <rabbit:exchange> to specify which connection factory to use or admin to use.\nThis is required systems which &apos;straddles&apos; two different application environments each of which have their own RabbitMQ brokers. You can&apos;t run both environments on the same broker as there will be name clashing and messages going to the wrong system.\nOne work around for this is to use the code configuration instead of XML application context configuration.", "project": "Spring", "sub_project": "AMQP", "version": "v1.1.3.RELEASE", "fixed_version": "v1.2.0.RC1", "fixed_files": ["org.springframework.amqp.rabbit.config.BindingFactoryBean.java", "org.springframework.amqp.core.Binding.java", "org.springframework.amqp.rabbit.core.RabbitAdmin.java", "org.springframework.amqp.core.Exchange.java", "org.springframework.amqp.rabbit.config.HeadersExchangeParser.java", "org.springframework.amqp.rabbit.config.AbstractExchangeParser.java", "org.springframework.amqp.rabbit.config.FederatedExchangeParser.java", "org.springframework.amqp.core.Queue.java", "org.springframework.amqp.rabbit.config.FanoutExchangeParser.java", "org.springframework.amqp.rabbit.config.ExchangeParserTests.java", "org.springframework.amqp.rabbit.config.DirectExchangeParser.java", "org.springframework.amqp.rabbit.config.QueueParser.java", "org.springframework.amqp.rabbit.config.TopicExchangeParser.java", "org.springframework.amqp.core.AbstractExchange.java", "org.springframework.amqp.rabbit.config.QueueParserTests.java", "org.springframework.amqp.rabbit.config.NamespaceUtils.java"], "label": 1, "es_results": []}, {"bug_id": 331, "bug_title": "When java.lang.Error(e.g.OutOfMemory) occurs, threads do not abort.", "bug_description": "When an OutOfMemory occurs, threads do not abort.\nBut, it is expected that threads will be abort in this situation.", "project": "Spring", "sub_project": "AMQP", "version": "v1.2.0.RELEASE", "fixed_version": "v1.2.1.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java"], "label": 1, "es_results": []}, {"bug_id": 330, "bug_title": "String array can not be converted back from message ", "bug_description": "the following code used to work in 1.0.0 but as of 1.1.x upwards it throws an exception:\n\n\n\n\n\n\n\n\n\n\n\n// Jackson2JsonMessageConverter converter=new Jackson2JsonMessageConverter();\n\n\n\n\n        JsonMessageConverter converter = new JsonMessageConverter();\n\n\n\n\n        Message message = converter.toMessage(new String[] { \"test\" }, new MessageProperties());\n\n\n\n\n        Object fromMessage = converter.fromMessage(message);\n\n\n\n\n\n ", "project": "Spring", "sub_project": "AMQP", "version": "v1.2.0.RELEASE", "fixed_version": "v1.2.1.RELEASE", "fixed_files": ["org.springframework.amqp.support.converter.JsonMessageConverterTests.java", "org.springframework.amqp.support.converter.Jackson2JsonMessageConverterTests.java", "org.springframework.amqp.support.converter.DefaultJackson2JavaTypeMapper.java", "org.springframework.amqp.support.converter.DefaultJavaTypeMapper.java"], "label": 1, "es_results": []}, {"bug_id": 332, "bug_title": "Adding multiple listeners inside a single listener-container element is not working", "bug_description": "If I am adding multiple listener classes inside a single <listener-container> element, only one of them is working i.e. the listener class which is defined at the last in XML e.g Please refer to the stackoverflow link above", "project": "Spring", "sub_project": "AMQP", "version": "v1.2.0.RELEASE", "fixed_version": "v1.2.1.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.config.ListenerContainerParser.java", "org.springframework.amqp.rabbit.config.ListenerContainerPlaceholderParserTests.java", "org.springframework.amqp.rabbit.config.ListenerContainerParserTests.java"], "label": 1, "es_results": []}, {"bug_id": 338, "bug_title": "Container Does not Stop Consuming", "bug_description": "When the container is stopped, it should stop consuming from the queue, while allowing the prefetched messages to be processed (until the shutdown timeout).", "project": "Spring", "sub_project": "AMQP", "version": "v1.2.0.RELEASE", "fixed_version": "v1.2.1.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.MessageListenerContainerLifecycleIntegrationTests.java", "org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 340, "bug_title": "Wrong RabbitMQ credentials are not considered as fatal error by SimpleMessageListenerContainer", "bug_description": "If I connect an AmqpInboundChannelAdapter to the RabbitMQ (3.1.0) with wrong credentials (wrong password) the SimpleMessageListenerContainer doesn&apos;t detect the real reason and continues to restart the consumer in a seemingly endless loop. As AmqpInboundChannelAdapter user, I don&apos;t notice (except if I turn on debug logging level) any problem, its start() method returns after a pause with no error, but the connection remains not established.\nIt would be nice to have a notification (exception) about wrong credentials right away (there is a comprehensive rabbit client exception that gets swallowed in SimpleMessageListenerContainer.run())", "project": "Spring", "sub_project": "AMQP", "version": "v1.2.0.RELEASE", "fixed_version": "v1.2.1.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.MessageListenerContainerLifecycleIntegrationTests.java", "org.springframework.amqp.rabbit.support.RabbitExceptionTranslator.java", "org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 355, "bug_title": "Fix Race Condition in Test Case", "bug_description": "The ErrorHandler is not called unless the container is active; the container is stopped as soon as the 3rd message is received (on exit from the onMessage method).\nSo there is a (small) possibility that the container is not active when it&apos;s time to invoke the ErrorHandler and the verify fails because it was only called twice.", "project": "Spring", "sub_project": "AMQP", "version": "v1.2.0.RELEASE", "fixed_version": "v1.3.0.M1", "fixed_files": ["org.springframework.amqp.rabbit.listener.MessageListenerContainerErrorHandlerIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 359, "bug_title": "AmqpAppenderIntegrationTests should revert to default log4j.properties after its work", "bug_description": "CI build logs (see Reference URL) show that DEBUG logging level for org.springframework.amqp.rabbit is switched by log4j-amqp.properties and isn&apos;t returned back to INFO.\nIt causes very slow test results parsing process on Bamboo.", "project": "Spring", "sub_project": "AMQP", "version": "v1.2.1.RELEASE", "fixed_version": "v1.3.0.M2", "fixed_files": ["org.springframework.amqp.rabbit.log4j.AmqpAppenderIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 363, "bug_title": "AmqpAppender appends throwable information even when layout handles it", "bug_description": "I use AmqpAppender to log messages to RabbitMQ. I have a custom layout which converts data to JSON. Even though my layout handles ThrowableInformation, AmqpAppender re-appends ThrowableInformation and is messing up the message body. I use org.springframework.amqp spring-rabbit 1.2.0.RELEASE.\nPlease find log4j.xml and my custom Layout class SimpleJsonLayout attached.\nFix is to add just the first line below in AmqpAppender.EventSender.run()\n\n\n\n\n\n\nif (layout.ignoresThrowable())\n\n\n\n\n{\n\n\n\n\n\n\n\n\n\nif (null != logEvent.getThrowableInformation()) {\n\n\n\n\n\t\t\t\t\t\tThrowableInformation tinfo = logEvent.getThrowableInformation();\n\n\n\n\n\t\t\t\t\t\tfor (String line : tinfo.getThrowableStrRep()) {\n\n\n\n\n\t\t\t\t\t\t\tmsgBody.append(String.format(\"%s%n\", line));\n\n\n\n\n\t\t\t\t\t\t}\n\n\n\n\n\t\t\t\t\t}\n\n\n\n\n}\n\n\n\n\n\n\nPFA java project with log4j.xml. Please set values for AMQPAppender in Log4j.xml and run Test.class to reproduce error.", "project": "Spring", "sub_project": "AMQP", "version": "v1.2.1.RELEASE", "fixed_version": "v1.3.0.M2", "fixed_files": ["org.springframework.amqp.rabbit.log4j.AmqpAppender.java", "org.springframework.amqp.rabbit.log4j.AmqpAppenderIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 365, "bug_title": "Fix Race Condition in Consumer Recovery Test Cases", "bug_description": "Analysis\nThe test cases send 10 messages. The consumer uses manual ack and does a double ack on the first message causing the channel to be closed. It is a race between the broker closing the channel and the second message being received by the listener.\nThe listener counts down a latch that the test is waiting on.\nIf the broker loses the race, the channel is closed AFTER the second message is received by the listener and the message is redelivered after the consumer is recovered.\nThis results in 11 messages being received and the final test for a null message fails.\nSolution\nIn the listener, only count down the latch if the message is NOT a redelivery (because the latch would have already been counted down for this message). However, we can&apos;t just test the redelivered flag - the message might have been sent by the broker but not actually received by the listener.\nAdd the message to a Set and only countdown the latch if it&apos;s not already received.", "project": "Spring", "sub_project": "AMQP", "version": "v1.2.1.RELEASE", "fixed_version": "v1.3.0.M2", "fixed_files": ["org.springframework.amqp.rabbit.connection.RabbitUtils.java", "org.springframework.amqp.rabbit.listener.MessageListenerContainerLifecycleIntegrationTests.java", "org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java", "org.springframework.amqp.rabbit.listener.MessageListenerRecoveryCachingConnectionIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 385, "bug_title": "AmqpRejectAndDontRequeueException kills pub-sub-channel consumer", "bug_description": "One documented way in spring-amqp to avoid message redelivery on listener failure is to inject an error-handler that throws AmqpRejectAndDontRequeueException. This works fine for SI amqp-backed point-to-point channels, but causes the consumer to attempt to restart for SI amqp-backed pub-sub channels. This attempt to restart fails because \n\"there is a problem with the pub-sub channel in that if the consumer fails, it does not redeclare its queue - this has been partially addressed in 4.0.0.M3 (https://jira.spring.io/browse/INT-3278) in that the queue and binding will be redeclared in the event of a connection failure, but it still won&apos;t handle a fatal (but recoverable) consumer exception because it only redeclares when it detects a new connection is created.\"\nSee reference URL forum posting for more details from Gary Russell, who understands this stuff way better than I do.\nFrom the forum posting: \n\"In order for the AmqpRejectAndDontRequeueException to be non-fatal to the consumer, when thrown from an ErrorHandler, it needs to be wrapped in a ListenerExecutionFailedException (we should probably document that, or even treat ARADREs in the same way as LEFEs; please open up a JIRA issue: https://jira.spring.io/browse/AMQP).\"", "project": "Spring", "sub_project": "AMQP", "version": "v1.2.1.RELEASE", "fixed_version": "v1.2.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.MessageListenerContainerErrorHandlerIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 369, "bug_title": "AmqpAdmin <-> Declarable classes tangle", "bug_description": "", "project": "Spring", "sub_project": "AMQP", "version": "v1.3.0.M1", "fixed_version": "v1.3.0.M2", "fixed_files": ["org.springframework.amqp.rabbit.config.BindingFactoryBean.java", "org.springframework.amqp.rabbit.core.RabbitAdminDeclarationTests.java", "org.springframework.amqp.core.Declarable.java", "org.springframework.amqp.core.AbstractDeclarable.java"], "label": 1, "es_results": []}, {"bug_id": 396, "bug_title": "Prefetch count = 0 after passive declare failure in BlockingQueueConsumer", "bug_description": "When using the SimpleMessageListenerContainer and consuming from multiple passively declared queues and one of the queues does not exist.\nPrefetch count for the channel remains at 0, which is not correct.", "project": "Spring", "sub_project": "AMQP", "version": "v1.3.3.RELEASE", "fixed_version": "v1.3.4.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.BlockingQueueConsumerTests.java", "org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 398, "bug_title": "Direct exchange binding default routing key fails on queue name with dot char", "bug_description": "If the queue name for rabbit direct-exchange binding contains a dot character, the default routing key evaluation will fail on non-existing bean <string_before_first_dot>, which unfortunately breaks all our direct-exchange bindings with such queue names.\nFollowing configuration fails on unresolvable bean &apos;communication&apos;.\n\n\n\n\n\n\n<rabbit:direct-exchange name=\"communication.mail\">\n\n\n\n\n  <rabbit:bindings>\n\n\n\n\n    <rabbit:binding queue=\"communication.mail\" />\n\n\n\n\n  </rabbit:bindings>\n\n\n\n\n</rabbit:direct-exchange>\n\n\n\n\n\n\nWhat was the reason behind using SPEL expression as default routing key in DirectExchangeParser?\nIt would be ok if it&apos;s used with explicitly set key to take advantage of spel evaluation to provide flexible routing keys, but as a default, it&apos;s somehow undocumented hidden breaking magic.", "project": "Spring", "sub_project": "AMQP", "version": "v1.3.4.RELEASE", "fixed_version": "v1.3.5.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.config.ExchangeParserIntegrationTests.java", "org.springframework.amqp.rabbit.config.DirectExchangeParser.java"], "label": 1, "es_results": []}, {"bug_id": 421, "bug_title": "RabbitTemplate Memory Leak with Publisher Confirms/Returns", "bug_description": "The wrong key is being used for `RabbitTemplate.pendingConfirms`. The rabbit template\nstores the pending confirms map under the `ChannelProxy` whereas the\nremoval uses the actual `PublisherCallbackChannel` so the map\nentry is not removed.", "project": "Spring", "sub_project": "AMQP", "version": "v1.3.5.RELEASE", "fixed_version": "v1.4.0.M1", "fixed_files": ["org.springframework.amqp.rabbit.core.RabbitTemplate.java", "org.springframework.amqp.rabbit.core.RabbitTemplatePublisherCallbacksIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 432, "bug_title": "Default MessagePropertiesConverter should convert LongString values nested in lists", "bug_description": "Multi-valued headers are contained as List<Object> in BasicProperties.headers. LongString values contained in these Lists will not be converted.\nAMQP header values are converted from LongString to either a String or a DataInputStream depending on their length:\n\n\n\n\n\n\nfor (Map.Entry<String, Object> entry : headers.entrySet()) {\n\n\n\n\n\tObject value = entry.getValue();\n\n\n\n\n\tif (value instanceof LongString) {\n\n\n\n\n\t\tvalue = this.convertLongString((LongString) value, charset);\n\n\n\n\n\t}\n\n\n\n\n\ttarget.setHeader(entry.getKey(), value);\n\n\n\n\n}\n\n\n\n\n\n\nA possible solution would be:\n\n\n\n\n\n\nfor (Map.Entry<String, Object> entry : headers.entrySet()) {\n\n\n\n\n\tObject value = entry.getValue();\n\n\n\n\n\tif (value instanceof LongString) {\n\n\n\n\n\t\tvalue = this.convertLongString((LongString) value, charset);\n\n\n\n\n\t} else if (value instanceof List) {\n\n\n\n\n\t\tList listValue = new ArrayList<Object>();\n\n\n\n\n\t\tfor (Object v : (List)value) {\n\n\n\n\n\t\t\tif (v instanceof LongString) {\n\n\n\n\n\t\t\t\tv = this.convertLongString((LongString) v, charset);\n\n\n\n\n\t\t\t}\n\n\n\n\n\t\t\tlistValue.add(v);\n\n\n\n\n\t\t}\n\n\n\n\n\t\tvalue = listValue;\n\n\n\n\n\t}\n\n\n\n\n\ttarget.setHeader(entry.getKey(), value);\n\n\n\n\n}\n\n\n\n\n\n", "project": "Spring", "sub_project": "AMQP", "version": "v1.3.6.RELEASE", "fixed_version": "v1.3.7.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.support.DefaultMessagePropertiesConverter.java"], "label": 1, "es_results": []}, {"bug_id": 463, "bug_title": "PublisherCallbackChannelImpl Incompatible with JRockit", "bug_description": "JRockit&apos;s TreeMap doesn&apos;t allow getValue() after remove on an Iterator.\n\n\n\n\n\n\n\t\t\t\t\t\twhile (iterator.hasNext()) {\n\n\n\n\n\t\t\t\t\t\t\tEntry<Long, PendingConfirm> entry = iterator.next();\n\n\n\n\n\t\t\t\t\t\t\titerator.remove();\n\n\n\n\n\t\t\t\t\t\t\tdoHandleConfirm(ack, involvedListener, entry.getValue());\n\n\n\n\n\t\t\t\t\t\t}\n\n\n\n\n\n", "project": "Spring", "sub_project": "AMQP", "version": "v1.3.7.RELEASE", "fixed_version": "v1.3.8.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.support.PublisherCallbackChannelImpl.java", "org.springframework.amqp.rabbit.core.RabbitTemplatePublisherCallbacksIntegrationTests.java", "org.springframework.amqp.rabbit.core.RabbitAdminDeclarationTests.java"], "label": 1, "es_results": []}, {"bug_id": 446, "bug_title": "RabbitAdmin warnings for autodeclaring anonymous queue", "bug_description": "As I see it - these warnings just waste some disk space.\n\nAuto-declaring an exclusive Queue ...\nAuto-declaring an auto-delete Queue ...\nAuto-declaring a non-durable Queue ...\n\n1. if queue is anonymous then always 3 warnings are logged, makes no sense.\n2. it makes no sense logging this in production, only as a hint during development\nIf these warnings are considered useful, then I would suggest adding a property \"boolean enableHints\", so I can disable them on production.", "project": "Spring", "sub_project": "AMQP", "version": "v1.4.0.RELEASE", "fixed_version": "v1.4.1.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.core.RabbitAdminTests.java", "org.springframework.amqp.rabbit.core.RabbitAdmin.java"], "label": 1, "es_results": []}, {"bug_id": 447, "bug_title": "AbstractConnectionFactory.setAddresses(String) should ignore empty string", "bug_description": "At the moment if an empty string is passed it is parsed to an array with size 1 and empty host in Address (due to use of the String.split() method), which makes java connecting to localhost.\nThis is completely unexpected.\nI&apos;ve raised this question in rabbitmq-java-client https://groups.google.com/forum/#!topic/rabbitmq-users/yIMnrcR9_o4 and throwing an exception is a possible option there.\nI would suggest a hasText() check before in spring-amqp code.", "project": "Spring", "sub_project": "AMQP", "version": "v1.4.0.RELEASE", "fixed_version": "v1.5.0.M1", "fixed_files": ["org.springframework.amqp.rabbit.connection.AbstractConnectionFactory.java", "org.springframework.amqp.rabbit.connection.CachingConnectionFactoryTests.java"], "label": 1, "es_results": []}, {"bug_id": 481, "bug_title": "@RabbitListener cannot be a scoped proxy", "bug_description": "Because of the way the endpoints are detected in a BeanPostProcessor and the message listener container is started in a SmartInitializingBean, the listener is never registered if it is a scoped proxy (e.g. @Scope(proxyMode=TARGET_CLASS) @Lazy). The BeanPostProcessor is not presented with the actual listener until it is instantiated, and the instantiation happens lazily, the container is never created or started (I believe it&apos;s because the call to RabbitListenerEndpointRegistrar.afterPropertiesSet() in RabbitListenerAnnotationBeanPostProcessor.afterSingletonsInstantiated() is never made).\nSee https://github.com/spring-cloud/spring-cloud-config/issues/96 for original user&apos;s issue.", "project": "Spring", "sub_project": "AMQP", "version": "v1.4.3.RELEASE", "fixed_version": "v1.5.0.M1", "fixed_files": ["org.springframework.amqp.rabbit.listener.RabbitListenerEndpointRegistry.java", "org.springframework.amqp.rabbit.listener.RabbitListenerEndpointRegistrar.java", "org.springframework.amqp.rabbit.annotation.EnableRabbitTests.java"], "label": 1, "es_results": []}, {"bug_id": 498, "bug_title": "Dynamic Consumer Adjustment Does Not Work when Channel is Transacted", "bug_description": "", "project": "Spring", "sub_project": "AMQP", "version": "v1.4.5.RELEASE", "fixed_version": "v1.5.0.RC1", "fixed_files": ["org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java", "org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainerLongTests.java"], "label": 1, "es_results": []}, {"bug_id": 496, "bug_title": "Some threads are not interrupted when Workers did not finish before shutdownTimeout", "bug_description": "Please see http://stackoverflow.com/questions/30124537/spring-amqp-with-rabbitmq-does-not-shutdown-properly for details", "project": "Spring", "sub_project": "AMQP", "version": "v1.4.5.RELEASE", "fixed_version": "v1.5.0.RC1", "fixed_files": ["org.springframework.amqp.rabbit.listener.MessageListenerContainerLifecycleIntegrationTests.java", "org.springframework.amqp.rabbit.connection.CachingConnectionFactory.java"], "label": 1, "es_results": []}, {"bug_id": 506, "bug_title": "ConcurrentModificationException on RabbitTemplate.getUnconfirmed()", "bug_description": "Although we&apos;re synchronized on pendingConfirms, the values are maps and we fail to synchronize on those while iterating/removing.", "project": "Spring", "sub_project": "AMQP", "version": "v1.4.5.RELEASE", "fixed_version": "v1.5.0.RC1", "fixed_files": ["org.springframework.amqp.rabbit.support.PublisherCallbackChannelImpl.java", "org.springframework.amqp.rabbit.core.RabbitTemplate.java", "org.springframework.amqp.rabbit.core.RabbitTemplatePublisherCallbacksIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 508, "bug_title": "Admin Hang After IllegalArgumentException on declareQueue()", "bug_description": "", "project": "Spring", "sub_project": "AMQP", "version": "v1.4.5.RELEASE", "fixed_version": "v1.5.0.RC1", "fixed_files": ["org.springframework.amqp.rabbit.listener.BlockingQueueConsumerIntegrationTests.java", "org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java", "org.springframework.amqp.rabbit.core.RabbitAdmin.java", "org.springframework.amqp.rabbit.core.RabbitAdminTests.java"], "label": 1, "es_results": []}, {"bug_id": 516, "bug_title": "Setting autoDelete or exclusive to anything, including \"true\" in @Queue without a queue name results in them being disabled", "bug_description": "The following queue declaration will result in a queue being declared with auto delete and exclusive set to false:\n\n\n\n\n\n\n@RabbitListener(bindings = @QueueBinding(\n\n\n\n\n    value = @Queue(autoDelete = \"true\", exclusive = \"true\"),\n\n\n\n\n    exchange = @Exchange(value = \"myFanout\", type = ExchangeTypes.FANOUT, durable = \"true\")\n\n\n\n\n))\n\n\n\n\n\n\ndue to the following code in RabbitListenerAnnotationBeanProcessor:\n\n\n\n\n\n\nif (!StringUtils.hasText(queueName)) {\n\n\n\n\n    queueName = UUID.randomUUID().toString();\n\n\n\n\n    if (!StringUtils.hasText(bindingQueue.exclusive())) {\n\n\n\n\n        exclusive = true;\n\n\n\n\n    }\n\n\n\n\n    if (!StringUtils.hasText(bindingQueue.autoDelete())) {\n\n\n\n\n        autoDelete = true;\n\n\n\n\n    }\n\n\n\n\n}\n\n\n\n\nelse {\n\n\n\n\n    exclusive = resolveExpressionAsBoolean(bindingQueue.exclusive());\n\n\n\n\n    autoDelete = resolveExpressionAsBoolean(bindingQueue.autoDelete());\n\n\n\n\n}\n\n\n\n\n\n\nMaking them exclusive and auto delete by default when using a random name seems like a good idea, but it should probably be changed to something like:\n\n\n\n\n\n\nString e = bindingQueue.exclusive();\n\n\n\n\nif (!StringUtils.hasText(e) || resolveExpressionAsBoolean(e)) {\n\n\n\n\n    exclusive = true\n\n\n\n\n}\n\n\n\n\n\n", "project": "Spring", "sub_project": "AMQP", "version": "v1.5.0.M1", "fixed_version": "v1.5.0.RC1", "fixed_files": ["org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor.java", "org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 546, "bug_title": "Closing the CachingConnectionFactory at application context close is too abrupt", "bug_description": "From version 1.5 and onwards the CachingConnectionFactory will not allow any connections to be created once the application context has been closed. This is implemented by closing the connection factory as soon as closing of the application context is initiated (when the ContextClosedEvent is fired).\nThis is quite abrupt and causes problems for code that may want to process requests and publish messages while the application context is still in it&apos;s closing stage.\nAs an example we use spring-integration AMQP components (AmqpInboundChannelAdapter) for consuming inbound messages and these components implement the Spring SmartLifeCycle. This means that message consumers are not stopped until you reach the life-cycle processing stage in the application context closing (which happens after the ContextClosedEvent is fired). This causes problems as you cannot easily prevent incoming messages from arriving but you cannot send any outbound messages (since the connection factory is already closed). \nFor us this is major issue since it now limits our possibility to perform a graceful shutdown of our application, which worked well with prior versions.\nIf closing is really required then I think it could make sense to defer it to a later stage by e.g. implementing Spring SmartLifeCycle and use the stop() method. Combined with a configurable phase that would give us the possibility to co-ordinate the closing with other components that should be stopped before we let the application context start destroying beans.", "project": "Spring", "sub_project": "AMQP", "version": "v1.5.0.RELEASE", "fixed_version": "v1.5.3.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.connection.CachingConnectionFactory.java"], "label": 1, "es_results": []}, {"bug_id": 540, "bug_title": "RabbitListenerAnnotationBeanPostProcessor cannot find Class-Level @RabbitListener annotations on beans with @Transactional annotations", "bug_description": "RabbitListenerAnnotationBeanPostProcessor .postProcessAfterInitialization:\n\n\n\n\n\n\nClass<?> targetClass = AopUtils.getTargetClass(bean);\n\n\n\n\nfinal RabbitListener classLevelListener = AnnotationUtils.findAnnotation(bean.getClass(), RabbitListener.class);\n\n\n\n\n\n\nbean.getClass() in this case refers to proxy. Why not use AnnotationUtils.findAnnotation(targetClass) ? It&apos;s interesting because 2 lines below targetClass is used to find annotations on methods:\nReflectionUtils.doWithMethods(targetClass, ...)", "project": "Spring", "sub_project": "AMQP", "version": "v1.5.1.RELEASE", "fixed_version": "v1.5.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor.java", "org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 542, "bug_title": "Class-level @RabbitListener causes INFO log message about NullPointerException", "bug_description": "If you have a class-level @RabbitListener annotation and some @RabbitHandler methods, when you start your application you&apos;ll get a log message like:\n2015-10-15 18:50:15.430  INFO o.s.core.annotation.AnnotationUtils: Failed to introspect annotations on [null]: java.lang.NullPointerException\nThis appears to be completely harmless and the handler methods are all correctly called for incoming messages, but it makes my team nervous to have NullPointerException messages in our logs that we&apos;re supposed to just ignore.\nThe current version of Spring Boot&apos;s AMQP sample app (spring-boot-sample-amqp) demonstrates the problem with no changes needed; fire up the example and look for the error just after the \"Registering beans for JMX exposure on startup\" message.", "project": "Spring", "sub_project": "AMQP", "version": "v1.5.1.RELEASE", "fixed_version": "v1.5.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.MultiMethodRabbitListenerEndpoint.java", "org.springframework.amqp.rabbit.annotation.RabbitListener.java", "org.springframework.amqp.rabbit.listener.MethodRabbitListenerEndpoint.java", "org.springframework.amqp.rabbit.listener.adapter.DelegatingInvocableHandler.java", "org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor.java", "org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 541, "bug_title": "@Header and @Payload Annotations Not Recognized on Proxy", "bug_description": "See AMQP-540.\nThe PR fixes the reported problem but it is incomplete because annotated handler methods are not matched.", "project": "Spring", "sub_project": "AMQP", "version": "v1.5.1.RELEASE", "fixed_version": "v1.5.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor.java", "org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 556, "bug_title": "NPE in SMLC (Recoverable)", "bug_description": "consumerActive = this.consumers != null && this.consumers.get(consumer);\nIf the container is stopped and times out waiting for the consumers, the consumer is no longer in the map when he next calls isActive().", "project": "Spring", "sub_project": "AMQP", "version": "v1.5.2.RELEASE", "fixed_version": "v1.5.3.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainerIntegration2Tests.java"], "label": 1, "es_results": []}, {"bug_id": 561, "bug_title": "AmqpAppender causes application error when declareExchange=true and no RabbitMQ connection available", "bug_description": "When the declareExchange property is set for the AmqpAppender in log4j.properties, a call to org.apache.log4j.Logger.getLogger(..) will cause an application exception if the connection to RabbitMQ is not available. Stack trace below:\n\n\n\n\n\n\n\torg.springframework.amqp.AmqpConnectException: java.net.ConnectException: Connection timed out: connect\n\n\n\n\n\torg.springframework.amqp.rabbit.support.RabbitExceptionTranslator.convertRabbitAccessException(RabbitExceptionTranslator.java:58)\n\n\n\n\n\torg.springframework.amqp.rabbit.connection.AbstractConnectionFactory.createBareConnection(AbstractConnectionFactory.java:273)\n\n\n\n\n\torg.springframework.amqp.rabbit.connection.CachingConnectionFactory.createConnection(CachingConnectionFactory.java:500)\n\n\n\n\n\torg.springframework.amqp.rabbit.connection.ConnectionFactoryUtils$1.createConnection(ConnectionFactoryUtils.java:85)\n\n\n\n\n\torg.springframework.amqp.rabbit.connection.ConnectionFactoryUtils.doGetTransactionalResourceHolder(ConnectionFactoryUtils.java:135)\n\n\n\n\n\torg.springframework.amqp.rabbit.connection.ConnectionFactoryUtils.getTransactionalResourceHolder(ConnectionFactoryUtils.java:71)\n\n\n\n\n\torg.springframework.amqp.rabbit.core.RabbitTemplate.doExecute(RabbitTemplate.java:1278)\n\n\n\n\n\torg.springframework.amqp.rabbit.core.RabbitTemplate.execute(RabbitTemplate.java:1271)\n\n\n\n\n\torg.springframework.amqp.rabbit.core.RabbitTemplate.execute(RabbitTemplate.java:1247)\n\n\n\n\n\torg.springframework.amqp.rabbit.core.RabbitAdmin.declareExchange(RabbitAdmin.java:126)\n\n\n\n\n\torg.springframework.amqp.rabbit.log4j.AmqpAppender.maybeDeclareExchange(AmqpAppender.java:434)\n\n\n\n\n\torg.springframework.amqp.rabbit.log4j.AmqpAppender.activateOptions(AmqpAppender.java:407)\n\n\n\n\n\torg.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\n\n\n\n\torg.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n\n\n\n\n\torg.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n\n\n\n\n\torg.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n\n\n\n\n\torg.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n\n\n\n\n\torg.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)\n\n\n\n\n\torg.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)\n\n\n\n\n\torg.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n\n\n\n\n\torg.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\n\n\n\n\torg.apache.log4j.LogManager.&lt;clinit&gt;(LogManager.java:127)\n\n\n\n\n\torg.apache.log4j.Logger.getLogger(Logger.java:117)\n\n\n\n\n\n\nA workaround is to set the declareExchange property to false, however a better solution would be to wrap the declareExchange(..) method in a try/catch and follow the log4j error handler pattern when the connection is not available.", "project": "Spring", "sub_project": "AMQP", "version": "v1.5.2.RELEASE", "fixed_version": "v1.5.4.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.log4j.AmqpAppender.java", "org.springframework.amqp.rabbit.log4j.AmqpAppenderIntegrationTests.java", "org.springframework.amqp.rabbit.logback.AmqpAppender.java"], "label": 1, "es_results": []}, {"bug_id": 570, "bug_title": "SimpleAmqpHeaderMapper does not handle org.springframework.util.MimeType", "bug_description": "In my case I&apos;m sending messages using Spring messaging, with a custom converter based on org.springframework.messaging.converter.AbstractMessageConverter. This class (and the other converters I&apos;ve looked at in Spring messaging) seem to all use org.springframework.util.MimeType for encoding the content type, so it seems spring-amqp should handle that case in SimpleAmqpHeaderMapper.extractContentTypeAsString.\nCurrently it logs a warning and ignores the content type header. However due to a related bug it does pick up the contentType header further down in fromHeaders and treats it as a custom header (as for some reason org.springframework.amqp.support.AmqpHeaders.CONTENT_TYPE doesn&apos;t use the \"amqp_\" prefix).\nIt seems to me the correct behaviour would be to\n\nhandle in org.springframework.util.MimeType extractContentTypeAsString\nexclude contentType from custom header processing\n\nLinks: https://jira.spring.io/browse/INT-2713", "project": "Spring", "sub_project": "AMQP", "version": "v1.5.3.RELEASE", "fixed_version": "v1.5.4.RELEASE", "fixed_files": ["org.springframework.amqp.support.SimpleAmqpHeaderMapper.java", "org.springframework.amqp.support.SimpleAmqpHeaderMapperTests.java"], "label": 1, "es_results": []}, {"bug_id": 572, "bug_title": "Send of message via tx-channel is commited early with (JTA) transaction and TransactionTemplate", "bug_description": "When an external JTA/EJB transaction calls a spring bean using TransactionTemplate with JtaTransactionManager, which in turn sends a RabbitMQ-message (using a transacted RabbitTemplate), then the message is committed on the channel when the spring transaction bracket ends (not as expected when the outer JTA transaction is committed).\nThis only occurs when TransactionTemplate  is used, but not with AOP transaction proxies (@Transactional). Everything works as expected with AOP proxies.\nThe reason seems to be, that the ConnectionFactoryUtils use TransactionAspectSupport.currentTransactionStatus().isNewTransaction() (line 177) to check for the presence of a new/nested transaction - unfortunately it seems TransactionAspectSupport can only be used with AOP based transaction handling. When used with TransactionTemplate, TransactionAspectSupport.currentTransactionStatus() throws a NoTransactionException causing the Utils to assume a local transaction.\nWorkaround for users: Use AOP based transaction handling.\nSee AMQP-479 for the history (solved the main issue, but left the problem in the special case of TransactionTemplate s being used)", "project": "Spring", "sub_project": "AMQP", "version": "v1.5.3.RELEASE", "fixed_version": "v1.6.0.RC1", "fixed_files": ["org.springframework.amqp.rabbit.core.RabbitTemplateIntegrationTests.java", "org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils.java"], "label": 1, "es_results": []}, {"bug_id": 621, "bug_title": "Spring AMQP Consumer pauses after running for sometime", "bug_description": "We have a 2 node RabbitMQ cluster with Ha-all policy. We use Spring AMQP in our application to talk to RabbitMQ. Producer part is working fine, but consumer works for some time and pauses. Producer and consumer are running as different applications. More information on Consumer part.\n\nwe use SimpleMessageListenerContainer with ChannelAwareMessageListener, use Manual ack mode and default prefetch(1)\nIn our application we create queue (on-demand) and add it to the listener\nWhen we started with 10 ConcurrentConsumers and 20 MaxConcurrentConsumers, consumption happens for around 15 hours and pauses. This situation happens within 1 hour when we increase the MaxConcurrentConsumers to 75.\n\nOn RabbitMQ UI, we see channels with 3/4 unacked messages on the channel tab when this situation occurs, until then it just have 1 unacked message.\nHaving heartbeat set to 60 did not help improve this situation.\nWe dynamically add and remove queues to SimpleMessageListenerContainer and we suspect this is causing some problem, because every time we add or remove a queue from the listener, all the BlockingQueueConsumer are removed and created again. \n@GaryRussell, suggested to create Jira ticket. Please let me know if you need more information.\nSpring AMQP version: 1.6.0.RELEASE\nThread Dump: pastebin.com/UrBLfn2C \nStackoverflow  Link: http://stackoverflow.com/questions/38134687/spring-amqp-consumer-pauses-after-running-for-sometime?noredirect=1#comment63848781_38134687", "project": "Spring", "sub_project": "AMQP", "version": "v1.6.0.RELEASE", "fixed_version": "v1.6.1.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java", "org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.java", "org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainerIntegration2Tests.java", "org.springframework.amqp.rabbit.listener.ContainerInitializationTests.java"], "label": 1, "es_results": []}, {"bug_id": 625, "bug_title": "CachingConnectionFactory onClose notification", "bug_description": "The problem is happening when using CachingConnectionFactory with cache mode CacheMode.CHANNEL. When connection is closed for the first time the connection listener is notified normally, but if the connection is recreated and closed again the listener is not notified.\nI created a sample project on Github to show this problem: https://github.com/marcospy/spring-amqp-listener-issue.", "project": "Spring", "sub_project": "AMQP", "version": "1.5.6.RELEASE", "fixed_version": "v1.6.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.connection.CachingConnectionFactoryTests.java", "org.springframework.amqp.rabbit.connection.CachingConnectionFactory.java"], "label": 1, "es_results": []}, {"bug_id": 633, "bug_title": "Non Transactional RabbitTemplate Uses Container Transactional Channel", "bug_description": "When running a RabbitTemplate on a transactional container thread, the container channel is used, even if the RabbitTemplate is not marked transactional.\nConsider the case where you wish to publish a message when there&apos;s an error, while rejecting the inbound message. If the container is transactional, the published message is rolled back.\nIf you publish with a template that is not transactional, the publish should occur on a new channel.\n\n\n\n\n\n\nRabbitResourceHolder resourceHolder = (RabbitResourceHolder) TransactionSynchronizationManager\n\n\n\n\n\t\t.getResource(connectionFactory);\n\n\n\n\nif (resourceHolder != null) {\n\n\n\n\n\tChannel channel = resourceFactory.getChannel(resourceHolder);\n\n\n\n\n\tif (channel != null) {\n\n\n\n\n\t\treturn resourceHolder;\n\n\n\n\n\t}\n\n\n\n\n}\n\n\n\n\n\n\nWe should never return the resourceHolder if the resourceFactory.isSynchedLocalTransactionAllowed() is false.", "project": "Spring", "sub_project": "AMQP", "version": "v1.6.1.RELEASE", "fixed_version": "v1.6.2.RELEASE", "fixed_files": ["org.springframework.amqp.rabbit.core.RabbitTemplate.java", "org.springframework.amqp.rabbit.listener.LocallyTransactedTests.java"], "label": 1, "es_results": []}, {"bug_id": 648, "bug_title": "Container Fails to Recover", "bug_description": "Container configured to listen on multiple queues.\nDelete a queue, suspend the consumer thread in handleCancel.\nStop/start the broker.\nThe issue is that we detect there are other queues so we attempt to cancel those consumers (basicCacncel) but we&apos;ll never get a handleCancelOk call because the broker had been restarted.\nWhen we cancel the other consumers, we need to properly deal with failure to cancel.\nVESC-715", "project": "Spring", "sub_project": "AMQP", "version": "v1.6.2.RELEASE", "fixed_version": "v1.2.0.M1", "fixed_files": ["org.springframework.amqp.rabbit.listener.BlockingQueueConsumerTests.java", "org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.java"], "label": 1, "es_results": []}, {"bug_id": 661, "bug_title": "NPE with null QueueName in SMLC", "bug_description": "", "project": "Spring", "sub_project": "AMQP", "version": "v1.6.3.RELEASE", "fixed_version": "v1.2.0.M1", "fixed_files": ["org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainerIntegrationTests.java", "org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.java"], "label": 1, "es_results": []}, {"bug_id": 55, "bug_title": "REST Crash on HTC Mytouch 4g", "bug_description": "See issue: http://code.google.com/p/google-gson/issues/detail?id=255\nEssentially, HTC has bundled an (old, broken) copy of gson with some builds of HTC Sense and exposed it on the classpath.  As a result, the REST client sees it on the classpath and attempts to load it.  Unfortunately, this crashes.\n", "project": "Spring", "sub_project": "ANDROID", "version": "v1.0.0.M4", "fixed_version": "v1.0.0.RC1", "fixed_files": ["org.springframework.web.client.RestTemplate.java"], "label": 1, "es_results": []}, {"bug_id": 59, "bug_title": "Issues sending gzip'd requests", "bug_description": "The outputMessage body is not properly being closed. See the forum reference for more information.", "project": "Spring", "sub_project": "ANDROID", "version": "v1.0.0.M4", "fixed_version": "v1.0.0.RC1", "fixed_files": ["org.springframework.http.client.HttpComponentsClientHttpResponse.java", "org.springframework.http.client.AbstractClientHttpRequest.java", "org.springframework.http.client.SimpleClientHttpResponse.java", "org.springframework.http.client.AbstractHttpRequestFactoryTestCase.java", "org.springframework.http.client.AbstractClientHttpResponse.java", "org.springframework.http.client.CommonsClientHttpResponse.java"], "label": 1, "es_results": []}, {"bug_id": 85, "bug_title": "HttpBasicAuthentication utilizes an unsupported Base64 library", "bug_description": "HttpBasicAuthentication is using the org.apache.commons.codec.binary.Base64 library for Base64 encoding. This library is included in Android, but is not supported, and is not listed in the Android API reference. It will most likely be available to an application, however it is not guaranteed to be present on a given Android device. Android Froyo (2.2) and newer include a supported Base64 library, android.util.Base64. However, earlier versions do not. To support Eclair (2.1) devices we&apos;ll need to include a third party library.", "project": "Spring", "sub_project": "ANDROID", "version": "v1.0.0.RC1", "fixed_version": "v1.0.0.RELEASE", "fixed_files": ["org.springframework.http.HttpHeadersTests.java", "org.springframework.http.HttpBasicAuthentication.java", "org.springframework.http.HttpAuthentication.java"], "label": 1, "es_results": []}, {"bug_id": 109, "bug_title": "Failing Rest Template tests on Jelly Bean related to date formatting", "bug_description": "Date formatting appears to have changed (broken?) on Jelly Bean. Consider the following format:\n\"EEE, dd MMM yyyy HH:mm:ss zzz\"\nOn Android versions prior to Jelly Bean it would format like this:\n\"Thu, 18 Dec 2008 10:20:00 GMT+00:00\"\nOn Jelly Bean it now looks like this:\n\"Thu, 18 Dec 2008 10:20:00 GMT\"\nThe tests are currently verifying a date format, but that isn&apos;t what we are actually testing. The tests should be modified to add more date equality comparisons, and work around this change.", "project": "Spring", "sub_project": "ANDROID", "version": "v1.0.0.RELEASE", "fixed_version": "v1.0.1.RELEASE", "fixed_files": ["org.springframework.http.HttpHeadersTests.java"], "label": 1, "es_results": []}, {"bug_id": 101, "bug_title": "Backport SPR-9238 to Spring Android", "bug_description": "Please backport issue\nhttps://jira.springsource.org/browse/SPR-9238\nresolved in Spring Rest Template version 3.1.2 to Spring Android version 1.0.1.", "project": "Spring", "sub_project": "ANDROID", "version": "v1.0.0.RELEASE", "fixed_version": "v1.0.1.RELEASE", "fixed_files": ["org.springframework.http.converter.json.MappingJacksonHttpMessageConverter.java"], "label": 1, "es_results": []}, {"bug_id": 129, "bug_title": "Merge in changes from SPR-9273 - HttpStatusCodeException cannot be serialized", "bug_description": "", "project": "Spring", "sub_project": "ANDROID", "version": "v1.0.1.RELEASE", "fixed_version": "v2.0.0.M1", "fixed_files": ["org.springframework.web.client.HttpStatusCodeException.java", "org.springframework.web.client.HttpServerErrorException.java", "org.springframework.web.client.HttpClientErrorException.java"], "label": 1, "es_results": []}, {"bug_id": 252, "bug_title": "HibernateFailureJob does not fail as expected", "bug_description": "HibernateFailureJob testcase expects the job to end with exception, but does not check it really does. In fact the job doesn&apos;t fail and the assert in catch block is never executed (although it would have passed if it was).", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.M3", "fixed_version": "spring-batch-1.0.0.m4", "fixed_files": ["org.springframework.batch.sample.HibernateFailureJobFunctionalTests.java"], "label": 1, "es_results": []}, {"bug_id": 350, "bug_title": "Remove close() method from non ItemStreams", "bug_description": "The following classes have a default close method implementation which does not belong and should be removed:\norg.springframework.batch.execution.launch.EmptyItemWriter\norg.springframework.batch.item.writer.ItemWriterAdapter\norg.springframework.batch.item.reader.ItemReaderAdapter\nHere&apos;s an interesting question  how do you deal with something that needs to be both an ItemStream and a RepeatInterceptor?\nTake org.springframework.batch.io.support.HibernateAwareItemWriter for instance  suppose you want this to be transaction managed as an ItemStream, how would you do that?", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.0.m4", "fixed_version": "spring-batch-1.0.0.m5", "fixed_files": ["org.springframework.batch.execution.launch.EmptyItemWriter.java"], "label": 1, "es_results": []}, {"bug_id": 373, "bug_title": "jobExecution.getJobInstance().getJob() return null when re-run", "bug_description": "It is found that, if a job is re-run (by passing same JobParameters), the JobExecution created is referring to a JobInstance which refers to null for its Job.\ni.e.\nJobExecution x = jobLauncher.run(job, jobParam);\nx.getJobInstance().getJob();  // returns null\nIt seems that for re-run job, the Job passed in the JobLauncher is not set to the JobInstance restored from DB.\nAs I am trying to intercept the Job&apos;s execute() by AOP, the only method I can get know of which job being executed is by jobExecution.getJobInstance().getJob()\n", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.0.m4", "fixed_version": "spring-batch-1.0.0.m5", "fixed_files": ["org.springframework.batch.execution.repository.SimpleJobRepository.java"], "label": 1, "es_results": []}, {"bug_id": 400, "bug_title": "Batch support schemas does not include RI and primary keys ", "bug_description": "It looks like BATCH_JOB_PARAMS and BATCH_STEP_EXECUTION_ATTRS do not include primary keys. (It also appears that these two tables are not yet used anywhere). Also, the schema does include referential integrity between tables. \nIt becomes difficult to get an approval from DBA&apos;s to approve such table structure...\nOf course, RI can be manually added as an easy workaround, but still... ", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.0.m4", "fixed_version": "spring-batch-1.0.0.m5", "fixed_files": ["org.springframework.batch.sample.dao.JdbcJobRepositoryTests.java"], "label": 1, "es_results": []}, {"bug_id": 393, "bug_title": "DefaultFieldSet should not always trim whitespace from the value returned", "bug_description": "DefaultFieldSet should not always trim whitespace from the value returned.  (I think it&apos;s valid for trimming to occur on the \"name\" of a field.)  It&apos;s a valid use-case for people to want the raw data that was read with whitespace included.  (It also can slightly slow performance during the reading of a file when trimming is done).\nPlease allow users to decide which field(s) they would like trimming to occur on.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.0.m4", "fixed_version": "spring-batch-1.0.0.m5", "fixed_files": ["org.springframework.batch.io.file.mapping.FieldSet.java", "org.springframework.batch.io.file.FieldSetTests.java", "org.springframework.batch.io.file.mapping.DefaultFieldSet.java"], "label": 1, "es_results": []}, {"bug_id": 198, "bug_title": "Make backoff policy tests less sensitive to virtualisation", "bug_description": "Make backoff policy tests less sensitive to virtualisation - they keep failing because they run too slowly.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.M3", "fixed_version": "spring-batch-1.0.1.RELEASE", "fixed_files": ["org.springframework.batch.retry.backoff.StatelessBackOffPolicy.java", "org.springframework.batch.retry.backoff.ExponentialBackOffPolicyTests.java", "org.springframework.batch.retry.backoff.FixedBackOffPolicyTests.java", "org.springframework.batch.retry.backoff.FixedBackOffPolicy.java", "org.springframework.batch.retry.backoff.ExponentialBackOffPolicy.java"], "label": 1, "es_results": []}, {"bug_id": 616, "bug_title": "Possible overflow in exit description if a stream.open() throws exception", "bug_description": "Possible overflow in exit description if a stream.open() throws exception.  Since we only truncate the exitStatus.description on update, if an exception is thrown before the initial insert would normally happen (ItemOrientedStep.doExecute) then the description could overflow the database column.  Fix would be to truncate on insert as well.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.core.repository.dao.JdbcStepExecutionDaoTests.java", "org.springframework.batch.core.repository.dao.AbstractStepExecutionDaoTests.java", "org.springframework.batch.core.repository.dao.JdbcStepExecutionDao.java"], "label": 1, "es_results": []}, {"bug_id": 625, "bug_title": "SkipListener#onSkipInWrite(..) called multiple times for the same item and not called without rollback", "bug_description": "SkipListener#onSkipInWrite(..) is called every time the problematic item is encountered i.e. multiple times in case of rollbacks. It should instead be called once per skipped item (i.e. the listener should be called when the write error was encountered and the item was marked for future skipping).", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.core.SkipListener.java", "org.springframework.batch.core.step.item.ItemSkipPolicyItemHandlerTests.java", "org.springframework.batch.core.step.item.ItemSkipPolicyItemHandler.java"], "label": 1, "es_results": []}, {"bug_id": 638, "bug_title": "ItemSkipPolicyItemHandler does not count items", "bug_description": "In 1.0.1 release, the resolution of  \"http://jira.springframework.org/browse/BATCH-531\" issue causes another issue.\nThe item count has move to SimpleItemHandler. The write method calls contribution.incrementItemCount().\nThe problem is in ItemSkipPolicyItemHandler, subclass of SimpleItemHandler. This class doesn&apos;t call incrementItemCount.\nIn my application, using 1.0.0 release item count ends with one number more than total items, and in 1.0.1, ends always with 0 (zero).", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.ItemSkipPolicyItemHandlerTests.java", "org.springframework.batch.core.step.item.SkipLimitStepFactoryBeanTests.java", "org.springframework.batch.core.step.item.SimpleItemHandler.java"], "label": 1, "es_results": []}, {"bug_id": 623, "bug_title": "StatefulRetryStepFactoryBean causes item count to be lost in database", "bug_description": "StatefulRetryStepFactoryBean causes item count to be lost in database", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.StatefulRetryStepFactoryBeanTests.java"], "label": 1, "es_results": []}, {"bug_id": 660, "bug_title": " SimpleStepFactoryBean#setExceptionHandler() not working", "bug_description": "Copied from http://forum.springframework.org/showthread.php?t=55706\nIt appears that while SkipLimitFactoryBean and StatefulRetryFactoryBean propogate the ExceptionHandler set in setExceptionHandler(), SimpleStepFactoryBean does not. The resulting ItemOrientedStep does not have anything other than DefaultExceptionHandler.\nI believe that a call to ItemOrientedStep#setExceptionHandler() within SimpleStepFactoryBean#applyConfiguration() will fix the issue.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.SimpleStepFactoryBeanTests.java", "org.springframework.batch.core.step.item.SimpleStepFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 661, "bug_title": "broken job interruption logic", "bug_description": "Currenlty jobExecution#stop() is implemented by setting the \"terminateOnly\" flag on all step executions created so far. Therefore in case stop() is called e.g. in StepListener#afterStep(..) it has no effect on the following steps. Also the terminateOnly flag is checked only in ItemOrientedStep so a job consisting of arbitrary number of TaskletSteps can&apos;t be stopped unless the Tasklet implementations support interruption.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.core.job.SimpleJobTests.java", "org.springframework.batch.core.job.SimpleJob.java"], "label": 1, "es_results": []}, {"bug_id": 665, "bug_title": "ChunkListeners are registered on stepOperations in RepeatOperationsStepFactoryBean - should be chunkOperations", "bug_description": "In RepeatOperationsStepFactoryBean.applyConfiguration(), the chunk listeners are registered on the stepOperations, which means they are executed when the step starts and when it ends, which seems to be wrong : as name and javadoc mentions it, ChunkListeners are executed around a chunk, i.e. inside the transaction boundaries.\nThe other step factory bean SimpleStepFactoryBean correctly registers the chunk listeners on the chunkOperations.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.RepeatOperationsStepFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 654, "bug_title": "Ensure best efforts are made to commit StepExecution when commit fails", "bug_description": "When the commit operation fails in an ItemOrientedStep, the writing of the Spring Batch metadata fails (trying to write the rollback information). \nWhen trying to commit the transaction, SB has already set the information it&apos;s about to persist in the StepExecutionContext. Then, in another transaction, SB wants to store that a rollback ocurred. Nevertheless, SB hasn&apos;t read the current persisted state from the database, so it still has the Version it read when trying to commit. That&apos;s why we think it fails.\nFurthermore, even if it succeeded, the persisted information wouldn&apos;t be accurate, because, as I&apos;ve pointed out, the StepExecutionContext hasn&apos;t been reset.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.ItemOrientedStep.java", "org.springframework.batch.core.step.AbstractStep.java", "org.springframework.batch.core.job.SimpleJobTests.java", "org.springframework.batch.core.repository.support.SimpleJobRepository.java", "org.springframework.batch.item.ExecutionContext.java", "org.springframework.batch.item.ExecutionContextTests.java", "org.springframework.batch.core.step.item.ItemOrientedStepTests.java"], "label": 1, "es_results": []}, {"bug_id": 687, "bug_title": "BatchUpdateItemWriter should fail if any of the statements does not update any rows (at least by default).", "bug_description": "BatchUpdateItemWriter should fail if any of the statements does not update any rows (at least by default).", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.item.database.BatchSqlUpdateItemWriter.java", "org.springframework.batch.item.database.BatchSqlUpdateItemWriterTests.java"], "label": 1, "es_results": []}, {"bug_id": 703, "bug_title": "JdbcJobInstanceDao does not locate JOB_INSTANCE on Oracle where JOB_KEY is empty", "bug_description": "Oracle has this \"feature\" where an empty string is stored as NULL.  This causes the JdbcJobInstanceDao to fail to look up existing JOB_INSTANCE where the key is empty since the JOB_KEY is stored as NULL in the database and must be queried using JOB_KEY IS NULL.  The end result is that there will be a new JOB_INSTANCE created even though a matching one already exists.\nThe JdbcJobRepositoryTests expose this issue.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.core.repository.dao.JdbcJobInstanceDao.java"], "label": 1, "es_results": []}, {"bug_id": 705, "bug_title": "TradeJobFunctionalTests might fail since verification queries do not have ORDER clause", "bug_description": "TradeJobFunctionalTests might randomly fail since verification queries don&apos;t have ORDER clause.  When the data stored is verified it might fail if rows are retrieved in a random order that is different from the order the rows where added.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-1.1.0.RELEASE", "fixed_files": ["org.springframework.batch.sample.TradeJobFunctionalTests.java"], "label": 1, "es_results": []}, {"bug_id": 737, "bug_title": "JdbcCursorItemReader will spin through entire resultset if numberOfProcessRows=0", "bug_description": "If a step using jdbccursoritemreader to read fails in the processing-step (itemwriter), number of processed rows (0) will be persisted in ExecutionContext. When re-running the step, 0 will be read as the number of processed rows, and the code will call moveCursorToRow with 0 as param.\nThe way the private method moveCursorToRow is implemented now, it will always call next() and increment count once, before checking if count==rows. At this point count=1 and row=0, this resultset will be traveresed until next()=false.\nChecked the history of the file, and it seems this behavior has been there since before the private method was introduced.\nThe code below should solve this problem:\n\t/**\n\nMoves the cursor in the resultset to the position specified by the in param by\ntraversing the resultset\n@param row\n\t */\n\tprivate void moveCursorToRow(int row){\n\t\ttry \nUnknown macro: {\t\t\tfor (int skipped = 0; skipped < row && rs.next(); skipped++) {\n\t\t\t\t//Do nothing\n\t\t\t}\t\t} \n\t\tcatch (SQLException se) \n{\n\t\t\tthrow getExceptionTranslator().translate(\"Attempted to move ResultSet to last committed row\", sql, se);\n\t\t}\n \n\t}\n\n", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "1.1.1.RELEASE", "fixed_files": ["org.springframework.batch.item.database.JdbcCursorItemReader.java", "org.springframework.batch.item.database.AbstractDataSourceItemReaderIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 682, "bug_title": "Use SoftReference and/or expiry to store entries in RetryContextCache implementation(s)", "bug_description": "Use SoftReference and/or expiry to store entries in RetryContextCache implementation(s).  If the map-based cache is used in a multi-VM environment, stale cache entries can easily be accumulated inadvertently because the successful processing of a previously failed item happened on a different node than the original failure.  A good start would be to use SoftReferences in the map-based implementation.  Expiry and more complicated features would be best left to mature cache technologies, and custom implementations of the RetryContextCache interface.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "spring-batch-2.0.0.M2", "fixed_files": ["org.springframework.batch.retry.policy.MapRetryContextCache.java"], "label": 1, "es_results": []}, {"bug_id": 618, "bug_title": "DefaultJobParametersConverter does not parse parameters of type double", "bug_description": "Although JobParameters and JobParametersBuilder support parameters of type double, the DefaultJobParametersConverter does not handle double parameters. Other types (string, date, long) are correctly managed.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.0.1.RELEASE", "fixed_version": "2.0.0.RC1", "fixed_files": ["org.springframework.batch.core.converter.DefaultJobParametersConverter.java", "org.springframework.batch.core.converter.DefaultJobParametersConverterTests.java"], "label": 1, "es_results": []}, {"bug_id": 732, "bug_title": "FlatFileItemReader does not take \"firstLineIsHeader\" flag into account when restarting", "bug_description": "FlatFileItemReader doesn&apos;t take \"firstLineIsHeader\" flag into account when restarting so the restart begins with the last successfully processed item rather than the one following it.  This is only an issue if the flat file contains a header line.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.0.RELEASE", "fixed_version": "spring-batch-2.0.0.M1", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemReader.java", "org.springframework.batch.item.file.FlatFileItemReaderAdvancedTests.java"], "label": 1, "es_results": []}, {"bug_id": 741, "bug_title": "DefaultFieldSet should clone the tokens before exposing them in getValues()", "bug_description": "DefaultFieldSet should clone the tokens before exposing them in getValues()", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.0.RELEASE", "fixed_version": "1.1.1.RELEASE", "fixed_files": ["org.springframework.batch.item.file.mapping.DefaultFieldSet.java"], "label": 1, "es_results": []}, {"bug_id": 744, "bug_title": "restart.count is always 0 in FlatFileItemWriter", "bug_description": "", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.0.RELEASE", "fixed_version": "spring-batch-2.0.0.M1", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemWriterTests.java", "org.springframework.batch.item.file.FlatFileItemWriter.java"], "label": 1, "es_results": []}, {"bug_id": 751, "bug_title": "ensure StepExecution is saved before trying to save ExecutionContext", "bug_description": "http://forum.springframework.org/showthread.php?t=57647", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.0.RELEASE", "fixed_version": "1.1.1.RELEASE", "fixed_files": ["org.springframework.batch.core.step.JobRepositorySupport.java", "org.springframework.batch.core.step.AbstractStepTests.java"], "label": 1, "es_results": []}, {"bug_id": 752, "bug_title": "FlatFileItemReader restart broken for non-default RecordSeparatorPolicy (record > 1 line)", "bug_description": "Simple fix is to remove the overriden jumpToItem(int) method and use the default provided by superclass", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.0.RELEASE", "fixed_version": "1.1.1.RELEASE", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemReader.java", "org.springframework.batch.item.file.FlatFileItemReaderAdvancedTests.java"], "label": 1, "es_results": []}, {"bug_id": 734, "bug_title": "ItemReaders and ItemWriters using Resource(s) should check for file during ItemStream#open", "bug_description": "During bean initialization MultiResourceItemReader checks [in afterPropertiesSet()] that it actually has resources to read. If not, it throws an exception. There are use cases where the resources might not be created at bean initialization time (see http://forum.springframework.org/showthread.php?t=57502 for details).\nLucas Ward suggests that this check might better be made during ItemStream#open().", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.0.RELEASE", "fixed_version": "spring-batch-1.1.2.RELEASE", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemReader.java", "org.springframework.batch.item.file.MultiResourceItemReaderXmlTests.java", "org.springframework.batch.item.file.MultiResourceItemReader.java", "org.springframework.batch.item.file.MultiResourceItemReaderFlatFileTests.java", "org.springframework.batch.item.file.FlatFileItemWriter.java", "org.springframework.batch.item.xml.StaxEventItemWriter.java", "org.springframework.batch.item.file.MultiResourceItemReaderIntegrationTests.java", "org.springframework.batch.item.xml.StaxEventItemReader.java", "org.springframework.batch.item.xml.StaxEventItemReaderTests.java"], "label": 1, "es_results": []}, {"bug_id": 761, "bug_title": "StaxEventItemWriter writes extra end document tag with Woodstox 3.2.6", "bug_description": "Woodstox 3.2.6 (current stable)&apos;s StaxEventWriter implementation automatically writes end tags and end document tags that it detects as still open on close.  When StaxEventItemWriter wraps Woodstox with a NoStartEndDocumentStreamWriter for the chunk writer (eventWriter), and another Woodstox instance for the document writer (delegateEventWriter), the result is two end document tags being written.  This is because even though the NoStartEndDocumentStreamWriter prevents the end document event from being written to the chunk writer, it writes the end document tag on close() anyway, on top of the one being written by StaxEventItemWriter.endDocument(delegateEventWriter) itself.\nHere&apos;s the relevant stack trace:\nThread [main] (Suspended)\t\n\tcom.ctc.wstx.sw.SimpleNsStreamWriter(com.ctc.wstx.sw.BaseStreamWriter).finishDocument() line: 1672\t\n\tcom.ctc.wstx.sw.SimpleNsStreamWriter(com.ctc.wstx.sw.BaseStreamWriter).close() line: 288\t\n\tcom.ctc.wstx.evt.WstxEventWriter.close() line: 237\t\n\torg.springframework.batch.item.xml.stax.NoStartEndDocumentStreamWriter(org.springframework.batch.item.xml.stax.AbstractEventWriterWrapper).close() line: 32\t\n\torg.springframework.batch.item.xml.StaxEventItemWriter.close(org.springframework.batch.item.ExecutionContext) line: 376\t\nThis was captured with Spring 1.1.0, but I different&apos;ed StaxEventItemWriter and NoStartEndDocumentStreamWriter for 1.1.0 vs. 1.1.1 in FishEye, and am not seeing anything that would change the behavior.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.0.RELEASE", "fixed_version": "spring-batch-1.1.2.RELEASE", "fixed_files": ["org.springframework.batch.item.xml.StaxEventItemWriter.java"], "label": 1, "es_results": []}, {"bug_id": 870, "bug_title": "Cannot add description to empty ExitStatus", "bug_description": "Cannot add description to empty ExitStatus.  One result is that the stack trace is missing from JobExecution when an exception is thrown in a step.", "project": "Spring", "sub_project": "BATCH", "version": "1.1.1.RELEASE", "fixed_version": "spring-batch-1.1.3.RELEASE", "fixed_files": ["org.springframework.batch.repeat.ExitStatus.java", "org.springframework.batch.repeat.ExitStatusTests.java", "org.springframework.batch.core.step.AbstractStepTests.java", "org.springframework.batch.core.step.AbstractStep.java"], "label": 1, "es_results": []}, {"bug_id": 831, "bug_title": "id counter in MapJobInstanceDao should be declared static", "bug_description": "http://forum.springframework.org/showthread.php?t=60053", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.2.RELEASE", "fixed_version": "spring-batch-1.1.3.RELEASE", "fixed_files": ["org.springframework.batch.core.repository.dao.MapJobInstanceDao.java"], "label": 1, "es_results": []}, {"bug_id": 838, "bug_title": "TimeoutTerminationPolicy does not terminate when eof is encountered", "bug_description": "The isCompleted methods of TimeoutTerminationPolicy (both the one implemented in the class and the one inherited from CompletionPolicySupport) does not seem to take end-of-file into account., They returns false until the timeout-time is up, even if the exitCode of the ExitStatus parameter changes to \"COMPLETED\"  when an eof is reached (looking at the constants in ExitStatus I should have guessed that the status should be \"FINISHED\"....). The result is lots of extra reads that all gets end-of-file.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.2.RELEASE", "fixed_version": "spring-batch-1.1.3.RELEASE", "fixed_files": ["org.springframework.batch.repeat.policy.TimeoutTerminationPolicy.java", "org.springframework.batch.repeat.policy.TimeoutCompletionPolicyTests.java", "org.springframework.batch.repeat.policy.CompletionPolicySupport.java"], "label": 1, "es_results": []}, {"bug_id": 833, "bug_title": "TransactionAttributes swallows Exceptions", "bug_description": "If the TransactionAttributes determines that a Transaction shouldn&apos;t be rolled back, the exception is effectively swallowed:\ntry \n{\n\t\t\t\t\t\texitStatus = tasklet.execute(contribution, attributes);\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Error e) {\n\t\t\t\t\t\tif (transactionAttribute.rollbackOn(e)) \n{\n\t\t\t\t\t\t\tthrow e;\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\tlogger.error(\"Ecountered error that should not because rollback: \", e);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Exception e) {\n\t\t\t\t\t\tif (transactionAttribute.rollbackOn(e)) {\t\t\t\t\t\t\tthrow e;\t\t\t\t\t\t}\n\t\t\t\t\t\telse \n{\n\t\t\t\t\t\t\tlogger.error(\"Ecountered error that should not because rollback: \", e);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\nSo, for example, if the TransactionAttributes are set to not rollback a SkipLimitExceededException, it won&apos;t be rolledback, effectively preventing it from going up to the StepOperations and causing the step to fail.  Of course, someone shouldn&apos;t think to do that, but it&apos;s just one example.  If someone sets any exception to be on the fatal list, but messes up and puts an exception in the attribute list to cause rollback, the step won&apos;t fail.\nI propose that the rollback decision on transaction attribute be used solely to determine whether the transaction is committing, but that the exception should still be propagated up to the StepOperations to determine whether or not it should cause failure.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.2.RELEASE", "fixed_version": "spring-batch-1.1.3.RELEASE", "fixed_files": ["org.springframework.batch.core.step.tasklet.TaskletStep.java"], "label": 1, "es_results": []}, {"bug_id": 866, "bug_title": "Reference manual has invalid references to org.springframework.batch.io.file package", "bug_description": "The org.springframework.batch.io.file package doesn&apos;t exists - should reference org.springframework.batch.item.file instead", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.2.RELEASE", "fixed_version": "spring-batch-1.1.3.RELEASE", "fixed_files": ["org.springframework.batch.item.file.mapping.PassThroughFieldSetMapper.java", "org.springframework.batch.item.file.mapping.DefaultFieldSet.java"], "label": 1, "es_results": []}, {"bug_id": 892, "bug_title": "Thread visibility issues in repeat template", "bug_description": "ResultHolder implementation in TaskExecutorRepeatTemplate probably needs some volatile keywords (http://forum.springframework.org/showthread.php?t=61715).", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.2.RELEASE", "fixed_version": "spring-batch-1.1.3.RELEASE", "fixed_files": ["org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate.java"], "label": 1, "es_results": []}, {"bug_id": 836, "bug_title": "CompositeItemWriter should also implement ItemStream", "bug_description": "The StepFactoryBean automatically registers ItemReader and ItemWriter as streams if they implement the interface. It would be nice if the CompositeItemWriter could also implement this interface (and delegate the methods to its delegate if appropriate), so that a composite writer set as the writer in a StepFactoryBean will be registered as an ItemStream and delegate ItemStreams methods on its delegates automatically.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.2.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.item.support.CompositeItemWriterTests.java", "org.springframework.batch.item.support.CompositeItemWriter.java"], "label": 1, "es_results": []}, {"bug_id": 1122, "bug_title": "StaxEventWriter.startDocument() needs to be protected", "bug_description": "The method startDocument() in org.springframework.batch.item.xml.StaxEventWriter is private, whereas its javadoc mentions \"[...]If this is not sufficient for you, simply override this method.\"\nThis prevents one to cleanly override the creation of the root tag.\nThe simple fix is to make the method protected, as is endDocument().", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.4.RELEASE", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.item.xml.StaxEventItemWriter.java"], "label": 1, "es_results": []}, {"bug_id": 1245, "bug_title": "StaxEventItemWriter writes extra end document tag with Woodstox 3.2.9 plus", "bug_description": "This is a continuation of BATCH-761, which is closed and won&apos;t accept new attachments.  Feel free to close this as won&apos;t fix, but now that Woodstox allows disabling of \"auto-closing of unmatched start elements\" in versions >= 3.2.9 the attached patch against 1.1.4.RELEASE is my preferred solution to BATCH-761.  With this patch there is no burden on the Spring Batch user that chooses Woodstox >= 3.2.9 as their StAX implementation to override Spring Batch API, or even be aware that this issue ever existed.\nAgain apologies for the lack of a test case, but my plan is to flesh that out by taking a stab at BATCH-981 when we get a chance to migrate to 2.0.  For what it&apos;s worth we have an internal integration test case to trap this issue, and it passes with Woodstox 4.0.4 and this patch.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-1.1.4.RELEASE", "fixed_version": "2.0.1.RELEASE", "fixed_files": ["org.springframework.batch.item.xml.StaxEventItemWriter.java"], "label": 1, "es_results": []}, {"bug_id": 795, "bug_title": "JdbcJobExecutionDao output sorting", "bug_description": "There is minor bug in JdbcJobExecutionDao.findJobExecutions(JobInstance jobInstance). JobExecutionDao&apos;s javadoc says that executions returned from this method are sorted bacwards by start time, but there is no ORDER BY clause in sql query so executions are sorted the wrong way. I didn&apos;t see ORDER BY anywhere in the file so maybe other methods should be checked too.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M1", "fixed_version": "spring-batch-2.0.0.M2", "fixed_files": ["org.springframework.batch.core.repository.dao.JdbcJobExecutionDao.java", "org.springframework.batch.core.repository.dao.MapJobExecutionDao.java", "org.springframework.batch.core.repository.dao.AbstractJobExecutionDaoTests.java", "org.springframework.batch.core.repository.dao.JobExecutionDao.java"], "label": 1, "es_results": []}, {"bug_id": 843, "bug_title": "FlatFileItemWriter handling of failure in LineAggregator", "bug_description": "FFIW should first map all items to strings and only then write them. Currently failure in line aggregator can cause rollback when some lines from the current chunk are already written.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M1", "fixed_version": "spring-batch-2.0.0.M2", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemWriterTests.java", "org.springframework.batch.item.file.FlatFileItemWriter.java"], "label": 1, "es_results": []}, {"bug_id": 865, "bug_title": "CompositeItemProcessor should handle null properly", "bug_description": "Now that returning null from an ItemProcessor indicates filtering on an item, CompositeItemProcessor should return null if any processor in the chain returns null.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M1", "fixed_version": "spring-batch-2.0.0.M2", "fixed_files": ["org.springframework.batch.item.support.CompositeItemProcessorTests.java", "org.springframework.batch.item.support.CompositeItemProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 887, "bug_title": "onSkipInProcess called multiple times for same item using FaultTolerantChunkOrientedTasklet", "bug_description": "If there are skips during the writing of the chunk then the items in the chunk are processed twice again and the onSkipInProcess gets called for each time the chunk is processed.\nThe basic sequence of events:\n\nread chunk\n\n\nprocess chunk - some items because process skip and the items are removed from chunk and placed in skip list\nonSkipInProcess called for skipped items\nwrite chunk - exception thrown for one item\n\n\nprocess chunk without the process skips\nonSkipInProcess called again for skipped items\nwrite individual chunks - exception thrown for same item  and the item is removed from the chunk\nonSkipInWrite called\n\n\nprocess chunk without the process or write skips\nwrite chunk without the process or write skips\nonSkipInProcess called again for skipped items\n\nSeems that we should remove the skips from the skip list after the first onSkipInProcess is completed or supress the call once it has been made for the skip list.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M2", "fixed_version": "spring-batch-2.0.0.M3", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantChunkOrientedTaskletTests.java", "org.springframework.batch.core.step.item.FaultTolerantChunkOrientedTasklet.java"], "label": 1, "es_results": []}, {"bug_id": 912, "bug_title": "Thread safety issue in JobRegistryBackgroundJobRunner", "bug_description": "I noticed that the errors variable in JobRegistryBackgroundJobRunner is modified by one thread and accessed by another without synchronization. As the ArrayList isn&apos;t thread safe, I think this might cause visibility issues in the code that checks if errors has occurred during initialization of the application context. As with all visibility issues it is hard to prove this, but I believe that at least in theory this may be a problem.\nA way to fix this would be by replacing\nprivate static List<Exception> errors = new ArrayList<Exception>();\nwith\nprivate static List<Exception> errors = Collections.synchronizedCollection(new ArrayList<Exception>());\nNot sure if making the list volatile will work...as I&apos;m not all that confident on what the implications are of making a collection variable volatile. Don&apos;t think there will much of a performance hit this particular case anyway.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M2", "fixed_version": "spring-batch-2.0.0.M3", "fixed_files": ["org.springframework.batch.core.launch.support.JobRegistryBackgroundJobRunner.java"], "label": 1, "es_results": []}, {"bug_id": 948, "bug_title": "MapJobInstanceDao.getLastJobInstances ignores jobName parameter", "bug_description": "MapJobInstanceDao.getLastJobInstances ignores jobName parameter and returns  JobInstances for all jobs. \n", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.core.repository.dao.MapJobInstanceDao.java", "org.springframework.batch.core.repository.dao.AbstractJobInstanceDaoTests.java"], "label": 1, "es_results": []}, {"bug_id": 951, "bug_title": "MapJobInstanceDao.getLastJobInstances does not return the last job instance ", "bug_description": "Wrong order of job instances in MapJobInstanceDao.getLastJobInstances. MapJobInstanceDao.getLastJobInstances returns always  &apos;old&apos; job instances.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.core.repository.dao.MapJobInstanceDao.java", "org.springframework.batch.core.repository.dao.AbstractJobInstanceDaoTests.java"], "label": 1, "es_results": []}, {"bug_id": 954, "bug_title": "Failure on job stop", "bug_description": "Try to stop a job with SimpleJobOperator through JMX.\nA OptimisticLockingFailureException is thrown because the jobExecution version doesn&apos;t match with persisted one.\nBug origin :\nWhen the JobExecution status is synchronized, the version is not updated.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.core.repository.dao.JdbcJobExecutionDaoTests.java", "org.springframework.batch.core.repository.dao.MapJobExecutionDao.java", "org.springframework.batch.core.launch.support.SimpleJobOperator.java", "org.springframework.batch.core.repository.dao.JdbcJobExecutionDao.java", "org.springframework.batch.core.repository.dao.JobExecutionDao.java", "org.springframework.batch.core.repository.dao.AbstractJobExecutionDaoTests.java", "org.springframework.batch.core.job.SimpleJobTests.java", "org.springframework.batch.core.partition.support.PartitionStepTests.java"], "label": 1, "es_results": []}, {"bug_id": 969, "bug_title": "FlatFileItemWriters interference in CompositeItemWriter", "bug_description": "http://forum.springframework.org/showthread.php?t=64607\nTwo FFIW end up writing to the same resource when injected into CompositeItemWriter. No clue how that&apos;s possible yet, but I was able to recreate the problem.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.support.transaction.TransactionAwareBufferedWriter.java", "org.springframework.batch.support.transaction.TransactionAwareBufferedWriterTests.java", "org.springframework.batch.item.file.FlatFileItemWriter.java", "org.springframework.batch.item.xml.StaxEventItemWriter.java", "org.springframework.batch.item.util.ExecutionContextUserSupport.java", "org.springframework.batch.sample.CompositeItemWriterSampleFunctionalTests.java"], "label": 1, "es_results": []}, {"bug_id": 994, "bug_title": "BackOffPolicy is not applied for exceptions that because rollback", "bug_description": "Retryable exceptions that should cause rollback get re-thrown before BackOffPolicy applies.\nhttp://forum.springframework.org/showthread.php?t=65811", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.retry.support.RetryTemplate.java", "org.springframework.batch.retry.support.RetryTemplateTests.java"], "label": 1, "es_results": []}, {"bug_id": 952, "bug_title": "StagingItemReader is not restartable", "bug_description": "StagingItemReader is not restartable because it updates the process indicator in the read() method, which is only ever called once per item (used to be once per item per transaction in 1.x).  I&apos;m not sure the best way to fix this.  The reader knows the id of the record to be updated, but that information is lost further downstream in the processing pipeline.  Maybe it&apos;s time to get serious about ChunkContext or @ChunkAttribute (see BATCH-920)?\nIt would be a good time to replace the innards of StagingItemReader anyway - it can delegate most of its stateful behaviour to an off the shelf JDBC ItemReader.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.sample.common.StagingItemReaderTests.java", "org.springframework.batch.sample.common.StagingItemReader.java"], "label": 1, "es_results": []}, {"bug_id": 995, "bug_title": "unclear retry configuration in FaultTolerantStepFactoryBean", "bug_description": "if (retryPolicy == null) {\n\tSimpleRetryPolicy simpleRetryPolicy = new SimpleRetryPolicy(retryLimit);\n\tif (!retryableExceptionClasses.isEmpty()) \n{ // otherwise we\n\t\t// retry all exceptions\n\t\tsimpleRetryPolicy.setRetryableExceptionClasses(retryableExceptionClasses);\n\t}\n\tsimpleRetryPolicy.setFatalExceptionClasses(fatalExceptionClasses);\n\tExceptionClassifierRetryPolicy classifierRetryPolicy = new ExceptionClassifierRetryPolicy();\n\tHashMap<Class<? extends Throwable>, RetryPolicy> exceptionTypeMap = new HashMap<Class<? extends Throwable>, RetryPolicy>();\n\tfor (Class<? extends Throwable> cls : retryableExceptionClasses) \n{\n\t\texceptionTypeMap.put(cls, simpleRetryPolicy);\n\t}\n\tclassifierRetryPolicy.setPolicyMap(exceptionTypeMap);\n\tretryPolicy = classifierRetryPolicy;\n}\nSimpleRetryPolicy is for some reason wrapped in ExceptionClassifierRetryPolicy. This not only seems unnecessary, but also ignores the fatalExceptionClasses (unless they subclass the retryableExceptionClasses).\nThe most interesting thing however is that using simpleRetryPolicy directly severely breaks the FaultTolerantStepFactoryBeanTests.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantStepFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 1002, "bug_title": "Default behavior for a Job should be failure if a step fails", "bug_description": "Currently, the when the \"next\" attribute is used on a step in a job configuration with the namespace, such as the following:\n\t<step name=\"step1\" next=\"step2\"/>\n\t<step name=\"step2\" />\nthe interpretation is as follows:\n\t<step name=\"step1\">\n\t\t<next on=\"*\" to=\"step2\"/>\n\t</step>\n\t<step name=\"step2\" />\nHowever, this behavior may be confusing for users because \"step2\" will still execute even if \"step1\" fails.  Therefore, the correct default behavior should be:\n\t<step name=\"step1\">\n\t\t<end on=\"FAILED\" status=\"FAILED\"/>\n\t\t<next on=\"*\" to=\"step2\"/>\n\t</step>\n\t<step name=\"step2\" />\n", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepParser.java", "org.springframework.batch.core.configuration.xml.NextAttributeJobParserTests.java"], "label": 1, "es_results": []}, {"bug_id": 1001, "bug_title": "Make jobs restartable by default", "bug_description": "A Job should be restartable by default.  However, the Spring Batch schema, spring-batch-2.0.xsd, and the AbstractJob class list the \"restartable\" attribute has being false by default.\nA patch has been created and attached to make these changes.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.core.job.AbstractJob.java"], "label": 1, "es_results": []}, {"bug_id": 1021, "bug_title": "AssertFile.assertFileEquals(File,File) parameters in the wrong order", "bug_description": "In the AssertFile class in spring-batch-test, the method:\nassertFileEquals(File actual, File expected)\nshould be:\nassertFileEquals(File expected, File actual)\nPatches are attached to fix this.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.test.AssertFileTests.java", "org.springframework.batch.sample.iosample.MultiLineFunctionalTests.java", "org.springframework.batch.sample.iosample.MultiRecordTypeFunctionalTests.java", "org.springframework.batch.test.AssertFile.java"], "label": 1, "es_results": []}, {"bug_id": 1031, "bug_title": "FlatFileItemReader should identify missing resource in warning", "bug_description": "Current warning says: \"Input resource does not exist,\" but does not identify the resource, forcing debugging to correct.  Patch adds the resource.toString() to the message.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.item.xml.StaxEventItemReader.java", "org.springframework.batch.item.file.FlatFileItemReader.java"], "label": 1, "es_results": []}, {"bug_id": 1007, "bug_title": "JobRepository default is inconsistent between job and step", "bug_description": "The way that the job repository property is defaulted in job and step is inconsistent.  With Step, it&apos;s done the right way, by defaulting in the namespace, with good documentation.  However, job is done in the parser:\n\t\tString repositoryAttribute = element.getAttribute(\"repository\");\n\t\tif (!StringUtils.hasText(repositoryAttribute)) \n{\n\t\t\trepositoryAttribute = \"jobRepository\";\n\t\t}\n\t\tbuilder.addPropertyReference(\"jobRepository\", repositoryAttribute);\nThere are many others like this in job.", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "spring-batch-2.0.0.M4", "fixed_files": ["org.springframework.batch.core.configuration.xml.JobParser.java"], "label": 1, "es_results": []}, {"bug_id": 1030, "bug_title": "FlowJob replays failed steps on restart, even if the failure did not fail the job", "bug_description": "\t@Test\n\tpublic void testFailedStepRestarted() throws Exception {\n\t\tSimpleFlow flow = new SimpleFlow(\"job\");\n\t\tCollection<StateTransition> transitions = new ArrayList<StateTransition>();\n\t\ttransitions.add(StateTransition.createStateTransition(new StepState(new StepSupport(\"step1\") {\n\t\t\t@Override\n\t\t\tpublic void execute(StepExecution stepExecution) throws JobInterruptedException,\n\t\t\t\t\tUnexpectedJobExecutionException \n{\n\t\t\t\tstepExecution.setStatus(BatchStatus.FAILED);\n\t\t\t\tstepExecution.setExitStatus(ExitStatus.FAILED);\n\t\t\t\tjobRepository.update(stepExecution);\n\t\t\t}\n\t\t}), \"step2\"));\n\t\ttransitions.add(StateTransition.createEndStateTransition(new StepState(new StubStep(\"step2\") {\n\t\t\t@Override\n\t\t\tpublic void execute(StepExecution stepExecution) throws JobInterruptedException,\n\t\t\t\t\tUnexpectedJobExecutionException {\n\t\t\t\tif (fail) \n{\n\t\t\t\t\tstepExecution.setStatus(BatchStatus.FAILED);\n\t\t\t\t\tstepExecution.setExitStatus(ExitStatus.FAILED);\n\t\t\t\t\tjobRepository.update(stepExecution);\n\t\t\t\t}\n else \n{\n\t\t\t\t\tsuper.execute(stepExecution);\n\t\t\t\t}\n\t\t\t}\n\t\t})));\n\t\tflow.setStateTransitions(transitions);\n\t\tjob.setFlow(flow);\n\t\tjob.afterPropertiesSet();\n\t\tfail = true;\n\t\tjob.execute(jobExecution);\n\t\tassertEquals(ExitStatus.FAILED, jobExecution.getExitStatus());\n\t\tassertEquals(2, jobExecution.getStepExecutions().size());\n\t\tjobRepository.update(jobExecution);\n\t\tjobExecution = jobRepository.createJobExecution(\"job\", new JobParameters());\n\t\tfail = false;\n\t\tjob.execute(jobExecution);\n\t\tassertEquals(ExitStatus.COMPLETED, jobExecution.getExitStatus());\n\t\tassertEquals(1, jobExecution.getStepExecutions().size());\n\t}", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "2.0.0.RC1", "fixed_files": ["org.springframework.batch.core.job.flow.FlowJobTests.java", "org.springframework.batch.core.job.AbstractJob.java", "org.springframework.batch.core.BatchStatus.java", "org.springframework.batch.core.job.flow.FlowJob.java"], "label": 1, "es_results": []}, {"bug_id": 1061, "bug_title": "FlowJob.getLastStepExecution() puts arguments into isLater() in the wrong order", "bug_description": "FlowJob.getLastStepExecution() puts arguments into isLater() in the wrong order:\n\t\tfor (StepExecution stepExecution : execution.getStepExecutions()) {\n\t\t\tif (stepExecution.getStepName().equals(result.getName())\n\t\t\t\t\t&& stepExecution.getExitStatus().getExitCode().equals(result.getStatus())) \n{\n\t\t\t\tvalue = stepExecution;\n\t\t\t}\n\t\t\tif (isLater(backup, stepExecution)) \n{\n\t\t\t\tbackup = stepExecution;\n\t\t\t}\n", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M4", "fixed_version": "2.0.0.RC1", "fixed_files": ["org.springframework.batch.core.job.flow.FlowJob.java"], "label": 1, "es_results": []}, {"bug_id": 1082, "bug_title": "If file reader is lenient about resource existing on startup, it should also check when it is closed", "bug_description": "If file reader is lenient about resource existing on startup, it should also check when it is closed.  See forum post: http://forum.springframework.org/showthread.php?t=67490", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "2.0.0.RC1", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemReaderTests.java", "org.springframework.batch.item.file.FlatFileItemReader.java"], "label": 1, "es_results": []}, {"bug_id": 1086, "bug_title": "JdbcJobExecutionDao.getRunningJobExecutions() ignores jobName", "bug_description": "JdbcJobExecutionDao.getRunningJobExecutions() ignores jobName", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M4", "fixed_version": "2.0.0.RC1", "fixed_files": ["org.springframework.batch.core.repository.dao.AbstractStepExecutionDaoTests.java", "org.springframework.batch.core.repository.dao.JdbcJobExecutionDaoTests.java", "org.springframework.batch.core.repository.dao.JdbcJobExecutionDao.java", "org.springframework.batch.core.repository.dao.MapJobExecutionDaoTests.java", "org.springframework.batch.core.repository.dao.AbstractJobExecutionDaoTests.java"], "label": 1, "es_results": []}, {"bug_id": 1091, "bug_title": "Add strict flag to file readers (flat and XML).", "bug_description": "If file reader is lenient about resource existing on startup, it should also check when it is closed.  See forum post: http://forum.springframework.org/showthread.php?t=67490", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemReaderTests.java", "org.springframework.batch.item.xml.StaxEventItemReader.java", "org.springframework.batch.item.file.FlatFileItemReader.java", "org.springframework.batch.item.xml.StaxEventItemReaderTests.java"], "label": 1, "es_results": []}, {"bug_id": 1107, "bug_title": "Fix Date conversion in PlaceholderTargetSource", "bug_description": "Spring doesn&apos;t do type conversion to String very well, and for Date not at all.  This leads to problems for placeholders of type Date.  There are two issues in the current implementation:\n  <property name=\"foo\" value=\"#\n{jobParameters[runDate]}\n\"/>\nfails even if the \"foo\" property is of type Date because the String conversion is attempted too early; and \n  <property name=\"query\" value=\"select ... where start_date > &apos;#\n{jobParameters}\n&apos;\"/>\nfails because the placeholder is embedded in a literal and needs to be converted to String (which Spring doesn&apos;t do natively).", "project": "Spring", "sub_project": "BATCH", "version": "spring-batch-2.0.0.M3", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.core.scope.util.PlaceholderTargetSourceTests.java", "org.springframework.batch.core.scope.util.PlaceholderTargetSource.java"], "label": 1, "es_results": []}, {"bug_id": 1111, "bug_title": "ChunkListener called before WriteListener", "bug_description": "ChunkListener called before WriteListener:  http://forum.springframework.org/showthread.php?t=68280", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC1", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.core.step.item.SimpleStepFactoryBeanTests.java", "org.springframework.batch.core.step.tasklet.TaskletStep.java", "org.springframework.batch.core.step.item.BatchListenerFactoryHelper.java", "org.springframework.batch.core.step.item.SimpleStepFactoryBean.java", "org.springframework.batch.core.step.AbstractStep.java"], "label": 1, "es_results": []}, {"bug_id": 1119, "bug_title": "afterWrite() will only be called if an exception is raised during throttling", "bug_description": "If a chunk fails during write() then the recovery procedure will kick in.  But if all of the items pass during throttling, then no exception will be raised to try the chunk again.  Thus, the afterWrite() method will never be called because that method is only called by SimpleChunkProcessor.doWrite().", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC1", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantChunkProcessorTests.java", "org.springframework.batch.core.step.item.FaultTolerantChunkProcessor.java", "org.springframework.batch.core.step.item.SimpleChunkProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1123, "bug_title": "ExecutionContextPromotionListener may perform promotion multiple times", "bug_description": "The ExecutionContextPromotionListener  currently performs the promotion for every matched pattern.  Instead, it should break out of the loop after finding a match.  ", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC1", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.core.listener.ExecutionContextPromotionListener.java"], "label": 1, "es_results": []}, {"bug_id": 1128, "bug_title": "Make sure WRITE_COUNT and ROLLBACK_COUNT are being updated correctly", "bug_description": "From a forum post:\nUsing FlatFileItemReader and loading the records into database.  File has 55 records and 55th record is skipped. Commit interval is 100.\nResults in:\n    READ_COUNT: 55\n    WRITE_COUNT: 108\n    WRITE_SKIP_COUNT: 1\n    ROLLBACK_COUNT: 2\n    COMMIT_COUNT: 1\nDatabase was correctly loaded with 54 records. But WRITE_COUNT and ROLLBACK_COUNT are misleading.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC1", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.sample.SkipSampleFunctionalTests.java", "org.springframework.batch.core.step.item.FaultTolerantChunkProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1130, "bug_title": "Ensure Ordered is respected by generated listeners", "bug_description": "", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC1", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.core.listener.JobListenerFactoryBeanTests.java", "org.springframework.batch.core.listener.JobListenerFactoryBean.java", "org.springframework.batch.core.listener.StepListenerFactoryBean.java", "org.springframework.batch.core.listener.StepListenerFactoryBeanTests.java", "org.springframework.batch.core.listener.MethodInvokerMethodInterceptor.java"], "label": 1, "es_results": []}, {"bug_id": 1143, "bug_title": "Standalone <step/> should not be allowed to have \"tasklet\" attribute and <tasklet/> together", "bug_description": "Standalone <step/> should not be allowed to have \"tasklet\" attribute and <tasklet/> together.  There is logic to check for this on an inline step, but not a standalone.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC1", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.core.configuration.xml.AbstractStepParser.java", "org.springframework.batch.core.configuration.xml.InlineStepParser.java", "org.springframework.batch.core.configuration.xml.StandaloneStepParser.java"], "label": 1, "es_results": []}, {"bug_id": 1147, "bug_title": "StepExecution getFilterCount always return 0", "bug_description": "We use a Step FaultTolerantStepFactoryBean\nwith an ItemProcessor\nWe do a JUnit Test in 2.0.0 CI Snapshot\nand setup the test to have the ItemProcessor return null\nthe assert says that getFilterCount == 1 is true\nAfter move to 2.0.0 RC1 core SpringBatch the assert is false\nbecause getFilterCount() return 0\nIn all the other Junit Test getFilterCount() method return always 0.\nCan you explain if it is a bug or another way to configure FaultTolerantStepFactoryBean ?\n", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC1", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.sample.domain.trade.CustomerUpdateFieldSetMapper.java", "org.springframework.batch.core.step.item.SimpleChunkProcessor.java", "org.springframework.batch.core.step.item.SimpleChunkProcessorTests.java", "org.springframework.batch.sample.CustomerFilterJobFunctionalTests.java"], "label": 1, "es_results": []}, {"bug_id": 1131, "bug_title": "It is not possible to set transaction-attributes for a tasklet step", "bug_description": "When creating a chunk-oriented step, a \"transaction-attribute\"  can be configured on the <tasklet/> element.  However, since tasklet steps do not use the <tasklet/> element (they use the &apos;tasklet&apos; attribute) there is no place to configure a transaction-attribute.  If the user must use Spring&apos;s \"tx\" namespace and provide transaction advice, then this should be mentioned in the documentation.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC1", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepParserTests.java", "org.springframework.batch.core.step.tasklet.TaskletStep.java", "org.springframework.batch.core.configuration.xml.TaskletElementParser.java", "org.springframework.batch.core.configuration.xml.AbstractStepParser.java", "org.springframework.batch.core.step.item.SimpleStepFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 1160, "bug_title": "In Batch xsd, stepType and flowStepType should be unordered", "bug_description": "In Batch xsd, stepType and flwoStepType should be unordered.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC1", "fixed_version": "2.0.0.RC2", "fixed_files": ["org.springframework.batch.core.configuration.xml.SplitParser.java", "org.springframework.batch.core.configuration.xml.AbstractStepParser.java"], "label": 1, "es_results": []}, {"bug_id": 1171, "bug_title": "Interrupted step does not fail job.", "bug_description": "Interrupted step does not fail job.  This has to be wrong otherwise you can&apos;t restart it.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC2", "fixed_version": "2.0.0.RC3", "fixed_files": ["org.springframework.batch.core.job.flow.FlowJobTests.java", "org.springframework.batch.core.job.AbstractJob.java", "org.springframework.batch.core.job.SimpleJobTests.java", "org.springframework.batch.sample.launch.RemoteLauncherTests.java"], "label": 1, "es_results": []}, {"bug_id": 1174, "bug_title": "Late binding of jobParameters does not work if late binding expression is not preceded or trailed by string", "bug_description": "I stumbled upon some strange behavior when using late binding. I&apos;m trying to inject a string that is passed to the batch as a jobParameter. This string is however not inject, if I don&apos;t add a character in front-  of or after the late binding expression.\nTo replicate this bug try to do the following:\nin the restartSample.xml change the flatFileItemReader&apos;s resource from:\n<beans:property name=\"resource\" value=\"classpath:/data/skipJob/input/input#\n{jobParameters[run.id]}.txt\" />\nto\n<beans:property name=\"resource\" value=\"#{jobParameters[run.id]}\n\" />\nThis results in the following exception:\n java.lang.IllegalStateException: Input resource must exist (reader is in &apos;strict&apos; mode): class path resource [#\n{jobParameters[run.id]}]\n\nIf you add a character at the end (or the front), like this:\n<beans:property name=\"resource\" value=\"a#{jobParameters[run.id]}\n\" />\nThe following exception is thrown:\njava.lang.IllegalStateException: Input resource must exist (reader is in &apos;strict&apos; mode): class path resource [a1]\n(This makes sense, as there is no such classpath resource, and the jobParameter run.id which is &apos;1&apos; has been set.)\nSo, it seems that the late binding of jobParameters does not work if the jobParameter key is concatenated with some other string value...strange. Even stranger, it does not seem to be the case for late binding of stepExecutionContext parameters.\nWasn&apos;t able to locate the code that did the actual late binding, so I&apos;m afraid I don&apos;t have patch ready that you can apply.\n", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC2", "fixed_version": "2.0.0.RC3", "fixed_files": ["org.springframework.batch.sample.RestartFunctionalTests.java", "org.springframework.batch.core.scope.util.PlaceholderTargetSource.java"], "label": 1, "es_results": []}, {"bug_id": 1178, "bug_title": "<step/> with \"ref=\" silently ignores other attributes", "bug_description": "If the InlineStepParser finds a \"ref=\" attribute on the <step/>, it will ignore all other attributes.  Instead, it should be throwing errors if any other attributes are defined.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC2", "fixed_version": "2.0.0.RC3", "fixed_files": ["org.springframework.batch.core.configuration.xml.InlineStepParser.java", "org.springframework.batch.core.configuration.xml.FlowParser.java"], "label": 1, "es_results": []}, {"bug_id": 1180, "bug_title": "Error occurs if parent= attribute appears on inline <step/> without tasket", "bug_description": "An error occurs if the parent= attribute appears on an inline <step/> without tasket= or <tasklet/> because no BeanDefinition is returned from AbstractStepParser.parseTasklet().  \nUpdate parseTasklet() so that a BeanDefinition (even an empty one) is always returned.  This will simplify the step parser hierarchy a bit too because calling methods will no longer have to check whether a BeanDefinition was returned or not.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC2", "fixed_version": "2.0.0.RC3", "fixed_files": ["org.springframework.batch.core.configuration.xml.TopLevelStepParser.java", "org.springframework.batch.core.configuration.xml.StopRestartOnCompletedStepJobParserTests.java", "org.springframework.batch.core.configuration.xml.TopLevelStepListenerParser.java", "org.springframework.batch.core.configuration.xml.StandaloneStepParser.java", "org.springframework.batch.core.configuration.xml.InlineStepParser.java", "org.springframework.batch.core.configuration.xml.AbstractStepParser.java", "org.springframework.batch.core.configuration.xml.TopLevelJobListenerParser.java"], "label": 1, "es_results": []}, {"bug_id": 1185, "bug_title": "Job slows when step scope is used", "bug_description": "When scope=\"step\" appears on a flat file item reader, the job takes longer.\nIn the football job, if player.file.name=player.csv, the job takes 15 seconds.  If scope=\"step\" is added to playerFileItemReader, the job takes 26 seconds.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC2", "fixed_version": "2.0.0.RELEASE", "fixed_files": ["org.springframework.batch.item.file.ResourceAwareItemReaderItemStream.java", "org.springframework.batch.core.scope.util.PlaceholderTargetSource.java", "org.springframework.batch.core.scope.util.PlaceholderProxyFactoryBean.java", "org.springframework.batch.core.scope.StepScope.java"], "label": 1, "es_results": []}, {"bug_id": 1197, "bug_title": "Rerunning a job sometimes creates new job instance", "bug_description": "An attempt to rerun stopped job using following code sometimes leads to creation of new JobInstance with same JOB_KEY but with entries in different order.\nList<JobInstance> jobInstances = jobExplorer.getJobInstances(jobName, 0, 1);\nif (!jobInstances.isEmpty()) {\n    jobLauncher.run(job, jobInstances.get(0).getJobParameters());\n}\nI tried to debug it and I think the problem may be in JdbcJobInstanceDao.FIND_PARAMS_FROM_ID used in getJobParameters(). This query relies on the fact that job parameters are retrieved from database in same order as they have been inserted - which does not have to be true. Maybe ordering by job parameter name would help. Maybe it would be best to add this behavior to the JobParameters instead of relying on LinkedHashMap and insert order.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC3", "fixed_version": "2.0.0.RELEASE", "fixed_files": ["org.springframework.batch.core.repository.dao.JdbcJobInstanceDaoTests.java", "org.springframework.batch.core.repository.dao.JdbcJobInstanceDao.java"], "label": 1, "es_results": []}, {"bug_id": 1198, "bug_title": "processSkipCount and filterCount mixed up", "bug_description": "There is another issue around process phase. Skips are sometimes counted as filtered items. And there is also pretty strange filter count behaviour: i&apos;ve got a file with 10 items. All get processed,  everything is ok. If I fiddle with one of the records so that processor throws exception. I get 9 written, 1 skipped and 1 filtered. When there are two invalid items, i get 8 written, 2 skipped and 1 filtered too... sometimes filter counter shows more filtered items ... by counter i mean the one in StepExecution instance that is passed to afterStep() method. I believe my skippable/fatal/retriable exceptions are configured correctly (see BATCH-1159). ", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC3", "fixed_version": "2.0.0.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantChunkProcessorTests.java", "org.springframework.batch.core.step.item.SimpleChunkProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1196, "bug_title": "ExecutionContext not re-hydrated by JdbcJobExecutionDao", "bug_description": "ExecutionContext not re-hydrated by JdbcJobExecutionDao", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC2", "fixed_version": "2.0.0.RELEASE", "fixed_files": ["org.springframework.batch.core.exlore.support.SimpleJobExplorerTests.java", "org.springframework.batch.core.explore.support.JobExplorerFactoryBean.java", "org.springframework.batch.core.explore.support.MapJobExplorerFactoryBean.java", "org.springframework.batch.core.explore.support.SimpleJobExplorer.java", "org.springframework.batch.core.explore.support.AbstractJobExplorerFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 1201, "bug_title": "Listener Annotations do not allow parameters to be subtypes of expected types", "bug_description": "Listener Annotations don&apos;t allow parameters to be subtypes of expected types.  For example, consider the AfterProcess method.  Its enum value in StepListenerMetaData is defined as: \nAFTER_PROCESS(\"afterProcess\", \"after-process-method\", AfterProcess.class, ItemProcessListener.class, Object.class, Object.class).\nshowing that it expects two Objects as parameters.  However, if the method is defined with subtypes of Object as its parameters, then an error is shown indicating the the signature is incompatible with what is expected:\n@AfterProcess\npublic void afterProcess(String item, String result){ }", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC3", "fixed_version": "2.0.0.RELEASE", "fixed_files": ["org.springframework.batch.support.MethodInvokerUtils.java", "org.springframework.batch.core.listener.StepListenerFactoryBeanTests.java"], "label": 1, "es_results": []}, {"bug_id": 1172, "bug_title": "Register RangeArrayPropertyEditor automatically", "bug_description": "RangeArrayPropertyEditor could be registered automatically when someone use the new namespace configuration.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC2", "fixed_version": "2.0.1.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.CoreNamespaceUtils.java", "org.springframework.batch.core.configuration.xml.TopLevelStepParser.java", "org.springframework.batch.core.configuration.xml.JobParser.java"], "label": 1, "es_results": []}, {"bug_id": 1205, "bug_title": "When readCount % commitInterval == 0, commitCount is one more than it should be", "bug_description": "When readCount % commitInterval == 0, commitCount is one more than it should be.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.0.1.RELEASE", "fixed_files": ["org.springframework.batch.core.step.tasklet.TaskletStepTests.java"], "label": 1, "es_results": []}, {"bug_id": 1208, "bug_title": "late-binding not being resolved in <list/>", "bug_description": "When late-binding is used within a <list/>, the expressions are not being resolved.\n<beans:bean class=\"org.springframework.batch.core.resource.ListPreparedStatementSetter\" scope=\"step\">\n    <beans:property name=\"parameters\">\n        <beans:list>\n            <beans:value>\"#\n{jobParameters[id1]}\n\"</beans:value>\n            <beans:value>\"#\n{jobParameters[id2]}\n\"</beans:value>\n   . . .", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.0.1.RELEASE", "fixed_files": ["org.springframework.batch.core.scope.util.PlaceholderTargetSourceTests.java", "org.springframework.batch.core.scope.util.PlaceholderTargetSourceErrorTests.java", "org.springframework.batch.core.resource.ListPreparedStatementSetterTests.java", "org.springframework.batch.core.scope.util.PlaceholderTargetSource.java"], "label": 1, "es_results": []}, {"bug_id": 1218, "bug_title": "item streams will not get registered if ItemStream reader is used with step", "bug_description": "I tried to use following configuration for a step:\n\t\t\t\t<bean id=\"myStep\" parent=\"skipLimitStep\">\n\t\t\t\t\t<property name=\"itemReader\" ref=\"mergingReader\" />\n\t\t\t\t\t...\n\t\t\t\t\t<property name=\"streams\">\n\t\t\t\t\t\t<list>\n\t\t\t\t\t\t\t<ref bean=\"file321Reader\"/>\n\t\t\t\t\t\t\t<ref bean=\"file324Reader\"/>\n\t\t\t\t\t\t\t<ref bean=\"mergingReader\" />\n\t\t\t\t\t\t</list>\n\t\t\t\t\t</property>\nmergingReader merges data from two FlatFileItemReader delegates. This reader needs to be stateful - have to implement ItemStream to allow restart. When the step is started (FaultTolerantStepFactoryBean) it correctly registers those readers but after that it automatically registers mergingReader (as it is reader which implements ItemStream) and replaces the stream definition so that only mergingReader&apos;s open() is called. \nI believe the problem is in FaultTolerantStepFactoryBean.registerStreams() method.\nMaybe something like this would help (ChunkMonitor.getItemStream() would have to be implemented):\nif (chunkMonitor.getItemStream() != null) {\n    composite.register(chunkMonitor.getItemStream());\n}\nchunkMonitor.setItemStream(composite);\n... grr, now i see, this would call the auto-registered reader&apos;s open() method twice, huh, at least I tried \n", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.0.1.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.ChunkMonitorTests.java", "org.springframework.batch.core.step.item.ChunkMonitor.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBean.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanTests.java"], "label": 1, "es_results": []}, {"bug_id": 1255, "bug_title": "Proxy with no target cannot be analysed for listener interfaces", "bug_description": "Proxy with no target cannot be analysed for listener interfaces.  Various NullPointerExceptions ensue in STepListenerFactoryBean and MethodInvokerUtils.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.0.1.RELEASE", "fixed_files": ["org.springframework.batch.support.MethodInvokerUtils.java", "org.springframework.batch.core.listener.StepListenerFactoryBeanTests.java", "org.springframework.batch.core.listener.AbstractListenerFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 1284, "bug_title": "Partition Step Stop is incorrectly setting the BatchStatus to COMPLETED.", "bug_description": "Please refer to the forum reference.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.0.2.RELEASE", "fixed_files": ["org.springframework.batch.core.partition.support.PartitionStepTests.java", "org.springframework.batch.core.step.AbstractStep.java"], "label": 1, "es_results": []}, {"bug_id": 1232, "bug_title": "Sybase 12.5 compatiblity when writing to the spring batch context tables", "bug_description": "In version 2.0.0, I am facing an error when persisting to the context tables (BATCH_JOB_EXECUTION_CONTEXT in specific) on the sybase version 12.5. The class JdbcExecutionContextDao shows that when the field SERIALIZED_CONTEXT is set to null, it is set with type 2005 (Types.CLOB). This type is not supported on sybase 12.5", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.0.2.RELEASE", "fixed_files": ["org.springframework.batch.core.repository.dao.JdbcExecutionContextDao.java", "org.springframework.batch.core.repository.support.JobRepositoryFactoryBeanTests.java", "org.springframework.batch.core.repository.support.JobRepositoryFactoryBean.java", "org.springframework.batch.core.repository.dao.AbstractJdbcBatchMetadataDao.java"], "label": 1, "es_results": []}, {"bug_id": 1304, "bug_title": "Filter counter not incremented whenever there is a skip", "bug_description": "The filterCount in StepExecution is not incremented properly whenever a skip (Exception) happens in the same chuck. See the forum link for details.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.0.2.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantChunkProcessorTests.java", "org.springframework.batch.core.step.item.FaultTolerantChunkProcessor.java", "org.springframework.batch.core.step.item.SimpleChunkProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1311, "bug_title": "SimpleJobExplorer should return null when a StepExecution cannot be found", "bug_description": "SimpleJobExplorer should return null when a StepExecution cannot be found - it needs to check the JobExecution first before sending it to the repository.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.0.2.RELEASE", "fixed_files": ["org.springframework.batch.core.exlore.support.SimpleJobExplorerTests.java", "org.springframework.batch.core.explore.support.SimpleJobExplorer.java"], "label": 1, "es_results": []}, {"bug_id": 1341, "bug_title": "DataSourceInitializer throws ArrayOutOfBoundException when any destroyScript is specified", "bug_description": "basically, this is a typo:\n91:\t\tfor (int i = 0; i < destroyScripts.length; i++) {\n92:\t\t\tResource destroyScript = initScripts[i];", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RC2", "fixed_version": "2.0.3.RELEASE", "fixed_files": ["org.springframework.batch.test.DataSourceInitializer.java"], "label": 1, "es_results": []}, {"bug_id": 1363, "bug_title": "Job stopped in split state does not finish with status = STOPPED", "bug_description": "The analysis on the forum post isn&apos;t quite correct because SimpleFlow is used even for sequential executions.  The problem lies in the SplitState: it needs to unwrap ExecutionExceptions and re-throw their because.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.0.3.RELEASE", "fixed_files": ["org.springframework.batch.core.job.flow.FlowJobTests.java", "org.springframework.batch.core.job.flow.support.SimpleFlow.java", "org.springframework.batch.core.job.flow.support.state.SplitState.java", "org.springframework.batch.core.job.flow.support.state.SplitStateTests.java"], "label": 1, "es_results": []}, {"bug_id": 1390, "bug_title": "ExecutionContextPromotionListener erases previous step", "bug_description": "ExecutionContextPromotionListener erases previous step.  It should check for the existence of each value by key before promting, otherwise it will erase information from previous steps.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.1.0.M1", "fixed_files": ["org.springframework.batch.core.listener.ExecutionContextPromotionListener.java", "org.springframework.batch.core.listener.ExecutionContextPromotionListenerTests.java"], "label": 1, "es_results": []}, {"bug_id": 1526, "bug_title": "Memory leak in web deployments because ThreadLocal is not nulled out in ChunkMonitor", "bug_description": "The Tomcat leak detection in 6.0.24 caught this one: it&apos;s a tiny leak but the thread local in ChunkMonitor is not nulled out so re-deployment of a web application can result in a memory leak.  Tomcat fixes it (from 6.0.24) so it&apos;s probably not a big deal for existing users.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.1.1.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.ChunkMonitor.java"], "label": 1, "es_results": []}, {"bug_id": 1602, "bug_title": "Empty string JobParameter would be re-hydrated as null by Oracle", "bug_description": "Empty string JobParameter would be re-hydrated as null by Oracle.  Probably not a very common scenario, but it&apos;s a feature of Oracle that isn&apos;t taken into account in the JdbcJobInstanceDao.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.core.repository.dao.JdbcJobInstanceDaoTests.java", "org.springframework.batch.core.repository.dao.JdbcJobInstanceDao.java", "org.springframework.batch.core.repository.dao.AbstractJobInstanceDaoTests.java"], "label": 1, "es_results": []}, {"bug_id": 1601, "bug_title": "The \"initialized\" field in org.springframework.batch.test.DataSourceInitializer should not be static.", "bug_description": "The initialized field in org.springframework.batch.test.DataSourceInitializer is declared as:\nprivate static boolean initialized = false;\nThis shouldn&apos;t be static as this causes issues when the DataSourceInitializer class is used more than once for initializing more than one DataSources.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.test.DataSourceInitializer.java"], "label": 1, "es_results": []}, {"bug_id": 1709, "bug_title": "BeanWrapperFieldSetMapper race condition in cache", "bug_description": "BeanWrapperFieldSetMapper occasionally fails in a concurrent setting with strange messages about duplicate properties and the distance limit, even if the column names are exact.  Must be a concurrency bug in the caching.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.0.RELEASE", "fixed_version": "2.1.7.RELEASE", "fixed_files": ["org.springframework.batch.item.file.mapping.BeanWrapperFieldSetMapperTests.java", "org.springframework.batch.item.file.mapping.BeanWrapperFieldSetMapper.java"], "label": 1, "es_results": []}, {"bug_id": 1264, "bug_title": "NPE in StepParserStepFactoryBean#configureTaskletStep() #289 when omitting \"isolation\" for <transaction-attributes>", "bug_description": "this is changed behaviour compared to 2.0.0, where the following job definition worked fine:\n <batch:job id=\"indexUpdater\">\n        <batch:step id=\"updateIndexes\">\n            <batch:tasklet ref=\"indexUpdaterTasklet\">\n                <batch:transaction-attributes propagation=\"NEVER\"/>\n            </batch:tasklet>\n        </batch:step>\n    </batch:job>\nafter changing to   <batch:transaction-attributes isolation=\"DEFAULT\" propagation=\"NEVER\"/> it worked again", "project": "Spring", "sub_project": "BATCH", "version": "2.0.1.RELEASE", "fixed_version": "2.0.2.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepParserStepFactoryBean.java", "org.springframework.batch.core.configuration.xml.StepParserStepFactoryBeanTests.java"], "label": 1, "es_results": []}, {"bug_id": 1285, "bug_title": "Raise an exception if a step cannot be reached.", "bug_description": "If a <job/> is defined with a <step/> that cannot be reached during the flow of the job, then an exception should be raised by the parser.\nFor example, the following will raise an exception because \"stepB\" can never be executed.\n\n\n\n\n\n\n<job id=\"job\">\n\n\n\n\n    <step id=\"stepA\"/>\n\n\n\n\n    <step id=\"stepB\"/>\n\n\n\n\n</job>\n\n\n\n\n\n", "project": "Spring", "sub_project": "BATCH", "version": "2.0.1.RELEASE", "fixed_version": "2.0.2.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.JobParserTests.java", "org.springframework.batch.core.configuration.xml.StepParserTests.java", "org.springframework.batch.core.configuration.xml.FlowParser.java"], "label": 1, "es_results": []}, {"bug_id": 1289, "bug_title": "Null pointer in CoreNamespaceUtils.rangeArrayEditorAlreadyDefined()", "bug_description": "A NullPointerException is thrown from CoreNamespaceUtils.rangeArrayEditorAlreadyDefined() if there is a custom editor configurer without a customEditor property because of the line:\n\n\n\n\n\n\nMap editors = (Map) bd.getPropertyValues().getPropertyValue(\"customEditors\").getValue();\n\n\n\n\n\n", "project": "Spring", "sub_project": "BATCH", "version": "2.0.1.RELEASE", "fixed_version": "2.0.2.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.CoreNamespaceUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1301, "bug_title": "ItemStream is not being opened correctly for multi-threaded Step when scope=\"step\"", "bug_description": "The following job definition works however if I change the reader to scope=\"step\" I will get an exception about the stream never being opened (from FlatFileItemReader.doRead()). It appears doOpen() is called at least once however I believe its a different instance than what is actually used for read() because when read() is called the reader is not initialized.\n            <job id=\"search-job\" restartable=\"true\">\n\t\t<step id=\"search-job-step\">\n\t\t\t<tasklet transaction-manager=\"transactionManager\" allow-start-if-complete=\"true\">\n\t\t\t\t<chunk reader=\"itemReader\" writer=\"itemWriter\" commit-interval=\"1\" task-executor=\"executor\" >\n\t\t\t\t\t<streams> <!-- tried with and without this -->\n\t\t\t\t\t\t<stream ref=\"itemReader\" />\n\t\t\t\t\t</streams>\n\t\t\t\t</chunk>\n\t\t\t</tasklet>\n\t\t</step>\n\t</job>\n\t<b:bean id=\"itemReader\" class=\"org.springframework.batch.item.file.FlatFileItemReader\">\n\t\t<b:property name=\"resource\" value=\"file:input.csv\" />\n\t\t<b:property name=\"lineMapper\">\n\t\t\t<b:bean class=\"org.springframework.batch.item.file.mapping.DefaultLineMapper\">\n\t\t\t\t<b:property name=\"lineTokenizer\">\n\t\t\t\t\t<b:bean class=\"org.springframework.batch.item.file.transform.DelimitedLineTokenizer\">\n\t\t\t\t\t\t<b:property name=\"names\" value=\"col1,col2,col3\" />\n\t\t\t\t\t</b:bean>\n\t\t\t\t</b:property>\n\t\t\t\t<b:property name=\"fieldSetMapper\">\n\t\t\t\t\t<b:bean class=\"com.company.prj.batch.ItemFieldSetMapper\" />\n\t\t\t\t</b:property>\n\t\t\t</b:bean>\n\t\t</b:property>\n\t</b:bean>\n\t<b:bean id=\"itemWriter\" class=\"org.springframework.batch.item.file.FlatFileItemWriter\">\n\t\t<b:property name=\"resource\" value=\"file:searchoutput.txt\" />\n\t\t<b:property name=\"lineAggregator\">\n\t\t\t<b:bean class=\"com.company.prj.batch.SearchResultsAggregator\" />\n\t\t</b:property>\n\t</b:bean>", "project": "Spring", "sub_project": "BATCH", "version": "2.0.1.RELEASE", "fixed_version": "2.0.2.RELEASE", "fixed_files": ["org.springframework.batch.core.scope.AsyncStepScopeIntegrationTests.java", "org.springframework.batch.core.scope.context.StepSynchronizationManager.java", "org.springframework.batch.core.scope.context.StepContextRepeatCallback.java", "org.springframework.batch.core.scope.context.StepSynchronizationManagerTests.java"], "label": 1, "es_results": []}, {"bug_id": 1313, "bug_title": "loopFlowSample's LimitDecider returns \"COMPLETE\" instead of \"COMPLETED\"", "bug_description": "", "project": "Spring", "sub_project": "BATCH", "version": "2.0.1.RELEASE", "fixed_version": "2.0.2.RELEASE", "fixed_files": ["org.springframework.batch.sample.loopFlow.LimitDecider.java", "org.springframework.batch.sample.AbstractBatchLauncherTests.java", "org.springframework.batch.sample.TradeJobFunctionalTests.java"], "label": 1, "es_results": []}, {"bug_id": 1314, "bug_title": "FFIW in tradeJob is pointing to classpath instead of the target", "bug_description": "", "project": "Spring", "sub_project": "BATCH", "version": "2.0.1.RELEASE", "fixed_version": "2.0.2.RELEASE", "fixed_files": ["org.springframework.batch.sample.loopFlow.LimitDecider.java", "org.springframework.batch.sample.AbstractBatchLauncherTests.java", "org.springframework.batch.sample.TradeJobFunctionalTests.java"], "label": 1, "es_results": []}, {"bug_id": 1345, "bug_title": "Fix error message for when <tasklet/> has no ref= or <chunk/>", "bug_description": "", "project": "Spring", "sub_project": "BATCH", "version": "2.0.2.RELEASE", "fixed_version": "2.0.3.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.AbstractStepParser.java"], "label": 1, "es_results": []}, {"bug_id": 1351, "bug_title": "An empty <*-exception-classes/> list does not override parent's list", "bug_description": "An empty <*-exception-classes/> list does not override parent&apos;s list.  This probably applies to other lists (listeners, streams, ...) as well.  This prevents a step or job from being about to inherit from a parent and remove its registered exception classes.\nFor example, the following results in step \"B\" having java.lang.Exception registered as skippable even though it specifies <skippable-exception-classes/> with merge=\"false\" (the default).\n\n\n\n\n\n\n<step id=\"A\">\n\n\n\n\n    <tasklet>\n\n\n\n\n        <chunk>\n\n\n\n\n            <skippable-exception-classes>\n\n\n\n\n                java.lang.Exception\n\n\n\n\n            </skippable-exception-classes>\n\n\n\n\n        </chunk>\n\n\n\n\n    </tasklet>\n\n\n\n\n</step>\n\n\n\n\n\n\n\n\n\n<step id=\"B\" parent=\"A\">\n\n\n\n\n    <tasklet>\n\n\n\n\n        <chunk>\n\n\n\n\n            <skippable-exception-classes/>\n\n\n\n\n        </chunk>\n\n\n\n\n    </tasklet>\n\n\n\n\n</step>\n\n\n\n\n\n", "project": "Spring", "sub_project": "BATCH", "version": "2.0.2.RELEASE", "fixed_version": "2.0.3.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepParserTests.java", "org.springframework.batch.core.configuration.xml.JobParserTests.java", "org.springframework.batch.core.configuration.xml.AbstractStepParser.java"], "label": 1, "es_results": []}, {"bug_id": 1354, "bug_title": "Infinite loop caused by throwing an Error from the ItemWriter of a skippable step", "bug_description": "If a java.lang.Error is thrown from the ItemWriter of a step with skipLimit > 0, then the framework falls into an infinite loop.  It appears that an Error thrown misses the catch block that increments the skip counter, but it still causes the step to skip.  So the effect is that we skip continuously.\nThe desired effect is that an Error is thrown out of the job and never skips.\nThis was discovered when using jMock since jMock throws an ExpectationError when an expectation fails.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.2.RELEASE", "fixed_version": "2.0.3.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanRollbackTests.java", "org.springframework.batch.core.step.item.FaultTolerantChunkProcessorTests.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBean.java", "org.springframework.batch.core.step.tasklet.TaskletStepTests.java", "org.springframework.batch.core.step.item.FaultTolerantChunkProcessor.java", "org.springframework.batch.core.step.item.AbstractExceptionThrowingItemHandlerStub.java"], "label": 1, "es_results": []}, {"bug_id": 1378, "bug_title": "Late binding of parameters in map value only happens once per ApplicationContext", "bug_description": "See thorough description in the forum, which also have the related classes, configuration and log attached.\nIn essence the binding of the parameters only happens once, so that any subsequent executions will run with stale values.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.2.RELEASE", "fixed_version": "2.0.3.RELEASE", "fixed_files": ["org.springframework.batch.core.scope.util.MultipleContextPlaceholderTargetSourceTests.java", "org.springframework.batch.core.scope.util.PlaceholderTargetSource.java"], "label": 1, "es_results": []}, {"bug_id": 1397, "bug_title": "Late binding only happens once per ApplicationContext if expression is in substring", "bug_description": "Late binding only happens once per ApplicationContext if expression is in substring (c.f. BATCH-1378).  Still slightly broken: e.g. value=\"#jobParameters[foo]\" works fine but value=\"foo-#jobParameters[foo]\" does not.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.2.RELEASE", "fixed_version": "2.1.0.M1", "fixed_files": ["org.springframework.batch.core.scope.util.MultipleContextPlaceholderTargetSourceTests.java", "org.springframework.batch.core.scope.util.PlaceholderTargetSource.java"], "label": 1, "es_results": []}, {"bug_id": 1392, "bug_title": "Throttle limit is not parsed in ChunkElementParser", "bug_description": "Spring Batch 2.0.3 has added the property throttle-limit property to the chunk element in the spring-batch-2.0.xsd\nHowever, ChunkElementParser does not parse this property at all.\nFix :\nIn method parse() of class ChunkElementParser  add the following lines:\nString throttleLimit = element.getAttribute(\"throttle-limit\");\nif (StringUtils.hasText(throttleLimit)) {\n       propertyValues.addPropertyValue(\"throttleLimit\", throttleLimit);\n}", "project": "Spring", "sub_project": "BATCH", "version": "2.0.3.RELEASE", "fixed_version": "2.1.0.M1", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepParserTests.java", "org.springframework.batch.core.configuration.xml.AbstractStepParser.java", "org.springframework.batch.core.configuration.xml.StepParserStepFactoryBeanTests.java"], "label": 1, "es_results": []}, {"bug_id": 1417, "bug_title": "Error in FlatFileItemReader when RecordSeparatorPolicy.preProcess or readLine returns null", "bug_description": "We think there&apos;s a bug in RecordSeparatorPolicy handling in FlatFileItemReader when RecordSeparatorPolicy .preProcess or readLine returns null :\n\nreturning null from recordSeparatorPolicy.preProcess is not handled correctly by FlatFileItemReader.\nalso on the same line, if readLine() returns null, FlatFileItemReader handles it incorrectly and we obtain a \"null\" string.\n\nCode in error ;\nwhile (line != null && !recordSeparatorPolicy.isEndOfRecord(record)) {\n   record = recordSeparatorPolicy.preProcess(record) + (line = readLine());\n}\nIf isEndOfRecord returns true, record can be \"null\" string.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.3.RELEASE", "fixed_version": "2.1.0.M1", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemReaderTests.java", "org.springframework.batch.item.file.FlatFileItemReader.java"], "label": 1, "es_results": []}, {"bug_id": 1423, "bug_title": "Upon job restart, step with FlatFileItemReader does not honor skippable-exception-classes", "bug_description": "Cloned from: BATCH-1418 to allow this to be closed for 2.0.4.\nUpon restarting a failed step, the reader (FlatFileItemReader) tries to parse the file from the beginning as it tries to reach the point from where it has to pick up after the last failure. Along the way, the reader invokes the lineTokenizer and the fieldSetMapper for each of the already processed records. This appears to be far too much work for no gain when the point is to quickly get to the last record read from the file that was successfully committed. \nMore importantly, during this recovery phase, the parsing exercise triggers parsing exceptions but the batch framework doesn&apos;t honor the \"skippable-exception-classes\" instruction. This causes exceptions (that should be skipped) to become fatal and the job fails to restart.\nMore information is available in the \"Spring Forum Reference\" thread.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.3.RELEASE", "fixed_version": "2.0.4.RELEASE", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemReader.java"], "label": 1, "es_results": []}, {"bug_id": 1418, "bug_title": "Upon job restart, step with FlatFileItemReader does not honor skippable-exception-classes", "bug_description": "Upon restarting a failed step, the reader (FlatFileItemReader) tries to parse the file from the beginning as it tries to reach the point from where it has to pick up after the last failure. Along the way, the reader invokes the lineTokenizer and the fieldSetMapper for each of the already processed records. This appears to be far too much work for no gain when the point is to quickly get to the last record read from the file that was successfully committed. \nMore importantly, during this recovery phase, the parsing exercise triggers parsing exceptions but the batch framework doesn&apos;t honor the \"skippable-exception-classes\" instruction. This causes exceptions (that should be skipped) to become fatal and the job fails to restart.\nMore information is available in the \"Spring Forum Reference\" thread.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.3.RELEASE", "fixed_version": "2.1.0.RC1", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemReader.java", "org.springframework.batch.item.xml.StaxEventItemReader.java", "org.springframework.batch.item.database.HibernateCursorItemReader.java"], "label": 1, "es_results": []}, {"bug_id": 1547, "bug_title": "ExecutionContextPromotionListener strict flag misinterpreted in listener code", "bug_description": "See BATCH-1309 for the original issue.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.3.RELEASE", "fixed_version": "2.1.1.RELEASE", "fixed_files": ["org.springframework.batch.core.listener.ExecutionContextPromotionListener.java", "org.springframework.batch.core.listener.ExecutionContextPromotionListenerTests.java"], "label": 1, "es_results": []}, {"bug_id": 1444, "bug_title": "ChunkMonitor warning message about stream state is inaccurate", "bug_description": "ChunkMonitor warning message about stream state is inaccurate: it warns the user if a multi-threaded access to ItemStream data is detected, and it checks for a null stream, but the stream is a composite and it is always null.  Should check for a registered stream inside the composite.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.4.RELEASE", "fixed_version": "2.1.0.M3", "fixed_files": ["org.springframework.batch.core.step.item.ChunkMonitorTests.java", "org.springframework.batch.core.step.item.ChunkMonitor.java"], "label": 1, "es_results": []}, {"bug_id": 1453, "bug_title": "OraclePagingQueryProvider generates wrong queries for pages", "bug_description": "OraclePagingQueryProvider generates wrong queries for pages.  It should use a sub select to select the whole range first, e.g. SELECT * FROM (SELECT ... WHERE ...) WHERE ROWNUM<=100.", "project": "Spring", "sub_project": "BATCH", "version": "2.0.4.RELEASE", "fixed_version": "2.1.0.M3", "fixed_files": ["org.springframework.batch.item.database.support.SqlPagingQueryUtilsTests.java", "org.springframework.batch.item.database.support.SqlPagingQueryUtils.java", "org.springframework.batch.item.database.support.OraclePagingQueryProviderTests.java"], "label": 1, "es_results": []}, {"bug_id": 1499, "bug_title": "SqlServerPagingQueryProvider needs an alias in the jump to subquery", "bug_description": "SqlServerPagingQueryProvider needs an alias in the jump to subquery.  Derby already has one; maybe Sybase needs it (someone will have to tell us if so).", "project": "Spring", "sub_project": "BATCH", "version": "2.0.4.RELEASE", "fixed_version": "2.1.0.RELEASE", "fixed_files": ["org.springframework.batch.item.database.support.SqlServerPagingQueryProvider.java", "org.springframework.batch.item.database.support.SqlServerPagingQueryProviderTests.java"], "label": 1, "es_results": []}, {"bug_id": 1497, "bug_title": "SqlServerPagingQueryProvider should use an alias for the inner query in a jump-to-item query", "bug_description": "", "project": "Spring", "sub_project": "BATCH", "version": "2.0.4.RELEASE", "fixed_version": "2.1.0.RELEASE", "fixed_files": ["org.springframework.batch.item.database.support.SqlServerPagingQueryProvider.java", "org.springframework.batch.item.database.support.SqlServerPagingQueryProviderTests.java"], "label": 1, "es_results": []}, {"bug_id": 1503, "bug_title": "JobExecution marked COMPLETE on failure to save step execution metadata", "bug_description": "Exception occurs in the writer, there&apos;s a failure in persisting step context- the job status is set to COMPLETE instead of FAILED.\nSteps to create\n\nChange the batch schema EXIT_MESSAGE VARCHAR(2500) to EXIT_MESSAGE VARCHAR(25).\nForce the writer to throw some exception.\nError occurs while persisting the step context but the job context gets updated to COMPLETE.\n\n", "project": "Spring", "sub_project": "BATCH", "version": "2.0.4.RELEASE", "fixed_version": "2.1.1.RELEASE", "fixed_files": ["org.springframework.batch.core.job.flow.support.state.EndState.java", "org.springframework.batch.core.step.item.TaskletStepExceptionTests.java", "org.springframework.batch.core.job.SimpleJobFailureTests.java"], "label": 1, "es_results": []}, {"bug_id": 1471, "bug_title": "Typo in FaultTolerantStepFactoryBean ", "bug_description": "\n\n\n\n\n\n18:56:34.578 [main] WARN  o.s.b.c.s.i.FaultTolerantStepFactoryBean - Synchronous TaskExecutor detected (class org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor) with ItemStream reader.  This is probably an error, and may lead to incorrect restart data being stored.\n\n\n\n\n\n\nShould read Asynchronous TaskExecutor detected", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.M4", "fixed_version": "2.1.0.RC1", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantStepFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 1476, "bug_title": "Filter counts too high when write skips happen", "bug_description": "Filter counts look too high when write skips happen", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.M4", "fixed_version": "2.1.0.RC1", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanRollbackTests.java", "org.springframework.batch.core.step.item.FaultTolerantChunkProcessorTests.java", "org.springframework.batch.core.step.item.FaultTolerantChunkProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1477, "bug_title": "Allow excludes (as well as includes) in retryable exceptions", "bug_description": "Allow excludes (as well as includes) in retryable exceptions", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.M4", "fixed_version": "2.1.0.RC1", "fixed_files": ["org.springframework.batch.core.configuration.xml.ChunkElementParserTests.java"], "label": 1, "es_results": []}, {"bug_id": 1490, "bug_title": "StaxEventItemWriter outputs invalid xml if step handling is failed and retried when handling the first chunk of data", "bug_description": "When reader fails handling the data of the first chunk defined by step commitInterval and the step is retried the writer starts the writing from wrong place in xml output file. The JUnit testcase to repeat the problem can be found from the Spring Batch forum post (http://forum.springsource.org/showthread.php?t=71239).", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RC1", "fixed_version": "2.1.0.RELEASE", "fixed_files": ["org.springframework.batch.item.xml.StaxEventItemWriter.java", "org.springframework.batch.item.xml.StaxEventItemWriterTests.java"], "label": 1, "es_results": []}, {"bug_id": 1498, "bug_title": "JdbcPagingItemReader does not apply parameter values correctly on restart", "bug_description": "JdbcPagingItemReader does not apply parameter values correctly on restart", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RC1", "fixed_version": "2.1.0.RELEASE", "fixed_files": ["org.springframework.batch.item.database.IbatisPagingItemReaderParameterTests.java", "org.springframework.batch.item.database.AbstractPagingItemReaderParameterTests.java", "org.springframework.batch.item.database.JpaPagingItemReaderNativeQueryIntegrationTests.java", "org.springframework.batch.item.database.JdbcPagingItemReaderClassicParameterTests.java", "org.springframework.batch.item.database.JpaPagingItemReaderParameterTests.java", "org.springframework.batch.item.database.JdbcPagingItemReaderNamedParameterTests.java", "org.springframework.batch.item.database.JdbcPagingItemReader.java", "org.springframework.batch.core.configuration.xml.AbstractStepParser.java", "org.springframework.batch.core.configuration.xml.PartitionStepParserTests.java"], "label": 1, "es_results": []}, {"bug_id": 1502, "bug_title": "HibernatePagingItemReader does not close sessions", "bug_description": "HibernatePagingItemReader is exhausting DB connections by never closing sessions.\nIt appears to open a new session for each chunk in &apos;createQuery&apos;, overwriting the previous in &apos;statefulSession&apos;, but never closes it.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RC1", "fixed_version": "2.1.0.RELEASE", "fixed_files": ["org.springframework.batch.item.database.HibernateItemReaderHelper.java"], "label": 1, "es_results": []}, {"bug_id": 1507, "bug_title": "FlowJob.getStep() only looks at state names, not step names", "bug_description": "FlowJob.getStep() only looks at state names, not step names, so anyone using the StepLocator interface is in for a surprise.  The state name happens to be the same as the step name in the existing uint tests (irony), but when created by the XML namespace parsers they are different.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.1.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepWithSimpleTaskJobParserTests.java", "org.springframework.batch.core.configuration.xml.StepNameTests.java", "org.springframework.batch.core.job.flow.FlowJobTests.java", "org.springframework.batch.core.job.flow.FlowJob.java"], "label": 1, "es_results": []}, {"bug_id": 1513, "bug_title": "HibernateItemReaderHelper requires queryProvider field to be an instance of AbstractHibernateQueryProvider", "bug_description": "HibernateItemReaderHelper.afterPropertiesSet() method contains the following code:\nAssert.state(queryProvider instanceof AbstractHibernateQueryProvider,\n\t\t\t\t\t\"Hibernate query provider must be set\");\nI think this is not needed as the queryProvider field is not cast to AbstractHibernateQueryProvider within HibernateItemReaderHelper.\nAlso if query provider defined as a scoped bean:\n<bean id=\"myQueryProvider\" class=\"MyQueryProvider\" scope=\"step\" /> \nproxy is not an instance of AbstractHibernateQueryProvider even though MyQueryProvider extends AbstractHibernateQueryProvider   ", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.1.RELEASE", "fixed_files": ["org.springframework.batch.item.database.HibernateItemReaderHelper.java"], "label": 1, "es_results": []}, {"bug_id": 1510, "bug_title": "List of stepnames incomplete for nested flow job", "bug_description": "The function  getStepNames on a job returns only those step names from a FlowJob, whose steps are defined directly within the flow.\nBut if the job contains nested flows the list of stepnames is incomplete. Only those of the outermost flow (the job itself) are returned but none of the nested flow(s).\nIf the job does only reference sub flows the list will be empty at all.\nThe function should return the complete list of steps, including nested flows within subflows (recursively)", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.1.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepNameTests.java", "org.springframework.batch.core.configuration.xml.SimpleFlowFactoryBean.java", "org.springframework.batch.core.job.flow.FlowJobTests.java", "org.springframework.batch.core.job.flow.FlowJob.java", "org.springframework.batch.core.job.flow.support.state.FlowState.java", "org.springframework.batch.core.job.flow.support.state.SplitState.java", "org.springframework.batch.core.configuration.xml.SplitJobParserTests.java"], "label": 1, "es_results": []}, {"bug_id": 1525, "bug_title": "ExitStatus description can be null when re-hyrated from  Oracle", "bug_description": "The equals implementation (or the toString) of ExitStatus may not work legitimately in all cases.\nIn Oracle, an empty String is considered as null and fails our build because we assert the exit status matched the expected (that is, a given exit code and an empty exit description). \nWe got this\njava.lang.AssertionError: expected:<exitCode=COMPLETED WITH ERROR;exitDescription=> but was:<exitCode=COMPLETED WITH ERROR;exitDescription=null>\nIt&apos;s not a big deal and we can deal with that in our test utilities but if the equals supported that use case, that would be great since we don&apos;t really want to put a &apos;null&apos; exit description. Besides the default constructor does this\npublic ExitStatus(String exitCode) {\n   this(exitCode, \"\");\n}\n", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.1.RELEASE", "fixed_files": ["org.springframework.batch.core.ExitStatusTests.java", "org.springframework.batch.core.ExitStatus.java"], "label": 1, "es_results": []}, {"bug_id": 1545, "bug_title": "FlatFileItemWriter logs as JdbcBatchItemWriter", "bug_description": "Updating your log configuration (log4j.properties) to see logs specifically from org.springframework.batch.item.file.FlatFileItemWriter does not work because that class incorrectly identifies itself as org.springframework.batch.item.database.JdbcBatchItemWriter.\nA patch is attached, which has been verified with a full maven build against the trunk.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.1.RELEASE", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemWriter.java"], "label": 1, "es_results": []}, {"bug_id": 1551, "bug_title": "Db2PagingQueryProvider needs an alias in the jump to subquery ", "bug_description": "same Problem as in BATCH-1499 but for DB2", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.1.RELEASE", "fixed_files": ["org.springframework.batch.item.database.support.Db2PagingQueryProviderTests.java", "org.springframework.batch.item.database.support.Db2PagingQueryProvider.java"], "label": 1, "es_results": []}, {"bug_id": 1574, "bug_title": "TaskExecutor configuration ignored in 2.1 namespace for <tasklet/> with no <chunk/>", "bug_description": "The task-executor attribute moved in the schema and the factory bean didn&apos;t accommodate the change.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.2.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepParserStepFactoryBeanTests.java", "org.springframework.batch.core.configuration.xml.StepParserStepFactoryBean.java", "org.springframework.batch.core.step.tasklet.TaskletStep.java"], "label": 1, "es_results": []}, {"bug_id": 1543, "bug_title": "OrderedComposite cannot register two items with the same order", "bug_description": "We have added a custom listener that extends StepListener and OrderedComposite is very handy to use when multiple listeners are involved. Unfortunately, it&apos;s not public. \nCan you make it public or part of the API?", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.2.RELEASE", "fixed_files": ["org.springframework.batch.core.listener.OrderedCompositeTests.java", "org.springframework.batch.core.listener.OrderedComposite.java"], "label": 1, "es_results": []}, {"bug_id": 1597, "bug_title": "DirectPoller only works with timeout in milliseconds", "bug_description": "DirectPoller only works with timeout in milliseconds.  Doesn&apos;t have any effect on mainstream Batch use, only if the poller is used and only if the timeout is not in millisecs.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.poller.DirectPoller.java", "org.springframework.batch.poller.DirectPollerTests.java"], "label": 1, "es_results": []}, {"bug_id": 1620, "bug_title": "FlowStep never fails", "bug_description": "", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.support.transaction.TransactionAwareMapFactoryTests.java", "org.springframework.batch.core.step.tasklet.TaskletStepTests.java", "org.springframework.batch.core.partition.support.SimpleStepExecutionSplitter.java", "org.springframework.batch.support.transaction.TransactionAwareProxyFactory.java", "org.springframework.batch.core.repository.dao.MapStepExecutionDao.java", "org.springframework.batch.core.job.flow.FlowStepTests.java", "org.springframework.batch.core.job.flow.FlowStep.java"], "label": 1, "es_results": []}, {"bug_id": 1598, "bug_title": "JobRepositoryTestUtils delete job execution fails if there is another execution with the same job instance", "bug_description": "JobRepositoryTestUtils delete job execution fails if there is another execution with the same job instance", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.test.JobRepositoryTestUtilsTests.java", "org.springframework.batch.test.JobRepositoryTestUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1632, "bug_title": "DefaultFieldSet#readBigDecimal(String, BigDecimal) and NumberFormatException", "bug_description": "Here the code of DefaultFieldSet#readBigDecimal(String, BigDecimal):\n\n\n\n\n\n\n466\t        public BigDecimal readBigDecimal(String name, BigDecimal defaultValue) {\n\n\n\n\n467\t                try {\n\n\n\n\n468\t                        return readBigDecimal(indexOf(name), defaultValue);\n\n\n\n\n469\t                }\n\n\n\n\n470\t                catch (IllegalArgumentException e) {\n\n\n\n\n471\t                        throw new IllegalArgumentException(e.getMessage() + \", name: [\" + name + \"]\");\n\n\n\n\n472\t                }\n\n\n\n\n473\t        }\n\n\n\n\n\n\nThe problem is that a NumberFormatException is also an IllegalArgumentException.\nSo at this point, we can not make the difference between between indexOf failure and BigDecimal conversion failure.\nThe fix:\n\n\n\n\n\n\n466\t        public BigDecimal readBigDecimal(String name, BigDecimal defaultValue) {\n\n\n\n\n467\t                try {\n\n\n\n\n468\t                        return readBigDecimal(indexOf(name), defaultValue);\n\n\n\n\n469\t                }\n\n\n\n\n470\t                catch (NumberFormatException e) {\n\n\n\n\n471\t                        throw new NumberFormatException(e.getMessage() + \", name: [\" + name + \"]\");\n\n\n\n\n472\t                }\n\n\n\n\n473\t                catch (IllegalArgumentException e) {\n\n\n\n\n474\t                        throw new IllegalArgumentException(e.getMessage() + \", name: [\" + name + \"]\");\n\n\n\n\n475\t                }\n\n\n\n\n476\t        }\n\n\n\n\n\n\n", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.4.RELEASE", "fixed_files": ["org.springframework.batch.item.file.transform.DefaultFieldSet.java"], "label": 1, "es_results": []}, {"bug_id": 1640, "bug_title": "File writers do not behave correctly on rollback", "bug_description": "File writers do not behave correctly on rollback.  It doesn&apos;t seem to affect regular users of FlatFileItemWriter (and XML) but if you write one item at a time instead of all at once in a transaction which rolls back, then no data are ever written by the writer.\nThe bug is in TransactionAwareBufferedWriter which fails to clear its transaction resource on rollback.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.4.RELEASE", "fixed_files": ["org.springframework.batch.support.transaction.TransactionAwareBufferedWriterTests.java", "org.springframework.batch.support.transaction.TransactionAwareBufferedWriter.java"], "label": 1, "es_results": []}, {"bug_id": 1717, "bug_title": "Failure in RetryPolicy leads to infinite loop in Step", "bug_description": "Failure in RetryPolicy leads to infinite loop in Step.  Really this is a corner case because none of the retry policies supplied by the framework should have this problem, but one provided by a user might.  The problem arises if a RetryPolicy throws an exception which according to it&apos;s own rules in retryable. In particular the problem is if registerThrowable() itself fails.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.7.RELEASE", "fixed_files": ["org.springframework.batch.retry.support.RetryTemplate.java"], "label": 1, "es_results": []}, {"bug_id": 1725, "bug_title": "SubclassClassifier should use ConcurrentHashMap", "bug_description": "SubclassClassifier should use ConcurrentHashMap, otherwise there can be concurrent modification exceptions when it is used concurrently.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.8.RELEASE", "fixed_files": ["org.springframework.batch.classify.SubclassClassifier.java"], "label": 1, "es_results": []}, {"bug_id": 1727, "bug_title": "Child contexts created by AutomaticJobRegistrar cannot easily use PropertyPlaceholderConfigurer", "bug_description": "See also BATCHADM-110.  ClassPathXmlApplicationContextFactory registers the parent PPC as a singleton (internal) BFPP using addBeanFactoryPostProcessor(), and those BFPP take precedence over those defined using bean definitions.   A workaround is to use SpEL instead of PPC in the child context.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.0.RELEASE", "fixed_version": "2.1.8.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.support.ClassPathXmlApplicationContextFactoryTests.java", "org.springframework.batch.core.configuration.support.ClassPathXmlApplicationContextFactory.java"], "label": 1, "es_results": []}, {"bug_id": 1569, "bug_title": "MultiResourceItemReader.getCurrentResource because java.lang.ArrayIndexOutOfBoundsException when .read() was not called", "bug_description": "Only after first .read(), variable currentResource got a value greather or equal to zero(0). So if stream was openned, or not read yet, one activation on getCurrentResource method generates ArrayIndexOutOfBoundsException.\nBelow a proposed fix:\n\tpublic Resource getCurrentResource() {\n\t\tif (currentResource >= resources.length ) \n{ //proposed: || currentResource  < 0\n\t\t\treturn null;\n\t\t}\n\t\treturn resources[currentResource];\n\t}", "project": "Spring", "sub_project": "BATCH", "version": "2.1.1.RELEASE", "fixed_version": "2.1.2.RELEASE", "fixed_files": ["org.springframework.batch.item.file.MultiResourceItemReaderIntegrationTests.java", "org.springframework.batch.item.file.MultiResourceItemReader.java"], "label": 1, "es_results": []}, {"bug_id": 1573, "bug_title": "End transition states will cause the batch job to finish with an Unknown status if the namespace prefix is used.", "bug_description": "If you run the following job:\n<batch:job id=\"job1\">\n\t\t<batch:step id=\"noopStep\" parent=\"noopStep1\">\n            <batch:next on=\"COMPLETED\" to=\"step4\" />\n            <batch:end on=\"NOOP\" />\n            <batch:fail on=\"*\" />\n        </batch:step>\n        <batch:step id=\"step4\" parent=\"step45\" />\n\t</batch:job>\n    <batch:step id=\"noopStep1\">\n        <batch:tasklet ref=\"noopTasklet\" />\n    </batch:step>\nAnd the first step returns an exit status of NOOP, the job will finish with a BatchStatus of UNKNOWN, and an ExitStatus of NOOP.  If you remove the batch namespace from the element, it will complete with a BatchStatus of COMPLETED and an Exit status of the same.\nThis is because in AbstractFlowParser.getBatchStatusFromEndTransitionName() line 393, the element name is checked to see if it is an &apos;End transition&apos;.  However, this check doesn&apos;t strip out the batch: from the front of the element first.\nI have attached a failing unit test as well.  ", "project": "Spring", "sub_project": "BATCH", "version": "2.1.1.RELEASE", "fixed_version": "2.1.2.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.AbstractFlowParser.java"], "label": 1, "es_results": []}, {"bug_id": 1582, "bug_title": "DefaultStepExecutionAggregator can simply ignore null or empty input", "bug_description": "Current implementation of DefaultStepExecutionAggregator validates the arguments and restricts null or empty sets of executions. In my opinion the validation is too strict and prohibit usage of DefaultStepExecutionAggregator in various cases when using namespace support for partitions (there is no possibility to set the aggregator when using batch namespace). I have implementation of the partitioner that ignores gridSize and in certain occasions return empty map of executions. I believe the framework should be able to deal with such situations.\nThe strict validation seems odd to me also in context of other default components provided by the framework to deal with partitioning. For instance, TaskExecutorPartitionHandler can process empty collections of executions returned by the splitter. SimpleStepExecutionSplitter is also fine.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.1.RELEASE", "fixed_version": "2.1.2.RELEASE", "fixed_files": ["org.springframework.batch.core.partition.support.DefaultStepExecutionAggregator.java", "org.springframework.batch.core.partition.support.DefaultStepExecutionAggregatorTests.java"], "label": 1, "es_results": []}, {"bug_id": 1587, "bug_title": "DefaultFieldSetFactory is not setting the numberFormat in the enhance() call", "bug_description": "enhance is testing for (numberFormat!=null) but then calling fieldSet.setDateFormat(dateFormat);\n\tprivate FieldSet enhance(DefaultFieldSet fieldSet) {\n\t\tif (dateFormat!=null) \n{\n\t\t\tfieldSet.setDateFormat(dateFormat);\n\t\t}\n\t\tif (numberFormat!=null) {\t\t\tfieldSet.setDateFormat(dateFormat);\t\t}\n \n\t\treturn fieldSet;\n\t}", "project": "Spring", "sub_project": "BATCH", "version": "2.1.1.RELEASE", "fixed_version": "2.1.2.RELEASE", "fixed_files": ["org.springframework.batch.item.file.transform.DefaultFieldSetFactory.java"], "label": 1, "es_results": []}, {"bug_id": 1588, "bug_title": "Job Excecution Listener - XML Namespace parsing fails for methods named different to \"beforeJob\", \"afterJob\"", "bug_description": "The following XML configuration for a job listener does not work as the two configured methods are not called:\n-------- XML - Configuration --------\n<job id=\"...\">\n  ...\n  <listener ref=\"pojoListener\" before-job-method=\"before\" after-job-method=\"after\"/>\n</job>\n<bean id=\"pojoListener\" class=\"com.example.MyListener\"/>\n--------\nMyListener is a simple POJO and does not implement JobExecutionListener and has no annotated methods either.\nI&apos;ve debugged the problem and it seems the offending code is in the private method\nAbstractListenerParser#getMethodNameAttributes\nThat method returns \n{\"beforeJob\", \"afterJob\"}\n instead of the XML attribute names \n{\"before-job-method\", \"after-job-method\"}\n.\nThe unit test \"JobExecutionListenerParserTests\" does not seem to test this case.\nI suspect that the same problem exists for Step Listeners as they share the same code base.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.1.RELEASE", "fixed_version": "2.1.2.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.TestListener.java", "org.springframework.batch.core.configuration.xml.AbstractListenerParser.java"], "label": 1, "es_results": []}, {"bug_id": 1594, "bug_title": "StepListenerSupport implements method onErrorInStep which is not declared in any of the implemented interfaces", "bug_description": "The onErrorInStep method is not declared in any of the listener interfaces, nor is it called as far as I can see from the code executing steps. onErrorInStep was removed from the StepExecutionListener in changeset 2338 and jira BATCH-825.\nWhile the onErrorInStep method in StepListenerSupport will not because problems directly, it is misleading for developers extending StepListenerSupport.\n(When editing the StepListenerSupport, you might want to update the non-javadoc referencing StepListener to reference StepExecutionListener instead as well)", "project": "Spring", "sub_project": "BATCH", "version": "2.1.1.RELEASE", "fixed_version": "2.1.2.RELEASE", "fixed_files": ["org.springframework.batch.sample.common.StagingItemWriter.java", "org.springframework.batch.core.step.NonAbstractStepTests.java", "org.springframework.batch.sample.common.StagingItemReader.java", "org.springframework.batch.core.listener.MulticasterBatchListenerTests.java", "org.springframework.batch.core.listener.StepListenerSupport.java"], "label": 1, "es_results": []}, {"bug_id": 1600, "bug_title": "CommandLineJobRunner cannot stop a Job execution that was restarted", "bug_description": "Mode execution restarting is not stopped with mode execution stop:\nThe error is this line from the file:\n\nClass: org.springframework.batch.core.launch.support. CommandLineJobRunner\nmethod: private List<JobExecution> getJobExecutionsWithStatusGreaterThan(String jobIdentifier, BatchStatus minStatus){}.\nline: 344: JobExecution jobExecution = jobExecutions.get(jobExecutions.size() - 1);\n\nPossible fix:\n\nline: 344: JobExecution jobExecution = jobExecutions.get(0);\n\n", "project": "Spring", "sub_project": "BATCH", "version": "2.1.1.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.core.launch.support.CommandLineJobRunnerTests.java", "org.springframework.batch.core.launch.support.CommandLineJobRunner.java"], "label": 1, "es_results": []}, {"bug_id": 1616, "bug_title": "A custom partitioner no longer restart the job properly upon failure", "bug_description": "The changes in BATCH-1531 introduced a regression. If the partitions are not named partition0, partition1, partition2, Spring Batch considers that the execution does not exist and start a fresh new instance without calling the partitioner, which fails since the partitioner sets mandatory value for the partition to run properly.\nFor the record we use the following\npublic static final String PARTITION_PREFIX = \"partition-\";\n// ...\nfinal String partitionNumberFormat = \"%0\" + String.valueOf(gridSize).length() + \"d\";\n// for partition i\nfinal String partitionName = PARTITION_PREFIX + String.format(partitionNumberFormat, i);", "project": "Spring", "sub_project": "BATCH", "version": "2.1.1.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.core.partition.support.SimpleStepExecutionSplitter.java", "org.springframework.batch.core.partition.support.SimpleStepExecutionSplitterTests.java", "org.springframework.batch.core.partition.support.PartitionStepTests.java"], "label": 1, "es_results": []}, {"bug_id": 1605, "bug_title": "HippyMethodInvoker candidate arguments repeated", "bug_description": "I&apos;m trying out the PropertyExtractingDelegatingItemWriter, and I&apos;m seeing that if the arguments are not of the exact types as the targetMethod, then HippyMethodInvoker#findMatchingMethod is called.\nWhen finding candidate arguments, the findMatchingMethod method will populate all fields with the first possible match.\nThe following is the sample I&apos;m using:\n\n\n\n\n\n\n<bean id=\"adaptedItemWriter\" class=\"org.springframework.batch.item.adapter.PropertyExtractingDelegatingItemWriter\">\n\n\n\n\n    <property name=\"targetObject\" ref=\"myWriter\" />\n\n\n\n\n    <property name=\"targetMethod\" value=\"doIt\" />\n\n\n\n\n    <property name=\"fieldsUsedAsTargetMethodArguments\">\n\n\n\n\n        <list>\n\n\n\n\n            <value>firstName</value>\n\n\n\n\n            <value>lastName</value>\n\n\n\n\n            <value>network</value>\n\n\n\n\n        </list>\n\n\n\n\n    </property>\n\n\n\n\n</bean>\n\n\n\n\n\t\n\n\n\n\n<bean id=\"myWriter\" class=\"com.mycom.writers.MyWriter\"/>\n\n\n\n\n\n\nThe item being passed has three properties: firstName, lastName, and network. All are Strings. In order to make MyWriter more reusable, I had the doIt(...) method use Object for one of the parameters:\n\n\n\n\n\n\npublic class MyWriter {\n\n\n\n\n\tpublic void doIt(String a, String b, Object c) {\n\n\n\n\n\t\tSystem.out.println(a + \", \" + b + \", and \" + c); \n\n\n\n\n\t}\n\n\n\n\n}\n\n\n\n\n\n\nIf the \"c\" argument is a String \"public void doIt(String a, String b, String c)\", then the output is as expected (i.e., firstName, lastName and network are printed out). When \"c\" is an Object \"public void doIt(String a, String b, Object c)\", then HippyMethodInvoker tries to find a method and selects candidate arguments. All three candidate arguments are the first value passed in (i.e., the result is firstName, firstName and firstName printed out).\nAny ideas on how to use more generic argument types? ", "project": "Spring", "sub_project": "BATCH", "version": "2.1.1.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.item.adapter.HippyMethodInvokerTests.java", "org.springframework.batch.item.adapter.HippyMethodInvoker.java"], "label": 1, "es_results": []}, {"bug_id": 1572, "bug_title": "Step not failing on org.springframework.transaction.UnexpectedRollbackException", "bug_description": "The transaction is timing out for a read by the reader in a step due to a lock. The transaction manager marks the out come of the transaction to be rollback only. Once the lock is released and the reader returns  a org.springframework.transaction.UnexpectedRollbackException is thrown while updating the chunk to database. Batch is considering this exception to be a non fatal exception and is continuing with the next chunk just by logging the exception at a debug level. Instead ity should fail the step I guess.\nProblem : I am loosing all the records in that chunk. \nI changed the org.springframework.batch.core.step.item.SimpleRetryExceptionHandler.java&apos;s constructor like \npublic SimpleRetryExceptionHandler(RetryPolicy retryPolicy, ExceptionHandler exceptionHandler, Collection<Class<? extends Throwable>> fatalExceptionClasses) \n{\n\t\tthis.retryPolicy = retryPolicy;\n\t\tthis.exceptionHandler = exceptionHandler;\n\t\tfatalExceptionClasses.add(org.springframework.transaction.UnexpectedRollbackException.class);\n\t\tthis.fatalExceptionClassifier = new BinaryExceptionClassifier(fatalExceptionClasses);\n\t}\n\nand tested. Now it is failing the step.\n", "project": "Spring", "sub_project": "BATCH", "version": "2.1.1.RELEASE", "fixed_version": "2.1.4.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.TaskletStepExceptionTests.java", "org.springframework.batch.core.step.tasklet.TaskletStep.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBean.java", "org.springframework.batch.core.Entity.java", "org.springframework.batch.core.repository.dao.MapStepExecutionDao.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanRollbackTests.java", "org.springframework.batch.core.step.AbstractStep.java"], "label": 1, "es_results": []}, {"bug_id": 1618, "bug_title": "MultiResourceItemWriter creates an empty file if the number of item to write is a multiple of itemCountLimitPerResource", "bug_description": "MultiResourceItemWriter creates files on the filesystem when setting the resource to the delegate. If the number of items to write is a multiple of itemCountLimitPerResource, the last file will be created, but nothing will be written into it.\nThe attached patch defers the file creation in the write method.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.2.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.item.file.MultiResourceItemWriter.java"], "label": 1, "es_results": []}, {"bug_id": 1623, "bug_title": "A chunk configured with processor-transactional=\"true\" should not require a retry- or skip-limit", "bug_description": "At the moment when you configure processor-transactional=\"true\" on a chunk the framework&apos;s validation forces you to also specify a retry-limit or skip-limit. First of all, when you specify a <retry-policy> element this requirement still exists, even though the policy overrules the configured retry-limit, so that&apos;s an error in the validation (StepParserStepFactoryBean.java:466). \nSecond of all, it might not really be necessary to enforce this requirement at all.\nFrom the brief private discussion on Skype with Dave about this:\n\n\n\n\n\n\n[12:33:45 PM] David Syer: I guess it makes sense that processor-transactional=\"true\" requires one or the other.  But maybe we should lift that restriction too (allow the middleware to deal with retry)?\n\n\n\n\n[12:35:20 PM] Joris Kuipers: could be, but wouldn&apos;t that only be the case if your ItemReader was transactional as well? I cannot easily imagine a scenario where a non-transactional reader would be followed by a transactional processor where middleware would take care of retries for the processor...\n\n\n\n\n[12:35:46 PM] Joris Kuipers: but maybe I haven&apos;t thought enough about what (non-)transactional processors really because the framework to do\n\n\n\n\n[12:36:09 PM] Joris Kuipers: it&apos;s just a way to prevent caching the items, right?\n\n\n\n\n[12:36:26 PM] Joris Kuipers: since a rollback on error won&apos;t lose items for tx-al processing\n\n\n\n\n[12:38:27 PM] David Syer: Right, I was mixing it up with reader-transactional\n\n\n\n\n[12:38:34 PM] David Syer: So there&apos;s no obvious link to retry.\n\n\n\n\n[12:38:55 PM] David Syer: You could set it true if you want and if there is no retry there is no re-processing either, but it&apos;s harmless.\n\n\n\n\n[12:39:18 PM] Joris Kuipers: that&apos;s what I thought as well, yes\n\n\n\n\n[12:39:41 PM] David Syer: OK.  Mention that in the JIRA and I&apos;ll remove the restriction completely.\n\n\n\n\n\n", "project": "Spring", "sub_project": "BATCH", "version": "2.1.2.RELEASE", "fixed_version": "2.1.3.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepParserStepFactoryBean.java", "org.springframework.batch.core.configuration.xml.ChunkElementParserTests.java"], "label": 1, "es_results": []}, {"bug_id": 1629, "bug_title": "FaultTolerantChunkProcessor contains dangerous log statements", "bug_description": "I&apos;ve just profiled an application that is making an extensive use of Spring Batch. And believe the biggest performance hog was....... \nthe StringBuilder conversions in FaultTolerantChunkProcessor class. There are couple of log statements there that do this:\nlogger.debug(\"Attempting to write: \" + inputs);\nwithout \nif(logger.isDebugEnabled()).\nI mean, guys, come on, inputs can be pretty wacky. In this case it was a list of Hibernate entities containing a lot of properties and all of them are in the toString() implementation. This \"innocent\" logging statement was the biggest performance hog in the app. \nIt probably would make sense to do an audit and see if there are any other places where such debug statement are not safe. I suppose I could do that. Would you take a patch?", "project": "Spring", "sub_project": "BATCH", "version": "2.1.3.RELEASE", "fixed_version": "2.1.4.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantChunkProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1633, "bug_title": "Dependency injection problem with step scoped anonymous inner bean", "bug_description": "XML spring context sample :\n    <bean id=\"john\" class=\"java.lang.String\">\n        <constructor-arg index=\"0\" value=\"John\"/>\n    </bean>\n    <bean id=\"jane\" class=\"java.lang.String\">\n        <constructor-arg index=\"0\" value=\"Jane\"/>\n    </bean>\n    <batch:job id=\"firstJob\" job-repository=\"jobRepository\">\n        <batch:step id=\"firstJobFirstStep\" next=\"firstJobSecondStep\">\n            <batch:tasklet>\n                <bean class=\"spring.batch.test.InnerBeanStepScopedTest$Hello\" scope=\"step\">\n                    <property name=\"name\" ref=\"jane\"/>\n                </bean>\n            </batch:tasklet>\n        </batch:step>\n        <batch:step id=\"firstJobSecondStep\">\n            <batch:tasklet>\n                <bean class=\"spring.batch.test.InnerBeanStepScopedTest$Hello\" scope=\"step\">\n                    <property name=\"name\" ref=\"john\"/>\n                </bean>\n            </batch:tasklet>\n        </batch:step>\n    </batch:job>\nThe output produced is:\n    19:08:03.885 [main] INFO  s.b.t.InnerBeanStepScopedTest$Hello - Hello John!\n    19:08:03.975 [main] INFO  s.b.t.InnerBeanStepScopedTest$Hello - Hello John!\nNow, the question is: where is Jane? :o)\nIf the two tasklets are unscoped (just remove scope=\"step\") then the ouput is:\n    19:13:20.801 [main] INFO  s.b.t.InnerBeanStepScopedTest$Hello - Hello Jane!\n    19:13:20.921 [main] INFO  s.b.t.InnerBeanStepScopedTest$Hello - Hello John!\nYes, Jan is back!\nSo, dependency injection fails on step scoped inner anonymous beans. \nWorkaround: name inner beans with an id attribute... but it&apos;s boring (me).", "project": "Spring", "sub_project": "BATCH", "version": "2.1.3.RELEASE", "fixed_version": "2.1.4.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.TaskletParser.java", "org.springframework.batch.core.configuration.xml.ChunkElementParser.java", "org.springframework.batch.core.configuration.xml.AbstractListenerParser.java"], "label": 1, "es_results": []}, {"bug_id": 1639, "bug_title": "Oracle jumpToItemQuery needs a tweak (again)", "bug_description": "OraclePagingQueryProvider.generateJumpToItemQuery generates an incorrect query, see BATCHADM-74.  It doesn&apos;t affect Batch users much, but it has a n impact on Spring Batch Admin with Oracle.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.3.RELEASE", "fixed_version": "2.1.4.RELEASE", "fixed_files": ["org.springframework.batch.item.database.support.SqlPagingQueryUtilsTests.java", "org.springframework.batch.item.database.JdbcPagingItemReader.java", "org.springframework.batch.item.database.support.SqlPagingQueryUtils.java", "org.springframework.batch.item.database.support.OraclePagingQueryProviderTests.java"], "label": 1, "es_results": []}, {"bug_id": 1643, "bug_title": "Unpredictable binding in BeanWrapperFieldSetMapper because of \"fuzzy\" property matching", "bug_description": "This bug has two distinct symptoms, due to the permissiveness of the fuzzy matching in BeanWrapperFieldSetMapper#findPropertyName().  See the attached Unit test and included javadocs.  Basically, the fuzzy matching can cause multiple columns of the input to match the same target bean property, thus wrongly failing to throw NotWritablePropertyException in some cases, and this also makes it unpredictable which competing \n{@link FieldSet}\n value will actually be set into the target bean property.\nAs a minimal work-around it would be nice to be able to turn off fuzzy matching (e.g. by exposing distanceLimit as a configurable property).  I would also suggest  defaulting its value to 0 instead of 5, as it is pretty dangerous.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.3.RELEASE", "fixed_version": "2.1.4.RELEASE", "fixed_files": ["org.springframework.batch.item.file.mapping.BeanWrapperFieldSetMapperTests.java", "org.springframework.batch.item.file.mapping.PropertyMatches.java", "org.springframework.batch.item.file.mapping.BeanWrapperFieldSetMapper.java"], "label": 1, "es_results": []}, {"bug_id": 1635, "bug_title": "Spring Batch and Hibernate Search do not work together", "bug_description": "We are having a problem when using spring batch and hibernate search together.   Basically when hibernate search needs to lazy load a entity using indexing (which occurs during the commit processing) we get a lazy load exception (org.hibernate.LazyInitializationException: could not initialize proxy - no Session).\nI&apos;m pretty sure this problem is caused by the call to hibernateTemplate.clear() in HibernateItemWriter:\n\tpublic final void write(List<? extends T> items) {\n\t\tdoWrite(hibernateTemplate, items);\n\t\ttry \n{\n\t\t\thibernateTemplate.flush();\n\t\t}\n\t\tfinally \n{\n\t\t\t// This should happen when the transaction commits anyway, but to be\n\t\t\t// sure...\n\t\t\thibernateTemplate.clear();\n\t\t}\n\t}\nAs the affect of this is also to clear the session on any hibernate proxies.  \nAs the comment says, the clear should not be needed and I would suggest either removing it or at least making it configurable.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.3.RELEASE", "fixed_version": "2.1.6.RELEASE", "fixed_files": ["org.springframework.batch.item.database.JpaItemWriter.java", "org.springframework.batch.item.database.HibernateItemWriterTests.java", "org.springframework.batch.item.database.HibernateItemWriter.java", "org.springframework.batch.item.database.JpaItemWriterTests.java"], "label": 1, "es_results": []}, {"bug_id": 1707, "bug_title": "MapJobInstanceDao.getJobInstances(String jobName, int start, int count) does not work", "bug_description": "In the code of MapJobInstanceDao :\npublic List<JobInstance> getJobInstances(String jobName, int start, int count) {\n\t...\n\treturn result.subList(start, count); // ERROR : should be result.subList(start, start+count) because subList parameters are fromIndex, toIndex\n}\nWORKAROUND = define a subclass with a correct implementation for this method :\npublic class MyMapJobInstanceDao extends MapJobInstanceDao {\n\t@Override\n\tpublic List<JobInstance> getJobInstances(String jobName, int start, int count) \n{\n\t\tint nbJobs = getJobNames().size();\n\t\tList<JobInstance> instances = super.getJobInstances(jobName, 0, nbJobs);\n\t\treturn instances.subList(start, Math.min(nbJobs, start+count));\n\t}\n}\nand in the applicationContext, use the new class instead of the spring one", "project": "Spring", "sub_project": "BATCH", "version": "2.1.3.RELEASE", "fixed_version": "2.1.7.RELEASE", "fixed_files": ["org.springframework.batch.core.repository.dao.MapJobInstanceDao.java", "org.springframework.batch.core.repository.dao.AbstractJobInstanceDaoTests.java"], "label": 1, "es_results": []}, {"bug_id": 1671, "bug_title": "static methods are not public in ExecutionContextTestUtils", "bug_description": "Currently all methods in org.springframework.batch.test.ExecutionContextTestUtils are not public(default/package scope now).\nCan you make them public, so that test class can have access to them.\nThanks,", "project": "Spring", "sub_project": "BATCH", "version": "2.1.4.RELEASE", "fixed_version": "2.1.6.RELEASE", "fixed_files": ["org.springframework.batch.test.ExecutionContextTestUtils.java"], "label": 1, "es_results": []}, {"bug_id": 1656, "bug_title": "Infinite loop on no-rollback-for exception when skipLimit is reached due to exception in ItemProcessor", "bug_description": "If java.lang.Exception is configured in the \"no-rollback-exception-classes\" this leads to an infinite loop when the skipLimit is reached, caused by an exception thrown in the ItemProcessor. \nExceptions thrown in the writer are handled  just fine. \nIt seems like this was introduced in 2.1 only (tested with 2.1.1 and 2.1.5) as it is working fine in 2.0.1. \n", "project": "Spring", "sub_project": "BATCH", "version": "2.1.5.RELEASE", "fixed_version": "2.1.6.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanRollbackTests.java", "org.springframework.batch.core.test.step.FaultTolerantStepFactoryBeanRollbackIntegrationTests.java", "org.springframework.batch.core.step.item.FaultTolerantChunkProcessor.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 1670, "bug_title": "Nested splits lead to invalid flow definition", "bug_description": "", "project": "Spring", "sub_project": "BATCH", "version": "2.1.5.RELEASE", "fixed_version": "2.1.6.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.SplitParser.java"], "label": 1, "es_results": []}, {"bug_id": 1681, "bug_title": "Restarting a job that generates XML output using StaxEventItemWriter with Woodstox fails", "bug_description": "When restarting a job that generates XML output using StaxEventItemWriter, the restart will fail if the Woodstox implementation of Stax is used. The problem is that by default, the Woodstox implementation of XMLEventWriter validates the XML structure as it is written. Since on a restart the root element is not written, when the second output item is written, the Woodstox implementation detects this as an invalid XML structure and throws:\njavax.xml.stream.XMLStreamException: Trying to output second root, <item>\nThe solution is to turn off structure validation in StaxEventItemWriter.open(long, boolean) in much the same way that the \"com.ctx.wstx.automaticEndElements\" feature is disabled:\nif (outputFactory.isPropertySupported(\"com.ctc.wstx.outputValidateStructure\")) {\n    outputFactory.setProperty(\"com.ctc.wstx.outputValidateStructure\", Boolean.FALSE);\n}\n", "project": "Spring", "sub_project": "BATCH", "version": "2.1.5.RELEASE", "fixed_version": "2.1.6.RELEASE", "fixed_files": ["org.springframework.batch.item.xml.StaxEventItemWriter.java", "org.springframework.batch.item.xml.StaxEventItemWriterTests.java"], "label": 1, "es_results": []}, {"bug_id": 1826, "bug_title": "Null pointer exception if optional parameter of type DATE is null", "bug_description": "Selecting some of the jobs in spring-batch-admin leads to the following exception:\n(Issue occurs when we use spring-batch-admin latest release version which internally uses spring-batch-core 2.1.5)\njava.lang.IllegalArgumentException: Cannot format given Object as a Date\n\tjava.text.DateFormat.format(DateFormat.java:301)\n\tjava.text.Format.format(Format.java:157)  \n        org.springframework.batch.core.converter.DefaultJobParametersConverter\n       .getProperties(DefaultJobParametersConverter.java:159)\n\torg.springframework.batch.admin.web.JobInstanceInfo.<init>(JobInstanceInfo.java:42)\n\torg.springframework.batch.admin.web.JobController.details(JobController.java:171)\nFound that the exception occurs in : DefaultJobParametersConverter.java line no:158 (in spring batch core v2.1.5)\nif (jobParameter.getType() == ParameterType.DATE) {\nresult.setProperty(key + DATE_TYPE, dateFormat.format(value));\n}\nwhen \"value\" is null.\nIdeally there should be check to ignore null valued parameters.\nThis can be fixed by :\n158 : if(value != null) \n{\n\n168 : }\n\nCan someone check this?\nThanks,\nGayathri", "project": "Spring", "sub_project": "BATCH", "version": "2.1.5.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.core.converter.DefaultJobParametersConverter.java", "org.springframework.batch.core.converter.DefaultJobParametersConverterTests.java"], "label": 1, "es_results": []}, {"bug_id": 1703, "bug_title": "MapStepExecutionDao does not add StepExecutions to a JobExecution correctly", "bug_description": "MapStepExecutionDao does not add StepExecutions to a JobExecution correctly.  Since the JobExecution has a hash-based collection of step executions, if you add more with the same ID they do not overwrite.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.6.RELEASE", "fixed_version": "2.1.8.RELEASE", "fixed_files": ["org.springframework.batch.core.repository.dao.MapStepExecutionDaoTests.java", "org.springframework.batch.core.JobExecution.java"], "label": 1, "es_results": []}, {"bug_id": 1849, "bug_title": "Item was not picked up after restarting a failed job!!!", "bug_description": "We have got 2 items needs to be processed by the job. Their keys are 3000000001659 and 3000000001661.\nPage size was set to 20 and commit-interval was set to 1 in the job configuration. At the first run, job failed but processed one of the items and second item rolledback due to RuntimeException. Again when we restarted the job, job reported as completed. We later realized the second item was not picked up by the job when we restarted it. We could see it in the batch tables.\nBelow are the entries from the batch_step_execution table for the both the failed and successful executions\nSTEP_EXECUTION_ID STATUS COMMIT_COUNT READ_COUNT WRITE_COUNT ROLLBACK_COUNT\n752510 FAILED 1 2 1 1\n752511 COMPLETED 1 0 0 0\nBelow are the entries from the batch_step_execution_context table for the both the failed and successful executions\nSTEP_EXECUTION_ID short_context\n752510 {\"map\":{\"entry\":[\n{\"string\":\"JdbcPagingItemReader.read.count\",\"int\": 1}\n,\n{\"string\":\"JdbcPagingItemReader.start.after\",\"l ong\":3000000001661}\n]}}\n752511 {\"map\":{\"entry\":[\n{\"string\":\"JdbcPagingItemReader.read.count\",\"int\": 2}\n,\n{\"string\":\"JdbcPagingItemReader.start.after\",\"l ong\":3000000001661}\n]}}\nI suspect the value for JdbcPagingItemReader.start.after should be 3000000001659 and not 3000000001661. But I am not sure.\nBut it works well if the size of commit-interval and page-size matches. I don&apos;t remember reading in doc that they should match.\nFollowing is the Job configuration but not a complete one\n<batch:job id=\"corporateActionEODJob\" parent=\"simpleJob\">\n<batch:step id=\"processCorporateActions\" parent=\"simpleStep\">\n<batch:tasklet>\n<batch:chunk reader =\"corporateActionEODItemReader\"\nwriter =\"corporateActionEODItemWriter\"\ncommit-interval=\"1\" />\n</batch:tasklet>\n</batch:step>\n</batch:job>\n<bean id=\"corporateActionEODItemReader\" class=\"org.springframework.batch.item.database.Jdb cPagingItemReader\">\n<property name=\"saveState\" value=\"true\"/>\n<property name=\"dataSource\" ref=\"dataSource\"/>\n<property name=\"pageSize\" value=\"20\"/>\n<property name=\"rowMapper\">\n<bean class=\"org.springframework.jdbc.core.simple.Parame terizedSingleColumnRowMapper\" factory-method=\"newInstance\">\n<constructor-arg>\n<null/>\n</constructor-arg>\n</bean>\n</property>\n<property name=\"queryProvider\">\n<bean class=\"org.springframework.batch.item.database.sup port.SqlPagingQueryProviderFactoryBean\">\n<property name=\"fromClause\" value=\"ca_corp_action ca\"/>\n<property name=\"selectClause\" value=\"ca.corp_action_id\"/>\n<property name=\"sortKey\" value=\"ca.corp_action_id\"/>\n<property name=\"whereClause\" value=\"ca.action_status = &apos;SCHEDULED&apos; and ca.effective_date = :businessDay\"/>\n</bean>\n</property>\n<property name=\"parameterValues\">\n<map>\n<entry key=\"businessDay\" value=\"2012-04-09\"/>\n</map>\n</property>\n</bean>\n<bean id=\"corporateActionEODItemWriter\" class=\"com.om.dh.batch.item.adapter.DelegatingItem WriterAdapterFactoryBean\">\n<property name=\"targetObject\" ref =\"com.dh.ca.services.BatchAdapterService\"/>\n<property name=\"targetMethod\" value=\"processAction\" />\n</bean>\nWe use SB version 2.1.6.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.6.RELEASE", "fixed_version": "2.2.1.RELEASE", "fixed_files": ["org.springframework.batch.item.database.JdbcPagingItemReader.java", "org.springframework.batch.item.database.AbstractDataSourceItemReaderIntegrationTests.java"], "label": 1, "es_results": []}, {"bug_id": 1742, "bug_title": "HippyMethodInvoker fails when target uses method overloading and there is no exact match for arguments", "bug_description": "See the failing test (testOverloadedMethodUsingInputWithoutExactMatch) at https://github.com/magott/spring-batch/blob/master/spring-batch-infrastructure/src/test/java/org/springframework/batch/item/adapter/HippyMethodInvokerTests.java\n The issue is that if you have an overloaded method where one of the methods is a match, but not an exact match (ie: foo(Set) and foo(List) with TreeSet being passed as the argument). An IllegalArgumentException is thrown.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.7.RELEASE", "fixed_version": "2.1.8.RELEASE", "fixed_files": ["org.springframework.batch.item.adapter.HippyMethodInvokerTests.java", "org.springframework.batch.item.adapter.HippyMethodInvoker.java"], "label": 1, "es_results": []}, {"bug_id": 1743, "bug_title": "Use step scope for PartitionHandler (so gridSize can be a job parameter) - broken in 2.1.7.", "bug_description": "Cloned from: BATCH-1612: Pull gridSize from job parameters\nhttps://jira.springsource.org/browse/BATCH-1612", "project": "Spring", "sub_project": "BATCH", "version": "2.1.7.RELEASE", "fixed_version": "2.1.8.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepParserStepFactoryBean.java", "org.springframework.batch.core.configuration.xml.StepParserStepFactoryBeanTests.java", "org.springframework.batch.core.partition.support.TaskExecutorPartitionHandler.java"], "label": 1, "es_results": []}, {"bug_id": 1739, "bug_title": "Inheriting from parent step with skip-limit/retry-limit causes IllegalArgumentException when the inheriting bean does not define exception-classes.", "bug_description": "The error for the supplied example job (needs infrastructure) is: \n\"IllegalArgumentException: The field &apos;skip-limit&apos; is not permitted on the step [myStep] because there is no &apos;skippable-exception-classes&apos;\"", "project": "Spring", "sub_project": "BATCH", "version": "2.1.7.RELEASE", "fixed_version": "2.1.8.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.ParentStepFactoryBeanParserTests.java", "org.springframework.batch.core.configuration.xml.ChunkElementParser.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanRollbackTests.java", "org.springframework.batch.core.configuration.xml.StepParserStepFactoryBeanTests.java", "org.springframework.batch.core.configuration.xml.ChunkElementParserTests.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanTests.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 1744, "bug_title": "Revert retry-limit and skip-limit changes from BATCH-1396.", "bug_description": "The changes in BATCH-1396 for retry-limit and skip-limit caused too many problems with step inheritance (parent=\"...\" in a step).  This task is to track the release of a new version that reverts those changes.  You can still do late binding of those values by injecting a retry-policy or skip-policy, and that might be as far as we ever go. ", "project": "Spring", "sub_project": "BATCH", "version": "2.1.7.RELEASE", "fixed_version": "2.1.8.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.ParentStepFactoryBeanParserTests.java", "org.springframework.batch.core.configuration.xml.ChunkElementParser.java"], "label": 1, "es_results": []}, {"bug_id": 1756, "bug_title": "Make  round-trip JobParameters->Properties->JobParameters work for double parameters", "bug_description": "Make  round-trip JobParameters->Properties->JobParameters work for double parameters. Here is the customer&apos;s use case for this - \"I want to use the JobOperator interface to start jobs and that requires passing the parameters in the string format. So I need a way to generate a correct string representation of JobParameters that will be accepted by JobOperator. I assumed DefaultJobParametersConverter is the way to go. But to be even more specific I&apos;m trying to build a mechanism for starting jobs in a clustered environment and the approach is to use JMS messages. The client would invoke an API (passing JobParameters) which behind the scenes will take the JobParameters, convert it to the String format and put it in a JMS messages that gets placed on a queue. On the other side of the queue a component receives the message, extracts the String of job parameters and calls JobOperator. I don&apos;t want to be responsible for creating the correct string format of JobParameters so I&apos;m counting on classes provided by the framework.\" \nCode snippet for demonstration:\n@Test\npublic void testDefaultJobParametersConverter() {\nDefaultJobParametersConverter converter = new DefaultJobParametersConverter();\nJobParametersBuilder builder = new JobParametersBuilder();\nDouble val = Double.valueOf(222);\nbuilder.addDouble(\"doubleParam\", val);\nJobParameters params = builder.toJobParameters();\nMap<String, JobParameter> map = params.getParameters();\nJobParameter jp = map.get(\"doubleParam\");\nassertNotNull(jp);\nassertEquals(ParameterType.DOUBLE, jp.getType());\nProperties props = converter.getProperties(params);\nparams = converter.getJobParameters(props);\nmap = params.getParameters();\njp = map.get(\"doubleParam\");\nassertNotNull(jp);\nassertEquals(ParameterType.DOUBLE, jp.getType());\nassertEquals(val, jp.getValue());\n}", "project": "Spring", "sub_project": "BATCH", "version": "2.1.7.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.core.converter.DefaultJobParametersConverter.java", "org.springframework.batch.core.converter.DefaultJobParametersConverterTests.java"], "label": 1, "es_results": []}, {"bug_id": 1753, "bug_title": "Problems With FlatFileItemWriter: error while trying to restart an execution ", "bug_description": "We are having problems with property \"shouldDeleteIfEmpty\" from FlatFileItemWriter.\nIf an error occurs during an execution, we are unable to restart application because the re-execution cannot find the files created at first time. For example:\nApplication functionality:\nWe have an application which receive an input file \"INPUT.TXT\". The invalid data from input file gets a \"rejected\" status and are recorded in a file named \"INPUT.TXT.REJ\". The valid data are processed and generate an output file \"OUTPUT.TXT\".\nBoth files, output and rejected, are generated by FlatFileItemWriter, and they have property \"shouldDeleteIfEmpty\" with value=true.\nError scenario:\nIf we didn&apos;t give reading permission and execute the application, it will not be able to read INPUT.TXT file. So an exception is thrown. Then we give reading permission, and try to re-execute the application. Another Exception is thrown because the application cannot read the output file or the rejected file that were created at first execution. The files were deleted because when the error occurred both files were empty.\nBecause of these problems we can&apos;t execute reprocessing. I believe the re-execution should be able to create a new file if it doesn&apos;t exists.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.7.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemWriterTests.java"], "label": 1, "es_results": []}, {"bug_id": 1775, "bug_title": "Inner beans of same type inside <chunk/> elements with scope =\"step\" leads to mistaken override of bean definitions", "bug_description": "I&apos;v the following job configuration: \n\n\n\n\n\n\n<batch:job id=\"file2fileJob\">\n\n\n\n\n\t<batch:step id=\"file2fileJobStep1\">\n\n\n\n\n\t\t<batch:tasklet>\n\n\n\n\n\t\t\t<batch:chunk commit-interval=\"1\" writer=\"itemToStringFlatFileItemWriter\">\n\n\n\n\n\t\t\t\t<batch:reader>\n\n\n\n\n\t\t\t\t\t<bean class=\"org.springframework.batch.item.file.FlatFileItemReader\" scope=\"step\">\n\n\n\n\n\t\t\t\t\t\t<property name=\"resource\" value=\"#{jobParameters[&apos;input.file.name&apos;]}\" />\n\n\n\n\n\t\t\t\t\t\t<property name=\"lineMapper\">\n\n\n\n\n\t\t\t\t\t\t\t<bean class=\"org.springframework.batch.item.file.mapping.PassThroughLineMapper\" />\n\n\n\n\n\t\t\t\t\t\t</property>\n\n\n\n\n\t\t\t\t\t</bean>\n\n\n\n\n\t\t\t\t</batch:reader>\n\n\n\n\n\t\t\t</batch:chunk>\n\n\n\n\n\t\t</batch:tasklet>\n\n\n\n\n\t</batch:step>\n\n\n\n\n</batch:job>\n\n\n\n\n\n\nand the following test: \n\n\n\n\n\n\n@Test\n\n\n\n\npublic void file2fileJob() throws Exception {\n\n\n\n\n\t/* setup */\n\n\n\n\n\tMap<String, JobParameter> parameters = new HashMap<String, JobParameter>();\n\n\n\n\n\tparameters.put(\"input.file.name\", new JobParameter(\"users.csv\"));\n\n\n\n\n\tFile output = testFolder.newFile(\"output.txt\");\n\n\n\n\n\toutput.createNewFile();\n\n\n\n\n\tparameters.put(\"output.file.name\", new JobParameter(\"file:\" + output.getAbsolutePath()));\n\n\n\n\n\t/* exercise */\n\n\n\n\n\tlauncher.run(file2fileJob, new JobParameters(parameters));\n\n\n\n\n\t/* verify */\n\n\n\n\n\tResource input = new ClassPathResource(\"users.csv\");\n\n\n\n\n\tassertEquals(\"Input and output should be equal\", FileUtils.readLines(input.getFile()), FileUtils.readLines(output));\n\n\n\n\n}\n\n\n\n\n\n\nThe test is successful. \nBut if I had the following job configuration:\n\n\n\n\n\n\n<batch:job id=\"file2DatabaseJob\">\n\n\n\n\n\t<batch:step id=\"file2DatabaseJobStep1\">\n\n\n\n\n\t\t<batch:tasklet>\n\n\n\n\n\t\t\t<batch:chunk commit-interval=\"1\" writer=\"itemToStringFlatFileItemWriter\">\n\n\n\n\n\t\t\t<!-- TODO - save the User in the database -->\n\n\n\n\n\t\t\t\t<batch:reader>\n\n\n\n\n\t\t\t\t\t<bean class=\"org.springframework.batch.item.file.FlatFileItemReader\" scope=\"step\">\n\n\n\n\n\t\t\t\t\t\t<property name=\"resource\" value=\"#{jobParameters[&apos;input.file.name&apos;]}\" />\n\n\n\n\n\t\t\t\t\t\t<property name=\"lineMapper\">\n\n\n\n\n\t\t\t\t\t\t\t<bean class=\"org.springframework.batch.item.file.mapping.DefaultLineMapper\">\n\n\n\n\n\t\t\t\t\t\t\t\t<property name=\"lineTokenizer\">\n\n\n\n\n\t\t\t\t\t\t\t\t\t<bean class=\"org.springframework.batch.item.file.transform.DelimitedLineTokenizer\" />\n\n\n\n\n\t\t\t\t\t\t\t\t</property>\n\n\n\n\n\t\t\t\t\t\t\t\t<property name=\"fieldSetMapper\">\n\n\n\n\n\t\t\t\t\t\t\t\t\t<bean class=\"org.springframework.batch.UserFieldSetMapper\" />\n\n\n\n\n\t\t\t\t\t\t\t\t</property>\n\n\n\n\n\t\t\t\t\t\t\t</bean>\n\n\n\n\n\t\t\t\t\t\t</property>\n\n\n\n\n\t\t\t\t\t</bean>\n\n\n\n\n\t\t\t\t</batch:reader>\n\n\n\n\n\t\t\t</batch:chunk>\n\n\n\n\n\t\t</batch:tasklet>\n\n\n\n\n\t</batch:step>\n\n\n\n\n</batch:job>\n\n\n\n\n\n\nthe same test fails because the &apos;file2fileJobStep1&apos; lineMapper is no more a PassThroughLineMapper but the same as the file2DatabaseJobStep1 lineMapper... \nIf I remove the scope attribute (and update the resource value to an hard-coded value) of the file2fileJobStep1 reader, the test is successful again.  ", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.InlineItemHandlerParserTests.java", "org.springframework.batch.core.configuration.xml.ChunkElementParser.java"], "label": 1, "es_results": []}, {"bug_id": 1783, "bug_title": "Throwing exceptions inside a ChunkListener results in endless loop", "bug_description": "If an exception is thrown in beforeChunk in a ChunkListener and the job is configured with skips (but not to skip the exception that is thrown from the ChunkListener) it will result in an endless loop. Same will happen if you have configured your job for retries.\nIf neither retry nor skip is configured, the job will end with ExitStatus.FAILED, as expected. Which makes me suspect the issue is in the exception handling in some of the components added when skip or retry comes into play.\nI&apos;ve attached two files based on the Spring Batch template from STS that can be dropped in to reproduce the issue.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanRollbackTests.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBean.java", "org.springframework.batch.core.step.item.SimpleStepFactoryBean.java"], "label": 1, "es_results": []}, {"bug_id": 1841, "bug_title": "Upgrading to spring batch 2.1.8 causes error in processing xml configuration", "bug_description": "We are just upgrading to 2.1.8 and our existing xml batch configuration will no longer load.  \nHere is a snippet of the configuration that is failing:\n\n\n\n\n\n\n    <bean id=\"simpleStep\" class=\"org.springframework.batch.core.step.item.FaultTolerantStepFactoryBean\"\n\n\n\n\n          abstract=\"true\">\n\n\n\n\n        <property name=\"transactionManager\" ref=\"transactionManager\"/>\n\n\n\n\n        <property name=\"jobRepository\" ref=\"jobRepository\"/>\n\n\n\n\n        <property name=\"startLimit\" value=\"100\"/>\n\n\n\n\n        <property name=\"commitInterval\" value=\"1\"/>\n\n\n\n\n        <property name=\"backOffPolicy\">\n\n\n\n\n            <bean class=\"org.springframework.batch.retry.backoff.ExponentialBackOffPolicy\">\n\n\n\n\n                <property name=\"initialInterval\" value=\"1000\"/>\n\n\n\n\n            </bean>\n\n\n\n\n        </property>\n\n\n\n\n        <property name=\"retryLimit\" value=\"5\"/>\n\n\n\n\n        <property name=\"retryableExceptionClasses\">\n\n\n\n\n            <map>\n\n\n\n\n                <entry key=\"org.springframework.dao.ConcurrencyFailureException\" value=\"true\"/>\n\n\n\n\n            </map>\n\n\n\n\n        </property>\n\n\n\n\n    </bean>\n\n\n\n\n\n\n\n\n\n    <step id=\"createCatalogueValidateStep\" parent=\"simpleStep\" next=\"createCataloguePostValidateStep\">\n\n\n\n\n         <tasklet transaction-manager=\"transactionManager\">\n\n\n\n\n             <chunk reader=\"csvStagedProductReader\" writer=\"hibernateStagedProductWriter\" commit-interval=\"10\"/>\n\n\n\n\n             <listeners>\n\n\n\n\n                 <listener ref=\"createCatalogueValidateItemListener\"/>\n\n\n\n\n             </listeners>\n\n\n\n\n         </tasklet>\n\n\n\n\n    </step>\n\n\n\n\n\n\nAnd we are getting the error:\nThe field &apos;retry-limit&apos; is not permitted on the step [createCatalogueValidateStep] because there is no &apos;retryable-exception-classes&apos;.\nWhen I debug the code, I can see that the StepParserStepFactoryBean has a retryLimit which it got from the parent bean but no retryableExceptionClasses.  Further investigation lead me to this code in ChunkElementParser:\n\n\n\n\n\n\n\t// Even if there is no retryLimit, we can still accept exception\n\n\n\n\n\t// classes for an abstract parent bean definition\n\n\n\n\n\tpropertyValues.addPropertyValue(\"retryableExceptionClasses\", retryableExceptions);\n\n\n\n\n\n\nThe problem is that this always sets the retryableExceptionClasses property even if it is not provided.  When the bean definitions are merged, the parent bean&apos;s definition of retryableExceptionClasses is overridden by an empty definition.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.ChunkElementParser.java", "org.springframework.batch.core.configuration.xml.ChunkElementParserTests.java"], "label": 1, "es_results": []}, {"bug_id": 1804, "bug_title": "Retry does not work if additional exception occurs in the ItemWriter during scan for failure", "bug_description": "I expect the configuration <chunk commit-interval=\"5\" retry-limit=\"5\" skip-limit=\"5\"> to be applied for both processor and writer. This does not seem to work as expected with the writer, where retry is non at all, if the writer runs in \"recoverer\".\nThe attachment contains a maven-project that demonstrates the issue:\n1. springbatch.test.components.batch.retry_in_writer.RetryInWriterTest\n2. springbatch.test.components.batch.retry_in_processor.RetryInProcessorTest\nThe first test will do all work in the writer and the batch fails because a functional error causes the writer to be run in recoverer. The first item will be skipped. The second item will get a deadlock on the first try, this is not handled with a retry and causes the batch to fail.\nThe second test does the work in the processor (which seems like the right thing to do, but that is not the point. The first item will be skipped. The second item will get the deadlock, retried and the processor will continue to process the rest of the chunk.\nIt seems that the errorhandling works as expected in the processor, but not in the writer. Our solution is to use the processor and just do flush in the writer, but it would be nice to have the same errorhandling in the writer or an explanation on why not.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantChunkProcessorTests.java", "org.springframework.batch.core.step.item.FaultTolerantChunkProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1761, "bug_title": "Only first item in chunk is re-processed on retry of failed write", "bug_description": "http://forum.springsource.org/showthread.php?110196-itemprocessor-recalled-only-for-first-item-in-chunk-when-retryable-exception\nAll items are eventually re-processed if the chunk fails the same way deterministically, or if retry is not used (skip may be though).", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanTests.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanRetryTests.java", "org.springframework.batch.core.step.item.FaultTolerantStepFactoryBeanRollbackTests.java", "org.springframework.batch.core.step.item.BatchRetryTemplate.java", "org.springframework.batch.core.step.item.FaultTolerantChunkProcessor.java"], "label": 1, "es_results": []}, {"bug_id": 1813, "bug_title": "BeanWrapperFieldSetMapper properties caching is broken", "bug_description": "The fix for BATCH-1709 broke the caching of the property name mapping cache in getBeanProperties().\nOn the first run through an empty ConcurrentHashMap is put in \"propertiesMatched\" at the top of getBeanProperties(), the \"matches\" Map is then seeded with this empty ConcurrentHashMap but at the bottom of getBeanProperties the updated \"matches\" Map isn&apos;t written back to that ConcurrentHashMap.\nOn large numbers of items with a lot of properties, this really hurts performance.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.item.file.mapping.BeanWrapperFieldSetMapper.java"], "label": 1, "es_results": []}, {"bug_id": 1848, "bug_title": "JdbcPagingItemReader does not support table or column aliases due to sortKey being used in where clause, order by clause and for retrieval of result set column", "bug_description": "The SqlPagingQueryProviderFactoryBean class takes a parameter sortKey used to enable paged queries.  This is used in the where clause (to select skip rows already selected in subsequent queries), in an order by clause and to retrieve the result set value for the last item read.  The exact use is determined by the database type, but the general pattern remains the same.\nThe examples in the tutorial appear fairly simple, involving a single table only.  We have a use case where two aliased tables are involved.  As the table alias prefix is required in the where clause, appears to be optional in the order by clause and cannot be present when retrieving the result set column by name we have a problem as sortKey is used in all three cases.\nFor a page size of 2 we set the reader&apos;s queryProvider properties set as follows:\n\t\t<property name=\"queryProvider\">\n\t\t\t<bean class=\"org.springframework.batch.item.database.support.SqlPagingQueryProviderFactoryBean\">\n\t\t\t\t<property name=\"selectClause\" value=\"select  t1.id, t1.field, t2.other_field\" />\n\t\t\t\t<property name=\"dataSource\" ref=\"dataSource\" />\n\t\t\t\t<property name=\"fromClause\" value=\"from TABLE_1 t1, TABLE_2 t2 \" />\n\t\t\t\t<property name=\"whereClause\" value=\"t1.id = t2.id\" />\n\t\t\t\t<property name=\"sortKey\" value=\"t1.id\" />\n\t\t\t</bean>\n\t\t</property>\nThe Derby query resulting for the first page is:\nSELECT * FROM ( SELECT t1.id, t1.field, t2.other_field, ROW_NUMBER() OVER () AS ROW_NUMBER FROM TABLE_1 t1, TABLE_2 t2 WHERE t1.id = t2.id ORDER BY t1.id ASC) AS TMP_SUB WHERE TMP_SUB.ROW_NUMBER <= 2\nAnd queries for subsequent pages are:\nSELECT * FROM ( SELECT t1.id, t1.field, t2.other_field, ROW_NUMBER() OVER () AS ROW_NUMBER FROM TABLE_1 t1, TABLE_2 t2 WHERE t1.id = t2.id  AND t1.id > ? ORDER BY t1.id ASC) AS TMP_SUB WHERE TMP_SUB.ROW_NUMBER <= 2\nIn H2 the query for the initial page is:\nSELECT TOP 2 t1.id, t1.field, t2.other_field FROM TABLE_1 t1, TABLE_2 t2 WHERE t1.id = t2.id ORDER BY t1.id ASC\nAnd queries for sebsequent pages are:\nSELECT TOP 2 t1.id, t1.field, t2.other_field FROM TABLE_1 t1, TABLE_2 t2 WHERE t1.id = t2.id AND t1.id > :_sortKey ORDER BY id ASC\nIn both Derby and H2 the result set column retrieval fails.\nWe&apos;ve worked around the problem by subclassing JdbcPagingItemReader and with tricks with reflection effectively changing the line in the inner class JdbcPagingItemReader.PagingRowMapper \"startAfterValue = rs.getObject(queryProvider.getSortKey());\" to \"startAfterValue = rs.getObject(stripAlias(queryProvider.getSortKey()));\" where the method stripAlias is defined as:\n        private String stripAlias(String column) {\n            int separator = column.indexOf(&apos;.&apos;);\n            if(separator > 0) {\n                int columnIndex = separator + 1;\n                if(columnIndex < column.length()) \n{\n                    column = column.substring(columnIndex);\n                }\n            }\n            return column;\n        }\nWe&apos;d like this change made to this class directly or something else with similar effect.\nAnother issue you might wish to consider is that a column alias would further confuse things (as it would be required to be used for the order by clause and to retrieve the value from the result set, but not for the where clause).  Perhaps an additional optional property \"sortKeyAlias\"?", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.item.database.support.AbstractSqlPagingQueryProvider.java", "org.springframework.batch.item.database.support.SqlPagingQueryUtils.java", "org.springframework.batch.item.database.support.SqlWindowingPagingQueryProvider.java", "org.springframework.batch.item.database.JdbcPagingItemReader.java", "org.springframework.batch.item.database.PagingQueryProvider.java"], "label": 1, "es_results": []}, {"bug_id": 1822, "bug_title": "Job execution marked as STOPPED when exception occurs while committing StepExecution", "bug_description": "When an exception occurs while committing StepExecution (in org.springframework.batch.core.step.tasklet.TaskletStep.ChunkTransactionCallback#doInTransaction()) setTerminateOnly is being called on the step execution. This results in the job execution being marked as STOPPED. I think it&apos;s better to mark the job execution as FAILED? The STOPPED status in general indicates the job has been stopped in a controlled way (via spring-batch gui, programmatically via the JobOperator API, ...).", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.1.9.RELEASE", "fixed_files": ["org.springframework.batch.core.job.flow.FlowJobTests.java", "org.springframework.batch.core.JobInterruptedException.java", "org.springframework.batch.core.job.AbstractJob.java", "org.springframework.batch.core.job.SimpleJobTests.java", "org.springframework.batch.core.job.flow.JobFlowExecutor.java"], "label": 1, "es_results": []}, {"bug_id": 1916, "bug_title": "RecordSeparatorPolicy#isEndOfRecord wrong javadoc?", "bug_description": "I have an issue using FlatFileItemReader with a custom RecordSeparatorPolicy. The javadoc of RecordSeparatorPolicy#isEndOfRecord  tells \n\nSignal the end of a record based on the content of a line, being the latest line read from an input source. The input is what you would expect from BufferedReader.readLine() - i.e. no line separator character at the end. But it might have line separators embedded in it.\nI would think the parameter is last read line from file. If I see the code of \nFlatFileItemReader#applyRecordSeparatorPolicy\n\n\n\n\n\n\n \n\n\n\n\nwhile (line != null && !recordSeparatorPolicy.isEndOfRecord(record)) {\n\n\n\n\n\n\nActually this is not a line, this is whole record. It makes FlatFileItemReader unusable for my purposes. ", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.2.0.RELEASE", "fixed_files": ["org.springframework.batch.item.file.separator.RecordSeparatorPolicy.java"], "label": 1, "es_results": []}, {"bug_id": 1959, "bug_title": "Problem with FlatFileItemWriter restart using multi-byte encoding", "bug_description": "With FlatFileItemWriter, size saved in the step_context on update (under current.count key) is the sum of fileChannel.size (current file size in bytes) and Buffered string length (see FlatFileItemWriter.OutputState.position() method)\nWith an out file encoded in UTF-8 and buffer string containing two bytes caracters, the saved position is wrong => restart will erase out file content.\nIn attachment, maven project with :\n\na first test case comparing real file size after a 1rst job run with failure and current.countsaved data\na second test case comparing in file size and out file size after a 1rst job run with failure and a 2nd job restart with no failure\n\n", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.2.0.RELEASE", "fixed_files": ["org.springframework.batch.item.file.FlatFileItemWriterTests.java", "org.springframework.batch.support.transaction.TransactionAwareBufferedWriterTests.java", "org.springframework.batch.support.transaction.TransactionAwareBufferedWriter.java"], "label": 1, "es_results": []}, {"bug_id": 1865, "bug_title": "SimpleChunkProvider calls afterRead listener even if the file is finished", "bug_description": "the doRead method of SimpleChunkProvider always calls listener.afterRead even if the returned item is null.\nA null returned item indicates the file is complete, so the doRead shouldn&apos;t be calling the afterRead listener.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.2.0.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.SimpleChunkProvider.java"], "label": 1, "es_results": []}, {"bug_id": 2172, "bug_title": "Spring batch fails to autodetect database type DB2ZOS", "bug_description": "The auto-detection code in DatabaseType.fromMetaData() stopped working when we switched from jdbc driver versjon 3.64.111 to 3.65.102.\nWhile the older jdbc driver getDatabaseProductName() returns \"DB2\", the new one returns \"DB2 for DB2 UDB for z/OS\". getDatabaseProductVersion() returns \"DSN10015\" in both cases. \nSuggestion: use startsWith() instead of equals()", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "2.2.6.RELEASE", "fixed_files": ["org.springframework.batch.support.DatabaseTypeTests.java", "org.springframework.batch.support.DatabaseType.java"], "label": 1, "es_results": []}, {"bug_id": 1863, "bug_title": "The batch namespace can not be used with allowBeanDefinitionOverriding=false", "bug_description": "Problem description\nIt is not possible to use the batch namespace in spring configuration files if the allowBeanDefinitionOverriding is set to false in the application context. The application context loading fails with a org.springframework.beans.factory.parsing.BeanDefinitionParsingException exception.\nThis is caused by the JobParser and InlineFlowParser from package org.springframework.batch.core.configuration.xml. JobParser creates for non abstract job definition a SimpleFlow which is unfortunately registered under the same bean name as the job parsed by the JobParser - the problematic code is on line 120 of the JobParser. The application context loading fails as 2 beans with the same names are going to be registered. The first one is a factory bean for SimpleFlow and the second one is a factory bean for FlowJob.\nThis fact is visible also from logs when the allowBeanDefinitionOverriding is set to true:\n\n\n\n\n\n\norg.springframework.beans.factory.support.DefaultListableBeanFactory registerBeanDefinition\n\n\n\n\nINFO: Overriding bean definition for bean &apos;dummyJob&apos;: replacing [Generic bean: class [org.springframework.batch.core.configuration.xml.SimpleFlowFactoryBean]; scope=singleton; abstract=false; lazyInit=false; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=null] with [Generic bean: class [org.springframework.batch.core.configuration.xml.JobParserJobFactoryBean]; scope=singleton; abstract=false; lazyInit=false; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=null]\n\n\n\n\n\n\nI have attached a very simple application to this issue, which contains 2 unit tests. One with allowBeanDefinitionOverriding=true and another one with allowBeanDefinitionOverriding=false. The second one fails with the org.springframework.beans.factory.parsing.BeanDefinitionParsingException exception.\nPossible fix\nThe factory bean for SimpleFlow could be represented by a random name (or by a prefix/suffix added to the bean name of the FlowJob&apos;s factroy bean), as the SimpleFlow is just a bean which will be not used as a \"main\", standalone bean, but it is used only as the flow attribute of the SimpleFlow. So its bean name is not important, however it should named differently as its owner FlowJob. The current implementation of the InlineFlowParser uses the same value for the flow name and for the bean name. If flow name is important and it makes no sense to use a random name or a modified name, then an additional value should be provided for the InlineFlowParser&apos;s constructor, which will be the bean name to use.\n", "project": "Spring", "sub_project": "BATCH", "version": "2.1.8.RELEASE", "fixed_version": "3.0.2.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.InlineFlowParser.java"], "label": 1, "es_results": []}, {"bug_id": 1903, "bug_title": "SQL compatibility breakage with HSQL", "bug_description": "I was testing Spring Batch for the first time with 2.1.9 and an HSQL database.  It was working fine until I tried to restart a failed job.  At that point, a NPE was thrown from the Spring Batch core code.  The reason was that a DAO query for fetching the last job execution details did not return any rows, even though there was a row in the database.\nI tried Spring Batch 2.1.8 and it worked fine.  I traced it down to the SQL query itself, which is defined here:\norg.springframework.batch.core.repository.dao.JdbcJobExecutionDao.GET_LAST_EXECUTION\nThis query was recently changed in commit 16e23fc on May 16, 2012.  While part of the change was good (changing to use an ID rather than timestamp to match rows), it broke compatibility with HSQLDB.  Here is the SQL that returns no results:\nSELECT JOB_EXECUTION_ID, START_TIME, END_TIME, STATUS, EXIT_CODE, EXIT_MESSAGE, CREATE_TIME, LAST_UPDATED, VERSION\nfrom BATCH_JOB_EXECUTION E where JOB_INSTANCE_ID = 6 and \nJOB_EXECUTION_ID = (SELECT max(JOB_EXECUTION_ID) \n\tfrom BATCH_JOB_EXECUTION E2 where E.JOB_INSTANCE_ID = E2.JOB_INSTANCE_ID)\nThe problem is that HSQLDB supports correlated subqueries, but apparently not when using the \"=\" operator.  This query works properly in HSQLDB when making one of the following changes:\n1. Change the comparison operator for JOB_EXECUTION_ID from \"=\" to \"IN\".  HSQLDB supports the IN clause w/ correlated subqueries.\n2. OR remove the subquery correlation by repeating the JDBC parameter instead of referencing E_JOB_INSTANCE_ID.  This is apparently how the query used to work.\nI do not know if either proposed change would cause issues in the other supported databases, but I would suspect not.  I also do not know if there are other SQL compatibility issues in 2.1.9, but I can say I didn&apos;t notice any problems when my jobs succeeded.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.9.RELEASE", "fixed_version": "2.2.0.RELEASE", "fixed_files": ["org.springframework.batch.core.repository.dao.JdbcJobExecutionDao.java"], "label": 1, "es_results": []}, {"bug_id": 1908, "bug_title": "Inefficient storage of StepExecutionContexts when using partitioning", "bug_description": "When using a PartitionStep, each StepExecutionContext created for the corresponding partitions are saved and committed individually.  When a job has a large number of partitions, this leads to large delays.  Look into batching the inserts of these records.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.9.RELEASE", "fixed_version": "2.2.0.RELEASE", "fixed_files": ["org.springframework.batch.core.repository.support.SimpleJobRepositoryTests.java", "org.springframework.batch.core.repository.dao.AbstractStepExecutionDaoTests.java", "org.springframework.batch.core.repository.dao.MapExecutionContextDao.java", "org.springframework.batch.core.repository.dao.AbstractExecutionContextDaoTests.java", "org.springframework.batch.core.repository.dao.ExecutionContextDao.java", "org.springframework.batch.core.repository.JobRepository.java", "org.springframework.batch.core.repository.dao.JdbcExecutionContextDao.java", "org.springframework.batch.core.step.item.TaskletStepExceptionTests.java", "org.springframework.batch.core.repository.support.SimpleJobRepository.java", "org.springframework.batch.core.step.JobRepositorySupport.java", "org.springframework.batch.core.configuration.xml.DummyJobRepository.java", "org.springframework.batch.core.repository.dao.StepExecutionDao.java", "org.springframework.batch.core.partition.support.SimpleStepExecutionSplitter.java", "org.springframework.batch.core.repository.dao.JdbcStepExecutionDao.java", "org.springframework.batch.core.repository.dao.MapStepExecutionDao.java"], "label": 1, "es_results": []}, {"bug_id": 1924, "bug_title": "Restarting a stopped job in COMPLETED state prevents progress", "bug_description": "A job that is programatically stopped and restarted at a step prior to the step it was stopped in will not advance past the step it was previously stopped.  Below is an example of a job with the issue:\n\n\n\n\n\n\n\n\n\n\n\n\t<job id=\"restart.job6\">\n\n\n\n\n\t\t<step id=\"job6.step1\" next=\"job6.step2\">\n\n\n\n\n\t\t\t<tasklet allow-start-if-complete=\"true\" >\n\n\n\n\n\t\t\t\t<chunk reader=\"customerFileReader\" writer=\"xmlOutputWriter\"\n\n\n\n\n\t\t\t\t\tcommit-interval=\"10\" />\n\n\n\n\n\t\t\t</tasklet>\n\n\n\n\n\t\t</step>\n\n\n\n\n\t\t<step id=\"job6.step2\"  parent=\"formatFileStep\" >\n\n\n\n\n\t\t\t<next on=\"ES3\" to=\"job6.step3\" />\n\n\n\n\n\t\t\t<stop on=\"ES4\" restart=\"job6.step4\" />\n\n\n\n\n\t\t\t<listeners>\n\n\n\n\n\t\t\t\t<listener ref=\"translator\"/>\n\n\n\n\n\t\t\t</listeners>\n\n\n\n\n\t\t</step>\n\n\n\n\n\t\t<step id=\"job6.step3\" next=\"job6.step4\"  parent=\"formatFileStep\"/>\n\n\n\n\n\t\t<step id=\"job6.step4\"  parent=\"formatFileStep\"/>\n\n\n\n\n\t</job>\n\n\n\n\n\n\nThe full export of this job can be found here: https://gist.github.com/4259471", "project": "Spring", "sub_project": "BATCH", "version": "2.1.9.RELEASE", "fixed_version": "2.2.0.RELEASE", "fixed_files": ["org.springframework.batch.core.job.flow.support.SimpleFlow.java", "org.springframework.batch.core.configuration.xml.StopRestartOnCompletedStepJobParserTests.java", "org.springframework.batch.core.configuration.xml.StopRestartOnFailedStepJobParserTests.java", "org.springframework.batch.core.job.flow.JobFlowExecutor.java", "org.springframework.batch.core.configuration.xml.StopAndRestartFailedJobParserTests.java"], "label": 1, "es_results": []}, {"bug_id": 2038, "bug_title": "DerbyPagingQueryProvider does not work with Derby 10.10.1.1", "bug_description": "An InvalidDataAccessResourceUsageException is thrown when using Derby 10.10.1.1\nThe following line returns true and throws the exception with 10.10.1.1:\nif (\"10.4.1.3\".compareTo(version) > 0) {", "project": "Spring", "sub_project": "BATCH", "version": "2.1.9.RELEASE", "fixed_version": "2.2.1.RELEASE", "fixed_files": ["org.springframework.batch.item.database.support.DerbyPagingQueryProviderTests.java", "org.springframework.batch.item.database.support.DerbyPagingQueryProvider.java"], "label": 1, "es_results": []}, {"bug_id": 2347, "bug_title": "Memory leak in DefaultJobLoader", "bug_description": "To load the jobs and populate the job registry, we use the DefaultJobLoader class provided by Spring batch: https://github.com/spring-projects/spring-batch/blob/587680ba56d03d1acd18a75ee00abea84e81038f/spring-batch-core/src/main/java/org/springframework/batch/core/configuration/support/DefaultJobLoader.java\nInternally the DefaultJobLoader maintains two hashmaps:\n1) contexts \n2) contextToJobNames\nWhen the clear() method in called, only the first hashmap is cleared.\nIn most cases it doesn&apos;t because any issues because applications will usually load their jobs only once. \nFor applications such as ours that need to clear loaded jobs and reload them, this causes a memory leak since the DefaultJobLoader retains a reference to the previously loaded jobs, preventing garbage collection.\nThe fix is trivial to implement and shouldn&apos;t have side effects:\nensure that the &apos;contextToJobNames&apos; map is cleared right after the other one.", "project": "Spring", "sub_project": "BATCH", "version": "2.1.9.RELEASE", "fixed_version": "3.0.4.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.support.DefaultJobLoaderTests.java", "org.springframework.batch.core.configuration.support.DefaultJobLoader.java"], "label": 1, "es_results": []}, {"bug_id": 1995, "bug_title": "Line ending in multiline delimiter not being processed correctly", "bug_description": "", "project": "Spring", "sub_project": "BATCH", "version": "2.2.0.RC1", "fixed_version": "2.2.0.RC2", "fixed_files": ["org.springframework.batch.item.file.transform.DelimitedLineTokenizerTests.java", "org.springframework.batch.item.file.transform.DelimitedLineTokenizer.java"], "label": 1, "es_results": []}, {"bug_id": 2019, "bug_title": "Support PropertySourcesPlaceholderConfigurer delegation to job contexts", "bug_description": "Spring Batch uses children of AbstractApplicationContextFactory to create child application contexts for each batch job. By default, it will pass PropertyPlaceholderConfigurer and CustomEditorConfigurer bean post processors to the child contexts as configured in the default constructor of AbstractApplicationContextFactory. However, it does not pass PropertySourcesPlaceholderConfigurer bean post processors by default. Therefore, using the Spring 3.1 Environment functionality in conjunction with <context:properties-placeholder/> does not work out of the box. You must manually add that class to the list of bean post processors. It would be great if this default behavior could be changed to support the Spring 3.1 convention.", "project": "Spring", "sub_project": "BATCH", "version": "2.2.0.RC1", "fixed_version": "2.2.0.RC2", "fixed_files": ["org.springframework.batch.core.configuration.support.GenericApplicationContextFactory.java", "org.springframework.batch.core.configuration.support.AbstractApplicationContextFactory.java", "org.springframework.batch.core.configuration.support.GenericApplicationContextFactoryTests.java"], "label": 1, "es_results": []}, {"bug_id": 2023, "bug_title": "@StepScope should default to ScopedProxyMode.TARGET_CLASS", "bug_description": "If I use @StepScope on a @Bean method that returns a FlatFileItemReader, I get a ClassCastException:  java.lang.ClassCastException: $Proxy16 cannot be cast to org.springframework.batch.item.file.FlatFileItemReader\nThat&apos;s because the ScopedProxyMode is INTERFACES, and the proxy created is based on of the interfaces of FlatFileItemReader, and not FlatFileItemReader itself. Since we always have CGLIB when using @Configuration, I see no reason for not changing the default proxyMode of @StepScope to TARGET_CLASS. \nOne more argument for the proxyMode TARGET_CLASS: when I change the return type of the @Bean method to ItemReader<xx>, but still return a FlatFileItemReader, I don&apos;t get a ClassCastException, but the ItemStream registration does not work, and the FlatFileItemReader throws an exception on the first read.\nAltogether those are serious stumble blocks for people not familiar with the proxying mechanisms in Spring, and we would avoid all of them with the default proxyMode TARGET_CLASS.", "project": "Spring", "sub_project": "BATCH", "version": "2.2.0.RC1", "fixed_version": "2.2.0.RC2", "fixed_files": ["org.springframework.batch.core.configuration.annotation.StepScope.java", "org.springframework.batch.core.configuration.annotation.StepScopeConfigurationTests.java"], "label": 1, "es_results": []}, {"bug_id": 2035, "bug_title": "Create parallel to simple-cli for pure Java configuration", "bug_description": "Coding a pure Java configuration for Spring Batch. Thought it would fit next to spring-cli in the archetypes.", "project": "Spring", "sub_project": "BATCH", "version": "2.2.0.RC2", "fixed_version": "3.0.0.RELEASE", "fixed_files": ["org.springframework.batch.core.launch.support.CommandLineJobRunner.java"], "label": 1, "es_results": []}, {"bug_id": 2050, "bug_title": "AbstractItemCountingItemStreamItemReader.read() should not be final", "bug_description": "when you use FlatfileItemWriter with StepScope (proxyTargetClass=true --> CGLib-Subclassing) the final method read() cannot be proxied correctly. So the AbstractItemCountingItemStreamItemReader.currentItemCount is always persisted as 0 to the jobRepository.\nCode:\n<bean class=\"org.springframework.batch.core.scope.StepScope\" p:proxyTargetClass=\"true\" />\n<bean id=\"reader\" class=\"org.springframework.batch.item.file.FlatFileItemReader\" scope=\"step\">\n<property name=\"resource\" value=\"#\n{jobParameters[pathToFile]}\n\"></property>\n<property name=\"lineMapper\" ref=\"lineMapper\"/>\n</bean>\nthanks \n", "project": "Spring", "sub_project": "BATCH", "version": "2.2.0.RELEASE", "fixed_version": "2.2.1.RELEASE", "fixed_files": ["org.springframework.batch.item.database.AbstractCursorItemReader.java", "org.springframework.batch.item.database.HibernateItemWriter.java", "org.springframework.batch.item.support.AbstractItemCountingItemStreamItemReader.java", "org.springframework.batch.item.database.JpaItemWriter.java", "org.springframework.batch.core.configuration.annotation.StepScope.java", "org.springframework.batch.item.file.transform.DelimitedLineTokenizer.java"], "label": 1, "es_results": []}, {"bug_id": 2086, "bug_title": "default writer implementations need public setter for name", "bug_description": "The default spring batch writer implementations need a public setter for the name. The name is used to prefix the execution context keys. In spring batch 2.1.x the setter for the name was available in FlatFileItemWriter, StaxEventItemWriter, MultiResourceItemWriter, ... because they extended from ExecutionContextUserSupport. In 2.2.x they extend from ItemStreamSupport where the setExecutionContextName method is protected.\nThis means that currently in spring batch 2.2.x it&apos;s no longer possible to write to multiple StaxEventItemWriters in the same step, because they would share the same execution context.\nedit: FlatFileItemWriter has a setName() method. StaxEventItemWriter and MultiResourceItemWriter don&apos;t.", "project": "Spring", "sub_project": "BATCH", "version": "2.2.0.RELEASE", "fixed_version": "2.2.2.RELEASE", "fixed_files": ["org.springframework.batch.item.ItemStreamSupport.java", "org.springframework.batch.item.support.AbstractItemCountingItemStreamItemReader.java", "org.springframework.batch.item.xml.StaxEventItemWriterTests.java", "org.springframework.batch.item.file.FlatFileItemWriter.java"], "label": 1, "es_results": []}, {"bug_id": 2040, "bug_title": "Db2PagingQueryProvider creates erroneous Statement in generateJumpToItemQuery", "bug_description": "Hi,\nwe want to use the spring-batch-admin on WAS70 with DB2-Database underlying.\nThe list of executions is delimited to 20 per default. If we try to jump on the next page, an internal server error 500 will occur. \nAs we hit the \"next\" button, the method generateJumpToItemQuery(int itemIndex, int pageSize) in org.springframework.batch.item.database.support.Db2PagingQueryProvider is called.\nThe generated SQL-Statement is:\nSELECT E.JOB_EXECUTION_ID FROM ( SELECT E.JOB_EXECUTION_ID, ROW_NUMBER() OVER ( ORDER BY E.JOB_EXECUTION_ID DESC) AS ROW_NUMBER FROM T1100Z.BATCH_JOB_EXECUTION E, T1100Z.BATCH_JOB_INSTANCE I WHERE E.JOB_INSTANCE_ID=I.JOB_INSTANCE_ID) AS TMP_SUB WHERE TMP_SUB.ROW_NUMBER = 20 ORDER BY E.JOB_EXECUTION_ID DESC\nAfter a few test i found out the right SQL:\nSELECT TMP_SUB.JOB_EXECUTION_ID FROM ( SELECT E.JOB_EXECUTION_ID, ROW_NUMBER() OVER ( ORDER BY E.JOB_EXECUTION_ID DESC) AS ROW_NUMBER FROM T1100Z.BATCH_JOB_EXECUTION E, T1100Z.BATCH_JOB_INSTANCE I WHERE E.JOB_INSTANCE_ID=I.JOB_INSTANCE_ID) AS TMP_SUB WHERE TMP_SUB.ROW_NUMBER = 20 ORDER BY TMP_SUB.JOB_EXECUTION_ID DESC\nAs you can see, the subquery uses the \"E.\", but comes out as TMP_SUB.\nThe \"E.\" is set by the batch-admin-manager: sortKeys.put(\"E.JOB_EXECUTION_ID\", Order.DESCENDING);\nI think a the generateJumpToItemQuery-Method in Db2PagingQueryProvider has to be overridden, or the method buildSortKeySelect(StringBuilder sql) could get a brother like: buildSortKeySelect(StringBuilder sql, String qualifierReplacement) (\"E.\" -> \"TMP_SUB.\")", "project": "Spring", "sub_project": "BATCH", "version": "2.2.0.RC2", "fixed_version": "2.2.2.RELEASE", "fixed_files": ["org.springframework.batch.item.database.support.SqlWindowingPagingQueryProvider.java", "org.springframework.batch.item.database.support.SqlPagingQueryUtils.java", "org.springframework.batch.item.database.support.SqlWindowingPagingQueryProviderTests.java"], "label": 1, "es_results": []}, {"bug_id": 2045, "bug_title": "Spring Batch version not incremented in XSD schema check warning message", "bug_description": "When Spring Batch 2.1 schema is used to define job context, the following message is shown:\nYou cannot use spring-batch-2.0.xsd with Spring Batch 2.1.  Please upgrade your schema declarations (or use the spring-batch.xsd alias if you are feeling lucky).\nThe numbers seem to be incremented accordingly to 2.1.xsd and Spring Batch 2.2\n(line 72 of the JobParser class)", "project": "Spring", "sub_project": "BATCH", "version": "2.2.0.RELEASE", "fixed_version": "3.0.0.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.JobParserExceptionTests.java", "org.springframework.batch.core.configuration.xml.JobParser.java"], "label": 1, "es_results": []}, {"bug_id": 2011, "bug_title": "ExitStatus.compareTo() delivers result out of given range [-1,1]", "bug_description": "If compareTo is used to compare two ExitStatus with non-standard exitCodes the result may be different then {-1,0,1}. If user defined ExitStatuses are used, this may lead to unexpected behavior.\nI would expect the following test to pass:\n@Test\npublic void testCompareTo(){\n final ExitStatus someExitStatus = new ExitStatus(\"SOME_STATUS\");\n final ExitStatus otherExitStatus = new ExitStatus(\"OTHER_STATUS\");\n assertThat(someExitStatus.compareTo(otherExitStatus),\n            either(equalTo(1)).or(equalTo(-1)));\n}", "project": "Spring", "sub_project": "BATCH", "version": "2.2.0.RC1", "fixed_version": "2.2.6.RELEASE", "fixed_files": ["org.springframework.batch.core.ExitStatus.java"], "label": 1, "es_results": []}, {"bug_id": 2204, "bug_title": "transactional reader does not work since 2.2", "bug_description": "Hello,\nIt seems that the property \"reader-transactional-queue\" on a chunk does not work since 2.2.\nThe value of the property is not set to FaultTolerantStepBuilder and then the property is ignored.\nThen when the chunk is in skipping process the items are not re-read if the property is setting to \"true\".\nAdding the line code:\nif (readerTransactionalQueue!=null && readerTransactionalQueue==true) \n{\n\t\t\tbuilder.readerIsTransactionalQueue();\n\t\t}\n\nin the method  createFaultTolerantStep() of org.springframework.batch.core.configuration.xml.StepParserStepFactoryBean fixes the problem. (see the attachement)", "project": "Spring", "sub_project": "BATCH", "version": "2.2.0.RC1", "fixed_version": "2.2.6.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepParserStepFactoryBean.java", "org.springframework.batch.core.configuration.xml.StepParserStepFactoryBeanTests.java"], "label": 1, "es_results": []}, {"bug_id": 2096, "bug_title": "'chunk-completion-policy' or 'commit-interval' with '#{jobParameters[...]}' is ignored when 'retry-limit' exists.", "bug_description": "When &apos;retry-limit&apos; is configured and  &apos;commit-interval&apos; is extracted from jobParameters, commit-interval is always recognized to be &apos;1&apos; regardless of jobParameters.\n\n\n\n\n\n\n<batch:job id=\"retryJob\">\n\n\n\n\n  <batch:step id=\"step1\">\n\n\n\n\n    <batch:tasklet>\n\n\n\n\n      <batch:chunk reader=\"reader\" writer=\"writer\" commit-interval=\"#{jobParameters[&apos;commit.interval&apos;]}\" retry-limit=\"5\" > \n\n\n\n\n      <batch:retryable-exception-classes>\n\n\n\n\n        <batch:include class=\"java.lang.Exception\" />\n\n\n\n\n       </batch:retryable-exception-classes>\n\n\n\n\n    </batch:chunk>\n\n\n\n\n  </batch:tasklet>\n\n\n\n\n</batch:step>\n\n\n\n\n</batch:job>\n\n\n\n\n\n\nAnd &apos;chunk-completion-policy&apos; also ignored when &apos;retry-limit&apos; exists.\nThese flows are fundamentally same after parsing the XML, because SimpleCompletionPolicy is registered when &apos;commit-interval&apos; starts with &apos;#\". it is processed in  &apos;org.springframework.batch.core.configuration.xml.ChunkElementParser.prase()&apos;\n\n\n\n\n\n\nif (StringUtils.hasText(commitInterval)) {\n\n\n\n\n  if (commitInterval.startsWith(\"#\")) {\n\n\n\n\n    // It&apos;s a late binding expression, so we need step scope...\n\n\n\n\n    BeanDefinitionBuilder completionPolicy = BeanDefinitionBuilder.genericBeanDefinition(SimpleCompletionPolicy.class);\n\n\n\n\n    completionPolicy.addConstructorArgValue(commitInterval);\n\n\n\n\n    completionPolicy.setScope(\"step\");\n\n\n\n\n    propertyValues.addPropertyValue(\"chunkCompletionPolicy\", completionPolicy.getBeanDefinition());\n\n\n\n\n  } else {\n\n\n\n\n    propertyValues.addPropertyValue(\"commitInterval\", commitInterval);\n\n\n\n\n  }\n\n\n\n\n}\n\n\n\n\n\n\nThe because is that StepParserStepFactoryBean omits to set ChunkCompletionPolicy when creating FaultTolerantStep.  &apos;builder.chunk(chunkCompletionPolicy);&apos; is called in &apos;StepParserStepFactoryBean.createSimpleStep()&apos;,\n but it is not in &apos;createFaultTolerantStep()&apos;.\nI reproduced the problem at https://github.com/benelog/batch-experiments/tree/master/batch-retry-test", "project": "Spring", "sub_project": "BATCH", "version": "2.2.1.RELEASE", "fixed_version": "2.2.2.RELEASE", "fixed_files": ["org.springframework.batch.core.configuration.xml.StepParserStepFactoryBean.java", "org.springframework.batch.core.configuration.xml.StepParserStepFactoryBeanTests.java"], "label": 1, "es_results": []}, {"bug_id": 2141, "bug_title": "RepositoryItemReader reads the first page twice when used with partitions.", "bug_description": "org.springframework.batch.item.data.RepositoryItemReader.jumpToItem() is called for each partition and initiates the processing of the first page by calling results = doPageRead(); but doesn&apos;t increment the page attribute which means that page 0 is processed twice. The fix is simply to have page++ after the call to doPageRead();", "project": "Spring", "sub_project": "BATCH", "version": "2.2.2.RELEASE", "fixed_version": "2.2.3.RELEASE", "fixed_files": ["org.springframework.batch.item.data.RepositoryItemReader.java"], "label": 1, "es_results": []}, {"bug_id": 2126, "bug_title": "DefaultJobParametersConverter is not thread-safe and the SimpleJobOperator either ", "bug_description": "DefaultJobParametersConverter is not thread-safe due to usage of instance variable for numberFormat, dateFormat and longNumberFormat. Then as the SimpleJobOperator use a JobParametersConverter as a instance variable it is not thread-safe either. Thus means that we cannot use it with a SimpleAsyncTaskExecutor or any asynchronous TaskExecutor. \nCan you fix it or can you explain which setup as be evaluated to make it work properly with an asynchronous TaskExecutor ?", "project": "Spring", "sub_project": "BATCH", "version": "2.2.2.RELEASE", "fixed_version": "3.0.0.RELEASE", "fixed_files": ["org.springframework.batch.core.converter.DefaultJobParametersConverter.java"], "label": 1, "es_results": []}, {"bug_id": 2151, "bug_title": "Skip issues on restart", "bug_description": "See attached test project", "project": "Spring", "sub_project": "BATCH", "version": "2.2.3.RELEASE", "fixed_version": "2.2.4.RELEASE", "fixed_files": ["org.springframework.batch.core.step.item.ChunkMonitorTests.java", "org.springframework.batch.core.step.item.ChunkMonitor.java"], "label": 1, "es_results": []}, {"bug_id": 2179, "bug_title": "DefaultJobParametersConverter#getProperties ignores NON_IDENTIFYING_FLAG", "bug_description": "When converting non-identifying JobParameters using \nDefaultJobParametersConverter#getProperties(JobParameters params) the NON_IDENTIFYING_FLAG is not prepended to the resulting property name.", "project": "Spring", "sub_project": "BATCH", "version": "2.2.4.RELEASE", "fixed_version": "3.0.0.RELEASE", "fixed_files": ["org.springframework.batch.core.converter.DefaultJobParametersConverter.java", "org.springframework.batch.core.converter.DefaultJobParametersConverterTests.java"], "label": 1, "es_results": []}, {"bug_id": 2189, "bug_title": "DefaultBatchConfigurer violates PostConstruct rules", "bug_description": "When trying to deploy a Spring Batch project in a web container it fails to deploy due to DefaultBatchConfigurer violating the @PostConstruct annotation rule.\nhttp://docs.oracle.com/javaee/5/api/javax/annotation/PostConstruct.html\nThe DefaultBatchConfigurer can&apos;t throw a checked exception in the initialize method. My suggestion would be to catch whatever exception is being thrown and wrap it in a run time exception.", "project": "Spring", "sub_project": "BATCH", "version": "2.2.5.RELEASE", "fixed_version": "2.2.6.RELEASE", "fixed_files": ["org.springframework.batch.core.job.flow.support.SimpleFlow.java", "org.springframework.batch.core.configuration.annotation.DefaultBatchConfigurer.java"], "label": 1, "es_results": []}, {"bug_id": 2206, "bug_title": "MongoItemReader needs property to set collection", "bug_description": "The MongoItemReader needs the ability to set the collection explicitly. This can&apos;t be done through the query and from the look of the current code it is relying on the type to determine the collection. This will only work if the mongo data contains the \"_class column - ie the data was created through spring originally. \nIn our case we need to read all records as raw JSON and process that. The data does not contain the \"_class (and this would not work anyway as we want to read as string). \nWe want to set query=\"{ }\" and targetType=\"java.lang.String\" \nTo get this to work I have had to create a copy of the MongoItemReader, add a collection property and modify the line\n\n\n\n\n\n\nreturn (Iterator<T>) template.find(mongoQuery, type).iterator(); \n\n\n\n\n\n\nto \n\n\n\n\n\n\nreturn (Iterator<T>) template.find(mongoQuery, type, collection).iterator();\n\n\n\n\n\n\nThis then allows us to set the desired collection on the bean and then everything works correctly.", "project": "Spring", "sub_project": "BATCH", "version": "2.2.5.RELEASE", "fixed_version": "2.2.7.RELEASE", "fixed_files": ["org.springframework.batch.item.data.MongoItemReader.java", "org.springframework.batch.item.data.MongoItemReaderTests.java"], "label": 1, "es_results": []}, {"bug_id": 2205, "bug_title": "MethodInvokingTaskletAdaper does not support primitive arguments", "bug_description": "The argument check in MethodInvokingTaskletAdapter fails for primitive paramters. Eg: I have a method that takes a single double primitive parameter. When I set the arguments property on the bean it takes an Object[] so it will be a Double object that is passed here. This causes an error when Spring creates the bean as it checks the types of arguments against the type of parameters, but double is not assignable from Double. If I extend the class and override afterPropertiesSet to remove the check it all works as expected.", "project": "Spring", "sub_project": "BATCH", "version": "2.2.5.RELEASE", "fixed_version": "3.0.4.RELEASE", "fixed_files": ["org.springframework.batch.sample.TaskletJobFunctionalTests.java", "org.springframework.batch.item.adapter.AbstractMethodInvokingDelegator.java"], "label": 1, "es_results": []}, {"bug_id": 2235, "bug_title": "ExitStatus.EXECUTING.isRunning() does not return true", "bug_description": "Implementation checks for two status codes/strings (\"RUNNING\" and \"UNKNOWN\") but there is no predefined RUNNING ExitStatus. Instead of \"RUNNING\" it should check for \"EXECUTING\".", "project": "Spring", "sub_project": "BATCH", "version": "2.2.6.RELEASE", "fixed_version": "2.2.7.RELEASE", "fixed_files": ["org.springframework.batch.core.ExitStatus.java"], "label": 1, "es_results": []}]